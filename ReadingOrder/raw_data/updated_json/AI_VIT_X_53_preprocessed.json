{
    "id": "32957ed4-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1406.1078v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 428,
                    "y": 253
                },
                {
                    "x": 2060,
                    "y": 253
                },
                {
                    "x": 2060,
                    "y": 382
                },
                {
                    "x": 428,
                    "y": 382
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Learning Phrase Representations using RNN Encoder-Decoder<br>for Statistical Machine Translation</p>",
            "id": 0,
            "page": 1,
            "text": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
        },
        {
            "bounding_box": [
                {
                    "x": 390,
                    "y": 414
                },
                {
                    "x": 1272,
                    "y": 414
                },
                {
                    "x": 1272,
                    "y": 526
                },
                {
                    "x": 390,
                    "y": 526
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Kyunghyun Cho<br>Bart van Merrienboer Caglar Gulcehre</p>",
            "id": 1,
            "page": 1,
            "text": "Kyunghyun Cho Bart van Merrienboer Caglar Gulcehre"
        },
        {
            "bounding_box": [
                {
                    "x": 592,
                    "y": 531
                },
                {
                    "x": 1065,
                    "y": 531
                },
                {
                    "x": 1065,
                    "y": 582
                },
                {
                    "x": 592,
                    "y": 582
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:18px'>Universite de Montreal</p>",
            "id": 2,
            "page": 1,
            "text": "Universite de Montreal"
        },
        {
            "bounding_box": [
                {
                    "x": 478,
                    "y": 599
                },
                {
                    "x": 1181,
                    "y": 599
                },
                {
                    "x": 1181,
                    "y": 640
                },
                {
                    "x": 478,
                    "y": 640
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:16px'>firstname lastname@umontreal ca</p>",
            "id": 3,
            "page": 1,
            "text": "firstname lastname@umontreal ca"
        },
        {
            "bounding_box": [
                {
                    "x": 456,
                    "y": 686
                },
                {
                    "x": 2122,
                    "y": 686
                },
                {
                    "x": 2122,
                    "y": 801
                },
                {
                    "x": 456,
                    "y": 801
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>Fethi Bougares Holger Schwenk Yoshua Bengio<br>Universite du Maine, France Universite de Montreal, CIFAR Senior Fellow</p>",
            "id": 4,
            "page": 1,
            "text": "Fethi Bougares Holger Schwenk Yoshua Bengio Universite du Maine, France Universite de Montreal, CIFAR Senior Fellow"
        },
        {
            "bounding_box": [
                {
                    "x": 401,
                    "y": 815
                },
                {
                    "x": 1260,
                    "y": 815
                },
                {
                    "x": 1260,
                    "y": 856
                },
                {
                    "x": 401,
                    "y": 856
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:14px'>firstname lastname@ 1 ium. uni v-l emans fr</p>",
            "id": 5,
            "page": 1,
            "text": "firstname lastname@ 1 ium. uni v-l emans fr"
        },
        {
            "bounding_box": [
                {
                    "x": 657,
                    "y": 881
                },
                {
                    "x": 852,
                    "y": 881
                },
                {
                    "x": 852,
                    "y": 933
                },
                {
                    "x": 657,
                    "y": 933
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:20px'>Abstract</p>",
            "id": 6,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 359,
                    "y": 975
                },
                {
                    "x": 1154,
                    "y": 975
                },
                {
                    "x": 1154,
                    "y": 2165
                },
                {
                    "x": 359,
                    "y": 2165
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>In this paper, we propose a novel neu-<br>ral network model called RNN Encoder-<br>Decoder that consists of two recurrent<br>neural networks (RNN). One RNN en-<br>codes a sequence of symbols into a fixed-<br>length vector representation, and the other<br>decodes the representation into another se-<br>quence of symbols. The encoder and de-<br>coder of the proposed model are jointly<br>trained to maximize the conditional prob-<br>ability of a target sequence given a source<br>sequence. The performance of a statisti-<br>cal machine translation system is empiri-<br>cally found to improve by using the con-<br>ditional probabilities of phrase pairs com-<br>puted by the RNN Encoder-Decoder as an<br>additional feature in the existing log-linear<br>model. Qualitatively, we show that the<br>proposed model learns a semantically and<br>syntactically meaningful representation of<br>linguistic phrases.</p>",
            "id": 7,
            "page": 1,
            "text": "In this paper, we propose a novel neural network model called RNN EncoderDecoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2202
                },
                {
                    "x": 650,
                    "y": 2202
                },
                {
                    "x": 650,
                    "y": 2259
                },
                {
                    "x": 293,
                    "y": 2259
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>1 Introduction</p>",
            "id": 8,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 1446,
                    "y": 473
                },
                {
                    "x": 1868,
                    "y": 473
                },
                {
                    "x": 1868,
                    "y": 524
                },
                {
                    "x": 1446,
                    "y": 524
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:22px'>Dzmitry Bahdanau</p>",
            "id": 9,
            "page": 1,
            "text": "Dzmitry Bahdanau"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2291
                },
                {
                    "x": 1217,
                    "y": 2291
                },
                {
                    "x": 1217,
                    "y": 3197
                },
                {
                    "x": 291,
                    "y": 3197
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:18px'>Deep neural networks have shown great success in<br>various applications such as objection recognition<br>(see, e.g., (Krizhevsky et al., 2012)) and speech<br>recognition (see, e.g., (Dahl et al., 2012)). Fur-<br>thermore, many recent works showed that neu-<br>ral networks can be successfully used in a num-<br>ber of tasks in natural language processing (NLP).<br>These include, but are not limited to, language<br>modeling (Bengio et al., 2003), paraphrase detec-<br>tion (Socher et al., 2011) and word embedding ex-<br>traction (Mikolov et al., 2013). In the field of sta-<br>tistical machine translation (SMT), deep neural<br>networks have begun to show promising results.<br>(Schwenk, 2012) summarizes a successful usage<br>of feedforward neural networks in the framework<br>of phrase-based SMT system.</p>",
            "id": 10,
            "page": 1,
            "text": "Deep neural networks have shown great success in various applications such as objection recognition (see, e.g., (Krizhevsky , 2012)) and speech recognition (see, e.g., (Dahl , 2012)). Furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing (NLP). These include, but are not limited to, language modeling (Bengio , 2003), paraphrase detection (Socher , 2011) and word embedding extraction (Mikolov , 2013). In the field of statistical machine translation (SMT), deep neural networks have begun to show promising results. (Schwenk, 2012) summarizes a successful usage of feedforward neural networks in the framework of phrase-based SMT system."
        },
        {
            "bounding_box": [
                {
                    "x": 1373,
                    "y": 531
                },
                {
                    "x": 1945,
                    "y": 531
                },
                {
                    "x": 1945,
                    "y": 585
                },
                {
                    "x": 1373,
                    "y": 585
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:20px'>Jacobs University, Germany</p>",
            "id": 11,
            "page": 1,
            "text": "Jacobs University, Germany"
        },
        {
            "bounding_box": [
                {
                    "x": 1305,
                    "y": 601
                },
                {
                    "x": 2013,
                    "y": 601
                },
                {
                    "x": 2013,
                    "y": 642
                },
                {
                    "x": 1305,
                    "y": 642
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:16px'>d.bahdanau@ jacobs-university . de</p>",
            "id": 12,
            "page": 1,
            "text": "d.bahdanau@ jacobs-university . de"
        },
        {
            "bounding_box": [
                {
                    "x": 1455,
                    "y": 817
                },
                {
                    "x": 1866,
                    "y": 817
                },
                {
                    "x": 1866,
                    "y": 857
                },
                {
                    "x": 1455,
                    "y": 857
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:14px'>find.me@on · the. web</p>",
            "id": 13,
            "page": 1,
            "text": "find.me@on · the. web"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 882
                },
                {
                    "x": 2197,
                    "y": 882
                },
                {
                    "x": 2197,
                    "y": 1840
                },
                {
                    "x": 1272,
                    "y": 1840
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:16px'>Along this line of research on using neural net-<br>works for SMT, this paper focuses on a novel neu-<br>ral network architecture that can be used as a part<br>of the conventional phrase-based SMT system.<br>The proposed neural network architecture, which<br>we will refer to as an RNN Encoder-Decoder, con-<br>sists of two recurrent neural networks (RNN) that<br>act as an encoder and a decoder pair. The en-<br>coder maps a variable-length source sequence to a<br>fixed-length vector, and the decoder maps the vec-<br>tor representation back to a variable-length target<br>sequence. The two networks are trained jointly to<br>maximize the conditional probability of the target<br>sequence given a source sequence. Additionally,<br>we propose to use a rather sophisticated hidden<br>unit in order to improve both the memory capacity<br>and the ease of training.</p>",
            "id": 14,
            "page": 1,
            "text": "Along this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as a part of the conventional phrase-based SMT system. The proposed neural network architecture, which we will refer to as an RNN Encoder-Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence. The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. Additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1868
                },
                {
                    "x": 2198,
                    "y": 1868
                },
                {
                    "x": 2198,
                    "y": 2490
                },
                {
                    "x": 1272,
                    "y": 2490
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:18px'>The proposed RNN Encoder-Decoder with a<br>novel hidden unit is empirically evaluated on the<br>task of translating from English to French. We<br>train the model to learn the translation probabil-<br>ity of an English phrase to a corresponding French<br>phrase. The model is then used as a part of a stan-<br>dard phrase-based SMT system by scoring each<br>phrase pair in the phrase table. The empirical eval-<br>uation reveals that this approach of scoring phrase<br>pairs with an RNN Encoder-Decoder improves<br>the translation performance.</p>",
            "id": 15,
            "page": 1,
            "text": "The proposed RNN Encoder-Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French. We train the model to learn the translation probability of an English phrase to a corresponding French phrase. The model is then used as a part of a standard phrase-based SMT system by scoring each phrase pair in the phrase table. The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder-Decoder improves the translation performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2516
                },
                {
                    "x": 2198,
                    "y": 2516
                },
                {
                    "x": 2198,
                    "y": 3196
                },
                {
                    "x": 1272,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:18px'>We qualitatively analyze the trained RNN<br>Encoder-Decoder by comparing its phrase scores<br>with those given by the existing translation model.<br>The qualitative analysis shows that the RNN<br>Encoder-Decoder is better at capturing the lin-<br>guistic regularities in the phrase table, indirectly<br>explaining the quantitative improvements in the<br>overall translation performance. The further anal-<br>ysis of the model reveals that the RNN Encoder-<br>Decoder learns a continuous space representation<br>of a phrase that preserves both the semantic and<br>syntactic structure of the phrase.</p>",
            "id": 16,
            "page": 1,
            "text": "We qualitatively analyze the trained RNN Encoder-Decoder by comparing its phrase scores with those given by the existing translation model. The qualitative analysis shows that the RNN Encoder-Decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance. The further analysis of the model reveals that the RNN EncoderDecoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase."
        },
        {
            "bounding_box": [
                {
                    "x": 60,
                    "y": 1137
                },
                {
                    "x": 148,
                    "y": 1137
                },
                {
                    "x": 148,
                    "y": 2496
                },
                {
                    "x": 60,
                    "y": 2496
                }
            ],
            "category": "footer",
            "html": "<br><footer id='17' style='font-size:14px'>2014<br>Sep<br>3<br>[cs.CL]<br>arXiv:1406.1078v3</footer>",
            "id": 17,
            "page": 1,
            "text": "2014 Sep 3 [cs.CL] arXiv:1406.1078v3"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 263
                },
                {
                    "x": 880,
                    "y": 263
                },
                {
                    "x": 880,
                    "y": 319
                },
                {
                    "x": 292,
                    "y": 319
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>2 RNN Encoder-Decoder</p>",
            "id": 18,
            "page": 2,
            "text": "2 RNN Encoder-Decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 353
                },
                {
                    "x": 1212,
                    "y": 353
                },
                {
                    "x": 1212,
                    "y": 407
                },
                {
                    "x": 293,
                    "y": 407
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:18px'>2.1 Preliminary: Recurrent Neural Networks</p>",
            "id": 19,
            "page": 2,
            "text": "2.1 Preliminary: Recurrent Neural Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 428
                },
                {
                    "x": 1216,
                    "y": 428
                },
                {
                    "x": 1216,
                    "y": 764
                },
                {
                    "x": 291,
                    "y": 764
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:16px'>A recurrent neural network (RNN) is a neural net-<br>work that consists of a hidden state h and an<br>optional output y which operates on a variable-<br>length sequence x = (x1, · · · , XT). At each time<br>step t, the hidden state h<t> of the RNN is updated<br>by</p>",
            "id": 20,
            "page": 2,
            "text": "A recurrent neural network (RNN) is a neural network that consists of a hidden state h and an optional output y which operates on a variablelength sequence x = (x1, · · · , XT). At each time step t, the hidden state h<t> of the RNN is updated by"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 908
                },
                {
                    "x": 1213,
                    "y": 908
                },
                {
                    "x": 1213,
                    "y": 1186
                },
                {
                    "x": 291,
                    "y": 1186
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:16px'>where f is a non-linear activation func-<br>tion. f may be as simple as an element-<br>wise logistic sigmoid function and as com-<br>plex as a long short-term memory (LSTM)<br>unit (Hochreiter and Schmidhuber, 1997).</p>",
            "id": 21,
            "page": 2,
            "text": "where f is a non-linear activation function. f may be as simple as an elementwise logistic sigmoid function and as complex as a long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1194
                },
                {
                    "x": 1217,
                    "y": 1194
                },
                {
                    "x": 1217,
                    "y": 1583
                },
                {
                    "x": 291,
                    "y": 1583
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:16px'>An RNN can learn a probability distribution<br>over a sequence by being trained to predict the<br>next symbol in a sequence. In that case, the output<br>at each timestep t is the conditional distribution<br>p(xt I Xt-1, · · · , x1). For example, a multinomial<br>distribution (1-of-K coding) can be output using a<br>softmax activation function</p>",
            "id": 22,
            "page": 2,
            "text": "An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. In that case, the output at each timestep t is the conditional distribution p(xt I Xt-1, · · · , x1). For example, a multinomial distribution (1-of-K coding) can be output using a softmax activation function"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1904
                },
                {
                    "x": 1217,
                    "y": 1904
                },
                {
                    "x": 1217,
                    "y": 2129
                },
                {
                    "x": 291,
                    "y": 2129
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:16px'>for all possible symbols j = 1, · · · , K, where Wj<br>are the rows of a weight matrix W. By combining<br>these probabilities, we can compute the probabil-<br>ity of the sequence x using</p>",
            "id": 23,
            "page": 2,
            "text": "for all possible symbols j = 1, · · · , K, where Wj are the rows of a weight matrix W. By combining these probabilities, we can compute the probability of the sequence x using"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2351
                },
                {
                    "x": 1216,
                    "y": 2351
                },
                {
                    "x": 1216,
                    "y": 2520
                },
                {
                    "x": 291,
                    "y": 2520
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>From this learned distribution, it is straightfor-<br>ward to sample a new sequence by iteratively sam-<br>pling a symbol at each time step.</p>",
            "id": 24,
            "page": 2,
            "text": "From this learned distribution, it is straightforward to sample a new sequence by iteratively sampling a symbol at each time step."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2557
                },
                {
                    "x": 864,
                    "y": 2557
                },
                {
                    "x": 864,
                    "y": 2610
                },
                {
                    "x": 291,
                    "y": 2610
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>2.2 RNN Encoder-Decoder</p>",
            "id": 25,
            "page": 2,
            "text": "2.2 RNN Encoder-Decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2632
                },
                {
                    "x": 1216,
                    "y": 2632
                },
                {
                    "x": 1216,
                    "y": 3199
                },
                {
                    "x": 291,
                    "y": 3199
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:14px'>In this paper, we propose a novel neural network<br>architecture that learns to encode a variable-length<br>sequence into a fixed-length vector representation<br>and to decode a given fixed-length vector rep-<br>resentation back into a variable-length sequence.<br>From a probabilistic perspective, this new model<br>is a general method to learn the conditional dis-<br>tribution over a variable-length sequence condi-<br>tioned on yet another variable-length sequence,<br>e.g. p(y1, · · · , YT' X1, ... , xT), where one</p>",
            "id": 26,
            "page": 2,
            "text": "In this paper, we propose a novel neural network architecture that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector representation back into a variable-length sequence. From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence, e.g. p(y1, · · · , YT' X1, ... , xT), where one"
        },
        {
            "bounding_box": [
                {
                    "x": 1361,
                    "y": 254
                },
                {
                    "x": 2114,
                    "y": 254
                },
                {
                    "x": 2114,
                    "y": 1035
                },
                {
                    "x": 1361,
                    "y": 1035
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='27' style='font-size:18px' alt=\"Decoder\nyT\nC\nX1 X2 XT\nEncoder\" data-coord=\"top-left:(1361,254); bottom-right:(2114,1035)\" /></figure>",
            "id": 27,
            "page": 2,
            "text": "Decoder yT C X1 X2 XT Encoder"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1066
                },
                {
                    "x": 2193,
                    "y": 1066
                },
                {
                    "x": 2193,
                    "y": 1174
                },
                {
                    "x": 1274,
                    "y": 1174
                }
            ],
            "category": "caption",
            "html": "<br><caption id='28' style='font-size:18px'>Figure 1: An illustration of the proposed RNN<br>Encoder-Decoder.</caption>",
            "id": 28,
            "page": 2,
            "text": "Figure 1: An illustration of the proposed RNN Encoder-Decoder."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1226
                },
                {
                    "x": 2194,
                    "y": 1226
                },
                {
                    "x": 2194,
                    "y": 1333
                },
                {
                    "x": 1274,
                    "y": 1333
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:18px'>should note that the input and output sequence<br>lengths T and T' may differ.</p>",
            "id": 29,
            "page": 2,
            "text": "should note that the input and output sequence lengths T and T' may differ."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1342
                },
                {
                    "x": 2198,
                    "y": 1342
                },
                {
                    "x": 2198,
                    "y": 1730
                },
                {
                    "x": 1272,
                    "y": 1730
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:18px'>The encoder is an RNN that reads each symbol<br>of an input sequence x sequentially. As it reads<br>each symbol, the hidden state of the RNN changes<br>according to Eq. (1). After reading the end of<br>the sequence (marked by an end-of-sequence sym-<br>bol), the hidden state of the RNN is a summary c<br>of the whole input sequence.</p>",
            "id": 30,
            "page": 2,
            "text": "The encoder is an RNN that reads each symbol of an input sequence x sequentially. As it reads each symbol, the hidden state of the RNN changes according to Eq. (1). After reading the end of the sequence (marked by an end-of-sequence symbol), the hidden state of the RNN is a summary c of the whole input sequence."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1736
                },
                {
                    "x": 2198,
                    "y": 1736
                },
                {
                    "x": 2198,
                    "y": 2183
                },
                {
                    "x": 1271,
                    "y": 2183
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:16px'>The decoder of the proposed model is another<br>RNN which is trained to generate the output se-<br>quence by predicting the next symbol Yt given the<br>hidden state h<t>. However, unlike the RNN de-<br>scribed in Sec. 2.1, both Yt and h<t> are also con-<br>ditioned on yt-1 and on the summary c of the input<br>sequence. Hence, the hidden state of the decoder<br>at time t is computed by,</p>",
            "id": 31,
            "page": 2,
            "text": "The decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol Yt given the hidden state h<t>. However, unlike the RNN described in Sec. 2.1, both Yt and h<t> are also conditioned on yt-1 and on the summary c of the input sequence. Hence, the hidden state of the decoder at time t is computed by,"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2312
                },
                {
                    "x": 2196,
                    "y": 2312
                },
                {
                    "x": 2196,
                    "y": 2421
                },
                {
                    "x": 1273,
                    "y": 2421
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>and similarly, the conditional distribution of the<br>next symbol is</p>",
            "id": 32,
            "page": 2,
            "text": "and similarly, the conditional distribution of the next symbol is"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2552
                },
                {
                    "x": 2195,
                    "y": 2552
                },
                {
                    "x": 2195,
                    "y": 2717
                },
                {
                    "x": 1273,
                    "y": 2717
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>for given activation functions f and g (the latter<br>must produce valid probabilities, e.g. with a soft-<br>max).</p>",
            "id": 33,
            "page": 2,
            "text": "for given activation functions f and g (the latter must produce valid probabilities, e.g. with a softmax)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2724
                },
                {
                    "x": 2194,
                    "y": 2724
                },
                {
                    "x": 2194,
                    "y": 2829
                },
                {
                    "x": 1271,
                    "y": 2829
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:16px'>See Fig. 1 for a graphical depiction of the pro-<br>posed model architecture.</p>",
            "id": 34,
            "page": 2,
            "text": "See Fig. 1 for a graphical depiction of the proposed model architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2836
                },
                {
                    "x": 2198,
                    "y": 2836
                },
                {
                    "x": 2198,
                    "y": 3001
                },
                {
                    "x": 1272,
                    "y": 3001
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:16px'>The two components of the proposed RNN<br>Encoder-Decoder are jointly trained to maximize<br>the conditional log-likelihood</p>",
            "id": 35,
            "page": 2,
            "text": "The two components of the proposed RNN Encoder-Decoder are jointly trained to maximize the conditional log-likelihood"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 267
                },
                {
                    "x": 1216,
                    "y": 267
                },
                {
                    "x": 1216,
                    "y": 603
                },
                {
                    "x": 290,
                    "y": 603
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:14px'>where 0 is the set of the model parameters and<br>each (Xn, yn) is an (input sequence, output se-<br>quence) pair from the training set. In our case,<br>as the output of the decoder, starting from the in-<br>put, is differentiable, we can use a gradient-based<br>algorithm to estimate the model parameters.</p>",
            "id": 36,
            "page": 3,
            "text": "where 0 is the set of the model parameters and each (Xn, yn) is an (input sequence, output sequence) pair from the training set. In our case, as the output of the decoder, starting from the input, is differentiable, we can use a gradient-based algorithm to estimate the model parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 607
                },
                {
                    "x": 1215,
                    "y": 607
                },
                {
                    "x": 1215,
                    "y": 1003
                },
                {
                    "x": 292,
                    "y": 1003
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:14px'>Once the RNN Encoder-Decoder is trained, the<br>model can be used in two ways. One way is to use<br>the model to generate a target sequence given an<br>input sequence. On the other hand, the model can<br>be used to score a given pair of input and output<br>sequences, where the score is simply a probability<br>po (y I x) from Eqs. (3) and (4).</p>",
            "id": 37,
            "page": 3,
            "text": "Once the RNN Encoder-Decoder is trained, the model can be used in two ways. One way is to use the model to generate a target sequence given an input sequence. On the other hand, the model can be used to score a given pair of input and output sequences, where the score is simply a probability po (y I x) from Eqs. (3) and (4)."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1035
                },
                {
                    "x": 1208,
                    "y": 1035
                },
                {
                    "x": 1208,
                    "y": 1146
                },
                {
                    "x": 294,
                    "y": 1146
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:18px'>2.3 Hidden Unit that Adaptively Remembers<br>and Forgets</p>",
            "id": 38,
            "page": 3,
            "text": "2.3 Hidden Unit that Adaptively Remembers and Forgets"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1166
                },
                {
                    "x": 1215,
                    "y": 1166
                },
                {
                    "x": 1215,
                    "y": 1499
                },
                {
                    "x": 292,
                    "y": 1499
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:16px'>In addition to a novel model architecture, we also<br>propose a new type of hidden unit (f in Eq. (1))<br>that has been motivated by the LSTM unit but is<br>much simpler to compute and implement. 1 Fig. 2<br>shows the graphical depiction of the proposed hid-<br>den unit.</p>",
            "id": 39,
            "page": 3,
            "text": "In addition to a novel model architecture, we also propose a new type of hidden unit (f in Eq. (1)) that has been motivated by the LSTM unit but is much simpler to compute and implement. 1 Fig. 2 shows the graphical depiction of the proposed hidden unit."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1505
                },
                {
                    "x": 1215,
                    "y": 1505
                },
                {
                    "x": 1215,
                    "y": 1673
                },
                {
                    "x": 292,
                    "y": 1673
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:14px'>Let us describe how the activation of the j-th<br>hidden unit is computed. First, the reset gate rj is<br>computed by</p>",
            "id": 40,
            "page": 3,
            "text": "Let us describe how the activation of the j-th hidden unit is computed. First, the reset gate rj is computed by"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1818
                },
                {
                    "x": 1214,
                    "y": 1818
                },
                {
                    "x": 1214,
                    "y": 2098
                },
                {
                    "x": 291,
                    "y": 2098
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>where 0 is the logistic sigmoid function, and [·],<br>denotes the j-th element of a vector. X and ht-1<br>are the input and the previous hidden state, respec-<br>tively. Wr and Ur are weight matrices which are<br>learned.</p>",
            "id": 41,
            "page": 3,
            "text": "where 0 is the logistic sigmoid function, and [·], denotes the j-th element of a vector. X and ht-1 are the input and the previous hidden state, respectively. Wr and Ur are weight matrices which are learned."
        },
        {
            "bounding_box": [
                {
                    "x": 338,
                    "y": 2103
                },
                {
                    "x": 1156,
                    "y": 2103
                },
                {
                    "x": 1156,
                    "y": 2158
                },
                {
                    "x": 338,
                    "y": 2158
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:18px'>Similarly, the update gate Zj is computed by</p>",
            "id": 42,
            "page": 3,
            "text": "Similarly, the update gate Zj is computed by"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2307
                },
                {
                    "x": 1215,
                    "y": 2307
                },
                {
                    "x": 1215,
                    "y": 2418
                },
                {
                    "x": 290,
                    "y": 2418
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:16px'>The actual activation of the proposed unit hj is<br>then computed by</p>",
            "id": 43,
            "page": 3,
            "text": "The actual activation of the proposed unit hj is then computed by"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2555
                },
                {
                    "x": 416,
                    "y": 2555
                },
                {
                    "x": 416,
                    "y": 2601
                },
                {
                    "x": 294,
                    "y": 2601
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:14px'>where</p>",
            "id": 44,
            "page": 3,
            "text": "where"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2742
                },
                {
                    "x": 1214,
                    "y": 2742
                },
                {
                    "x": 1214,
                    "y": 2910
                },
                {
                    "x": 292,
                    "y": 2910
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:14px'>In this formulation, when the reset gate is close<br>to 0, the hidden state is forced to ignore the pre-<br>vious hidden state and reset with the current input</p>",
            "id": 45,
            "page": 3,
            "text": "In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the previous hidden state and reset with the current input"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2937
                },
                {
                    "x": 1214,
                    "y": 2937
                },
                {
                    "x": 1214,
                    "y": 3192
                },
                {
                    "x": 293,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:14px'>1 The LSTM unit, which has shown impressive results in<br>several applications such as speech recognition, has a mem-<br>ory cell and four gating units that adaptively control the in-<br>formation flow inside the unit, compared to only two gating<br>units in the proposed hidden unit. For details on LSTM net-<br>works, see, e.g., (Graves, 2012).</p>",
            "id": 46,
            "page": 3,
            "text": "1 The LSTM unit, which has shown impressive results in several applications such as speech recognition, has a memory cell and four gating units that adaptively control the information flow inside the unit, compared to only two gating units in the proposed hidden unit. For details on LSTM networks, see, e.g., (Graves, 2012)."
        },
        {
            "bounding_box": [
                {
                    "x": 1404,
                    "y": 251
                },
                {
                    "x": 2065,
                    "y": 251
                },
                {
                    "x": 2065,
                    "y": 589
                },
                {
                    "x": 1404,
                    "y": 589
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='47' style='font-size:18px' alt=\"Z\nh h X\" data-coord=\"top-left:(1404,251); bottom-right:(2065,589)\" /></figure>",
            "id": 47,
            "page": 3,
            "text": "Z h h X"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 621
                },
                {
                    "x": 2197,
                    "y": 621
                },
                {
                    "x": 2197,
                    "y": 1013
                },
                {
                    "x": 1271,
                    "y": 1013
                }
            ],
            "category": "caption",
            "html": "<caption id='48' style='font-size:14px'>Figure 2: An illustration of the proposed hidden<br>activation function. The update gate 2 selects<br>whether the hidden state is to be updated with<br>a new hidden state h. The reset gate r decides<br>whether the previous hidden state is ignored. See<br>Eqs. (5)-(8) for the detailed equations of r, z, h<br>and h.</caption>",
            "id": 48,
            "page": 3,
            "text": "Figure 2: An illustration of the proposed hidden activation function. The update gate 2 selects whether the hidden state is to be updated with a new hidden state h. The reset gate r decides whether the previous hidden state is ignored. See Eqs. (5)-(8) for the detailed equations of r, z, h and h."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1077
                },
                {
                    "x": 2196,
                    "y": 1077
                },
                {
                    "x": 2196,
                    "y": 1299
                },
                {
                    "x": 1270,
                    "y": 1299
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:14px'>only. This effectively allows the hidden state to<br>drop any information that is found to be irrelevant<br>later in the future, thus, allowing a more compact<br>representation.</p>",
            "id": 49,
            "page": 3,
            "text": "only. This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1307
                },
                {
                    "x": 2196,
                    "y": 1307
                },
                {
                    "x": 2196,
                    "y": 1753
                },
                {
                    "x": 1271,
                    "y": 1753
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:14px'>On the other hand, the update gate controls how<br>much information from the previous hidden state<br>will carry over to the current hidden state. This<br>acts similarly to the memory cell in the LSTM<br>network and helps the RNN to remember long-<br>term information. Furthermore, this may be con-<br>sidered an adaptive variant of a leaky-integration<br>unit (Bengio et al., 2013).</p>",
            "id": 50,
            "page": 3,
            "text": "On the other hand, the update gate controls how much information from the previous hidden state will carry over to the current hidden state. This acts similarly to the memory cell in the LSTM network and helps the RNN to remember longterm information. Furthermore, this may be considered an adaptive variant of a leaky-integration unit (Bengio , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1761
                },
                {
                    "x": 2197,
                    "y": 1761
                },
                {
                    "x": 2197,
                    "y": 2151
                },
                {
                    "x": 1271,
                    "y": 2151
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:14px'>As each hidden unit has separate reset and up-<br>date gates, each hidden unit will learn to capture<br>dependencies over different time scales. Those<br>units that learn to capture short-term dependencies<br>will tend to have reset gates that are frequently ac-<br>tive, but those that capture longer-term dependen-<br>cies will have update gates that are mostly active.</p>",
            "id": 51,
            "page": 3,
            "text": "As each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales. Those units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2159
                },
                {
                    "x": 2196,
                    "y": 2159
                },
                {
                    "x": 2196,
                    "y": 2382
                },
                {
                    "x": 1272,
                    "y": 2382
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:14px'>In our preliminary experiments, we found that<br>it is crucial to use this new unit with gating units.<br>We were not able to get meaningful result with an<br>oft-used tanh unit without any gating.</p>",
            "id": 52,
            "page": 3,
            "text": "In our preliminary experiments, we found that it is crucial to use this new unit with gating units. We were not able to get meaningful result with an oft-used tanh unit without any gating."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2430
                },
                {
                    "x": 2024,
                    "y": 2430
                },
                {
                    "x": 2024,
                    "y": 2488
                },
                {
                    "x": 1273,
                    "y": 2488
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>3 Statistical Machine Translation</p>",
            "id": 53,
            "page": 3,
            "text": "3 Statistical Machine Translation"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2527
                },
                {
                    "x": 2196,
                    "y": 2527
                },
                {
                    "x": 2196,
                    "y": 2747
                },
                {
                    "x": 1272,
                    "y": 2747
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:14px'>In a commonly used statistical machine translation<br>system (SMT), the goal of the system (decoder,<br>specifically) is to find a translation f given a source<br>sentence e, which maximizes</p>",
            "id": 54,
            "page": 3,
            "text": "In a commonly used statistical machine translation system (SMT), the goal of the system (decoder, specifically) is to find a translation f given a source sentence e, which maximizes"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2913
                },
                {
                    "x": 2197,
                    "y": 2913
                },
                {
                    "x": 2197,
                    "y": 3194
                },
                {
                    "x": 1272,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>where the first term at the right hand side is called<br>translation model and the latter language model<br>(see, e.g., (Koehn, 2005)). In practice, however,<br>most SMT systems model logp(f I e) as a log-<br>linear model with additional features and corre-</p>",
            "id": 55,
            "page": 3,
            "text": "where the first term at the right hand side is called translation model and the latter language model (see, e.g., (Koehn, 2005)). In practice, however, most SMT systems model logp(f I e) as a loglinear model with additional features and corre-"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 270
                },
                {
                    "x": 638,
                    "y": 270
                },
                {
                    "x": 638,
                    "y": 319
                },
                {
                    "x": 292,
                    "y": 319
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:22px'>sponding weights:</p>",
            "id": 56,
            "page": 4,
            "text": "sponding weights:"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 541
                },
                {
                    "x": 1217,
                    "y": 541
                },
                {
                    "x": 1217,
                    "y": 817
                },
                {
                    "x": 292,
                    "y": 817
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>where fn and Wn are the n-th feature and weight,<br>respectively. Z(e) is a normalization constant that<br>does not depend on the weights. The weights are<br>often optimized to maximize the BLEU score on a<br>development set.</p>",
            "id": 57,
            "page": 4,
            "text": "where fn and Wn are the n-th feature and weight, respectively. Z(e) is a normalization constant that does not depend on the weights. The weights are often optimized to maximize the BLEU score on a development set."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 823
                },
                {
                    "x": 1215,
                    "y": 823
                },
                {
                    "x": 1215,
                    "y": 1325
                },
                {
                    "x": 292,
                    "y": 1325
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>In the phrase-based SMT framework<br>introduced in (Koehn et al., 2003) and<br>(Marcu and Wong, 2002), the translation model<br>logp(e I f) is factorized into the translation<br>probabilities of matching phrases in the source<br>and target sentences.2 These probabilities are<br>once again considered additional features in the<br>log-linear model (see Eq. (9)) and are weighted<br>accordingly to maximize the BLEU score.</p>",
            "id": 58,
            "page": 4,
            "text": "In the phrase-based SMT framework introduced in (Koehn , 2003) and (Marcu and Wong, 2002), the translation model logp(e I f) is factorized into the translation probabilities of matching phrases in the source and target sentences.2 These probabilities are once again considered additional features in the log-linear model (see Eq. (9)) and are weighted accordingly to maximize the BLEU score."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1332
                },
                {
                    "x": 1216,
                    "y": 1332
                },
                {
                    "x": 1216,
                    "y": 1949
                },
                {
                    "x": 291,
                    "y": 1949
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:16px'>Since the neural net language model was pro-<br>posed in (Bengio et al., 2003), neural networks<br>have been used widely in SMT systems. In<br>many cases, neural networks have been used to<br>rescore translation hypotheses (n-best lists) (see,<br>e.g., (Schwenk et al., 2006)). Recently, however,<br>there has been interest in training neural networks<br>to score the translated sentence (or phrase pairs)<br>using a representation of the source sentence as<br>an additional input. See, e.g., (Schwenk, 2012),<br>(Son et al., 2012) and (Zou et al., 2013).</p>",
            "id": 59,
            "page": 4,
            "text": "Since the neural net language model was proposed in (Bengio , 2003), neural networks have been used widely in SMT systems. In many cases, neural networks have been used to rescore translation hypotheses (n-best lists) (see, e.g., (Schwenk , 2006)). Recently, however, there has been interest in training neural networks to score the translated sentence (or phrase pairs) using a representation of the source sentence as an additional input. See, e.g., (Schwenk, 2012), (Son , 2012) and (Zou , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1985
                },
                {
                    "x": 1022,
                    "y": 1985
                },
                {
                    "x": 1022,
                    "y": 2093
                },
                {
                    "x": 292,
                    "y": 2093
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:18px'>3.1 Scoring Phrase Pairs with RNN<br>Encoder-Decoder</p>",
            "id": 60,
            "page": 4,
            "text": "3.1 Scoring Phrase Pairs with RNN Encoder-Decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2117
                },
                {
                    "x": 1216,
                    "y": 2117
                },
                {
                    "x": 1216,
                    "y": 2392
                },
                {
                    "x": 291,
                    "y": 2392
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:16px'>Here we propose to train the RNN Encoder-<br>Decoder (see Sec. 2.2) on a table of phrase pairs<br>and use its scores as additional features in the log-<br>linear model in Eq. (9) when tuning the SMT de-<br>coder.</p>",
            "id": 61,
            "page": 4,
            "text": "Here we propose to train the RNN EncoderDecoder (see Sec. 2.2) on a table of phrase pairs and use its scores as additional features in the loglinear model in Eq. (9) when tuning the SMT decoder."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2400
                },
                {
                    "x": 1215,
                    "y": 2400
                },
                {
                    "x": 1215,
                    "y": 3074
                },
                {
                    "x": 291,
                    "y": 3074
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:18px'>When we train the RNN Encoder-Decoder, we<br>ignore the (normalized) frequencies of each phrase<br>pair in the original corpora. This measure was<br>taken in order (1) to reduce the computational ex-<br>pense of randomly selecting phrase pairs from a<br>large phrase table according to the normalized fre-<br>quencies and (2) to ensure that the RNN Encoder-<br>Decoder does not simply learn to rank the phrase<br>pairs according to their numbers of occurrences.<br>One underlying reason for this choice was that the<br>existing translation probability in the phrase ta-<br>ble already reflects the frequencies of the phrase</p>",
            "id": 62,
            "page": 4,
            "text": "When we train the RNN Encoder-Decoder, we ignore the (normalized) frequencies of each phrase pair in the original corpora. This measure was taken in order (1) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and (2) to ensure that the RNN EncoderDecoder does not simply learn to rank the phrase pairs according to their numbers of occurrences. One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 3102
                },
                {
                    "x": 1216,
                    "y": 3102
                },
                {
                    "x": 1216,
                    "y": 3191
                },
                {
                    "x": 291,
                    "y": 3191
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:14px'>2 Without loss of generality, from here on, we refer to<br>p(e I f) for each phrase pair as a translation model as well</p>",
            "id": 63,
            "page": 4,
            "text": "2 Without loss of generality, from here on, we refer to p(e I f) for each phrase pair as a translation model as well"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 268
                },
                {
                    "x": 2196,
                    "y": 268
                },
                {
                    "x": 2196,
                    "y": 659
                },
                {
                    "x": 1271,
                    "y": 659
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:18px'>pairs in the original corpus. With a fixed capacity<br>of the RNN Encoder-Decoder, we try to ensure<br>that most of the capacity of the model is focused<br>toward learning linguistic regularities, i.e., distin-<br>guishing between plausible and implausible trans-<br>lations, or learning the \"manifold\" (region of prob-<br>ability concentration) of plausible translations.</p>",
            "id": 64,
            "page": 4,
            "text": "pairs in the original corpus. With a fixed capacity of the RNN Encoder-Decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities, i.e., distinguishing between plausible and implausible translations, or learning the \"manifold\" (region of probability concentration) of plausible translations."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 669
                },
                {
                    "x": 2197,
                    "y": 669
                },
                {
                    "x": 2197,
                    "y": 947
                },
                {
                    "x": 1271,
                    "y": 947
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:16px'>Once the RNN Encoder-Decoder is trained, we<br>add a new score for each phrase pair to the exist-<br>ing phrase table. This allows the new scores to en-<br>ter into the existing tuning algorithm with minimal<br>additional overhead in computation.</p>",
            "id": 65,
            "page": 4,
            "text": "Once the RNN Encoder-Decoder is trained, we add a new score for each phrase pair to the existing phrase table. This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 956
                },
                {
                    "x": 2196,
                    "y": 956
                },
                {
                    "x": 2196,
                    "y": 1517
                },
                {
                    "x": 1272,
                    "y": 1517
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:18px'>As Schwenk pointed out in (Schwenk, 2012),<br>it is possible to completely replace the existing<br>phrase table with the proposed RNN Encoder-<br>Decoder. In that case, for a given source phrase,<br>the RNN Encoder-Decoder will need to generate<br>a list of (good) target phrases. This requires, how-<br>ever, an expensive sampling procedure to be per-<br>formed repeatedly. In this paper, thus, we only<br>consider rescoring the phrase pairs in the phrase<br>table.</p>",
            "id": 66,
            "page": 4,
            "text": "As Schwenk pointed out in (Schwenk, 2012), it is possible to completely replace the existing phrase table with the proposed RNN EncoderDecoder. In that case, for a given source phrase, the RNN Encoder-Decoder will need to generate a list of (good) target phrases. This requires, however, an expensive sampling procedure to be performed repeatedly. In this paper, thus, we only consider rescoring the phrase pairs in the phrase table."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1576
                },
                {
                    "x": 2188,
                    "y": 1576
                },
                {
                    "x": 2188,
                    "y": 1685
                },
                {
                    "x": 1273,
                    "y": 1685
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>3.2 Related Approaches: Neural Networks in<br>Machine Translation</p>",
            "id": 67,
            "page": 4,
            "text": "3.2 Related Approaches: Neural Networks in Machine Translation"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1719
                },
                {
                    "x": 2197,
                    "y": 1719
                },
                {
                    "x": 2197,
                    "y": 1883
                },
                {
                    "x": 1272,
                    "y": 1883
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:14px'>Before presenting the empirical results, we discuss<br>a number of recent works that have proposed to<br>use neural networks in the context of SMT.</p>",
            "id": 68,
            "page": 4,
            "text": "Before presenting the empirical results, we discuss a number of recent works that have proposed to use neural networks in the context of SMT."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1893
                },
                {
                    "x": 2196,
                    "y": 1893
                },
                {
                    "x": 2196,
                    "y": 2736
                },
                {
                    "x": 1272,
                    "y": 2736
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:18px'>Schwenk in (Schwenk, 2012) proposed a simi-<br>lar approach of scoring phrase pairs. Instead of the<br>RNN-based neural network, he used a feedforward<br>neural network that has fixed-size inputs (7 words<br>in his case, with zero-padding for shorter phrases)<br>and fixed-size outputs (7 words in the target lan-<br>guage). When it is used specifically for scoring<br>phrases for the SMT system, the maximum phrase<br>length is often chosen to be small. However, as the<br>length of phrases increases or as we apply neural<br>networks to other variable-length sequence data,<br>it is important that the neural network can han-<br>dle variable-length input and output. The pro-<br>posed RNN Encoder-Decoder is well-suited for<br>these applications.</p>",
            "id": 69,
            "page": 4,
            "text": "Schwenk in (Schwenk, 2012) proposed a similar approach of scoring phrase pairs. Instead of the RNN-based neural network, he used a feedforward neural network that has fixed-size inputs (7 words in his case, with zero-padding for shorter phrases) and fixed-size outputs (7 words in the target language). When it is used specifically for scoring phrases for the SMT system, the maximum phrase length is often chosen to be small. However, as the length of phrases increases or as we apply neural networks to other variable-length sequence data, it is important that the neural network can handle variable-length input and output. The proposed RNN Encoder-Decoder is well-suited for these applications."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2746
                },
                {
                    "x": 2196,
                    "y": 2746
                },
                {
                    "x": 2196,
                    "y": 3194
                },
                {
                    "x": 1271,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:16px'>Similar to (Schwenk, 2012), Devlin et al.<br>(Devlin et al., 2014) proposed to use a feedfor-<br>ward neural network to model a translation model,<br>however, by predicting one word in a target phrase<br>at a time. They reported an impressive improve-<br>ment, but their approach still requires the maxi-<br>mum length of the input phrase (or context words)<br>to be fixed a priori.</p>",
            "id": 70,
            "page": 4,
            "text": "Similar to (Schwenk, 2012), Devlin  (Devlin , 2014) proposed to use a feedforward neural network to model a translation model, however, by predicting one word in a target phrase at a time. They reported an impressive improvement, but their approach still requires the maximum length of the input phrase (or context words) to be fixed a priori."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 267
                },
                {
                    "x": 1217,
                    "y": 267
                },
                {
                    "x": 1217,
                    "y": 661
                },
                {
                    "x": 291,
                    "y": 661
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:18px'>Although it is not exactly a neural network they<br>train, the authors of (Zou et al., 2013) proposed<br>to learn a bilingual embedding of words/phrases.<br>They use the learned embedding to compute the<br>distance between a pair of phrases which is used<br>as an additional score of the phrase pair in an SMT<br>system.</p>",
            "id": 71,
            "page": 5,
            "text": "Although it is not exactly a neural network they train, the authors of (Zou , 2013) proposed to learn a bilingual embedding of words/phrases. They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 668
                },
                {
                    "x": 1215,
                    "y": 668
                },
                {
                    "x": 1215,
                    "y": 1680
                },
                {
                    "x": 291,
                    "y": 1680
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:14px'>In (Chandar et al., 2014), a feedforward neural<br>network was trained to learn a mapping from a<br>bag-of-words representation of an input phrase to<br>an output phrase. This is closely related to both the<br>proposed RNN Encoder-Decoder and the model<br>proposed in (Schwenk, 2012), except that their in-<br>put representation of a phrase is a bag-of-words.<br>A similar approach of using bag-of-words repre-<br>sentations was proposed in (Gao et al., 2013) as<br>well. Earlier, a similar encoder-decoder model us-<br>ing two recursive neural networks was proposed<br>in (Socher et al., 2011), but their model was re-<br>stricted to a monolingual setting, i.e. the model<br>reconstructs an input sentence. More recently, an-<br>other encoder-decoder model using an RNN was<br>proposed in (Auli et al., 2013), where the de-<br>coder is conditioned on a representation of either<br>a source sentence or a source context.</p>",
            "id": 72,
            "page": 5,
            "text": "In (Chandar , 2014), a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase. This is closely related to both the proposed RNN Encoder-Decoder and the model proposed in (Schwenk, 2012), except that their input representation of a phrase is a bag-of-words. A similar approach of using bag-of-words representations was proposed in (Gao , 2013) as well. Earlier, a similar encoder-decoder model using two recursive neural networks was proposed in (Socher , 2011), but their model was restricted to a monolingual setting, i.e. the model reconstructs an input sentence. More recently, another encoder-decoder model using an RNN was proposed in (Auli , 2013), where the decoder is conditioned on a representation of either a source sentence or a source context."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1687
                },
                {
                    "x": 1216,
                    "y": 1687
                },
                {
                    "x": 1216,
                    "y": 2192
                },
                {
                    "x": 292,
                    "y": 2192
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:16px'>One important difference between the pro-<br>posed RNN Encoder-Decoder and the approaches<br>in (Zou et al., 2013) and (Chandar et al., 2014) is<br>that the order of the words in source and tar-<br>get phrases is taken into account. The RNN<br>Encoder-Decoder naturally distinguishes between<br>sequences that have the same words but in a differ-<br>ent order, whereas the aforementioned approaches<br>effectively ignore order information.</p>",
            "id": 73,
            "page": 5,
            "text": "One important difference between the proposed RNN Encoder-Decoder and the approaches in (Zou , 2013) and (Chandar , 2014) is that the order of the words in source and target phrases is taken into account. The RNN Encoder-Decoder naturally distinguishes between sequences that have the same words but in a different order, whereas the aforementioned approaches effectively ignore order information."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2202
                },
                {
                    "x": 1215,
                    "y": 2202
                },
                {
                    "x": 1215,
                    "y": 2934
                },
                {
                    "x": 291,
                    "y": 2934
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:16px'>The closest approach related to the proposed<br>RNN Encoder-Decoder is the Recurrent Contin-<br>uous Translation Model (Model 2) proposed in<br>(Kalchbrenner and Blunsom, 2013). In their pa-<br>per, they proposed a similar model that consists<br>of an encoder and decoder. The difference with<br>our model is that they used a convolutional n-gram<br>model (CGM) for the encoder and the hybrid of<br>an inverse CGM and a recurrent neural network<br>for the decoder. They, however, evaluated their<br>model on rescoring the n-best list proposed by the<br>conventional SMT system and computing the per-<br>plexity of the gold standard translations.</p>",
            "id": 74,
            "page": 5,
            "text": "The closest approach related to the proposed RNN Encoder-Decoder is the Recurrent Continuous Translation Model (Model 2) proposed in (Kalchbrenner and Blunsom, 2013). In their paper, they proposed a similar model that consists of an encoder and decoder. The difference with our model is that they used a convolutional n-gram model (CGM) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder. They, however, evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2983
                },
                {
                    "x": 652,
                    "y": 2983
                },
                {
                    "x": 652,
                    "y": 3041
                },
                {
                    "x": 292,
                    "y": 3041
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:20px'>4 Experiments</p>",
            "id": 75,
            "page": 5,
            "text": "4 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 3082
                },
                {
                    "x": 1215,
                    "y": 3082
                },
                {
                    "x": 1215,
                    "y": 3193
                },
                {
                    "x": 292,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>We evaluate our approach on the English/French<br>translation task of the WMT' 14 workshop.</p>",
            "id": 76,
            "page": 5,
            "text": "We evaluate our approach on the English/French translation task of the WMT' 14 workshop."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 267
                },
                {
                    "x": 1889,
                    "y": 267
                },
                {
                    "x": 1889,
                    "y": 320
                },
                {
                    "x": 1270,
                    "y": 320
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:18px'>4.1 Data and Baseline System</p>",
            "id": 77,
            "page": 5,
            "text": "4.1 Data and Baseline System"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 349
                },
                {
                    "x": 2198,
                    "y": 349
                },
                {
                    "x": 2198,
                    "y": 966
                },
                {
                    "x": 1272,
                    "y": 966
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:16px'>Large amounts of resources are available to build<br>an English/French SMT system in the framework<br>of the WMT' 14 translation task. The bilingual<br>corpora include Europarl (61M words), news com-<br>mentary (5.5M), UN (421M), and two crawled<br>corpora of 90M and 780M words respectively.<br>The last two corpora are quite noisy. To train<br>the French language model, about 712M words of<br>crawled newspaper material is available in addi-<br>tion to the target side of the bitexts. All the word<br>counts refer to French words after tokenization.</p>",
            "id": 78,
            "page": 5,
            "text": "Large amounts of resources are available to build an English/French SMT system in the framework of the WMT' 14 translation task. The bilingual corpora include Europarl (61M words), news commentary (5.5M), UN (421M), and two crawled corpora of 90M and 780M words respectively. The last two corpora are quite noisy. To train the French language model, about 712M words of crawled newspaper material is available in addition to the target side of the bitexts. All the word counts refer to French words after tokenization."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 977
                },
                {
                    "x": 2199,
                    "y": 977
                },
                {
                    "x": 2199,
                    "y": 2044
                },
                {
                    "x": 1272,
                    "y": 2044
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:14px'>It is commonly acknowledged that training sta-<br>tistical models on the concatenation of all this<br>data does not necessarily lead to optimal per-<br>formance, and results in extremely large mod-<br>els which are difficult to handle. Instead, one<br>should focus on the most relevant subset of the<br>data for a given task. We have done SO by<br>applying the data selection method proposed in<br>(Moore and Lewis, 2010), and its extension to bi-<br>texts (Axelrod et al., 2011). By these means we<br>selected a subset of 418M words out of more<br>than 2G words for language modeling and a<br>subset of 348M out of 850M words for train-<br>ing the RNN Encoder-Decoder. We used the<br>test set newstest2012 and 2013 for data<br>selection and weight tuning with MERT, and<br>newstest201 4 as our test set. Each set has<br>more than 70 thousand words and a single refer-<br>ence translation.</p>",
            "id": 79,
            "page": 5,
            "text": "It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance, and results in extremely large models which are difficult to handle. Instead, one should focus on the most relevant subset of the data for a given task. We have done SO by applying the data selection method proposed in (Moore and Lewis, 2010), and its extension to bitexts (Axelrod , 2011). By these means we selected a subset of 418M words out of more than 2G words for language modeling and a subset of 348M out of 850M words for training the RNN Encoder-Decoder. We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT, and newstest201 4 as our test set. Each set has more than 70 thousand words and a single reference translation."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2053
                },
                {
                    "x": 2198,
                    "y": 2053
                },
                {
                    "x": 2198,
                    "y": 2445
                },
                {
                    "x": 1271,
                    "y": 2445
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:18px'>For training the neural networks, including the<br>proposed RNN Encoder-Decoder, we limited the<br>source and target vocabulary to the most frequent<br>15,000 words for both English and French. This<br>covers approximately 93% of the dataset. All the<br>out-of-vocabulary words were mapped to a special<br>token (�).</p>",
            "id": 80,
            "page": 5,
            "text": "For training the neural networks, including the proposed RNN Encoder-Decoder, we limited the source and target vocabulary to the most frequent 15,000 words for both English and French. This covers approximately 93% of the dataset. All the out-of-vocabulary words were mapped to a special token (�)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2453
                },
                {
                    "x": 2197,
                    "y": 2453
                },
                {
                    "x": 2197,
                    "y": 2730
                },
                {
                    "x": 1271,
                    "y": 2730
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:16px'>The baseline phrase-based SMT system was<br>built using Moses with default settings. This sys-<br>tem achieves a BLEU score of 30.64 and 33.3 on<br>the development and test sets, respectively (see Ta-<br>ble 1).</p>",
            "id": 81,
            "page": 5,
            "text": "The baseline phrase-based SMT system was built using Moses with default settings. This system achieves a BLEU score of 30.64 and 33.3 on the development and test sets, respectively (see Table 1)."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2782
                },
                {
                    "x": 1878,
                    "y": 2782
                },
                {
                    "x": 1878,
                    "y": 2832
                },
                {
                    "x": 1274,
                    "y": 2832
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:16px'>4.1.1 RNN Encoder-Decoder</p>",
            "id": 82,
            "page": 5,
            "text": "4.1.1 RNN Encoder-Decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2857
                },
                {
                    "x": 2198,
                    "y": 2857
                },
                {
                    "x": 2198,
                    "y": 3195
                },
                {
                    "x": 1271,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:16px'>The RNN Encoder-Decoder used in the experi-<br>ment had 1000 hidden units with the proposed<br>gates at the encoder and at the decoder. The in-<br>put matrix between each input symbol x〈t〉 and the<br>hidden unit is approximated with two lower-rank<br>matrices, and the output matrix is approximated</p>",
            "id": 83,
            "page": 5,
            "text": "The RNN Encoder-Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder. The input matrix between each input symbol x〈t〉 and the hidden unit is approximated with two lower-rank matrices, and the output matrix is approximated"
        },
        {
            "bounding_box": [
                {
                    "x": 374,
                    "y": 254
                },
                {
                    "x": 1135,
                    "y": 254
                },
                {
                    "x": 1135,
                    "y": 618
                },
                {
                    "x": 374,
                    "y": 618
                }
            ],
            "category": "table",
            "html": "<table id='84' style='font-size:16px'><tr><td rowspan=\"2\">Models</td><td colspan=\"2\">BLEU</td></tr><tr><td>dev</td><td>test</td></tr><tr><td>Baseline</td><td>30.64</td><td>33.30</td></tr><tr><td>RNN</td><td>31.20</td><td>33.87</td></tr><tr><td>CSLM + RNN</td><td>31.48</td><td>34.64</td></tr><tr><td>CSLM + RNN + WP</td><td>31.50</td><td>34.54</td></tr></table>",
            "id": 84,
            "page": 6,
            "text": "Models BLEU  dev test  Baseline 30.64 33.30  RNN 31.20 33.87  CSLM + RNN 31.48 34.64  CSLM + RNN + WP 31.50"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 648
                },
                {
                    "x": 1216,
                    "y": 648
                },
                {
                    "x": 1216,
                    "y": 932
                },
                {
                    "x": 291,
                    "y": 932
                }
            ],
            "category": "caption",
            "html": "<caption id='85' style='font-size:16px'>Table 1: BLEU scores computed on the develop-<br>ment and test sets using different combinations of<br>approaches. WP denotes a word penalty, where<br>we penalizes the number of unknown words to<br>neural networks.</caption>",
            "id": 85,
            "page": 6,
            "text": "Table 1: BLEU scores computed on the development and test sets using different combinations of approaches. WP denotes a word penalty, where we penalizes the number of unknown words to neural networks."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 993
                },
                {
                    "x": 1215,
                    "y": 993
                },
                {
                    "x": 1215,
                    "y": 1496
                },
                {
                    "x": 291,
                    "y": 1496
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>similarly. We used rank-100 matrices, equivalent<br>to learning an embedding of dimension 100 for<br>each word. The activation function used for え in<br>Eq. (8) is a hyperbolic tangent function. The com-<br>putation from the hidden state in the decoder to<br>the output is implemented as a deep neural net-<br>work (Pascanu et al., 2014) with a single interme-<br>diate layer having 500 maxout units each pooling<br>2 inputs (Goodfellow et al., 2013).</p>",
            "id": 86,
            "page": 6,
            "text": "similarly. We used rank-100 matrices, equivalent to learning an embedding of dimension 100 for each word. The activation function used for え in Eq. (8) is a hyperbolic tangent function. The computation from the hidden state in the decoder to the output is implemented as a deep neural network (Pascanu , 2014) with a single intermediate layer having 500 maxout units each pooling 2 inputs (Goodfellow , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1504
                },
                {
                    "x": 1216,
                    "y": 1504
                },
                {
                    "x": 1216,
                    "y": 1951
                },
                {
                    "x": 291,
                    "y": 1951
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:18px'>All the weight parameters in the RNN Encoder-<br>Decoder were initialized by sampling from an<br>isotropic zero-mean (white) Gaussian distribution<br>with its standard deviation fixed to 0.01, except<br>for the recurrent weight parameters. For the re-<br>current weight matrices, we first sampled from a<br>white Gaussian distribution and used its left singu-<br>lar vectors matrix, following (Saxe et al., 2014).</p>",
            "id": 87,
            "page": 6,
            "text": "All the weight parameters in the RNN EncoderDecoder were initialized by sampling from an isotropic zero-mean (white) Gaussian distribution with its standard deviation fixed to 0.01, except for the recurrent weight parameters. For the recurrent weight matrices, we first sampled from a white Gaussian distribution and used its left singular vectors matrix, following (Saxe , 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1957
                },
                {
                    "x": 1216,
                    "y": 1957
                },
                {
                    "x": 1216,
                    "y": 2347
                },
                {
                    "x": 291,
                    "y": 2347
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:16px'>We used Adadelta and stochastic gradient<br>descent to train the RNN Encoder-Decoder<br>with hyperparameters 6 = 10-6 and P<br>0.95 (Zeiler, 2012). At each update, we used 64<br>randomly selected phrase pairs from a phrase ta-<br>ble (which was created from 348M words). The<br>model was trained for approximately three days.</p>",
            "id": 88,
            "page": 6,
            "text": "We used Adadelta and stochastic gradient descent to train the RNN Encoder-Decoder with hyperparameters 6 = 10-6 and P 0.95 (Zeiler, 2012). At each update, we used 64 randomly selected phrase pairs from a phrase table (which was created from 348M words). The model was trained for approximately three days."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2356
                },
                {
                    "x": 1216,
                    "y": 2356
                },
                {
                    "x": 1216,
                    "y": 2519
                },
                {
                    "x": 291,
                    "y": 2519
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='89' style='font-size:18px'>Details of the architecture used in the experi-<br>ments are explained in more depth in the supple-<br>mentary material.</p>",
            "id": 89,
            "page": 6,
            "text": "Details of the architecture used in the experiments are explained in more depth in the supplementary material."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2560
                },
                {
                    "x": 914,
                    "y": 2560
                },
                {
                    "x": 914,
                    "y": 2613
                },
                {
                    "x": 292,
                    "y": 2613
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:20px'>4.1.2 Neural Language Model</p>",
            "id": 90,
            "page": 6,
            "text": "4.1.2 Neural Language Model"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2633
                },
                {
                    "x": 1216,
                    "y": 2633
                },
                {
                    "x": 1216,
                    "y": 3196
                },
                {
                    "x": 291,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>In order to assess the effectiveness of scoring<br>phrase pairs with the proposed RNN Encoder-<br>Decoder, we also tried a more traditional approach<br>of using a neural network for learning a target<br>language model (CSLM) (Schwenk, 2007). Espe-<br>cially, the comparison between the SMT system<br>using CSLM and that using the proposed approach<br>of phrase scoring by RNN Encoder-Decoder will<br>clarify whether the contributions from multiple<br>neural networks in different parts of the SMT sys-</p>",
            "id": 91,
            "page": 6,
            "text": "In order to assess the effectiveness of scoring phrase pairs with the proposed RNN EncoderDecoder, we also tried a more traditional approach of using a neural network for learning a target language model (CSLM) (Schwenk, 2007). Especially, the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder-Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys-"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 271
                },
                {
                    "x": 1807,
                    "y": 271
                },
                {
                    "x": 1807,
                    "y": 318
                },
                {
                    "x": 1273,
                    "y": 318
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:14px'>tem add up or are redundant.</p>",
            "id": 92,
            "page": 6,
            "text": "tem add up or are redundant."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 327
                },
                {
                    "x": 2200,
                    "y": 327
                },
                {
                    "x": 2200,
                    "y": 1342
                },
                {
                    "x": 1272,
                    "y": 1342
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:18px'>We trained the CSLM model on 7-grams<br>from the target corpus. Each input word<br>was projected into the embedding space R512<br>,<br>and they were concatenated to form a 3072-<br>dimensional vector. The concatenated vector was<br>fed through two rectified layers (of size 1536 and<br>1024) (Glorot et al., 2011). The output layer was<br>a simple softmax layer (see Eq. (2)). All the<br>weight parameters were initialized uniformly be-<br>tween -0.01 and 0.01, and the model was trained<br>until the validation perplexity did not improve for<br>10 epochs. After training, the language model<br>achieved a perplexity of 45.80. The validation set<br>was a random selection of 0.1% of the corpus. The<br>model was used to score partial translations dur-<br>ing the decoding process, which generally leads to<br>higher gains in BLEU score than n-best list rescor-<br>ing (Vaswani et al., 2013).</p>",
            "id": 93,
            "page": 6,
            "text": "We trained the CSLM model on 7-grams from the target corpus. Each input word was projected into the embedding space R512 , and they were concatenated to form a 3072dimensional vector. The concatenated vector was fed through two rectified layers (of size 1536 and 1024) (Glorot , 2011). The output layer was a simple softmax layer (see Eq. (2)). All the weight parameters were initialized uniformly between -0.01 and 0.01, and the model was trained until the validation perplexity did not improve for 10 epochs. After training, the language model achieved a perplexity of 45.80. The validation set was a random selection of 0.1% of the corpus. The model was used to score partial translations during the decoding process, which generally leads to higher gains in BLEU score than n-best list rescoring (Vaswani , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1349
                },
                {
                    "x": 2199,
                    "y": 1349
                },
                {
                    "x": 2199,
                    "y": 1853
                },
                {
                    "x": 1271,
                    "y": 1853
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:18px'>To address the computational complexity of<br>using a CSLM in the decoder a buffer was<br>used to aggregate n-grams during the stack-<br>search performed by the decoder. Only when<br>the buffer is full, or a stack is about to<br>be pruned, the n-grams are scored by the<br>CSLM. This allows us to perform fast matrix-<br>matrix multiplication on GPU using Theano<br>(Bergstra et al., 2010; Bastien et al., 2012).</p>",
            "id": 94,
            "page": 6,
            "text": "To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stacksearch performed by the decoder. Only when the buffer is full, or a stack is about to be pruned, the n-grams are scored by the CSLM. This allows us to perform fast matrixmatrix multiplication on GPU using Theano (Bergstra , 2010; Bastien , 2012)."
        },
        {
            "bounding_box": [
                {
                    "x": 1347,
                    "y": 1938
                },
                {
                    "x": 2082,
                    "y": 1938
                },
                {
                    "x": 2082,
                    "y": 2511
                },
                {
                    "x": 1347,
                    "y": 2511
                }
            ],
            "category": "figure",
            "html": "<figure><img id='95' style='font-size:14px' alt=\"0\n-2\n4\n(6ol)\n-6\nScores\n-8\nTM\n-10\nx\n-12\nx\n-14\n-60 -50 -40 -30 -20 -10 0\nRNN Scores (log)\" data-coord=\"top-left:(1347,1938); bottom-right:(2082,2511)\" /></figure>",
            "id": 95,
            "page": 6,
            "text": "0 -2 4 (6ol) -6 Scores -8 TM -10 x -12 x -14 -60 -50 -40 -30 -20 -10 0 RNN Scores (log)"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2559
                },
                {
                    "x": 2196,
                    "y": 2559
                },
                {
                    "x": 2196,
                    "y": 2732
                },
                {
                    "x": 1272,
                    "y": 2732
                }
            ],
            "category": "caption",
            "html": "<caption id='96' style='font-size:18px'>Figure 3: The visualization of phrase pairs accord-<br>ing to their scores (log-probabilities) by the RNN<br>Encoder-Decoder and the translation model.</caption>",
            "id": 96,
            "page": 6,
            "text": "Figure 3: The visualization of phrase pairs according to their scores (log-probabilities) by the RNN Encoder-Decoder and the translation model."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2803
                },
                {
                    "x": 1807,
                    "y": 2803
                },
                {
                    "x": 1807,
                    "y": 2855
                },
                {
                    "x": 1273,
                    "y": 2855
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:22px'>4.2 Quantitative Analysis</p>",
            "id": 97,
            "page": 6,
            "text": "4.2 Quantitative Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2883
                },
                {
                    "x": 1963,
                    "y": 2883
                },
                {
                    "x": 1963,
                    "y": 2936
                },
                {
                    "x": 1275,
                    "y": 2936
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='98' style='font-size:18px'>We tried the following combinations:</p>",
            "id": 98,
            "page": 6,
            "text": "We tried the following combinations:"
        },
        {
            "bounding_box": [
                {
                    "x": 1307,
                    "y": 2988
                },
                {
                    "x": 2145,
                    "y": 2988
                },
                {
                    "x": 2145,
                    "y": 3196
                },
                {
                    "x": 1307,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:16px'>1. Baseline configuration<br>2. Baseline + RNN<br>3. Baseline + CSLM + RNN<br>4. Baseline + CSLM + RNN + Word penalty</p>",
            "id": 99,
            "page": 6,
            "text": "1. Baseline configuration 2. Baseline + RNN 3. Baseline + CSLM + RNN 4. Baseline + CSLM + RNN + Word penalty"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 248
                },
                {
                    "x": 2233,
                    "y": 248
                },
                {
                    "x": 2233,
                    "y": 751
                },
                {
                    "x": 292,
                    "y": 751
                }
            ],
            "category": "table",
            "html": "<table id='100' style='font-size:14px'><tr><td>Source</td><td>Translation Model</td><td>RNN Encoder-Decoder</td></tr><tr><td>at the end of the</td><td>[a la fin de la] [r la fin des annees] [etre sup- primes a la fin de la]</td><td>[a la fin du] [a la fin des] [a la fin de la]</td></tr><tr><td>for the first time</td><td>[r ⓒ pour la premirere fois] [ete donnes pour la premiere fois] [ete commemoree pour la premiere fois]</td><td>[pour la premiere fois] [pour la premiere fois ,] [pour la premiere fois que]</td></tr><tr><td>in the United States and</td><td>[? aux ?tats-Unis et] [ete ouvertes aux Etats- Unis et] [ete constatees aux Etats-Unis et]</td><td>[aux Etats-Unis et] [des Etats-Unis et] [des Etats-Unis et]</td></tr><tr><td>, as well as</td><td>[?s , qu'] [?s , ainsi que] [?re aussi bien que]</td><td>[, ainsi qu'] [, ainsi que] [, ainsi que les]</td></tr><tr><td>one of the most</td><td>[?t ?1' un des plus] [?1' un des plus] [etre retenue comme un de ses plus]</td><td>[1' un des] [le] [un des]</td></tr></table>",
            "id": 100,
            "page": 7,
            "text": "Source Translation Model RNN Encoder-Decoder  at the end of the [a la fin de la] [r la fin des annees] [etre sup- primes a la fin de la] [a la fin du] [a la fin des] [a la fin de la]  for the first time [r ⓒ pour la premirere fois] [ete donnes pour la premiere fois] [ete commemoree pour la premiere fois] [pour la premiere fois] [pour la premiere fois ,] [pour la premiere fois que]  in the United States and [? aux ?tats-Unis et] [ete ouvertes aux Etats- Unis et] [ete constatees aux Etats-Unis et] [aux Etats-Unis et] [des Etats-Unis et] [des Etats-Unis et]  , as well as [?s , qu'] [?s , ainsi que] [?re aussi bien que] [, ainsi qu'] [, ainsi que] [, ainsi que les]  one of the most [?t ?1' un des plus] [?1' un des plus] [etre retenue comme un de ses plus]"
        },
        {
            "bounding_box": [
                {
                    "x": 1004,
                    "y": 743
                },
                {
                    "x": 1520,
                    "y": 743
                },
                {
                    "x": 1520,
                    "y": 783
                },
                {
                    "x": 1004,
                    "y": 783
                }
            ],
            "category": "caption",
            "html": "<br><caption id='101' style='font-size:16px'>(a) Long, frequent source phrases</caption>",
            "id": 101,
            "page": 7,
            "text": "(a) Long, frequent source phrases"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 815
                },
                {
                    "x": 2232,
                    "y": 815
                },
                {
                    "x": 2232,
                    "y": 1395
                },
                {
                    "x": 294,
                    "y": 1395
                }
            ],
            "category": "table",
            "html": "<table id='102' style='font-size:14px'><tr><td>Source</td><td>Translation Model</td><td>RNN Encoder-Decoder</td></tr><tr><td>, Minister of Commu- nications and Trans- port</td><td>[Secretaire aux communications et aux trans- ports :] [Secretaire aux communications et aux transports]</td><td>[Secretaire aux communications et aux trans- ports] [Secretaire aux communications et aux transports :]</td></tr><tr><td>did not comply with the</td><td>[vestimentaire , ne correspondaient pas a des] [susmentionnee n etait pas conforme aux] [presentees n' etaient pas conformes a la]</td><td>[n' ont pas respecte les] [n' etait pas conforme aux] [n' ont pas respecte la]</td></tr><tr><td>parts of the world</td><td>[ⓒ gions du monde .] [regions du monde con- siderees .] [region du monde consideree .]</td><td>[parties du monde .] [les parties du monde .] [des parties du monde .]</td></tr><tr><td>the past few days ·</td><td>[le petit texte ] [cours des tout derniers jours .] [les tout derniers jours .]</td><td>[ces derniers jours ] [les derniers jours .] [cours des derniers jours .]</td></tr><tr><td>on Friday and Satur- day</td><td>[vendredi et samedi a la] [vendredi et samedi a] [se deroulera vendredi et samedi ,]</td><td>[le vendredi etle samedi] [le vendredi et samedi] [vendredi et samedi]</td></tr></table>",
            "id": 102,
            "page": 7,
            "text": "Source Translation Model RNN Encoder-Decoder  , Minister of Commu- nications and Trans- port [Secretaire aux communications et aux trans- ports :] [Secretaire aux communications et aux transports] [Secretaire aux communications et aux trans- ports] [Secretaire aux communications et aux transports :]  did not comply with the [vestimentaire , ne correspondaient pas a des] [susmentionnee n etait pas conforme aux] [presentees n' etaient pas conformes a la] [n' ont pas respecte les] [n' etait pas conforme aux] [n' ont pas respecte la]  parts of the world [ⓒ gions du monde .] [regions du monde con- siderees .] [region du monde consideree .] [parties du monde .] [les parties du monde .] [des parties du monde .]  the past few days · [le petit texte ] [cours des tout derniers jours .] [les tout derniers jours .] [ces derniers jours ] [les derniers jours .] [cours des derniers jours .]  on Friday and Satur- day [vendredi et samedi a la] [vendredi et samedi a] [se deroulera vendredi et samedi ,]"
        },
        {
            "bounding_box": [
                {
                    "x": 1040,
                    "y": 1387
                },
                {
                    "x": 1483,
                    "y": 1387
                },
                {
                    "x": 1483,
                    "y": 1424
                },
                {
                    "x": 1040,
                    "y": 1424
                }
            ],
            "category": "caption",
            "html": "<br><caption id='103' style='font-size:16px'>(b) Long, rare source phrases</caption>",
            "id": 103,
            "page": 7,
            "text": "(b) Long, rare source phrases"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1465
                },
                {
                    "x": 2197,
                    "y": 1465
                },
                {
                    "x": 2197,
                    "y": 1692
                },
                {
                    "x": 290,
                    "y": 1692
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:20px'>Table 2: The top scoring target phrases for a small set of source phrases according to the translation<br>model (direct translation probability) and by the RNN Encoder-Decoder. Source phrases were randomly<br>selected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a Cyrillic<br>letter ghe.</p>",
            "id": 104,
            "page": 7,
            "text": "Table 2: The top scoring target phrases for a small set of source phrases according to the translation model (direct translation probability) and by the RNN Encoder-Decoder. Source phrases were randomly selected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a Cyrillic letter ghe."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1746
                },
                {
                    "x": 1214,
                    "y": 1746
                },
                {
                    "x": 1214,
                    "y": 1969
                },
                {
                    "x": 293,
                    "y": 1969
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:18px'>The results are presented in Table 1. As ex-<br>pected, adding features computed by neural net-<br>works consistently improves the performance over<br>the baseline performance.</p>",
            "id": 105,
            "page": 7,
            "text": "The results are presented in Table 1. As expected, adding features computed by neural networks consistently improves the performance over the baseline performance."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1973
                },
                {
                    "x": 1215,
                    "y": 1973
                },
                {
                    "x": 1215,
                    "y": 2650
                },
                {
                    "x": 291,
                    "y": 2650
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='106' style='font-size:18px'>The best performance was achieved when we<br>used both CSLM and the phrase scores from the<br>RNN Encoder-Decoder. This suggests that the<br>contributions of the CSLM and the RNN Encoder-<br>Decoder are not too correlated and that one can<br>expect better results by improving each method in-<br>dependently. Furthermore, we tried penalizing the<br>number of words that are unknown to the neural<br>networks (i.e. words which are not in the short-<br>list). We do SO by simply adding the number of<br>unknown words as an additional feature the log-<br>linear model in Eq. (9).3 However, in this case we</p>",
            "id": 106,
            "page": 7,
            "text": "The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder-Decoder. This suggests that the contributions of the CSLM and the RNN EncoderDecoder are not too correlated and that one can expect better results by improving each method independently. Furthermore, we tried penalizing the number of words that are unknown to the neural networks (i.e. words which are not in the shortlist). We do SO by simply adding the number of unknown words as an additional feature the loglinear model in Eq. (9).3 However, in this case we"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2675
                },
                {
                    "x": 1214,
                    "y": 2675
                },
                {
                    "x": 1214,
                    "y": 2892
                },
                {
                    "x": 293,
                    "y": 2892
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>3 To understand the effect of the penalty, consider the set<br>of all words in the 15,000 large shortlist, SL. All words xi /<br>SL are replaced by a special token � before being scored<br>by the neural networks. Hence, the conditional probability of<br>any xit / SL is actually given by the model as</p>",
            "id": 107,
            "page": 7,
            "text": "3 To understand the effect of the penalty, consider the set of all words in the 15,000 large shortlist, SL. All words xi / SL are replaced by a special token � before being scored by the neural networks. Hence, the conditional probability of any xit / SL is actually given by the model as"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 3147
                },
                {
                    "x": 1080,
                    "y": 3147
                },
                {
                    "x": 1080,
                    "y": 3191
                },
                {
                    "x": 294,
                    "y": 3191
                }
            ],
            "category": "caption",
            "html": "<caption id='108' style='font-size:14px'>where X<t is a shorthand notation for xt-1,..., x1.</caption>",
            "id": 108,
            "page": 7,
            "text": "where X<t is a shorthand notation for xt-1,..., x1."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1749
                },
                {
                    "x": 2195,
                    "y": 1749
                },
                {
                    "x": 2195,
                    "y": 1858
                },
                {
                    "x": 1274,
                    "y": 1858
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:16px'>were not able to achieve better performance on the<br>test set, but only on the development set.</p>",
            "id": 109,
            "page": 7,
            "text": "were not able to achieve better performance on the test set, but only on the development set."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1911
                },
                {
                    "x": 1780,
                    "y": 1911
                },
                {
                    "x": 1780,
                    "y": 1962
                },
                {
                    "x": 1273,
                    "y": 1962
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:22px'>4.3 Qualitative Analysis</p>",
            "id": 110,
            "page": 7,
            "text": "4.3 Qualitative Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1992
                },
                {
                    "x": 2197,
                    "y": 1992
                },
                {
                    "x": 2197,
                    "y": 2776
                },
                {
                    "x": 1273,
                    "y": 2776
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>In order to understand where the performance im-<br>provement comes from, we analyze the phrase pair<br>scores computed by the RNN Encoder-Decoder<br>against the corresponding p(f I e) from the trans-<br>lation model. Since the existing translation model<br>relies solely on the statistics of the phrase pairs in<br>the corpus, we expect its scores to be better esti-<br>mated for the frequent phrases but badly estimated<br>for rare phrases. Also, as we mentioned earlier<br>in Sec. 3.1, we further expect the RNN Encoder-<br>Decoder which was trained without any frequency<br>information to score the phrase pairs based rather<br>on the linguistic regularities than on the statistics<br>of their occurrences in the corpus.</p>",
            "id": 111,
            "page": 7,
            "text": "In order to understand where the performance improvement comes from, we analyze the phrase pair scores computed by the RNN Encoder-Decoder against the corresponding p(f I e) from the translation model. Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus, we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases. Also, as we mentioned earlier in Sec. 3.1, we further expect the RNN EncoderDecoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2787
                },
                {
                    "x": 2196,
                    "y": 2787
                },
                {
                    "x": 2196,
                    "y": 2897
                },
                {
                    "x": 1274,
                    "y": 2897
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='112' style='font-size:20px'>We focus on those pairs whose source phrase is<br>long (more than 3 words per source phrase) and</p>",
            "id": 112,
            "page": 7,
            "text": "We focus on those pairs whose source phrase is long (more than 3 words per source phrase) and"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2938
                },
                {
                    "x": 2197,
                    "y": 2938
                },
                {
                    "x": 2197,
                    "y": 3191
                },
                {
                    "x": 1274,
                    "y": 3191
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:14px'>As a result, the probability of words not in the shortlist is<br>always overestimated. It is possible to address this issue by<br>backing off to an existing model that contain non-shortlisted<br>words (see (Schwenk, 2007)) In this paper, however, we opt<br>for introducing a word penalty instead, which counteracts the<br>word probability overestimation.</p>",
            "id": 113,
            "page": 7,
            "text": "As a result, the probability of words not in the shortlist is always overestimated. It is possible to address this issue by backing off to an existing model that contain non-shortlisted words (see (Schwenk, 2007)) In this paper, however, we opt for introducing a word penalty instead, which counteracts the word probability overestimation."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 254
                },
                {
                    "x": 2295,
                    "y": 254
                },
                {
                    "x": 2295,
                    "y": 570
                },
                {
                    "x": 291,
                    "y": 570
                }
            ],
            "category": "table",
            "html": "<table id='114' style='font-size:16px'><tr><td>Source</td><td>Samples from RNN Encoder-Decoder</td></tr><tr><td>at the end of the</td><td>[a la fin de la] (x11)</td></tr><tr><td>for the first time</td><td>[pour la premiere fois] (x24) [pour la premiere fois que] (x2)</td></tr><tr><td>in the United States and</td><td>[aux Etats-Unis et] (x6) [dans les Etats-Unis et] (x4)</td></tr><tr><td>, as well as</td><td>[, ainsi que] [,] [ainsi que] [, ainsi qu'] [et UNK]</td></tr><tr><td>one of the most</td><td>[1' un des plus] (x9) [1' un des] (x5) [1' une des plus] (x2)</td></tr><tr><td colspan=\"2\">(a) Long, frequent source phrases</td></tr></table>",
            "id": 114,
            "page": 8,
            "text": "Source Samples from RNN Encoder-Decoder  at the end of the [a la fin de la] (x11)  for the first time [pour la premiere fois] (x24) [pour la premiere fois que] (x2)  in the United States and [aux Etats-Unis et] (x6) [dans les Etats-Unis et] (x4)  , as well as [, ainsi que] [,] [ainsi que] [, ainsi qu'] [et UNK]  one of the most [1' un des plus] (x9) [1' un des] (x5) [1' une des plus] (x2)"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 614
                },
                {
                    "x": 2296,
                    "y": 614
                },
                {
                    "x": 2296,
                    "y": 947
                },
                {
                    "x": 295,
                    "y": 947
                }
            ],
            "category": "table",
            "html": "<table id='115' style='font-size:16px'><tr><td>Source</td><td>Samples from RNN Encoder-Decoder</td></tr><tr><td>, Minister of Communica- tions and Transport</td><td>[ , ministre des communications et le transport] (x13)</td></tr><tr><td>did not comply with the</td><td>[n tait pas conforme aux」 [n a pas respect 1'] (x2) [n' a pas respect la] (x3)</td></tr><tr><td>parts of the world ·</td><td>[arts du monde · I (x11) [des arts du monde .] (x7)</td></tr><tr><td>the past few days ·</td><td>[quelques jours I (x5) [les derniers jours .] (x5) [ces derniers jours · I (x2)</td></tr><tr><td>on Friday and Saturday</td><td>[vendredi et samedi] (x5) [le vendredi et samedi] (x7) [le vendredi et le samedi] (x4)</td></tr></table>",
            "id": 115,
            "page": 8,
            "text": "Source Samples from RNN Encoder-Decoder  , Minister of Communica- tions and Transport [ , ministre des communications et le transport] (x13)  did not comply with the [n tait pas conforme aux」 [n a pas respect 1'] (x2) [n' a pas respect la] (x3)  parts of the world · [arts du monde · I (x11) [des arts du monde .] (x7)  the past few days · [quelques jours I (x5) [les derniers jours .] (x5) [ces derniers jours · I (x2)  on Friday and Saturday"
        },
        {
            "bounding_box": [
                {
                    "x": 1073,
                    "y": 931
                },
                {
                    "x": 1518,
                    "y": 931
                },
                {
                    "x": 1518,
                    "y": 966
                },
                {
                    "x": 1073,
                    "y": 966
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:18px'>(b) Long, rare source phrases</p>",
            "id": 116,
            "page": 8,
            "text": "(b) Long, rare source phrases"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1009
                },
                {
                    "x": 2197,
                    "y": 1009
                },
                {
                    "x": 2197,
                    "y": 1122
                },
                {
                    "x": 290,
                    "y": 1122
                }
            ],
            "category": "caption",
            "html": "<caption id='117' style='font-size:20px'>Table 3: Samples generated from the RNN Encoder-Decoder for each source phrase used in Table 2. We<br>show the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder-Decoder scores.</caption>",
            "id": 117,
            "page": 8,
            "text": "Table 3: Samples generated from the RNN Encoder-Decoder for each source phrase used in Table 2. We show the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder-Decoder scores."
        },
        {
            "bounding_box": [
                {
                    "x": 1339,
                    "y": 1134
                },
                {
                    "x": 1428,
                    "y": 1134
                },
                {
                    "x": 1428,
                    "y": 1157
                },
                {
                    "x": 1339,
                    "y": 1157
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:14px'>Federal</p>",
            "id": 118,
            "page": 8,
            "text": "Federal"
        },
        {
            "bounding_box": [
                {
                    "x": 389,
                    "y": 1149
                },
                {
                    "x": 2190,
                    "y": 1149
                },
                {
                    "x": 2190,
                    "y": 1781
                },
                {
                    "x": 389,
                    "y": 1781
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='119' style='font-size:14px' alt=\"Isr\nsiatance France\nbo'\n40 risk\nocus 7 Chinaussian\n30\n8 Ekensh\ndop\nEte\n20\nleft\n9\neeffeeectist\nsuphpeckpt\n10 langu ownimited\nnsi violencoNtewscooctmed pmotproposed\npositi -10 King\ners table crisis\n</saverage et\nlocation want Germany Iraq\n0 minutes discrimination gooo Ontario\nhours arliament\nfamily\nlight -11\necretariatNK agreed Aster Japan\nrelations\n10 Code\n\nArgetitute ective\n-12\nOrgangaindialogue wate Tooto\n-20 tranciss\nhttpsmarsstater\nBritish\nHuman North\nH -13 Canadida\n-30\nCE####Sapital nternet\nBang South\nthecononly\nYork 14\n-40 -30 -20 -10 fficia 120 30 40 50 35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25\nEast\" data-coord=\"top-left:(389,1149); bottom-right:(2190,1781)\" /></figure>",
            "id": 119,
            "page": 8,
            "text": "Isr siatance France bo' 40 risk ocus 7 Chinaussian 30 8 Ekensh dop Ete 20 left 9 eeffeeectist suphpeckpt 10 langu ownimited nsi violencoNtewscooctmed pmotproposed positi -10 King ers table crisis </saverage et location want Germany Iraq 0 minutes discrimination gooo Ontario hours arliament family light -11 ecretariatNK agreed Aster Japan relations 10 Code  Argetitute ective -12 Organgaindialogue wate Tooto -20 tranciss httpsmarsstater British Human North H -13 Canadida -30 CE####Sapital nternet Bang South thecononly York 14 -40 -30 -20 -10 fficia 120 30 40 50 35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25 East"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1853
                },
                {
                    "x": 2197,
                    "y": 1853
                },
                {
                    "x": 2197,
                    "y": 2023
                },
                {
                    "x": 290,
                    "y": 2023
                }
            ],
            "category": "caption",
            "html": "<caption id='120' style='font-size:20px'>Figure 4: 2-D embedding of the learned word representation. The left one shows the full embedding<br>space, while the right one shows a zoomed-in view of one region (color-coded). For more plots, see the<br>supplementary material.</caption>",
            "id": 120,
            "page": 8,
            "text": "Figure 4: 2-D embedding of the learned word representation. The left one shows the full embedding space, while the right one shows a zoomed-in view of one region (color-coded). For more plots, see the supplementary material."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2077
                },
                {
                    "x": 1216,
                    "y": 2077
                },
                {
                    "x": 1216,
                    "y": 2414
                },
                {
                    "x": 288,
                    "y": 2414
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:22px'>frequent. For each such source phrase, we look<br>at the target phrases that have been scored high<br>either by the translation probability p(f T e) or<br>by the RNN Encoder-Decoder. Similarly, we per-<br>form the same procedure with those pairs whose<br>source phrase is long but rare in the corpus.</p>",
            "id": 121,
            "page": 8,
            "text": "frequent. For each such source phrase, we look at the target phrases that have been scored high either by the translation probability p(f T e) or by the RNN Encoder-Decoder. Similarly, we perform the same procedure with those pairs whose source phrase is long but rare in the corpus."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2077
                },
                {
                    "x": 2197,
                    "y": 2077
                },
                {
                    "x": 2197,
                    "y": 2469
                },
                {
                    "x": 1271,
                    "y": 2469
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:22px'>other phrase pairs that were scored radically dif-<br>ferent (see Fig. 3). This could arise from the<br>proposed approach of training the RNN Encoder-<br>Decoder on a set of unique phrase pairs, discour-<br>aging the RNN Encoder-Decoder from learning<br>simply the frequencies of the phrase pairs from the<br>corpus, as explained earlier.</p>",
            "id": 122,
            "page": 8,
            "text": "other phrase pairs that were scored radically different (see Fig. 3). This could arise from the proposed approach of training the RNN EncoderDecoder on a set of unique phrase pairs, discouraging the RNN Encoder-Decoder from learning simply the frequencies of the phrase pairs from the corpus, as explained earlier."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2430
                },
                {
                    "x": 1217,
                    "y": 2430
                },
                {
                    "x": 1217,
                    "y": 2710
                },
                {
                    "x": 290,
                    "y": 2710
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:20px'>Table 2 lists the top-3 target phrases per source<br>phrase favored either by the translation model<br>or by the RNN Encoder-Decoder. The source<br>phrases were randomly chosen among long ones<br>having more than 4 or 5 words.</p>",
            "id": 123,
            "page": 8,
            "text": "Table 2 lists the top-3 target phrases per source phrase favored either by the translation model or by the RNN Encoder-Decoder. The source phrases were randomly chosen among long ones having more than 4 or 5 words."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2729
                },
                {
                    "x": 1215,
                    "y": 2729
                },
                {
                    "x": 1215,
                    "y": 3008
                },
                {
                    "x": 291,
                    "y": 3008
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='124' style='font-size:20px'>In most cases, the choices of the target phrases<br>by the RNN Encoder-Decoder are closer to ac-<br>tual or literal translations. We can observe that the<br>RNN Encoder-Decoder prefers shorter phrases in<br>general.</p>",
            "id": 124,
            "page": 8,
            "text": "In most cases, the choices of the target phrases by the RNN Encoder-Decoder are closer to actual or literal translations. We can observe that the RNN Encoder-Decoder prefers shorter phrases in general."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 3028
                },
                {
                    "x": 1215,
                    "y": 3028
                },
                {
                    "x": 1215,
                    "y": 3195
                },
                {
                    "x": 291,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:20px'>Interestingly, many phrase pairs were scored<br>similarly by both the translation model and the<br>RNN Encoder-Decoder, but there were as many</p>",
            "id": 125,
            "page": 8,
            "text": "Interestingly, many phrase pairs were scored similarly by both the translation model and the RNN Encoder-Decoder, but there were as many"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2515
                },
                {
                    "x": 2197,
                    "y": 2515
                },
                {
                    "x": 2197,
                    "y": 3195
                },
                {
                    "x": 1271,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:20px'>Furthermore, in Table 3, we show for each of<br>the source phrases in Table 2, the generated sam-<br>ples from the RNN Encoder-Decoder. For each<br>source phrase, we generated 50 samples and show<br>the top-five phrases accordingly to their scores.<br>We can see that the RNN Encoder-Decoder is<br>able to propose well-formed target phrases with-<br>out looking at the actual phrase table. Importantly,<br>the generated phrases do not overlap completely<br>with the target phrases from the phrase table. This<br>encourages us to further investigate the possibility<br>of replacing the whole or a part of the phrase table</p>",
            "id": 126,
            "page": 8,
            "text": "Furthermore, in Table 3, we show for each of the source phrases in Table 2, the generated samples from the RNN Encoder-Decoder. For each source phrase, we generated 50 samples and show the top-five phrases accordingly to their scores. We can see that the RNN Encoder-Decoder is able to propose well-formed target phrases without looking at the actual phrase table. Importantly, the generated phrases do not overlap completely with the target phrases from the phrase table. This encourages us to further investigate the possibility of replacing the whole or a part of the phrase table"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 254
                },
                {
                    "x": 2199,
                    "y": 254
                },
                {
                    "x": 2199,
                    "y": 1614
                },
                {
                    "x": 285,
                    "y": 1614
                }
            ],
            "category": "figure",
            "html": "<figure><img id='127' style='font-size:14px' alt=\"M v\nbefore the fall of theo the capith&Pfesident of the\nfor the launch of the at the feet of the thepeonle 8\naccording to the records of the\n11 theeeppeoffthe\nshot from\n60\no the evolution off then the edgethepense.of the the scene of the\n50 theiser eespethed the tdiltberFarbafstbe the\nthe evolution of the ofference\n40 for the love of the the destruction\nthe core of the\n9.5 the love of the the exit the Yalley of the the Chairperson of the s\nincluding the portionofthe the\n30\nwith the approval of the the\ntheyers of the the ithehatper/scantipripreof the\nof\nwww.grevers of the thlessmaeasiographesingsi 8f the\nthe the sister of the\nofithe Ministry of the\n10\n8.5 the moneemberation of the\nof the\nthe work of managemangemanche thealthersfirthe of the\n3\n现 the\nabablishgnanoythewake the tilaw lines of\n0 the  fifethe of the\nthe\nof\nthe 이용하여야에서따라서 the\nthe \nthe\nthe\nthe mentallementative of the\n8 Birgstor of theurn of the of the thethe\nS the Chief of the the return of\n-30\nstudy the of the\nof the 媚 Headsohtbiltbeisk of the the Go\nACT\nthe 이제한국민의원본인이나무소하서도록하고도록하는알려져위해서버\nthe di\n-40 Hapbolebephotothst the\nthe\nthe specialtsediek.ne afrither of the including the\nthe dafabase Doesheng/cartels.tion\n-40 2 。 atotsteofight time 20 40 60 the proxinfly-attlespoccom/ofallehander maribadbaoropouth of.the the absence of4the the altiga Alcha\nthewinner of the Pressid\na few mds akiniderbaterinationiatie endof its kindhe therkdorld.\nhashordscherreart of the tile/cinguthstandseoff three the\nto the head of thead of the S\non the shores of 据 restructuring of the the httln ....therrestance othAhe as the\nthe\nんへ\n, artists actors\n-2 Radinglesinglantaniga\nBrazil\none to three months Another day , another\n17.8\n-2.2 C\n1g those\nof the two groups Malawi Mozambique\n17.7\n-2.4 &quot; the two groups\nthe two Russia IrldtaotObinalalawi\ntwo days before\n17.6\n-2.6\nfor nearly two months\nover the last two decades Georgia, Florida\n17.5\nehicles\n-2.8\n17.4\nRpartingations past\nthe last twewnmenths before\nming.past.two two just a few months before 17.3\nIn the past two within a few months Austria\nFrance,\n17.2 Russia, Fr\n/O\n-3.4\nin. France.Germany\na few days ago a few months - 17.1\n-3.6\n17 Framplay Russia , F\nin the next few months\nthe nextsixmonths\n-3.8\nthe next few months 16.9\nthat a few days\n16.8\n4 the past few ghonths -2 18 18.5 19 19.5 France, Germany20.5\n-5.5 -5 insthiestehetwindaydsays -3.5\nover the last few months in the six months\" data-coord=\"top-left:(285,254); bottom-right:(2199,1614)\" /></figure>",
            "id": 127,
            "page": 9,
            "text": "M v before the fall of theo the capith&Pfesident of the for the launch of the at the feet of the thepeonle 8 according to the records of the 11 theeeppeoffthe shot from 60 o the evolution off then the edgethepense.of the the scene of the 50 theiser eespethed the tdiltberFarbafstbe the the evolution of the ofference 40 for the love of the the destruction the core of the 9.5 the love of the the exit the Yalley of the the Chairperson of the s including the portionofthe the 30 with the approval of the the theyers of the the ithehatper/scantipripreof the of www.grevers of the thlessmaeasiographesingsi 8f the the the sister of the ofithe Ministry of the 10 8.5 the moneemberation of the of the the work of managemangemanche thealthersfirthe of the 3 现 the abablishgnanoythewake the tilaw lines of 0 the  fifethe of the the of the 이용하여야에서따라서 the the  the the the mentallementative of the 8 Birgstor of theurn of the of the thethe S the Chief of the the return of -30 study the of the of the 媚 Headsohtbiltbeisk of the the Go ACT the 이제한국민의원본인이나무소하서도록하고도록하는알려져위해서버 the di -40 Hapbolebephotothst the the the specialtsediek.ne afrither of the including the the dafabase Doesheng/cartels.tion -40 2 。 atotsteofight time 20 40 60 the proxinfly-attlespoccom/ofallehander maribadbaoropouth of.the the absence of4the the altiga Alcha thewinner of the Pressid a few mds akiniderbaterinationiatie endof its kindhe therkdorld. hashordscherreart of the tile/cinguthstandseoff three the to the head of thead of the S on the shores of 据 restructuring of the the httln ....therrestance othAhe as the the んへ , artists actors -2 Radinglesinglantaniga Brazil one to three months Another day , another 17.8 -2.2 C 1g those of the two groups Malawi Mozambique 17.7 -2.4 &quot; the two groups the two Russia IrldtaotObinalalawi two days before 17.6 -2.6 for nearly two months over the last two decades Georgia, Florida 17.5 ehicles -2.8 17.4 Rpartingations past the last twewnmenths before ming.past.two two just a few months before 17.3 In the past two within a few months Austria France, 17.2 Russia, Fr /O -3.4 in. France.Germany a few days ago a few months - 17.1 -3.6 17 Framplay Russia , F in the next few months the nextsixmonths -3.8 the next few months 16.9 that a few days 16.8 4 the past few ghonths -2 18 18.5 19 19.5 France, Germany20.5 -5.5 -5 insthiestehetwindaydsays -3.5 over the last few months in the six months"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1652
                },
                {
                    "x": 2197,
                    "y": 1652
                },
                {
                    "x": 2197,
                    "y": 1822
                },
                {
                    "x": 291,
                    "y": 1822
                }
            ],
            "category": "caption",
            "html": "<caption id='128' style='font-size:18px'>Figure 5: 2-D embedding of the learned phrase representation. The top left one shows the full represen-<br>tation space (5000 randomly selected points), while the other three figures show the zoomed-in view of<br>specific regions (color-coded).</caption>",
            "id": 128,
            "page": 9,
            "text": "Figure 5: 2-D embedding of the learned phrase representation. The top left one shows the full representation space (5000 randomly selected points), while the other three figures show the zoomed-in view of specific regions (color-coded)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1879
                },
                {
                    "x": 1217,
                    "y": 1879
                },
                {
                    "x": 1217,
                    "y": 1984
                },
                {
                    "x": 291,
                    "y": 1984
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>with the proposed RNN Encoder-Decoder in the<br>future.</p>",
            "id": 129,
            "page": 9,
            "text": "with the proposed RNN Encoder-Decoder in the future."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2038
                },
                {
                    "x": 1072,
                    "y": 2038
                },
                {
                    "x": 1072,
                    "y": 2092
                },
                {
                    "x": 291,
                    "y": 2092
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:18px'>4.4 Word and Phrase Representations</p>",
            "id": 130,
            "page": 9,
            "text": "4.4 Word and Phrase Representations"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2117
                },
                {
                    "x": 1217,
                    "y": 2117
                },
                {
                    "x": 1217,
                    "y": 2337
                },
                {
                    "x": 292,
                    "y": 2337
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:18px'>Since the proposed RNN Encoder-Decoder is not<br>specifically designed only for the task of machine<br>translation, here we briefly look at the properties<br>of the trained model.</p>",
            "id": 131,
            "page": 9,
            "text": "Since the proposed RNN Encoder-Decoder is not specifically designed only for the task of machine translation, here we briefly look at the properties of the trained model."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2347
                },
                {
                    "x": 1217,
                    "y": 2347
                },
                {
                    "x": 1217,
                    "y": 2852
                },
                {
                    "x": 292,
                    "y": 2852
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:16px'>It has been known for some time that<br>continuous space language models using<br>neural networks are able to learn seman-<br>tically meaningful embeddings (See, e.g.,<br>(Bengio et al., 2003; Mikolov et al., 2013)). Since<br>the proposed RNN Encoder-Decoder also projects<br>to and maps back from a sequence of words into<br>a continuous space vector, we expect to see a<br>similar property with the proposed model as well.</p>",
            "id": 132,
            "page": 9,
            "text": "It has been known for some time that continuous space language models using neural networks are able to learn semantically meaningful embeddings (See, e.g., (Bengio , 2003; Mikolov , 2013)). Since the proposed RNN Encoder-Decoder also projects to and maps back from a sequence of words into a continuous space vector, we expect to see a similar property with the proposed model as well."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2859
                },
                {
                    "x": 1215,
                    "y": 2859
                },
                {
                    "x": 1215,
                    "y": 3192
                },
                {
                    "x": 291,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:20px'>The left plot in Fig. 4 shows the 2-D embedding<br>of the words using the word embedding matrix<br>learned by the RNN Encoder-Decoder. The pro-<br>jection was done by the recently proposed Barnes-<br>Hut-SNE (van der Maaten, 2013). We can clearly<br>see that semantically similar words are clustered</p>",
            "id": 133,
            "page": 9,
            "text": "The left plot in Fig. 4 shows the 2-D embedding of the words using the word embedding matrix learned by the RNN Encoder-Decoder. The projection was done by the recently proposed BarnesHut-SNE (van der Maaten, 2013). We can clearly see that semantically similar words are clustered"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1878
                },
                {
                    "x": 2193,
                    "y": 1878
                },
                {
                    "x": 2193,
                    "y": 1930
                },
                {
                    "x": 1274,
                    "y": 1930
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:20px'>with each other (see the zoomed-in plots in Fig. 4).</p>",
            "id": 134,
            "page": 9,
            "text": "with each other (see the zoomed-in plots in Fig. 4)."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1938
                },
                {
                    "x": 2196,
                    "y": 1938
                },
                {
                    "x": 2196,
                    "y": 2324
                },
                {
                    "x": 1273,
                    "y": 2324
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='135' style='font-size:16px'>The proposed RNN Encoder-Decoder naturally<br>generates a continuous-space representation of a<br>phrase. The representation (c in Fig. 1) in this<br>case is a 1000-dimensional vector. Similarly to the<br>word representations, we visualize the representa-<br>tions of the phrases that consists of four or more<br>words using the Barnes-Hut-SNE in Fig. 5.</p>",
            "id": 135,
            "page": 9,
            "text": "The proposed RNN Encoder-Decoder naturally generates a continuous-space representation of a phrase. The representation (c in Fig. 1) in this case is a 1000-dimensional vector. Similarly to the word representations, we visualize the representations of the phrases that consists of four or more words using the Barnes-Hut-SNE in Fig. 5."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2331
                },
                {
                    "x": 2196,
                    "y": 2331
                },
                {
                    "x": 2196,
                    "y": 2895
                },
                {
                    "x": 1272,
                    "y": 2895
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:18px'>From the visualization, it is clear that the RNN<br>Encoder-Decoder captures both semantic and syn-<br>tactic structures of the phrases. For instance, in<br>the bottom-left plot, most of the phrases are about<br>the duration of time, while those phrases that are<br>syntactically similar are clustered together. The<br>bottom-right plot shows the cluster of phrases that<br>are semantically similar (countries or regions). On<br>the other hand, the top-right plot shows the phrases<br>that are syntactically similar.</p>",
            "id": 136,
            "page": 9,
            "text": "From the visualization, it is clear that the RNN Encoder-Decoder captures both semantic and syntactic structures of the phrases. For instance, in the bottom-left plot, most of the phrases are about the duration of time, while those phrases that are syntactically similar are clustered together. The bottom-right plot shows the cluster of phrases that are semantically similar (countries or regions). On the other hand, the top-right plot shows the phrases that are syntactically similar."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2936
                },
                {
                    "x": 1597,
                    "y": 2936
                },
                {
                    "x": 1597,
                    "y": 2989
                },
                {
                    "x": 1274,
                    "y": 2989
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:22px'>5 Conclusion</p>",
            "id": 137,
            "page": 9,
            "text": "5 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 3028
                },
                {
                    "x": 2198,
                    "y": 3028
                },
                {
                    "x": 2198,
                    "y": 3196
                },
                {
                    "x": 1272,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:14px'>In this paper, we proposed a new neural network<br>architecture, called an RNN Encoder-Decoder<br>that is able to learn the mapping from a sequence</p>",
            "id": 138,
            "page": 9,
            "text": "In this paper, we proposed a new neural network architecture, called an RNN Encoder-Decoder that is able to learn the mapping from a sequence"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 266
                },
                {
                    "x": 1215,
                    "y": 266
                },
                {
                    "x": 1215,
                    "y": 831
                },
                {
                    "x": 291,
                    "y": 831
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:16px'>of an arbitrary length to another sequence, possi-<br>bly from a different set, of an arbitrary length. The<br>proposed RNN Encoder-Decoder is able to either<br>score a pair of sequences (in terms of a conditional<br>probability) or generate a target sequence given a<br>source sequence. Along with the new architecture,<br>we proposed a novel hidden unit that includes a re-<br>set gate and an update gate that adaptively control<br>how much each hidden unit remembers or forgets<br>while reading/generating a sequence.</p>",
            "id": 139,
            "page": 10,
            "text": "of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length. The proposed RNN Encoder-Decoder is able to either score a pair of sequences (in terms of a conditional probability) or generate a target sequence given a source sequence. Along with the new architecture, we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 836
                },
                {
                    "x": 1216,
                    "y": 836
                },
                {
                    "x": 1216,
                    "y": 1284
                },
                {
                    "x": 291,
                    "y": 1284
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:18px'>We evaluated the proposed model with the task<br>of statistical machine translation, where we used<br>the RNN Encoder-Decoder to score each phrase<br>pair in the phrase table. Qualitatively, we were<br>able to show that the new model is able to cap-<br>ture linguistic regularities in the phrase pairs well<br>and also that the RNN Encoder-Decoder is able to<br>propose well-formed target phrases.</p>",
            "id": 140,
            "page": 10,
            "text": "We evaluated the proposed model with the task of statistical machine translation, where we used the RNN Encoder-Decoder to score each phrase pair in the phrase table. Qualitatively, we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder-Decoder is able to propose well-formed target phrases."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1291
                },
                {
                    "x": 1215,
                    "y": 1291
                },
                {
                    "x": 1215,
                    "y": 1851
                },
                {
                    "x": 291,
                    "y": 1851
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:18px'>The scores by the RNN Encoder-Decoder were<br>found to improve the overall translation perfor-<br>mance in terms of BLEU scores. Also, we<br>found that the contribution by the RNN Encoder-<br>Decoder is rather orthogonal to the existing ap-<br>proach of using neural networks in the SMT sys-<br>tem, SO that we can improve further the perfor-<br>mance by using, for instance, the RNN Encoder-<br>Decoder and the neural net language model to-<br>gether.</p>",
            "id": 141,
            "page": 10,
            "text": "The scores by the RNN Encoder-Decoder were found to improve the overall translation performance in terms of BLEU scores. Also, we found that the contribution by the RNN EncoderDecoder is rather orthogonal to the existing approach of using neural networks in the SMT system, SO that we can improve further the performance by using, for instance, the RNN EncoderDecoder and the neural net language model together."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1859
                },
                {
                    "x": 1216,
                    "y": 1859
                },
                {
                    "x": 1216,
                    "y": 2248
                },
                {
                    "x": 291,
                    "y": 2248
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:18px'>Our qualitative analysis of the trained model<br>shows that it indeed captures the linguistic regu-<br>larities in multiple levels i.e. at the word level as<br>well as phrase level. This suggests that there may<br>be more natural language related applications that<br>may benefit from the proposed RNN Encoder-<br>Decoder.</p>",
            "id": 142,
            "page": 10,
            "text": "Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i.e. at the word level as well as phrase level. This suggests that there may be more natural language related applications that may benefit from the proposed RNN EncoderDecoder."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2255
                },
                {
                    "x": 1214,
                    "y": 2255
                },
                {
                    "x": 1214,
                    "y": 2821
                },
                {
                    "x": 291,
                    "y": 2821
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:18px'>The proposed architecture has large potential<br>for further improvement and analysis. One ap-<br>proach that was not investigated here is to re-<br>place the whole, or a part of the phrase table by<br>letting the RNN Encoder-Decoder propose target<br>phrases. Also, noting that the proposed model is<br>not limited to being used with written language,<br>it will be an important future research to apply the<br>proposed architecture to other applications such as<br>speech transcription.</p>",
            "id": 143,
            "page": 10,
            "text": "The proposed architecture has large potential for further improvement and analysis. One approach that was not investigated here is to replace the whole, or a part of the phrase table by letting the RNN Encoder-Decoder propose target phrases. Also, noting that the proposed model is not limited to being used with written language, it will be an important future research to apply the proposed architecture to other applications such as speech transcription."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2873
                },
                {
                    "x": 694,
                    "y": 2873
                },
                {
                    "x": 694,
                    "y": 2930
                },
                {
                    "x": 293,
                    "y": 2930
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:22px'>Acknowledgments</p>",
            "id": 144,
            "page": 10,
            "text": "Acknowledgments"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2969
                },
                {
                    "x": 1216,
                    "y": 2969
                },
                {
                    "x": 1216,
                    "y": 3194
                },
                {
                    "x": 292,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>KC, BM, CG, DB and YB would like to thank<br>NSERC, Calcul Quebec, Compute Canada, the<br>Canada Research Chairs and CIFAR. FB and HS<br>were partially funded by the European Commis-</p>",
            "id": 145,
            "page": 10,
            "text": "KC, BM, CG, DB and YB would like to thank NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR. FB and HS were partially funded by the European Commis-"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 270
                },
                {
                    "x": 2193,
                    "y": 270
                },
                {
                    "x": 2193,
                    "y": 376
                },
                {
                    "x": 1271,
                    "y": 376
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='146' style='font-size:18px'>sion under the project MateCat, and by DARPA<br>under the BOLT project.</p>",
            "id": 146,
            "page": 10,
            "text": "sion under the project MateCat, and by DARPA under the BOLT project."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 473
                },
                {
                    "x": 1517,
                    "y": 473
                },
                {
                    "x": 1517,
                    "y": 526
                },
                {
                    "x": 1272,
                    "y": 526
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:20px'>References</p>",
            "id": 147,
            "page": 10,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 555
                },
                {
                    "x": 2197,
                    "y": 555
                },
                {
                    "x": 2197,
                    "y": 831
                },
                {
                    "x": 1256,
                    "y": 831
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:16px'>[Auli et al.2013] Michael Auli, Michel Galley, Chris<br>Quirk, and Geoffrey Zweig. 2013. Joint language<br>and translation modeling with recurrent neural net-<br>works. In Proceedings of the ACL Conference on<br>Empirical Methods in Natural Language Processing<br>(EMNLP), pages 1044-1054.</p>",
            "id": 148,
            "page": 10,
            "text": "[Auli 2013] Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1044-1054."
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 866
                },
                {
                    "x": 2197,
                    "y": 866
                },
                {
                    "x": 2197,
                    "y": 1098
                },
                {
                    "x": 1256,
                    "y": 1098
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:16px'>[Axelrod et al.2011] Amittai Axelrod, Xiaodong He,<br>and Jianfeng Gao. 2011. Domain adaptation via<br>pseudo in-domain data selection. In Proceedings of<br>the ACL Conference on Empirical Methods in Natu-<br>ral Language Processing (EMNLP), pages 355-362.</p>",
            "id": 149,
            "page": 10,
            "text": "[Axelrod 2011] Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 355-362."
        },
        {
            "bounding_box": [
                {
                    "x": 1257,
                    "y": 1132
                },
                {
                    "x": 2197,
                    "y": 1132
                },
                {
                    "x": 2197,
                    "y": 1410
                },
                {
                    "x": 1257,
                    "y": 1410
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:16px'>[Bastien et al.2012] Frederic Bastien, Pascal Lamblin,<br>Razvan Pascanu, James Bergstra, Ian J. Goodfellow,<br>Arnaud Bergeron, Nicolas Bouchard, and Yoshua<br>Bengio. 2012. Theano: new features and speed im-<br>provements. Deep Learning and Unsupervised Fea-<br>ture Learning NIPS 2012 Workshop.</p>",
            "id": 150,
            "page": 10,
            "text": "[Bastien 2012] Frederic Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. 2012. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop."
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 1445
                },
                {
                    "x": 2195,
                    "y": 1445
                },
                {
                    "x": 2195,
                    "y": 1630
                },
                {
                    "x": 1254,
                    "y": 1630
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>[Bengio et al.2003] Yoshua Bengio, Rejean Ducharme,<br>Pascal Vincent, and Christian Janvin. 2003. A neu-<br>ral probabilistic language model. J. Mach. Learn.<br>Res., 3:1137-1155, March.</p>",
            "id": 151,
            "page": 10,
            "text": "[Bengio 2003] Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137-1155, March."
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 1666
                },
                {
                    "x": 2196,
                    "y": 1666
                },
                {
                    "x": 2196,
                    "y": 1941
                },
                {
                    "x": 1256,
                    "y": 1941
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>[Bengio et al.2013] Y. Bengio, N. Boulanger-<br>Lewandowski, and R. Pascanu. 2013. Advances<br>in optimizing recurrent networks. In Proceedings<br>of the 38th International Conference on Acoustics,<br>Speech, and Signal Processing (ICASSP 2013),<br>May.</p>",
            "id": 152,
            "page": 10,
            "text": "[Bengio 2013] Y. Bengio, N. BoulangerLewandowski, and R. Pascanu. 2013. Advances in optimizing recurrent networks. In Proceedings of the 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2013), May."
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 1977
                },
                {
                    "x": 2196,
                    "y": 1977
                },
                {
                    "x": 2196,
                    "y": 2300
                },
                {
                    "x": 1256,
                    "y": 2300
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>[Bergstra et al.2010] James Bergstra, Olivier Breuleux,<br>Frederic Bastien, Pascal Lamblin, Razvan Pascanu,<br>Guillaume Desjardins, Joseph Turian, David Warde-<br>Farley, and Yoshua Bengio. 2010. Theano: a CPU<br>and GPU math expression compiler. In Proceedings<br>of the Python for Scientific Computing Conference<br>(SciPy), June. Oral Presentation.</p>",
            "id": 153,
            "page": 10,
            "text": "[Bergstra 2010] James Bergstra, Olivier Breuleux, Frederic Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David WardeFarley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June. Oral Presentation."
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 2336
                },
                {
                    "x": 2196,
                    "y": 2336
                },
                {
                    "x": 2196,
                    "y": 2612
                },
                {
                    "x": 1254,
                    "y": 2612
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:14px'>[Chandar et al.2014] Sarath Chandar, Stanislas Lauly,<br>Hugo Larochelle, Mitesh Khapra, Balaraman Ravin-<br>dran, Vikas Raykar, and Amrita Saha. 2014. An au-<br>toencoder approach to learning bilingual word repre-<br>sentations. arXiv:1 402 · 1454 [cs · CL], Febru-<br>ary.</p>",
            "id": 154,
            "page": 10,
            "text": "[Chandar 2014] Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. arXiv:1 402 · 1454 [cs · CL], February."
        },
        {
            "bounding_box": [
                {
                    "x": 1255,
                    "y": 2647
                },
                {
                    "x": 2195,
                    "y": 2647
                },
                {
                    "x": 2195,
                    "y": 2880
                },
                {
                    "x": 1255,
                    "y": 2880
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:16px'>[Dahl et al.2012] George E. Dahl, Dong Yu, Li Deng,<br>and Alex Acero. 2012. Context-dependent pre-<br>trained deep neural networks for large vocabulary<br>speech recognition. IEEE Transactions on Audio,<br>Speech, and Language Processing, 20(1):33-42.</p>",
            "id": 155,
            "page": 10,
            "text": "[Dahl 2012] George E. Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pretrained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):33-42."
        },
        {
            "bounding_box": [
                {
                    "x": 1255,
                    "y": 2914
                },
                {
                    "x": 2196,
                    "y": 2914
                },
                {
                    "x": 2196,
                    "y": 3195
                },
                {
                    "x": 1255,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:14px'>[Devlin et al.2014] Jacob Devlin, Rabih Zbib,<br>Zhongqiang Huang, Thomas Lamar, Richard<br>Schwartz, , and John Makhoul. 2014. Fast and<br>robust neural network joint models for statistical<br>machine translation. In Proceedings of the ACL<br>2014 Conference, ACL '14, pages 1370-1380.</p>",
            "id": 156,
            "page": 10,
            "text": "[Devlin 2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, , and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the ACL 2014 Conference, ACL '14, pages 1370-1380."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 270
                },
                {
                    "x": 1215,
                    "y": 270
                },
                {
                    "x": 1215,
                    "y": 455
                },
                {
                    "x": 273,
                    "y": 455
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:16px'>[Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau<br>Yih, and Li Deng. 2013. Learning semantic repre-<br>sentations for the phrase translation model. Techni-<br>cal report, Microsoft Research.</p>",
            "id": 157,
            "page": 11,
            "text": "[Gao 2013] Jianfeng Gao, Xiaodong He, Wen tau Yih, and Li Deng. 2013. Learning semantic representations for the phrase translation model. Technical report, Microsoft Research."
        },
        {
            "bounding_box": [
                {
                    "x": 272,
                    "y": 498
                },
                {
                    "x": 1215,
                    "y": 498
                },
                {
                    "x": 1215,
                    "y": 639
                },
                {
                    "x": 272,
                    "y": 639
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:14px'>[Glorot et al.2011] X. Glorot, A. Bordes, and Y. Ben-<br>gio. 2011. Deep sparse rectifier neural networks. In<br>AISTATS'2011.</p>",
            "id": 158,
            "page": 11,
            "text": "[Glorot 2011] X. Glorot, A. Bordes, and Y. Bengio. 2011. Deep sparse rectifier neural networks. In AISTATS'2011."
        },
        {
            "bounding_box": [
                {
                    "x": 272,
                    "y": 679
                },
                {
                    "x": 1215,
                    "y": 679
                },
                {
                    "x": 1215,
                    "y": 863
                },
                {
                    "x": 272,
                    "y": 863
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:14px'>[Goodfellow et al.2013] Ian J. Goodfellow, David<br>Warde-Farley, Mehdi Mirza, Aaron Courville, and<br>Yoshua Bengio. 2013. Maxout networks. In<br>ICML'2013.</p>",
            "id": 159,
            "page": 11,
            "text": "[Goodfellow 2013] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. 2013. Maxout networks. In ICML'2013."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 907
                },
                {
                    "x": 1214,
                    "y": 907
                },
                {
                    "x": 1214,
                    "y": 1049
                },
                {
                    "x": 273,
                    "y": 1049
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>[Graves2012] Alex Graves. 2012. Supervised Se-<br>quence Labelling with Recurrent Neural Networks.<br>Studies in Computational Intelligence. Springer.</p>",
            "id": 160,
            "page": 11,
            "text": "[Graves2012] Alex Graves. 2012. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in Computational Intelligence. Springer."
        },
        {
            "bounding_box": [
                {
                    "x": 272,
                    "y": 1090
                },
                {
                    "x": 1215,
                    "y": 1090
                },
                {
                    "x": 1215,
                    "y": 1230
                },
                {
                    "x": 272,
                    "y": 1230
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:16px'>[Hochreiter and Schmidhuber1997] S. Hochreiter and<br>J. Schmidhuber. 1997. Long short-term memory.<br>Neural Computation, 9(8):1735-1780.</p>",
            "id": 161,
            "page": 11,
            "text": "[Hochreiter and Schmidhuber1997] S. Hochreiter and J. Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 1274
                },
                {
                    "x": 1214,
                    "y": 1274
                },
                {
                    "x": 1214,
                    "y": 1504
                },
                {
                    "x": 273,
                    "y": 1504
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:16px'>[Kalchbrenner and Blunsom2013] Nal Kalchbrenner<br>and Phil Blunsom. 2013. Two recurrent continuous<br>translation models. In Proceedings of the ACL Con-<br>ference on Empirical Methods in Natural Language<br>Processing (EMNLP), pages 1700-1709.</p>",
            "id": 162,
            "page": 11,
            "text": "[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and Phil Blunsom. 2013. Two recurrent continuous translation models. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700-1709."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 1546
                },
                {
                    "x": 1214,
                    "y": 1546
                },
                {
                    "x": 1214,
                    "y": 1825
                },
                {
                    "x": 273,
                    "y": 1825
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:16px'>[Koehn et al.2003] Philipp Koehn, Franz Josef Och,<br>and Daniel Marcu. 2003. Statistical phrase-based<br>translation. In Proceedings of the 2003 Conference<br>of the North American Chapter of the Association<br>for Computational Linguistics on Human Language<br>Technology - Volume 1, NAACL '03, pages 48-54.</p>",
            "id": 163,
            "page": 11,
            "text": "[Koehn 2003] Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL '03, pages 48-54."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 1864
                },
                {
                    "x": 1216,
                    "y": 1864
                },
                {
                    "x": 1216,
                    "y": 2048
                },
                {
                    "x": 273,
                    "y": 2048
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:14px'>[Koehn2005] P. Koehn. 2005. Europarl: A parallel cor-<br>pus for statistical machine translation. In Machine<br>Translation Summit X, pages 79-86, Phuket, Thai-<br>land.</p>",
            "id": 164,
            "page": 11,
            "text": "[Koehn2005] P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine Translation Summit X, pages 79-86, Phuket, Thailand."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 2093
                },
                {
                    "x": 1214,
                    "y": 2093
                },
                {
                    "x": 1214,
                    "y": 2324
                },
                {
                    "x": 273,
                    "y": 2324
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:18px'>[Krizhevsky et al.2012] Alex Krizhevsky, Ilya<br>Sutskever, and Geoffrey Hinton. 2012. Ima-<br>geNet classification with deep convolutional neural<br>networks. In Advances in Neural Information<br>Processing Systems 25 (NIPS'2012).</p>",
            "id": 165,
            "page": 11,
            "text": "[Krizhevsky 2012] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25 (NIPS'2012)."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 2368
                },
                {
                    "x": 1215,
                    "y": 2368
                },
                {
                    "x": 1215,
                    "y": 2644
                },
                {
                    "x": 273,
                    "y": 2644
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:16px'>[Marcu and Wong2002] Daniel Marcu and William<br>Wong. 2002. A phrase-based, joint probability<br>model for statistical machine translation. In Pro-<br>ceedings of the ACL-02 Conference on Empirical<br>Methods in Natural Language Processing - Volume<br>10, EMNLP '02, pages 133-139.</p>",
            "id": 166,
            "page": 11,
            "text": "[Marcu and Wong2002] Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10, EMNLP '02, pages 133-139."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 2686
                },
                {
                    "x": 1215,
                    "y": 2686
                },
                {
                    "x": 1215,
                    "y": 2918
                },
                {
                    "x": 273,
                    "y": 2918
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:16px'>[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,<br>Kai Chen, Greg Corrado, and Jeff Dean. 2013. Dis-<br>tributed representations of words and phrases and<br>their compositionality. In Advances in Neural Infor-<br>mation Processing Systems 26, pages 3111-3119.</p>",
            "id": 167,
            "page": 11,
            "text": "[Mikolov 2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111-3119."
        },
        {
            "bounding_box": [
                {
                    "x": 275,
                    "y": 2959
                },
                {
                    "x": 1215,
                    "y": 2959
                },
                {
                    "x": 1215,
                    "y": 3192
                },
                {
                    "x": 275,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:18px'>[Moore and Lewis2010] Robert C. Moore and William<br>Lewis. 2010. Intelligent selection of language<br>model training data. In Proceedings of the ACL<br>2010 Conference Short Papers, ACLShort '10,<br>pages 220-224, Stroudsburg, PA, USA.</p>",
            "id": 168,
            "page": 11,
            "text": "[Moore and Lewis2010] Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort '10, pages 220-224, Stroudsburg, PA, USA."
        },
        {
            "bounding_box": [
                {
                    "x": 1255,
                    "y": 270
                },
                {
                    "x": 2197,
                    "y": 270
                },
                {
                    "x": 2197,
                    "y": 501
                },
                {
                    "x": 1255,
                    "y": 501
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='169' style='font-size:16px'>[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho,<br>and Y. Bengio. 2014. How to construct deep recur-<br>rent neural networks. In Proceedings of the Second<br>International Conference on Learning Representa-<br>tions (ICLR 2014), April.</p>",
            "id": 169,
            "page": 11,
            "text": "[Pascanu 2014] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. 2014. How to construct deep recurrent neural networks. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014), April."
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 531
                },
                {
                    "x": 2197,
                    "y": 531
                },
                {
                    "x": 2197,
                    "y": 809
                },
                {
                    "x": 1256,
                    "y": 809
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:16px'>[Saxe et al.2014] Andrew M. Saxe, James L. McClel-<br>land, and Surya Ganguli. 2014. Exact solutions<br>to the nonlinear dynamics of learning in deep lin-<br>ear neural networks. In Proceedings of the Second<br>International Conference on Learning Representa-<br>tions (ICLR 2014), April.</p>",
            "id": 170,
            "page": 11,
            "text": "[Saxe 2014] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. 2014. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. In Proceedings of the Second International Conference on Learning Representations (ICLR 2014), April."
        },
        {
            "bounding_box": [
                {
                    "x": 1255,
                    "y": 838
                },
                {
                    "x": 2196,
                    "y": 838
                },
                {
                    "x": 2196,
                    "y": 1022
                },
                {
                    "x": 1255,
                    "y": 1022
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:14px'>[Schwenk et al.2006] Holger Schwenk, Marta R. Costa-<br>Jussa, and Jose A. R. Fonollosa. 2006. Continuous<br>space language models for the iwslt 2006 task. In<br>IWSLT, pages 166-173.</p>",
            "id": 171,
            "page": 11,
            "text": "[Schwenk 2006] Holger Schwenk, Marta R. CostaJussa, and Jose A. R. Fonollosa. 2006. Continuous space language models for the iwslt 2006 task. In IWSLT, pages 166-173."
        },
        {
            "bounding_box": [
                {
                    "x": 1255,
                    "y": 1053
                },
                {
                    "x": 2196,
                    "y": 1053
                },
                {
                    "x": 2196,
                    "y": 1191
                },
                {
                    "x": 1255,
                    "y": 1191
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:22px'>[Schwenk2007] Holger Schwenk. 2007. Continuous<br>space language models. Comput. Speech Lang.,<br>21(3):492-518, July.</p>",
            "id": 172,
            "page": 11,
            "text": "[Schwenk2007] Holger Schwenk. 2007. Continuous space language models. Comput. Speech Lang., 21(3):492-518, July."
        },
        {
            "bounding_box": [
                {
                    "x": 1255,
                    "y": 1222
                },
                {
                    "x": 2197,
                    "y": 1222
                },
                {
                    "x": 2197,
                    "y": 1498
                },
                {
                    "x": 1255,
                    "y": 1498
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:16px'>[Schwenk2012] Holger Schwenk. 2012. Continuous<br>space translation models for phrase-based statisti-<br>cal machine translation. In Martin Kay and Chris-<br>tian Boitet, editors, Proceedings of the 24th Inter-<br>national Conference on Computational Linguistics<br>(COLIN), pages 1071-1080.</p>",
            "id": 173,
            "page": 11,
            "text": "[Schwenk2012] Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In Martin Kay and Christian Boitet, editors, Proceedings of the 24th International Conference on Computational Linguistics (COLIN), pages 1071-1080."
        },
        {
            "bounding_box": [
                {
                    "x": 1257,
                    "y": 1529
                },
                {
                    "x": 2197,
                    "y": 1529
                },
                {
                    "x": 2197,
                    "y": 1801
                },
                {
                    "x": 1257,
                    "y": 1801
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:20px'>[Socher et al.2011] Richard Socher, Eric H. Huang, Jef-<br>frey Pennington, Andrew Y. Ng, and Christopher D.<br>Manning. 2011. Dynamic pooling and unfolding<br>recursive autoencoders for paraphrase detection. In<br>Advances in Neural Information Processing Systems<br>24.</p>",
            "id": 174,
            "page": 11,
            "text": "[Socher 2011] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems 24."
        },
        {
            "bounding_box": [
                {
                    "x": 1257,
                    "y": 1834
                },
                {
                    "x": 2197,
                    "y": 1834
                },
                {
                    "x": 2197,
                    "y": 2157
                },
                {
                    "x": 1257,
                    "y": 2157
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:16px'>[Son et al.2012] Le Hai Son, Alexandre Allauzen, and<br>Fran�ois Yvon. 2012. Continuous space transla-<br>tion models with neural networks. In Proceedings of<br>the 2012 Conference of the North American Chap-<br>ter ofthe Associationfor Computational Linguistics:<br>Human Language Technologies, NAACL HLT '12,<br>pages 39-48, Stroudsburg, PA, USA.</p>",
            "id": 175,
            "page": 11,
            "text": "[Son 2012] Le Hai Son, Alexandre Allauzen, and Fran�ois Yvon. 2012. Continuous space translation models with neural networks. In Proceedings of the 2012 Conference of the North American Chapter ofthe Associationfor Computational Linguistics: Human Language Technologies, NAACL HLT '12, pages 39-48, Stroudsburg, PA, USA."
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 2188
                },
                {
                    "x": 2195,
                    "y": 2188
                },
                {
                    "x": 2195,
                    "y": 2370
                },
                {
                    "x": 1256,
                    "y": 2370
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:16px'>[van der Maaten2013] Laurens van der Maaten. 2013.<br>Barnes-hut-sne. In Proceedings of the First Inter-<br>national Conference on Learning Representations<br>(ICLR 2013), May.</p>",
            "id": 176,
            "page": 11,
            "text": "[van der Maaten2013] Laurens van der Maaten. 2013. Barnes-hut-sne. In Proceedings of the First International Conference on Learning Representations (ICLR 2013), May."
        },
        {
            "bounding_box": [
                {
                    "x": 1257,
                    "y": 2403
                },
                {
                    "x": 2196,
                    "y": 2403
                },
                {
                    "x": 2196,
                    "y": 2676
                },
                {
                    "x": 1257,
                    "y": 2676
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:16px'>[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,<br>Victoria Fossum, and David Chiang. 2013. De-<br>coding with large-scale neural language models im-<br>proves translation. Proceedings of the Conference<br>on Empirical Methods in Natural Language Pro-<br>cessing, pages 1387-1392.</p>",
            "id": 177,
            "page": 11,
            "text": "[Vaswani 2013] Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1387-1392."
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 2708
                },
                {
                    "x": 2195,
                    "y": 2708
                },
                {
                    "x": 2195,
                    "y": 2844
                },
                {
                    "x": 1256,
                    "y": 2844
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:14px'>[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:<br>an adaptive learning rate method. Technical report,<br>arXiv 1212.5701.</p>",
            "id": 178,
            "page": 11,
            "text": "[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. Technical report, arXiv 1212.5701."
        },
        {
            "bounding_box": [
                {
                    "x": 1257,
                    "y": 2877
                },
                {
                    "x": 2197,
                    "y": 2877
                },
                {
                    "x": 2197,
                    "y": 3155
                },
                {
                    "x": 1257,
                    "y": 3155
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:18px'>[Zou et al.2013] Will Y. Zou, Richard Socher,<br>Daniel M. Cer, and Christopher D. Manning.<br>2013. Bilingual word embeddings for phrase-based<br>machine translation. In Proceedings of the ACL<br>Conference on Empirical Methods in Natural<br>Language Processing (EMNLP), pages 1393-1398.</p>",
            "id": 179,
            "page": 11,
            "text": "[Zou 2013] Will Y. Zou, Richard Socher, Daniel M. Cer, and Christopher D. Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1393-1398."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 264
                },
                {
                    "x": 893,
                    "y": 264
                },
                {
                    "x": 893,
                    "y": 319
                },
                {
                    "x": 292,
                    "y": 319
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:18px'>A RNN Encoder-Decoder</p>",
            "id": 180,
            "page": 12,
            "text": "A RNN Encoder-Decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 352
                },
                {
                    "x": 2195,
                    "y": 352
                },
                {
                    "x": 2195,
                    "y": 460
                },
                {
                    "x": 290,
                    "y": 460
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:14px'>In this document, we describe in detail the architecture of the RNN Encoder-Decoder used in the exper-<br>iments.</p>",
            "id": 181,
            "page": 12,
            "text": "In this document, we describe in detail the architecture of the RNN Encoder-Decoder used in the experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 465
                },
                {
                    "x": 2197,
                    "y": 465
                },
                {
                    "x": 2197,
                    "y": 690
                },
                {
                    "x": 289,
                    "y": 690
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:14px'>Let us denote an source phrase by X = (X1,X2,...,XN) and a target phrase by Y =<br>(y1,y2, · · · ,yM). Each phrase is a sequence of K-dimensional one-hot vectors, such that only one<br>element of the vector is 1 and all the others are 0. The index of the active (1) element indicates the word<br>represented by the vector.</p>",
            "id": 182,
            "page": 12,
            "text": "Let us denote an source phrase by X = (X1,X2,...,XN) and a target phrase by Y = (y1,y2, · · · ,yM). Each phrase is a sequence of K-dimensional one-hot vectors, such that only one element of the vector is 1 and all the others are 0. The index of the active (1) element indicates the word represented by the vector."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 727
                },
                {
                    "x": 581,
                    "y": 727
                },
                {
                    "x": 581,
                    "y": 776
                },
                {
                    "x": 293,
                    "y": 776
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:14px'>A.1 Encoder</p>",
            "id": 183,
            "page": 12,
            "text": "A.1 Encoder"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 801
                },
                {
                    "x": 2196,
                    "y": 801
                },
                {
                    "x": 2196,
                    "y": 909
                },
                {
                    "x": 292,
                    "y": 909
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:14px'>Each word of the source phrase is embedded in a 500-dimensional vector space: e(xi) E R500 · e(x) is<br>used in Sec. 4.4 to visualize the words.</p>",
            "id": 184,
            "page": 12,
            "text": "Each word of the source phrase is embedded in a 500-dimensional vector space: e(xi) E R500 · e(x) is used in Sec. 4.4 to visualize the words."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 914
                },
                {
                    "x": 2194,
                    "y": 914
                },
                {
                    "x": 2194,
                    "y": 1022
                },
                {
                    "x": 292,
                    "y": 1022
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='185' style='font-size:14px'>The hidden state of an encoder consists of 1000 hidden units, and each one of them at time t is<br>computed by</p>",
            "id": 185,
            "page": 12,
            "text": "The hidden state of an encoder consists of 1000 hidden units, and each one of them at time t is computed by"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1156
                },
                {
                    "x": 417,
                    "y": 1156
                },
                {
                    "x": 417,
                    "y": 1202
                },
                {
                    "x": 293,
                    "y": 1202
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:14px'>where</p>",
            "id": 186,
            "page": 12,
            "text": "where"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1536
                },
                {
                    "x": 2196,
                    "y": 1536
                },
                {
                    "x": 2196,
                    "y": 1651
                },
                {
                    "x": 290,
                    "y": 1651
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:14px'>0 and ⊙ are a logistic sigmoid function and an element-wise multiplication, respectively. To make the<br>equations uncluttered, we omit biases. The initial hidden state h,(0) is fixed to 0.</p>",
            "id": 187,
            "page": 12,
            "text": "0 and ⊙ are a logistic sigmoid function and an element-wise multiplication, respectively. To make the equations uncluttered, we omit biases. The initial hidden state h,(0) is fixed to 0."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1655
                },
                {
                    "x": 2196,
                    "y": 1655
                },
                {
                    "x": 2196,
                    "y": 1765
                },
                {
                    "x": 290,
                    "y": 1765
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='188' style='font-size:14px'>Once the hidden state at the N step (the end of the source phrase) is computed, the representation of<br>the source phrase c is</p>",
            "id": 188,
            "page": 12,
            "text": "Once the hidden state at the N step (the end of the source phrase) is computed, the representation of the source phrase c is"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1907
                },
                {
                    "x": 612,
                    "y": 1907
                },
                {
                    "x": 612,
                    "y": 1957
                },
                {
                    "x": 292,
                    "y": 1957
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:14px'>A.1.1 Decoder</p>",
            "id": 189,
            "page": 12,
            "text": "A.1.1 Decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1974
                },
                {
                    "x": 1295,
                    "y": 1974
                },
                {
                    "x": 1295,
                    "y": 2027
                },
                {
                    "x": 293,
                    "y": 2027
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='190' style='font-size:14px'>The decoder starts by initializing the hidden state with</p>",
            "id": 190,
            "page": 12,
            "text": "The decoder starts by initializing the hidden state with"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2159
                },
                {
                    "x": 1895,
                    "y": 2159
                },
                {
                    "x": 1895,
                    "y": 2211
                },
                {
                    "x": 294,
                    "y": 2211
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:14px'>where we will use .1 to distinguish parameters of the decoder from those of the encoder.</p>",
            "id": 191,
            "page": 12,
            "text": "where we will use .1 to distinguish parameters of the decoder from those of the encoder."
        },
        {
            "bounding_box": [
                {
                    "x": 341,
                    "y": 2216
                },
                {
                    "x": 1375,
                    "y": 2216
                },
                {
                    "x": 1375,
                    "y": 2266
                },
                {
                    "x": 341,
                    "y": 2266
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='192' style='font-size:14px'>The hidden state at time t of the decoder is computed by</p>",
            "id": 192,
            "page": 12,
            "text": "The hidden state at time t of the decoder is computed by"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2408
                },
                {
                    "x": 417,
                    "y": 2408
                },
                {
                    "x": 417,
                    "y": 2455
                },
                {
                    "x": 293,
                    "y": 2455
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:14px'>where</p>",
            "id": 193,
            "page": 12,
            "text": "where"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2792
                },
                {
                    "x": 2194,
                    "y": 2792
                },
                {
                    "x": 2194,
                    "y": 2898
                },
                {
                    "x": 290,
                    "y": 2898
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:14px'>and e(yo) is an all-zero vector. Similarly to the case of the encoder, e(y) is an embedding of a target<br>word.</p>",
            "id": 194,
            "page": 12,
            "text": "and e(yo) is an all-zero vector. Similarly to the case of the encoder, e(y) is an embedding of a target word."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2904
                },
                {
                    "x": 2196,
                    "y": 2904
                },
                {
                    "x": 2196,
                    "y": 3015
                },
                {
                    "x": 292,
                    "y": 3015
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='195' style='font-size:16px'>Unlike the encoder which simply encodes the source phrase, the decoder is learned to generate a target<br>phrase. At each time t, the decoder computes the probability of generating j-th word by</p>",
            "id": 195,
            "page": 12,
            "text": "Unlike the encoder which simply encodes the source phrase, the decoder is learned to generate a target phrase. At each time t, the decoder computes the probability of generating j-th word by"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 266
                },
                {
                    "x": 833,
                    "y": 266
                },
                {
                    "x": 833,
                    "y": 328
                },
                {
                    "x": 293,
                    "y": 328
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:20px'>where the i-element of S<t><br>is</p>",
            "id": 196,
            "page": 13,
            "text": "where the i-element of S<t> is"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 502
                },
                {
                    "x": 370,
                    "y": 502
                },
                {
                    "x": 370,
                    "y": 545
                },
                {
                    "x": 293,
                    "y": 545
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:14px'>and</p>",
            "id": 197,
            "page": 13,
            "text": "and"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 700
                },
                {
                    "x": 2197,
                    "y": 700
                },
                {
                    "x": 2197,
                    "y": 884
                },
                {
                    "x": 290,
                    "y": 884
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:20px'>In short, the �� is a so-called maxout unit.<br>For the computational efficiency, instead of a single-matrix output weight G, we use a product of two<br>matrices such that</p>",
            "id": 198,
            "page": 13,
            "text": "In short, the �� is a so-called maxout unit. For the computational efficiency, instead of a single-matrix output weight G, we use a product of two matrices such that"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1027
                },
                {
                    "x": 1083,
                    "y": 1027
                },
                {
                    "x": 1083,
                    "y": 1083
                },
                {
                    "x": 290,
                    "y": 1083
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:18px'>where Gl E RKx500 and Gr E R500x1000</p>",
            "id": 199,
            "page": 13,
            "text": "where Gl E RKx500 and Gr E R500x1000"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1126
                },
                {
                    "x": 1116,
                    "y": 1126
                },
                {
                    "x": 1116,
                    "y": 1186
                },
                {
                    "x": 292,
                    "y": 1186
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:22px'>B Word and Phrase Representations</p>",
            "id": 200,
            "page": 13,
            "text": "B Word and Phrase Representations"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1215
                },
                {
                    "x": 1795,
                    "y": 1215
                },
                {
                    "x": 1795,
                    "y": 1275
                },
                {
                    "x": 289,
                    "y": 1275
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:20px'>Here, we show enlarged plots of the word and phrase representations in Figs. 4-5.</p>",
            "id": 201,
            "page": 13,
            "text": "Here, we show enlarged plots of the word and phrase representations in Figs. 4-5."
        },
        {
            "bounding_box": [
                {
                    "x": 1845,
                    "y": 57
                },
                {
                    "x": 1984,
                    "y": 57
                },
                {
                    "x": 1984,
                    "y": 90
                },
                {
                    "x": 1845,
                    "y": 90
                }
            ],
            "category": "header",
            "html": "<header id='202' style='font-size:18px'>Federal</header>",
            "id": 202,
            "page": 14,
            "text": "Federal"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 96
                },
                {
                    "x": 3178,
                    "y": 96
                },
                {
                    "x": 3178,
                    "y": 1067
                },
                {
                    "x": 443,
                    "y": 1067
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='203' style='font-size:14px' alt=\"-6 Isri\nbothotwelf abassiatence\nFrance\n40 risk efforts\n-7\nHealdingsp okiepticlosfeen\nChina\nm or tationfocus 50\nSrouress ickevated towa Russian\nSteddassanalysis\nInfgut kchin\napooter stuents cincetnvol.com\nresource essalde\n30 Ecograget oca nco 5 -8\n98 exa fo\nInter로 0,000원 www.doptedogmar\nmy Enemsh\n18\n20 experts,vv\nlent left heart\npart fence\neefiffeeedblet 민원 -9\nclegiona seet mge mcalled\nlanguadjuranda Exec homensit listablelences Ntewscoapetmed obir King\nSIO oriGe prectindOWhimited locaterear rea ct suphpecrot\nStizeuses\nenver e versi organuct 이미지역 stay compercomotproposed\n10\noms\nues\nriny cultured\ncent Houstove -10\npositive\nDirector\ncrisis tell www.govir et\nnge\n⌀ pletsident den partners\nsei Germany Iraq\nCounteign locatiowledge good Ontario\n0 minutes polignubition agurviess </saverage\nnge want\nhours justice Parliament discrimination free\nor 00 일\nIIM&DERAL Israfamily light knot의 agreed Asservation Japan\n-11\ncoart EnerMan SecretariatNK\nrelations 310분\n-10 Sermy bayKingdom Code\nreauch p enrotection\n&amp, BISCULARAN Anjuitute ellooming pelo www.gooperactions\n세상병원 08 30H\nhyunication effective\nrbailion\n-12\nOrganalogue wa\n-20 sellonet httlshinasstatere trarlspoical.n 인의 sa\nansports.gourgenorgent esuneration' North\nBritish\n00 Human ------------------------ magget Canadida\n89\nam hand -13\nful\n1529\nman rotorts.moni.gement\n-30\njast\nCE.comeSapitaInternetqia.no newslendow\nBangepungion thecononly South\n&cod annexFub T 14\nYork S\n-40 -30 -20 - 10 SDAPHY PHO UfficialM20 30 40 50 -35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25\nEast\" data-coord=\"top-left:(443,96); bottom-right:(3178,1067)\" /></figure>",
            "id": 203,
            "page": 14,
            "text": "-6 Isri bothotwelf abassiatence France 40 risk efforts -7 Healdingsp okiepticlosfeen China m or tationfocus 50 Srouress ickevated towa Russian Steddassanalysis Infgut kchin apooter stuents cincetnvol.com resource essalde 30 Ecograget oca nco 5 -8 98 exa fo Inter로 0,000원 www.doptedogmar my Enemsh 18 20 experts,vv lent left heart part fence eefiffeeedblet 민원 -9 clegiona seet mge mcalled languadjuranda Exec homensit listablelences Ntewscoapetmed obir King SIO oriGe prectindOWhimited locaterear rea ct suphpecrot Stizeuses enver e versi organuct 이미지역 stay compercomotproposed 10 oms ues riny cultured cent Houstove -10 positive Director crisis tell www.govir et nge ⌀ pletsident den partners sei Germany Iraq Counteign locatiowledge good Ontario 0 minutes polignubition agurviess </saverage nge want hours justice Parliament discrimination free or 00 일 IIM&DERAL Israfamily light knot의 agreed Asservation Japan -11 coart EnerMan SecretariatNK relations 310분 -10 Sermy bayKingdom Code reauch p enrotection &amp, BISCULARAN Anjuitute ellooming pelo www.gooperactions 세상병원 08 30H hyunication effective rbailion -12 Organalogue wa -20 sellonet httlshinasstatere trarlspoical.n 인의 sa ansports.gourgenorgent esuneration\" North British 00 Human ------------------------ magget Canadida 89 am hand -13 ful 1529 man rotorts.moni.gement -30 jast CE.comeSapitaInternetqia.no newslendow Bangepungion thecononly South &cod annexFub T 14 York S -40 -30 -20 - 10 SDAPHY PHO UfficialM20 30 40 50 -35 -34 -33 -32 -31 -30 -29 -28 -27 -26 -25 East"
        },
        {
            "bounding_box": [
                {
                    "x": 403,
                    "y": 1110
                },
                {
                    "x": 477,
                    "y": 1110
                },
                {
                    "x": 477,
                    "y": 1157
                },
                {
                    "x": 403,
                    "y": 1157
                }
            ],
            "category": "caption",
            "html": "<caption id='204' style='font-size:20px'>tvgo</caption>",
            "id": 204,
            "page": 14,
            "text": "tvgo"
        },
        {
            "bounding_box": [
                {
                    "x": 383,
                    "y": 1185
                },
                {
                    "x": 3079,
                    "y": 1185
                },
                {
                    "x": 3079,
                    "y": 2105
                },
                {
                    "x": 383,
                    "y": 2105
                }
            ],
            "category": "figure",
            "html": "<figure><img id='205' style='font-size:16px' alt=\"-20 -31.99\nseconql Aughyst\nthird\n-21 -31.995\nth⌀ee\n2210\n-22 -32 October\n1 30\n25 June\n-23 31 -32.005 Septengber\n27\n100 MarchJuiyanuary\n10 そそ\n-24 -32.01 November\n50\n1293 February\n40\n-25 8 1135 26 -32.015\n9\n1187 60\n-26 112 14\n-32.02\nSix\n-27 folur 7 -32.025\n28\nfi5e\nDecember\n-28 -32.03\n-38 -37 -36 -35 -34 -33 -32 -31 -30 -37.5 -37.49 -37.48 -37.47 -37.46 -37.45 -37.44 -37.43 -37.42 -37.41 -37.4\" data-coord=\"top-left:(383,1185); bottom-right:(3079,2105)\" /></figure>",
            "id": 205,
            "page": 14,
            "text": "-20 -31.99 seconql Aughyst third -21 -31.995 th⌀ee 2210 -22 -32 October 1 30 25 June -23 31 -32.005 Septengber 27 100 MarchJuiyanuary 10 そそ -24 -32.01 November 50 1293 February 40 -25 8 1135 26 -32.015 9 1187 60 -26 112 14 -32.02 Six -27 folur 7 -32.025 28 fi5e December -28 -32.03 -38 -37 -36 -35 -34 -33 -32 -31 -30 -37.5 -37.49 -37.48 -37.47 -37.46 -37.45 -37.44 -37.43 -37.42 -37.41 -37.4"
        },
        {
            "bounding_box": [
                {
                    "x": 320,
                    "y": 2212
                },
                {
                    "x": 3253,
                    "y": 2212
                },
                {
                    "x": 3253,
                    "y": 2327
                },
                {
                    "x": 320,
                    "y": 2327
                }
            ],
            "category": "caption",
            "html": "<caption id='206' style='font-size:22px'>Figure 6: 2-D embedding of the learned word representation. The top left one shows the full embedding space, while the other three figures show the zoomed-in<br>view of specific regions (color-coded).</caption>",
            "id": 206,
            "page": 14,
            "text": "Figure 6: 2-D embedding of the learned word representation. The top left one shows the full embedding space, while the other three figures show the zoomed-in view of specific regions (color-coded)."
        },
        {
            "bounding_box": [
                {
                    "x": 323,
                    "y": 49
                },
                {
                    "x": 3188,
                    "y": 49
                },
                {
                    "x": 3188,
                    "y": 2152
                },
                {
                    "x": 323,
                    "y": 2152
                }
            ],
            "category": "figure",
            "html": "<figure><img id='207' style='font-size:16px' alt=\"before the fall of theo the capitalopresident of the\nfor the launch of the thepeople 8\nat the feet of the\n11 theeeplae ofthee according to the records of the\nshot from\n60\nwallogrooche\n0 the www.⌀ withen the edgethepense.of the the scene of the\n0 at\nthe toinbation the\n50 has, 9890 서명 2007 ) Intel⌀er\nby)the Chanday tor the evolution of the\nthe timenigasm such\natcolloweslinatureprellf 드립니다 United\nfor the love of the the:destruction offinersterschience\n40\nunry the core of the\nisthe  www.m/shot.krang.com to\nwastastruing.gr the Chairperson of the S\nIn a speech your radintheet,ovyarot to and here'Rownf therYalley of the\nham http://www.iscombl 9.5 the love of the the exit\n30 ancielidoes 날인) the portion of the\neputies including the President of the\n나무 natua50s,syu Chalve maing Year  of the the ithenatpevicantipreof the\nwith the approval of the\nof the\nyouhave to be\n사용할 tuapos:s canr ※ year\ns consition\nONS' case\n. hutplinzerland; wheretuch/as Turkey he\n날인 edan\nthere, enclabbeen\n※30 then enal of And ⌀blince Joneshing of the threestatglogrograpresogra 8f the\nof the the sister of the\n비인등\nrael\nof the Ministry of the\n&quo\n10\nuot; thet 8.5 the moneeprereroation of the\nco⌀rs of the\nthe elife the th⌀e the workboo hparting the the aythors the\n3\ne ensiv denbe\nof the authersofthe the\n아닌 Whene the lines of the stensoffithe\nOT.\nB 0 Statablishored the wake the law of the = = Comperticationaly of Centin the\nhamahine\nharted States affeeting at all tike tecaurpationpe of the ��������������������� firfethe\nthe\nthe aamudicolesticredarionet stations the\nof the\none HESO the\nethe the\nthe rutingstationstatef\nomurytle\n2000 68 Marand man in the world the\nShok 일 me the.log arage  gidhallAhabworld:\nwww.go.kr oletime\nthe\n-20 www.gov.comfalf818.17g.)\n날인) been 28 Director of the managementatingbentative of the\ntheepring (인동) e had been forced\nge, HUMSULAS shoshing in the south of the of the the\nnas been.discuse 서울증명 the Chief of the the return of\n-30 acased in.1% take depens\nmpeejearsiagyears the decline of the\nstudy carried.out.in in 입 MIC·OR Contion(ICA) 3 of the 媚 deersBhtblibeisk of the SHIPPY\nver th⌀icateo bydr\non\nthe Go\napperacted avahableit Corruption (\nthe\nof the effartier\nWespecritit underighaphotor the\n-40\n, the di\ncomkter af [x] geergest year\nthe specialtsedialcha\nof the theLincluding the\npart 이 a 7일 starpention\nPPY 200 49 01010101011102020abe Bresbeogkcarvellsution\na copy\n6\nAlcha\nrotototsteofitythe time 20 40 60 the sha.to........................................ ot. the the absence of4the\n-40 -20\nthe thewinner of the Pressio\na few mdstinatetoldie end of\nits kindha tharldorld . onthenshorlisogriart of the theicinguins tranudresse off three , the\nto the head of the the S\non shores of the restructuring of the the htble ....theresisiance othahe as the head of the ,\nん㉦\nartists , actors\n-2 Adessissidongotorahiga\nBrazil\none to three months Another day another\n17.8\n-2.2 C those\n1g\nof the two groups Malawi Mozambique\n17.7\n-2.4 &quot; the two groups\nthe two , Russia , IrldisotObinblalawi\n, two days before\n17.6\n-2.6\nfor nearly two months\nover the last two decades Georgia , Florida\n17.5\nehicles -2.8\n17.4\n= = Reard nations past\nthe last twormonths before\n서 /eent\nthe\n���������� two just a few months before 17.3\nIn the past.wo within a few months\n, France , Austria\n17.2 Russia Fr\n/0\n-3.4 in France , Germany\na few days ago a few months - 17.1\n-3.6\n17 Frailling Russia F\nin the next few months\nthe next SIX months\n-3.8\nthe next few months 16.9\nthat a few days\n4 the past few ghonths -2 18 18.5 19 19.5 France , Germany\n16.8\n-5.5 -5 instliestehetwhelaydsays -3.5\n20.5\nover the last few months in the six months\" data-coord=\"top-left:(323,49); bottom-right:(3188,2152)\" /></figure>",
            "id": 207,
            "page": 15,
            "text": "before the fall of theo the capitalopresident of the for the launch of the thepeople 8 at the feet of the 11 theeeplae ofthee according to the records of the shot from 60 wallogrooche 0 the www.⌀ withen the edgethepense.of the the scene of the 0 at the toinbation the 50 has, 9890 서명 2007 ) Intel⌀er by)the Chanday tor the evolution of the the timenigasm such atcolloweslinatureprellf 드립니다 United for the love of the the:destruction offinersterschience 40 unry the core of the isthe  www.m/shot.krang.com to wastastruing.gr the Chairperson of the S In a speech your radintheet,ovyarot to and here\"Rownf therYalley of the ham http://www.iscombl 9.5 the love of the the exit 30 ancielidoes 날인) the portion of the eputies including the President of the 나무 natua50s,syu Chalve maing Year  of the the ithenatpevicantipreof the with the approval of the of the youhave to be 사용할 tuapos:s canr ※ year s consition ONS' case . hutplinzerland; wheretuch/as Turkey he 날인 edan there, enclabbeen ※30 then enal of And ⌀blince Joneshing of the threestatglogrograpresogra 8f the of the the sister of the 비인등 rael of the Ministry of the &quo 10 uot; thet 8.5 the moneeprereroation of the co⌀rs of the the elife the th⌀e the workboo hparting the the aythors the 3 e ensiv denbe of the authersofthe the 아닌 Whene the lines of the stensoffithe OT. B 0 Statablishored the wake the law of the = = Comperticationaly of Centin the hamahine harted States affeeting at all tike tecaurpationpe of the ��������������������� firfethe the the aamudicolesticredarionet stations the of the one HESO the ethe the the rutingstationstatef omurytle 2000 68 Marand man in the world the Shok 일 me the.log arage  gidhallAhabworld: www.go.kr oletime the -20 www.gov.comfalf818.17g.) 날인) been 28 Director of the managementatingbentative of the theepring (인동) e had been forced ge, HUMSULAS shoshing in the south of the of the the nas been.discuse 서울증명 the Chief of the the return of -30 acased in.1% take depens mpeejearsiagyears the decline of the study carried.out.in in 입 MIC·OR Contion(ICA) 3 of the 媚 deersBhtblibeisk of the SHIPPY ver th⌀icateo bydr on the Go apperacted avahableit Corruption ( the of the effartier Wespecritit underighaphotor the -40 , the di comkter af [x] geergest year the specialtsedialcha of the theLincluding the part 이 a 7일 starpention PPY 200 49 01010101011102020abe Bresbeogkcarvellsution a copy 6 Alcha rotototsteofitythe time 20 40 60 the sha.to........................................ ot. the the absence of4the -40 -20 the thewinner of the Pressio a few mdstinatetoldie end of its kindha tharldorld . onthenshorlisogriart of the theicinguins tranudresse off three , the to the head of the the S on shores of the restructuring of the the htble ....theresisiance othahe as the head of the , ん㉦ artists , actors -2 Adessissidongotorahiga Brazil one to three months Another day another 17.8 -2.2 C those 1g of the two groups Malawi Mozambique 17.7 -2.4 &quot; the two groups the two , Russia , IrldisotObinblalawi , two days before 17.6 -2.6 for nearly two months over the last two decades Georgia , Florida 17.5 ehicles -2.8 17.4 = = Reard nations past the last twormonths before 서 /eent the ���������� two just a few months before 17.3 In the past.wo within a few months , France , Austria 17.2 Russia Fr /0 -3.4 in France , Germany a few days ago a few months - 17.1 -3.6 17 Frailling Russia F in the next few months the next SIX months -3.8 the next few months 16.9 that a few days 4 the past few ghonths -2 18 18.5 19 19.5 France , Germany 16.8 -5.5 -5 instliestehetwhelaydsays -3.5 20.5 over the last few months in the six months"
        },
        {
            "bounding_box": [
                {
                    "x": 1356,
                    "y": 2161
                },
                {
                    "x": 1394,
                    "y": 2161
                },
                {
                    "x": 1394,
                    "y": 2177
                },
                {
                    "x": 1356,
                    "y": 2177
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='208' style='font-size:14px'>150</p>",
            "id": 208,
            "page": 15,
            "text": "150"
        },
        {
            "bounding_box": [
                {
                    "x": 317,
                    "y": 2211
                },
                {
                    "x": 3252,
                    "y": 2211
                },
                {
                    "x": 3252,
                    "y": 2328
                },
                {
                    "x": 317,
                    "y": 2328
                }
            ],
            "category": "caption",
            "html": "<caption id='209' style='font-size:20px'>Figure 7: 2-D embedding of the learned phrase representation. The top left one shows the full representation space (1000 randomly selected points), while the<br>other three figures show the zoomed-in view of specific regions (color-coded).</caption>",
            "id": 209,
            "page": 15,
            "text": "Figure 7: 2-D embedding of the learned phrase representation. The top left one shows the full representation space (1000 randomly selected points), while the other three figures show the zoomed-in view of specific regions (color-coded)."
        }
    ]
}