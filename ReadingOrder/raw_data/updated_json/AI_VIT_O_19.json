{
    "id": "629a9b34-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2105.15203v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 407
                },
                {
                    "x": 2107,
                    "y": 407
                },
                {
                    "x": 2107,
                    "y": 574
                },
                {
                    "x": 445,
                    "y": 574
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>SegFormer: Simple and Efficient Design for Semantic<br>Segmentation with Transformers</p>",
            "id": 0,
            "page": 1,
            "text": "SegFormer: Simple and Efficient Design for Semantic\nSegmentation with Transformers"
        },
        {
            "bounding_box": [
                {
                    "x": 476,
                    "y": 744
                },
                {
                    "x": 2151,
                    "y": 744
                },
                {
                    "x": 2151,
                    "y": 804
                },
                {
                    "x": 476,
                    "y": 804
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Enze Xie1, Wenhai Wang2, Zhiding Yu3, Anima Anandkumar3,4, Jose M. Alvarez3 , Ping Luo1</p>",
            "id": 1,
            "page": 1,
            "text": "Enze Xie1, Wenhai Wang2, Zhiding Yu3, Anima Anandkumar3,4, Jose M. Alvarez3 , Ping Luo1"
        },
        {
            "bounding_box": [
                {
                    "x": 695,
                    "y": 822
                },
                {
                    "x": 1940,
                    "y": 822
                },
                {
                    "x": 1940,
                    "y": 876
                },
                {
                    "x": 695,
                    "y": 876
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:22px'>1The University of Hong Kong 2Nanjing University 3NVIDIA 4Caltech</p>",
            "id": 2,
            "page": 1,
            "text": "1The University of Hong Kong 2Nanjing University 3NVIDIA 4Caltech"
        },
        {
            "bounding_box": [
                {
                    "x": 1173,
                    "y": 990
                },
                {
                    "x": 1374,
                    "y": 990
                },
                {
                    "x": 1374,
                    "y": 1048
                },
                {
                    "x": 1173,
                    "y": 1048
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:22px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 590,
                    "y": 1089
                },
                {
                    "x": 1960,
                    "y": 1089
                },
                {
                    "x": 1960,
                    "y": 1874
                },
                {
                    "x": 590,
                    "y": 1874
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>We present SegFormer, a simple, efficient yet powerful semantic segmentation<br>framework which unifies Transformers with lightweight multilayer perceptron<br>(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises<br>a novel hierarchically structured Transformer encoder which outputs multiscale<br>features. It does not need positional encoding, thereby avoiding the interpolation of<br>positional codes which leads to decreased performance when the testing resolution<br>differs from training. 2) SegFormer avoids complex decoders. The proposed<br>MLP decoder aggregates information from different layers, and thus combining<br>both local attention and global attention to render powerful representations. We<br>show that this simple and lightweight design is the key to efficient segmentation<br>on Transformers. We scale our approach up to obtain a series of models from<br>SegFormer-B0 to SegFormer-B5, reaching significantly better performance and<br>efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3%<br>mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than<br>the previous best method. Our best model, SegFormer-B5, achieves 84.0% mloU on<br>Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.<br>Code will be released at: gi thub.com/NVlabs /SegF ormer.</p>",
            "id": 4,
            "page": 1,
            "text": "We present SegFormer, a simple, efficient yet powerful semantic segmentation\nframework which unifies Transformers with lightweight multilayer perceptron\n(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises\na novel hierarchically structured Transformer encoder which outputs multiscale\nfeatures. It does not need positional encoding, thereby avoiding the interpolation of\npositional codes which leads to decreased performance when the testing resolution\ndiffers from training. 2) SegFormer avoids complex decoders. The proposed\nMLP decoder aggregates information from different layers, and thus combining\nboth local attention and global attention to render powerful representations. We\nshow that this simple and lightweight design is the key to efficient segmentation\non Transformers. We scale our approach up to obtain a series of models from\nSegFormer-B0 to SegFormer-B5, reaching significantly better performance and\nefficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3%\nmIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than\nthe previous best method. Our best model, SegFormer-B5, achieves 84.0% mloU on\nCityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.\nCode will be released at: gi thub.com/NVlabs /SegF ormer."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1946
                },
                {
                    "x": 798,
                    "y": 1946
                },
                {
                    "x": 798,
                    "y": 2004
                },
                {
                    "x": 445,
                    "y": 2004
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1 Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2049
                },
                {
                    "x": 1258,
                    "y": 2049
                },
                {
                    "x": 1258,
                    "y": 2554
                },
                {
                    "x": 442,
                    "y": 2554
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Semantic segmentation is a fundamental task in<br>computer vision and enables many downstream<br>applications. It is related to image classification<br>since it produces per-pixel category prediction<br>instead of image-level prediction. This relation-<br>ship is pointed out and systematically studied in<br>a seminal work [1], where the authors used fully<br>convolutional networks (FCNs) for semantic seg-<br>mentation tasks. Since then, FCN has inspired<br>many follow-up works and has become a predom-<br>inant design choice for dense prediction.</p>",
            "id": 6,
            "page": 1,
            "text": "Semantic segmentation is a fundamental task in\ncomputer vision and enables many downstream\napplications. It is related to image classification\nsince it produces per-pixel category prediction\ninstead of image-level prediction. This relation-\nship is pointed out and systematically studied in\na seminal work [1], where the authors used fully\nconvolutional networks (FCNs) for semantic seg-\nmentation tasks. Since then, FCN has inspired\nmany follow-up works and has become a predom-\ninant design choice for dense prediction."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2572
                },
                {
                    "x": 1260,
                    "y": 2572
                },
                {
                    "x": 1260,
                    "y": 2987
                },
                {
                    "x": 444,
                    "y": 2987
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:16px'>Since there is a strong relation between classi-<br>fication and semantic segmentation, many state-<br>of-the-art semantic segmentation frameworks are<br>variants of popular architectures for image classi-<br>fication on ImageNet. Therefore, designing back-<br>bone architectures has remained an active area<br>in semantic segmentation. Indeed, starting from<br>early methods using VGGs [1, 2], to the latest<br>backbones [3], the evolution of backbones has</p>",
            "id": 7,
            "page": 1,
            "text": "Since there is a strong relation between classi-\nfication and semantic segmentation, many state-\nof-the-art semantic segmentation frameworks are\nvariants of popular architectures for image classi-\nfication on ImageNet. Therefore, designing back-\nbone architectures has remained an active area\nin semantic segmentation. Indeed, starting from\nearly methods using VGGs [1, 2], to the latest\nbackbones [3], the evolution of backbones has"
        },
        {
            "bounding_box": [
                {
                    "x": 1285,
                    "y": 2044
                },
                {
                    "x": 2088,
                    "y": 2044
                },
                {
                    "x": 2088,
                    "y": 2753
                },
                {
                    "x": 1285,
                    "y": 2753
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='8' style='font-size:14px' alt=\"52\nSegFormer-B5\nB4\nTwins\nB3 SETR\n48\nSwin Transformer\nB2\nmloU\n44\nDeepLabV3+/R101\nHRNet-W48 + OCR\nADE20K\nB1\nPVT\n40 SemFPN\nmloU Params FLOPs FPS\nSegFormer-B0 37.4 3.7M 8.4G 50.5\nB0 FCN-R50 36.1 49.6M 198.0G 23.5\nSegFormer-B2 46.5 27.5M 62.4G 24.5\n36\nFCN-R50 DeeplabV3+/R101 44.1 62.7M 255.1G 14.1\nHRNet-W48 + OCR 43.0 70.5M 164.8G 17.0\nSegFormer-B4 50.3 64.1M 95.7G 15.4\nSETR 48.6 318.3M 362.1G 5.4\n32\n0 50 100 150 200 250 300 350\nParams (Millions)\" data-coord=\"top-left:(1285,2044); bottom-right:(2088,2753)\" /></figure>",
            "id": 8,
            "page": 1,
            "text": "52\nSegFormer-B5\nB4\nTwins\nB3 SETR\n48\nSwin Transformer\nB2\nmloU\n44\nDeepLabV3+/R101\nHRNet-W48 + OCR\nADE20K\nB1\nPVT\n40 SemFPN\nmloU Params FLOPs FPS\nSegFormer-B0 37.4 3.7M 8.4G 50.5\nB0 FCN-R50 36.1 49.6M 198.0G 23.5\nSegFormer-B2 46.5 27.5M 62.4G 24.5\n36\nFCN-R50 DeeplabV3+/R101 44.1 62.7M 255.1G 14.1\nHRNet-W48 + OCR 43.0 70.5M 164.8G 17.0\nSegFormer-B4 50.3 64.1M 95.7G 15.4\nSETR 48.6 318.3M 362.1G 5.4\n32\n0 50 100 150 200 250 300 350\nParams (Millions)"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 2757
                },
                {
                    "x": 2107,
                    "y": 2757
                },
                {
                    "x": 2107,
                    "y": 2890
                },
                {
                    "x": 1284,
                    "y": 2890
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>Figure 1: Performance vs. model efficiency on ADE20K. All results<br>are reported with single model and single-scale inference. SegFormer<br>achieves a new state-of-the-art 51.0% mloU while being significantly<br>more efficient than previous methods.</p>",
            "id": 9,
            "page": 1,
            "text": "Figure 1: Performance vs. model efficiency on ADE20K. All results\nare reported with single model and single-scale inference. SegFormer\nachieves a new state-of-the-art 51.0% mloU while being significantly\nmore efficient than previous methods."
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 2888
                },
                {
                    "x": 2108,
                    "y": 2888
                },
                {
                    "x": 2108,
                    "y": 2942
                },
                {
                    "x": 1213,
                    "y": 2942
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>methods with significantly deeper and more powerful</p>",
            "id": 10,
            "page": 1,
            "text": "methods with significantly deeper and more powerful"
        },
        {
            "bounding_box": [
                {
                    "x": 1242,
                    "y": 2936
                },
                {
                    "x": 2109,
                    "y": 2936
                },
                {
                    "x": 2109,
                    "y": 2987
                },
                {
                    "x": 1242,
                    "y": 2987
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>dramatically pushed the performance boundary of</p>",
            "id": 11,
            "page": 1,
            "text": "dramatically pushed the performance boundary of"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 3047
                },
                {
                    "x": 804,
                    "y": 3047
                },
                {
                    "x": 804,
                    "y": 3098
                },
                {
                    "x": 442,
                    "y": 3098
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:16px'>Preprint. Under review.</p>",
            "id": 12,
            "page": 1,
            "text": "Preprint. Under review."
        },
        {
            "bounding_box": [
                {
                    "x": 63,
                    "y": 890
                },
                {
                    "x": 149,
                    "y": 890
                },
                {
                    "x": 149,
                    "y": 2339
                },
                {
                    "x": 63,
                    "y": 2339
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2021<br>Oct<br>28<br>[cs.CV]<br>arXiv:2105.15203v3</footer>",
            "id": 13,
            "page": 1,
            "text": "2021\nOct\n28\n[cs.CV]\narXiv:2105.15203v3"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 309
                },
                {
                    "x": 2110,
                    "y": 309
                },
                {
                    "x": 2110,
                    "y": 489
                },
                {
                    "x": 442,
                    "y": 489
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:16px'>semantic segmentation. Besides backbone architectures, another line of work formulates semantic<br>segmentation as a structured prediction problem, and focuses on designing modules and operators,<br>which can effectively capture contextual information. A representative example in this area is dilated<br>convolution [4, 5], which increases the receptive field by \"inflating\" the kernel with holes.</p>",
            "id": 14,
            "page": 2,
            "text": "semantic segmentation. Besides backbone architectures, another line of work formulates semantic\nsegmentation as a structured prediction problem, and focuses on designing modules and operators,\nwhich can effectively capture contextual information. A representative example in this area is dilated\nconvolution [4, 5], which increases the receptive field by \"inflating\" the kernel with holes."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 510
                },
                {
                    "x": 2109,
                    "y": 510
                },
                {
                    "x": 2109,
                    "y": 786
                },
                {
                    "x": 441,
                    "y": 786
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:18px'>Witnessing the great success in natural language processing (NLP), there has been a recent surge of<br>interest to introduce Transformers to vision tasks. Dosovitskiy et al. [6] proposed vision Transformer<br>(ViT) for image classification. Following the Transformer design in NLP, the authors split an image<br>into multiple linearly embedded patches and feed them into a standard Transformer with positional<br>embeddings (PE), leading to an impressive performance on ImageNet. In semantic segmentation,<br>Zheng et al. [7] proposed SETR to demonstrate the feasibility of using Transformers in this task.</p>",
            "id": 15,
            "page": 2,
            "text": "Witnessing the great success in natural language processing (NLP), there has been a recent surge of\ninterest to introduce Transformers to vision tasks. Dosovitskiy et al. [6] proposed vision Transformer\n(ViT) for image classification. Following the Transformer design in NLP, the authors split an image\ninto multiple linearly embedded patches and feed them into a standard Transformer with positional\nembeddings (PE), leading to an impressive performance on ImageNet. In semantic segmentation,\nZheng et al. [7] proposed SETR to demonstrate the feasibility of using Transformers in this task."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 807
                },
                {
                    "x": 2108,
                    "y": 807
                },
                {
                    "x": 2108,
                    "y": 1173
                },
                {
                    "x": 441,
                    "y": 1173
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>SETR adopts ViT as a backbone and incorporates several CNN decoders to enlarge feature resolution.<br>Despite the good performance, ViT has some limitations: 1) ViT outputs single-scale low-resolution<br>features instead of multi-scale ones. 2) It has high computation cost on large images. To address these<br>limitations, Wang et al. [8] proposed a pyramid vision Transformer (PVT), a natural extension of ViT<br>with pyramid structures for dense prediction. PVT shows considerable improvements over the ResNet<br>counterpart on object detection and semantic segmentation. However, together with other emerging<br>methods such as Swin Transformer [9] and Twins [10], these methods mainly consider the design of<br>the Transformer encoder, neglecting the contribution of the decoder for further improvements.</p>",
            "id": 16,
            "page": 2,
            "text": "SETR adopts ViT as a backbone and incorporates several CNN decoders to enlarge feature resolution.\nDespite the good performance, ViT has some limitations: 1) ViT outputs single-scale low-resolution\nfeatures instead of multi-scale ones. 2) It has high computation cost on large images. To address these\nlimitations, Wang et al. [8] proposed a pyramid vision Transformer (PVT), a natural extension of ViT\nwith pyramid structures for dense prediction. PVT shows considerable improvements over the ResNet\ncounterpart on object detection and semantic segmentation. However, together with other emerging\nmethods such as Swin Transformer [9] and Twins [10], these methods mainly consider the design of\nthe Transformer encoder, neglecting the contribution of the decoder for further improvements."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1193
                },
                {
                    "x": 2108,
                    "y": 1193
                },
                {
                    "x": 2108,
                    "y": 1333
                },
                {
                    "x": 441,
                    "y": 1333
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:16px'>This paper introduces SegFormer, a cutting-edge Transformer framework for semantic segmentation<br>that jointly considers efficiency, accuracy, and robustness. In contrast to previous methods, our<br>framework redesigns both the encoder and the decoder. The key novelties of our approach are:</p>",
            "id": 17,
            "page": 2,
            "text": "This paper introduces SegFormer, a cutting-edge Transformer framework for semantic segmentation\nthat jointly considers efficiency, accuracy, and robustness. In contrast to previous methods, our\nframework redesigns both the encoder and the decoder. The key novelties of our approach are:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1369
                },
                {
                    "x": 2106,
                    "y": 1369
                },
                {
                    "x": 2106,
                    "y": 1652
                },
                {
                    "x": 441,
                    "y": 1652
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>· A novel positional-encoding-free and hierarchical Transformer encoder.<br>· A lightweight All-MLP decoder design that yields a powerful representation without complex and<br>computationally demanding modules.<br>· As shown in Figure 1, SegFormer sets new a state-of-the-art in terms of efficiency, accuracy and<br>robustness in three publicly available semantic segmentation datasets.</p>",
            "id": 18,
            "page": 2,
            "text": "· A novel positional-encoding-free and hierarchical Transformer encoder.\n· A lightweight All-MLP decoder design that yields a powerful representation without complex and\ncomputationally demanding modules.\n· As shown in Figure 1, SegFormer sets new a state-of-the-art in terms of efficiency, accuracy and\nrobustness in three publicly available semantic segmentation datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1687
                },
                {
                    "x": 2108,
                    "y": 1687
                },
                {
                    "x": 2108,
                    "y": 2146
                },
                {
                    "x": 441,
                    "y": 2146
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:16px'>First, the proposed encoder avoids interpolating positional codes when performing inference on<br>images with resolutions different from the training one. As a result, our encoder can easily adapt to<br>arbitrary test resolutions without impacting the performance. In addition, the hierarchical part enables<br>the encoder to generate both high-resolution fine features and low-resolution coarse features, this is<br>in contrast to ViT that can only produce single low-resolution feature maps with fixed resolutions.<br>Second, we propose a lightweight MLP decoder where the key idea is to take advantage of the<br>Transformer-induced features where the attentions of lower layers tend to stay local, whereas the<br>ones of the highest layers are highly non-local. By aggregating the information from different layers,<br>the MLP decoder combines both local and global attention. As a result, we obtain a simple and<br>straightforward decoder that renders powerful representations.</p>",
            "id": 19,
            "page": 2,
            "text": "First, the proposed encoder avoids interpolating positional codes when performing inference on\nimages with resolutions different from the training one. As a result, our encoder can easily adapt to\narbitrary test resolutions without impacting the performance. In addition, the hierarchical part enables\nthe encoder to generate both high-resolution fine features and low-resolution coarse features, this is\nin contrast to ViT that can only produce single low-resolution feature maps with fixed resolutions.\nSecond, we propose a lightweight MLP decoder where the key idea is to take advantage of the\nTransformer-induced features where the attentions of lower layers tend to stay local, whereas the\nones of the highest layers are highly non-local. By aggregating the information from different layers,\nthe MLP decoder combines both local and global attention. As a result, we obtain a simple and\nstraightforward decoder that renders powerful representations."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2165
                },
                {
                    "x": 2108,
                    "y": 2165
                },
                {
                    "x": 2108,
                    "y": 2580
                },
                {
                    "x": 441,
                    "y": 2580
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:18px'>We demonstrate the advantages of SegFormer in terms of model size, run-time, and accuracy on three<br>publicly available datasets: ADE20K, Cityscapes, and COCO-Stuff. On Citysapces, our lightweight<br>model, SegFormer-B0, without accelerated implementations such as TensorRT, yields 71.9% mIoU<br>at 48 FPS, which, compared to ICNet [11], represents a relative improvement of 60% and 4.2% in<br>latency and performance, respectively. Our largest model, SegFormer-B5, yields 84.0% mIoU, which<br>represents a relative 1.8% mIoU improvement while being 5 x faster than SETR [7]. On ADE20K,<br>this model sets a new state-of-the-art of 51.8% mIoU while being 4 x smaller than SETR. Moreover,<br>our approach is significantly more robust to common corruptions and perturbations than existing<br>methods, therefore being suitable for safety-critical applications. Code will be publicly available.</p>",
            "id": 20,
            "page": 2,
            "text": "We demonstrate the advantages of SegFormer in terms of model size, run-time, and accuracy on three\npublicly available datasets: ADE20K, Cityscapes, and COCO-Stuff. On Citysapces, our lightweight\nmodel, SegFormer-B0, without accelerated implementations such as TensorRT, yields 71.9% mIoU\nat 48 FPS, which, compared to ICNet [11], represents a relative improvement of 60% and 4.2% in\nlatency and performance, respectively. Our largest model, SegFormer-B5, yields 84.0% mIoU, which\nrepresents a relative 1.8% mIoU improvement while being 5 x faster than SETR [7]. On ADE20K,\nthis model sets a new state-of-the-art of 51.8% mIoU while being 4 x smaller than SETR. Moreover,\nour approach is significantly more robust to common corruptions and perturbations than existing\nmethods, therefore being suitable for safety-critical applications. Code will be publicly available."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2665
                },
                {
                    "x": 826,
                    "y": 2665
                },
                {
                    "x": 826,
                    "y": 2719
                },
                {
                    "x": 444,
                    "y": 2719
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:20px'>2 Related Work</p>",
            "id": 21,
            "page": 2,
            "text": "2 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2781
                },
                {
                    "x": 2110,
                    "y": 2781
                },
                {
                    "x": 2110,
                    "y": 3014
                },
                {
                    "x": 442,
                    "y": 3014
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:16px'>Semantic Segmentation. Semantic segmentation can be seen as an extension of image classification<br>from image level to pixel level. In the deep learning era [12-16], FCN [1] is the fundamental work of<br>semantic segmentation, which is a fully convolution network that performs pixel-to-pixel classification<br>in an end-to-end manner. After that, researchers focused on improving FCN from different aspects<br>such as: enlarging the receptive field [17-19, 5, 2, 4, 20]; refining the contextual information [21-</p>",
            "id": 22,
            "page": 2,
            "text": "Semantic Segmentation. Semantic segmentation can be seen as an extension of image classification\nfrom image level to pixel level. In the deep learning era [12-16], FCN [1] is the fundamental work of\nsemantic segmentation, which is a fully convolution network that performs pixel-to-pixel classification\nin an end-to-end manner. After that, researchers focused on improving FCN from different aspects\nsuch as: enlarging the receptive field [17-19, 5, 2, 4, 20]; refining the contextual information [21-"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3091
                },
                {
                    "x": 1289,
                    "y": 3091
                },
                {
                    "x": 1289,
                    "y": 3130
                },
                {
                    "x": 1260,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='23' style='font-size:14px'>2</footer>",
            "id": 23,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 591,
                    "y": 300
                },
                {
                    "x": 1940,
                    "y": 300
                },
                {
                    "x": 1940,
                    "y": 990
                },
                {
                    "x": 591,
                    "y": 990
                }
            ],
            "category": "figure",
            "html": "<figure><img id='24' style='font-size:14px' alt=\"Encoder Decoder\n4xW xC1 X V8C2 11⌀x W16 XC3 32 32 4 4 4 4\n브 x w XC4\nHI x w x4C 브 x w XNcls\nOverlap\nEmbeddings\nTransformer\nTransformer\nTransformer\nTransformer\nBlock\nBlock\nBlock\nBlock\nLayer\nPatch → MLP\n4\n2\n3\nH W H W 4x�xC\nx xC\nx xCi 2i+1 2i+1\nOverlap 2i+1 2i+1\nMix-FFN\nSelf-An\nEficent\nMerging\nML\nPatch\nP\nxN Upsample\" data-coord=\"top-left:(591,300); bottom-right:(1940,990)\" /></figure>",
            "id": 24,
            "page": 3,
            "text": "Encoder Decoder\n4xW xC1 X V8C2 11⌀x W16 XC3 32 32 4 4 4 4\n브 x w XC4\nHI x w x4C 브 x w XNcls\nOverlap\nEmbeddings\nTransformer\nTransformer\nTransformer\nTransformer\nBlock\nBlock\nBlock\nBlock\nLayer\nPatch → MLP\n4\n2\n3\nH W H W 4x�xC\nx xC\nx xCi 2i+1 2i+1\nOverlap 2i+1 2i+1\nMix-FFN\nSelf-An\nEficent\nMerging\nML\nPatch\nP\nxN Upsample"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1008
                },
                {
                    "x": 2108,
                    "y": 1008
                },
                {
                    "x": 2108,
                    "y": 1139
                },
                {
                    "x": 440,
                    "y": 1139
                }
            ],
            "category": "caption",
            "html": "<br><caption id='25' style='font-size:14px'>Figure 2: The proposed SegFormer framework consists of two main modules: A hierarchical Transformer<br>encoder to extract coarse and fine features; and a lightweight All-MLP decoder to directly fuse these multi-level<br>features and predict the semantic segmentation mask. \"FFN\" indicates feed-forward network.</caption>",
            "id": 25,
            "page": 3,
            "text": "Figure 2: The proposed SegFormer framework consists of two main modules: A hierarchical Transformer\nencoder to extract coarse and fine features; and a lightweight All-MLP decoder to directly fuse these multi-level\nfeatures and predict the semantic segmentation mask. \"FFN\" indicates feed-forward network."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1191
                },
                {
                    "x": 2107,
                    "y": 1191
                },
                {
                    "x": 2107,
                    "y": 1467
                },
                {
                    "x": 442,
                    "y": 1467
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:20px'>29]; introducing boundary information [30-37]; designing various attention modules [38-46]; or<br>using AutoML technologies [47-51]. These methods significantly improve semantic segmentation<br>performance at the expense of introducing many empirical modules, making the resulting framework<br>computationally demanding and complicated. More recent methods have proved the effectiveness of<br>Transformer-based architectures for semantic segmentation [7, 46]. However, these methods are still<br>computationally demanding.</p>",
            "id": 26,
            "page": 3,
            "text": "29]; introducing boundary information [30-37]; designing various attention modules [38-46]; or\nusing AutoML technologies [47-51]. These methods significantly improve semantic segmentation\nperformance at the expense of introducing many empirical modules, making the resulting framework\ncomputationally demanding and complicated. More recent methods have proved the effectiveness of\nTransformer-based architectures for semantic segmentation [7, 46]. However, these methods are still\ncomputationally demanding."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1487
                },
                {
                    "x": 2107,
                    "y": 1487
                },
                {
                    "x": 2107,
                    "y": 1763
                },
                {
                    "x": 441,
                    "y": 1763
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:20px'>Transformer backbones. ViT [6] is the first work to prove that a pure Transformer can achieve<br>state-of-the-art performance in image classification. ViT treats each image as a sequence of tokens and<br>then feeds them to multiple Transformer layers to make the classification. Subsequently, DeiT [52]<br>further explores a data-efficient training strategy and a distillation approach for ViT. More recent<br>methods such as T2T ViT [53], CPVT [54], TNT [55], Cross ViT [56] and LocalViT [57] introduce<br>tailored changes to ViT to further improve image classification performance.</p>",
            "id": 27,
            "page": 3,
            "text": "Transformer backbones. ViT [6] is the first work to prove that a pure Transformer can achieve\nstate-of-the-art performance in image classification. ViT treats each image as a sequence of tokens and\nthen feeds them to multiple Transformer layers to make the classification. Subsequently, DeiT [52]\nfurther explores a data-efficient training strategy and a distillation approach for ViT. More recent\nmethods such as T2T ViT [53], CPVT [54], TNT [55], Cross ViT [56] and LocalViT [57] introduce\ntailored changes to ViT to further improve image classification performance."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1784
                },
                {
                    "x": 2109,
                    "y": 1784
                },
                {
                    "x": 2109,
                    "y": 2012
                },
                {
                    "x": 441,
                    "y": 2012
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:18px'>Beyond classification, PVT [8] is the first work to introduce a pyramid structure in Transformer,<br>demonstrating the potential of a pure Transformer backbone compared to CNN counterparts in<br>dense prediction tasks. After that, methods such as Swin [9], CvT [58], CoaT [59], LeViT [60] and<br>Twins [10] enhance the local continuity of features and remove fixed size position embedding to<br>improve the performance of Transformers in dense prediction tasks.</p>",
            "id": 28,
            "page": 3,
            "text": "Beyond classification, PVT [8] is the first work to introduce a pyramid structure in Transformer,\ndemonstrating the potential of a pure Transformer backbone compared to CNN counterparts in\ndense prediction tasks. After that, methods such as Swin [9], CvT [58], CoaT [59], LeViT [60] and\nTwins [10] enhance the local continuity of features and remove fixed size position embedding to\nimprove the performance of Transformers in dense prediction tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2032
                },
                {
                    "x": 2108,
                    "y": 2032
                },
                {
                    "x": 2108,
                    "y": 2355
                },
                {
                    "x": 441,
                    "y": 2355
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:20px'>Transformers for specific tasks. DETR [52] is the first work using Transformers to build an end-to-<br>end object detection framework without non-maximum suppression (NMS). Other works have also<br>used Transformers in a variety of tasks such as tracking [61, 62], super-resolution [63], ReID [64],<br>Colorization [65], Retrieval [66] and multi-modal learning [67, 68]. For semantic segmentation,<br>SETR [7] adopts ViT [6] as a backbone to extract features, achieving impressive performance.<br>However, these Transformer-based methods have very low efficiency and, thus, difficult to deploy in<br>real-time applications.</p>",
            "id": 29,
            "page": 3,
            "text": "Transformers for specific tasks. DETR [52] is the first work using Transformers to build an end-to-\nend object detection framework without non-maximum suppression (NMS). Other works have also\nused Transformers in a variety of tasks such as tracking [61, 62], super-resolution [63], ReID [64],\nColorization [65], Retrieval [66] and multi-modal learning [67, 68]. For semantic segmentation,\nSETR [7] adopts ViT [6] as a backbone to extract features, achieving impressive performance.\nHowever, these Transformer-based methods have very low efficiency and, thus, difficult to deploy in\nreal-time applications."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2425
                },
                {
                    "x": 694,
                    "y": 2425
                },
                {
                    "x": 694,
                    "y": 2477
                },
                {
                    "x": 443,
                    "y": 2477
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:22px'>3 Method</p>",
            "id": 30,
            "page": 3,
            "text": "3 Method"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2531
                },
                {
                    "x": 2108,
                    "y": 2531
                },
                {
                    "x": 2108,
                    "y": 2762
                },
                {
                    "x": 441,
                    "y": 2762
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>This section introduces SegFormer, our efficient, robust, and powerful segmentation framework<br>without hand-crafted and computationally demanding modules. As depicted in Figure 2, SegFormer<br>consists of two main modules: (1) a hierarchical Transformer encoder to generate high-resolution<br>coarse features and low-resolution fine features; and (2) a lightweight All-MLP decoder to fuse these<br>multi-level features to produce the final semantic segmentation mask.</p>",
            "id": 31,
            "page": 3,
            "text": "This section introduces SegFormer, our efficient, robust, and powerful segmentation framework\nwithout hand-crafted and computationally demanding modules. As depicted in Figure 2, SegFormer\nconsists of two main modules: (1) a hierarchical Transformer encoder to generate high-resolution\ncoarse features and low-resolution fine features; and (2) a lightweight All-MLP decoder to fuse these\nmulti-level features to produce the final semantic segmentation mask."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2784
                },
                {
                    "x": 2108,
                    "y": 2784
                },
                {
                    "x": 2108,
                    "y": 3016
                },
                {
                    "x": 441,
                    "y": 3016
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='32' style='font-size:16px'>Given an image of size H x W x 3, we first divide it into patches of size 4 x 4. Contrary to ViT<br>that uses patches of size 16 x 16, using smaller patches favors the dense prediction task. We then<br>use these patches as input to the hierarchical Transformer encoder to obtain multi-level features at<br>{1/4, 1/8, 1/16, 1/32} of the original image resolution. We then pass these multi-level features to the<br>H W Ncls is the<br>All-MLP decoder to predict the segmentation mask at a x x Ncls resolution, where<br>4 4</p>",
            "id": 32,
            "page": 3,
            "text": "Given an image of size H x W x 3, we first divide it into patches of size 4 x 4. Contrary to ViT\nthat uses patches of size 16 x 16, using smaller patches favors the dense prediction task. We then\nuse these patches as input to the hierarchical Transformer encoder to obtain multi-level features at\n{1/4, 1/8, 1/16, 1/32} of the original image resolution. We then pass these multi-level features to the\nH W Ncls is the\nAll-MLP decoder to predict the segmentation mask at a x x Ncls resolution, where\n4 4"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3093
                },
                {
                    "x": 1290,
                    "y": 3093
                },
                {
                    "x": 1290,
                    "y": 3131
                },
                {
                    "x": 1259,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='33' style='font-size:16px'>3</footer>",
            "id": 33,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 309
                },
                {
                    "x": 2105,
                    "y": 309
                },
                {
                    "x": 2105,
                    "y": 399
                },
                {
                    "x": 442,
                    "y": 399
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:14px'>number of categories. In the rest of this section, we detail the proposed encoder and decoder designs<br>and summarize the main differences between our approach and SETR.</p>",
            "id": 34,
            "page": 4,
            "text": "number of categories. In the rest of this section, we detail the proposed encoder and decoder designs\nand summarize the main differences between our approach and SETR."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 451
                },
                {
                    "x": 1169,
                    "y": 451
                },
                {
                    "x": 1169,
                    "y": 499
                },
                {
                    "x": 443,
                    "y": 499
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>3.1 Hierarchical Transformer Encoder</p>",
            "id": 35,
            "page": 4,
            "text": "3.1 Hierarchical Transformer Encoder"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 536
                },
                {
                    "x": 2107,
                    "y": 536
                },
                {
                    "x": 2107,
                    "y": 718
                },
                {
                    "x": 442,
                    "y": 718
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:18px'>We design a series of Mix Transformer encoders (MiT), MiT-B0 to MiT-B5, with the same architecture<br>but different sizes. MiT-B0 is our lightweight model for fast inference, while MiT-B5 is the largest<br>model for the best performance. Our design for MiT is partly inspired by ViT but tailored and<br>optimized for semantic segmentation.</p>",
            "id": 36,
            "page": 4,
            "text": "We design a series of Mix Transformer encoders (MiT), MiT-B0 to MiT-B5, with the same architecture\nbut different sizes. MiT-B0 is our lightweight model for fast inference, while MiT-B5 is the largest\nmodel for the best performance. Our design for MiT is partly inspired by ViT but tailored and\noptimized for semantic segmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 739
                },
                {
                    "x": 2107,
                    "y": 739
                },
                {
                    "x": 2107,
                    "y": 1016
                },
                {
                    "x": 441,
                    "y": 1016
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:16px'>Hierarchical Feature Representation. Unlike ViT that can only generate a single-resolution feature<br>map, the goal of this module is, given an input image, to generate CNN-like multi-level features.<br>These features provide high-resolution coarse features and low-resolution fine-grained features that<br>usually boost the performance of semantic segmentation. More precisely, given an input image with<br>a resolution of H x W x 3, we perform patch merging to obtain a hierarchical feature map Fi with a<br>H W {1,2, 3, 4}, and Ci+1 is larger than Ci.<br>resolution of x x Ci, where i E<br>2i+1 2i+1</p>",
            "id": 37,
            "page": 4,
            "text": "Hierarchical Feature Representation. Unlike ViT that can only generate a single-resolution feature\nmap, the goal of this module is, given an input image, to generate CNN-like multi-level features.\nThese features provide high-resolution coarse features and low-resolution fine-grained features that\nusually boost the performance of semantic segmentation. More precisely, given an input image with\na resolution of H x W x 3, we perform patch merging to obtain a hierarchical feature map Fi with a\nH W {1,2, 3, 4}, and Ci+1 is larger than Ci.\nresolution of x x Ci, where i E\n2i+1 2i+1"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1035
                },
                {
                    "x": 2107,
                    "y": 1035
                },
                {
                    "x": 2107,
                    "y": 1497
                },
                {
                    "x": 442,
                    "y": 1497
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:14px'>Overlapped Patch Merging. Given an image patch, the patch merging process used in ViT, unifies<br>a N x N x 3 patch into a 1 x 1 x C vector. This can easily be extended to unify a 2 x 2 x Ci<br>feature path into a 1 x 1 x Ci+1 vector to obtain hierarchical feature maps. Using this, we can shrink<br>W C2), and then iterate for any<br>our hierarchical features from F1 (부 x W4 x C1) to F2 (� x x<br>8<br>other feature map in the hierarchy. This process was initially designed to combine non-overlapping<br>image or feature patches. Therefore, it fails to preserve the local continuity around those patches.<br>Instead, we use an overlapping patch merging process. To this end, we define K, S, and P, where<br>K is the patch size, S is the stride between two adjacent patches, and P is the padding size. In our<br>experiments, we set K = 7, S = 4, P = 3 ,and K = 3, S = 2, P = 1 to perform overlapping patch<br>merging to produces features with the same size as the non-overlapping process.</p>",
            "id": 38,
            "page": 4,
            "text": "Overlapped Patch Merging. Given an image patch, the patch merging process used in ViT, unifies\na N x N x 3 patch into a 1 x 1 x C vector. This can easily be extended to unify a 2 x 2 x Ci\nfeature path into a 1 x 1 x Ci+1 vector to obtain hierarchical feature maps. Using this, we can shrink\nW C2), and then iterate for any\nour hierarchical features from F1 (부 x W4 x C1) to F2 (� x x\n8\nother feature map in the hierarchy. This process was initially designed to combine non-overlapping\nimage or feature patches. Therefore, it fails to preserve the local continuity around those patches.\nInstead, we use an overlapping patch merging process. To this end, we define K, S, and P, where\nK is the patch size, S is the stride between two adjacent patches, and P is the padding size. In our\nexperiments, we set K = 7, S = 4, P = 3 ,and K = 3, S = 2, P = 1 to perform overlapping patch\nmerging to produces features with the same size as the non-overlapping process."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1518
                },
                {
                    "x": 2108,
                    "y": 1518
                },
                {
                    "x": 2108,
                    "y": 1654
                },
                {
                    "x": 443,
                    "y": 1654
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:16px'>Efficient Self-Attention. The main computation bottleneck of the encoders is the self-attention layer.<br>In the original multi-head self-attention process, each of the heads Q, K, V have the same dimensions<br>N x C, where N = H x W is the length of the sequence, the self-attention is estimated as:</p>",
            "id": 39,
            "page": 4,
            "text": "Efficient Self-Attention. The main computation bottleneck of the encoders is the self-attention layer.\nIn the original multi-head self-attention process, each of the heads Q, K, V have the same dimensions\nN x C, where N = H x W is the length of the sequence, the self-attention is estimated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1803
                },
                {
                    "x": 2107,
                    "y": 1803
                },
                {
                    "x": 2107,
                    "y": 1942
                },
                {
                    "x": 441,
                    "y": 1942
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:16px'>The computational complexity of this process is O(N2), which is prohibitive for large image<br>resolutions. Instead, we use the sequence reduction process introduced in [8]. This process uses a<br>reduction ratio R to reduce the length of the sequence of as follows:</p>",
            "id": 40,
            "page": 4,
            "text": "The computational complexity of this process is O(N2), which is prohibitive for large image\nresolutions. Instead, we use the sequence reduction process introduced in [8]. This process uses a\nreduction ratio R to reduce the length of the sequence of as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 2143
                },
                {
                    "x": 2106,
                    "y": 2143
                },
                {
                    "x": 2106,
                    "y": 2402
                },
                {
                    "x": 439,
                    "y": 2402
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>C · R)(K) refers to reshape K to the one with<br>where K is the sequence to be reduced, Reshape( 음 ,<br>shape of 음 x (C · R), and Linear(Cin, Cout)(·) refers to a linear layer taking a Cin-dimensional<br>tensor as input and generating a Cout-dimensional tensor as output. Therefore, the new K has<br>dimensions NR x C. As a result, the complexity of the self-attention mechanism is reduced from<br>O(N2) to O(N2). In our experiments, we set R to [64, 16, 4, 1] from stage-1 to stage-4.</p>",
            "id": 41,
            "page": 4,
            "text": "C · R)(K) refers to reshape K to the one with\nwhere K is the sequence to be reduced, Reshape( 음 ,\nshape of 음 x (C · R), and Linear(Cin, Cout)(·) refers to a linear layer taking a Cin-dimensional\ntensor as input and generating a Cout-dimensional tensor as output. Therefore, the new K has\ndimensions NR x C. As a result, the complexity of the self-attention mechanism is reduced from\nO(N2) to O(N2). In our experiments, we set R to [64, 16, 4, 1] from stage-1 to stage-4."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2419
                },
                {
                    "x": 2107,
                    "y": 2419
                },
                {
                    "x": 2107,
                    "y": 2741
                },
                {
                    "x": 442,
                    "y": 2741
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:16px'>Mix-FFN. ViT uses positional encoding (PE) to introduce the location information. However, the<br>resolution of PE is fixed. Therefore, when the test resolution is different from the training one, the<br>positional code needs to be interpolated and this often leads to dropped accuracy. To alleviate this<br>problem, CPVT [54] uses 3 x 3 Conv together with the PE to implement a data-driven PE. We argue<br>that positional encoding is actually not necessary for semantic segmentation. Instead, we introduce<br>Mix-FFN which considers the effect of zero padding to leak location information [69], by directly<br>using a 3 x 3 Conv in the feed-forward network (FFN). Mix-FFN can be formulated as:</p>",
            "id": 42,
            "page": 4,
            "text": "Mix-FFN. ViT uses positional encoding (PE) to introduce the location information. However, the\nresolution of PE is fixed. Therefore, when the test resolution is different from the training one, the\npositional code needs to be interpolated and this often leads to dropped accuracy. To alleviate this\nproblem, CPVT [54] uses 3 x 3 Conv together with the PE to implement a data-driven PE. We argue\nthat positional encoding is actually not necessary for semantic segmentation. Instead, we introduce\nMix-FFN which considers the effect of zero padding to leak location information [69], by directly\nusing a 3 x 3 Conv in the feed-forward network (FFN). Mix-FFN can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2830
                },
                {
                    "x": 2107,
                    "y": 2830
                },
                {
                    "x": 2107,
                    "y": 3013
                },
                {
                    "x": 441,
                    "y": 3013
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:14px'>where Xin is the feature from the self-attention module. Mix-FFN mixes a 3 x 3 convolution and<br>an MLP into each FFN. In our experiments, we will show that a 3 x 3 convolution is sufficient to<br>provide positional information for Transformers. In particular, we use depth-wise convolutions for<br>reducing the number of parameters and improving efficiency.</p>",
            "id": 43,
            "page": 4,
            "text": "where Xin is the feature from the self-attention module. Mix-FFN mixes a 3 x 3 convolution and\nan MLP into each FFN. In our experiments, we will show that a 3 x 3 convolution is sufficient to\nprovide positional information for Transformers. In particular, we use depth-wise convolutions for\nreducing the number of parameters and improving efficiency."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3096
                },
                {
                    "x": 1287,
                    "y": 3096
                },
                {
                    "x": 1287,
                    "y": 3126
                },
                {
                    "x": 1260,
                    "y": 3126
                }
            ],
            "category": "footer",
            "html": "<footer id='44' style='font-size:14px'>4</footer>",
            "id": 44,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 306
                },
                {
                    "x": 1090,
                    "y": 306
                },
                {
                    "x": 1090,
                    "y": 352
                },
                {
                    "x": 444,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:20px'>3.2 Lightweight AII-MLP Decoder</p>",
            "id": 45,
            "page": 5,
            "text": "3.2 Lightweight AII-MLP Decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 391
                },
                {
                    "x": 2108,
                    "y": 391
                },
                {
                    "x": 2108,
                    "y": 573
                },
                {
                    "x": 442,
                    "y": 573
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:18px'>SegFormer incorporates a lightweight decoder consisting only of MLP layers and this avoiding the<br>hand-crafted and computationally demanding components typically used in other methods. The key<br>to enabling such a simple decoder is that our hierarchical Transformer encoder has a larger effective<br>receptive field (ERF) than traditional CNN encoders.</p>",
            "id": 46,
            "page": 5,
            "text": "SegFormer incorporates a lightweight decoder consisting only of MLP layers and this avoiding the\nhand-crafted and computationally demanding components typically used in other methods. The key\nto enabling such a simple decoder is that our hierarchical Transformer encoder has a larger effective\nreceptive field (ERF) than traditional CNN encoders."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 595
                },
                {
                    "x": 2109,
                    "y": 595
                },
                {
                    "x": 2109,
                    "y": 870
                },
                {
                    "x": 441,
                    "y": 870
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:16px'>The proposed All-MLP decoder consists of four main steps. First, multi-level features Fi from<br>the MiT encoder go through an MLP layer to unify the channel dimension. Then, in a second<br>step, features are up-sampled to 1/4th and concatenated together. Third, a MLP layer is adopted to<br>fuse the concatenated features F. Finally, another MLP layer takes the fused feature to predict the<br>W Ncls resolution, where Ncls is the number of categories.<br>segmentation mask M with a 북 x x<br>4<br>This lets us formulate the decoder as:</p>",
            "id": 47,
            "page": 5,
            "text": "The proposed All-MLP decoder consists of four main steps. First, multi-level features Fi from\nthe MiT encoder go through an MLP layer to unify the channel dimension. Then, in a second\nstep, features are up-sampled to 1/4th and concatenated together. Third, a MLP layer is adopted to\nfuse the concatenated features F. Finally, another MLP layer takes the fused feature to predict the\nW Ncls resolution, where Ncls is the number of categories.\nsegmentation mask M with a 북 x x\n4\nThis lets us formulate the decoder as:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1212
                },
                {
                    "x": 2106,
                    "y": 1212
                },
                {
                    "x": 2106,
                    "y": 1302
                },
                {
                    "x": 442,
                    "y": 1302
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>where M refers to the predicted mask, and Linear(Cin, Cout )(·) refers to a linear layer with Cin and<br>Cout as input and output vector dimensions respectively.</p>",
            "id": 48,
            "page": 5,
            "text": "where M refers to the predicted mask, and Linear(Cin, Cout )(·) refers to a linear layer with Cin and\nCout as input and output vector dimensions respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1328
                },
                {
                    "x": 1079,
                    "y": 1328
                },
                {
                    "x": 1079,
                    "y": 1370
                },
                {
                    "x": 446,
                    "y": 1370
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:22px'>Effective Receptive Field Analysis.</p>",
            "id": 49,
            "page": 5,
            "text": "Effective Receptive Field Analysis."
        },
        {
            "bounding_box": [
                {
                    "x": 1120,
                    "y": 1338
                },
                {
                    "x": 2106,
                    "y": 1338
                },
                {
                    "x": 2106,
                    "y": 1693
                },
                {
                    "x": 1120,
                    "y": 1693
                }
            ],
            "category": "figure",
            "html": "<figure><img id='50' style='font-size:14px' alt=\"Stage-1 Stage-2 Stage-3 Stage-4 Head\nDeepLabv3+\n。 ·\nSegFormer\n* .\" data-coord=\"top-left:(1120,1338); bottom-right:(2106,1693)\" /></figure>",
            "id": 50,
            "page": 5,
            "text": "Stage-1 Stage-2 Stage-3 Stage-4 Head\nDeepLabv3+\n。 ·\nSegFormer\n* ."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1372
                },
                {
                    "x": 1081,
                    "y": 1372
                },
                {
                    "x": 1081,
                    "y": 1916
                },
                {
                    "x": 444,
                    "y": 1916
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:16px'>For semantic segmentation, maintain-<br>ing large receptive field to include con-<br>text information has been a central is-<br>sue [5, 19, 20]. Here, we use effec-<br>tive receptive field (ERF) [70] as a<br>toolkit to visualize and interpret why<br>our MLP decoder design is SO effec-<br>tive on Transformers. In Figure 3, we<br>visualize ERFs of the four encoder<br>stages and the decoder heads for both<br>DeepLabv3+ and SegFormer. We can<br>make the following observations:</p>",
            "id": 51,
            "page": 5,
            "text": "For semantic segmentation, maintain-\ning large receptive field to include con-\ntext information has been a central is-\nsue [5, 19, 20]. Here, we use effec-\ntive receptive field (ERF) [70] as a\ntoolkit to visualize and interpret why\nour MLP decoder design is SO effec-\ntive on Transformers. In Figure 3, we\nvisualize ERFs of the four encoder\nstages and the decoder heads for both\nDeepLabv3+ and SegFormer. We can\nmake the following observations:"
        },
        {
            "bounding_box": [
                {
                    "x": 1104,
                    "y": 1711
                },
                {
                    "x": 2108,
                    "y": 1711
                },
                {
                    "x": 2108,
                    "y": 1883
                },
                {
                    "x": 1104,
                    "y": 1883
                }
            ],
            "category": "caption",
            "html": "<br><caption id='52' style='font-size:14px'>Figure 3: Effective Receptive Field (ERF) on Cityscapes (aver-<br>age over 100 images). Top row: Deeplabv3+. Bottom row: Seg-<br>Former. ERFs of the four stages and the decoder heads of both<br>architectures are visualized. Best viewed with zoom in.</caption>",
            "id": 52,
            "page": 5,
            "text": "Figure 3: Effective Receptive Field (ERF) on Cityscapes (aver-\nage over 100 images). Top row: Deeplabv3+. Bottom row: Seg-\nFormer. ERFs of the four stages and the decoder heads of both\narchitectures are visualized. Best viewed with zoom in."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1947
                },
                {
                    "x": 2109,
                    "y": 1947
                },
                {
                    "x": 2109,
                    "y": 2222
                },
                {
                    "x": 443,
                    "y": 2222
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:16px'>· The ERF of DeepLabv3+ is relatively small even at Stage-4, the deepest stage.<br>· SegFormer's encoder naturally produces local attentions which resemble convolutions at lower<br>stages, while able to output highly non-local attentions that effectively capture contexts at Stage-4.<br>· As shown with the zoom-in patches in Figure 3, the ERF of the MLP head (blue box) differs from<br>Stage-4 (red box) with a significant stronger local attention besides the non-local attention.</p>",
            "id": 53,
            "page": 5,
            "text": "· The ERF of DeepLabv3+ is relatively small even at Stage-4, the deepest stage.\n· SegFormer's encoder naturally produces local attentions which resemble convolutions at lower\nstages, while able to output highly non-local attentions that effectively capture contexts at Stage-4.\n· As shown with the zoom-in patches in Figure 3, the ERF of the MLP head (blue box) differs from\nStage-4 (red box) with a significant stronger local attention besides the non-local attention."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2262
                },
                {
                    "x": 2107,
                    "y": 2262
                },
                {
                    "x": 2107,
                    "y": 2492
                },
                {
                    "x": 442,
                    "y": 2492
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:16px'>The limited receptive field in CNN requires one to resort to context modules such as ASPP [18]<br>that enlarge the receptive field but inevitably become heavy. Our decoder design benefits from the<br>non-local attention in Transformers and leads to a larger receptive field without being complex. The<br>same decoder design, however, does not work well on CNN backbones since the overall receptive<br>field is upper bounded by the limited one at Stage-4, and we will verify this later in Table 1d,</p>",
            "id": 54,
            "page": 5,
            "text": "The limited receptive field in CNN requires one to resort to context modules such as ASPP [18]\nthat enlarge the receptive field but inevitably become heavy. Our decoder design benefits from the\nnon-local attention in Transformers and leads to a larger receptive field without being complex. The\nsame decoder design, however, does not work well on CNN backbones since the overall receptive\nfield is upper bounded by the limited one at Stage-4, and we will verify this later in Table 1d,"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2513
                },
                {
                    "x": 2108,
                    "y": 2513
                },
                {
                    "x": 2108,
                    "y": 2744
                },
                {
                    "x": 441,
                    "y": 2744
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:18px'>More importantly, our decoder design essentially takes advantage of a Transformer induced feature<br>that produces both highly local and non-local attention at the same time. By unifying them, our MLP<br>decoder renders complementary and powerful representations by adding few parameters. This is<br>another key reason that motivated our design. Taking the non-local attention from Stage-4 alone is<br>not enough to produce good results, as will be verified in Table 1d.</p>",
            "id": 55,
            "page": 5,
            "text": "More importantly, our decoder design essentially takes advantage of a Transformer induced feature\nthat produces both highly local and non-local attention at the same time. By unifying them, our MLP\ndecoder renders complementary and powerful representations by adding few parameters. This is\nanother key reason that motivated our design. Taking the non-local attention from Stage-4 alone is\nnot enough to produce good results, as will be verified in Table 1d."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2801
                },
                {
                    "x": 946,
                    "y": 2801
                },
                {
                    "x": 946,
                    "y": 2848
                },
                {
                    "x": 443,
                    "y": 2848
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:18px'>3.3 Relationship to SETR.</p>",
            "id": 56,
            "page": 5,
            "text": "3.3 Relationship to SETR."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2887
                },
                {
                    "x": 1979,
                    "y": 2887
                },
                {
                    "x": 1979,
                    "y": 2935
                },
                {
                    "x": 443,
                    "y": 2935
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:20px'>SegFormer contains multiple more efficient and powerful designs compared with SETR [7]:</p>",
            "id": 57,
            "page": 5,
            "text": "SegFormer contains multiple more efficient and powerful designs compared with SETR [7]:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2965
                },
                {
                    "x": 2094,
                    "y": 2965
                },
                {
                    "x": 2094,
                    "y": 3010
                },
                {
                    "x": 444,
                    "y": 3010
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:16px'>· We only use ImageNet-1K for pre-training. ViT in SETR is pre-trained on larger ImageNet-22K.</p>",
            "id": 58,
            "page": 5,
            "text": "· We only use ImageNet-1K for pre-training. ViT in SETR is pre-trained on larger ImageNet-22K."
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3094
                },
                {
                    "x": 1289,
                    "y": 3094
                },
                {
                    "x": 1289,
                    "y": 3128
                },
                {
                    "x": 1261,
                    "y": 3128
                }
            ],
            "category": "footer",
            "html": "<footer id='59' style='font-size:14px'>5</footer>",
            "id": 59,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 302
                },
                {
                    "x": 2112,
                    "y": 302
                },
                {
                    "x": 2112,
                    "y": 718
                },
                {
                    "x": 439,
                    "y": 718
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:14px'>· SegFormer's encoder has a hierarchical architecture, which is smaller than ViT and can capture<br>both high-resolution coarse and low-resolution fine features. In contrast, SETR's ViT encoder can<br>only generate single low-resolution feature map.<br>● We remove Positional Embedding in encoder, while SETR uses fixed shape Positional Embedding<br>which decreases the accuracy when the resolution at inference differs from the training ones.<br>· Our MLP decoder is more compact and less computationally demanding than the one in SETR.<br>This leads to a negligible computational overhead. In contrast, SETR requires heavy decoders with<br>multiple 3x3 convolutions.</p>",
            "id": 60,
            "page": 6,
            "text": "· SegFormer's encoder has a hierarchical architecture, which is smaller than ViT and can capture\nboth high-resolution coarse and low-resolution fine features. In contrast, SETR's ViT encoder can\nonly generate single low-resolution feature map.\n● We remove Positional Embedding in encoder, while SETR uses fixed shape Positional Embedding\nwhich decreases the accuracy when the resolution at inference differs from the training ones.\n· Our MLP decoder is more compact and less computationally demanding than the one in SETR.\nThis leads to a negligible computational overhead. In contrast, SETR requires heavy decoders with\nmultiple 3x3 convolutions."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 801
                },
                {
                    "x": 801,
                    "y": 801
                },
                {
                    "x": 801,
                    "y": 852
                },
                {
                    "x": 443,
                    "y": 852
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:20px'>4 Experiments</p>",
            "id": 61,
            "page": 6,
            "text": "4 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 912
                },
                {
                    "x": 942,
                    "y": 912
                },
                {
                    "x": 942,
                    "y": 963
                },
                {
                    "x": 443,
                    "y": 963
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>4.1 Experimental Settings</p>",
            "id": 62,
            "page": 6,
            "text": "4.1 Experimental Settings"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1002
                },
                {
                    "x": 2111,
                    "y": 1002
                },
                {
                    "x": 2111,
                    "y": 1235
                },
                {
                    "x": 442,
                    "y": 1235
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:18px'>Datasets: We used three publicly available datasets: Cityscapes [71], ADE20K [72] and COCO-<br>Stuff [73]. ADE20K is a scene parsing dataset covering 150 fine-grained semantic concepts consisting<br>of 20210 images. Cityscapes is a driving dataset for semantic segmentation consisting of 5000 fine-<br>annotated high resolution images with 19 categories. COCO-Stuff covers 172 labels and consists of<br>164k images: 118k for training, 5k for validation, 20k for test-dev and 20k for the test-challenge.</p>",
            "id": 63,
            "page": 6,
            "text": "Datasets: We used three publicly available datasets: Cityscapes [71], ADE20K [72] and COCO-\nStuff [73]. ADE20K is a scene parsing dataset covering 150 fine-grained semantic concepts consisting\nof 20210 images. Cityscapes is a driving dataset for semantic segmentation consisting of 5000 fine-\nannotated high resolution images with 19 categories. COCO-Stuff covers 172 labels and consists of\n164k images: 118k for training, 5k for validation, 20k for test-dev and 20k for the test-challenge."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1253
                },
                {
                    "x": 2110,
                    "y": 1253
                },
                {
                    "x": 2110,
                    "y": 1896
                },
                {
                    "x": 441,
                    "y": 1896
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:16px'>Implementation details: We used the mmsegmentation codebase and train on a server with 8 Tesla<br>V100. We pre-train the encoder on the Imagenet-1K dataset and randomly initialize the decoder.<br>During training, we applied data augmentation through random resize with ratio 0.5-2.0, random<br>horizontal flipping, and random cropping to 512 x 512, 1024x 1024, 512 x 512 for ADE20K,<br>Cityscapes and COCO-Stuff, respectively. Following [9] we set crop size to 640 x 640 on ADE20K<br>for our largest model B5. We trained the models using Adam W optimizer for 160K iterations on<br>ADE20K, Cityscapes, and 80K iterations on COCO-Stuff. Exceptionally, for the ablation studies, we<br>trained the models for 40K iterations. We used a batch size of 16 for ADE20K and COCO-Stuff, and<br>a batch size of 8 for Cityscapes. The learning rate was set to an initial value of 0.00006 and then used<br>a \"poly\" LR schedule with factor 1.0 by default. For simplicity, we did not adopt widely-used tricks<br>such as OHEM, auxiliary losses or class balance loss. During evaluation, we rescale the short side<br>of the image to training cropping size and keep the aspect ratio for ADE20K and COCO-Stuff. For<br>Cityscapes, we do inference using sliding window test by cropping 1024 x 1024 windows. We report<br>semantic segmentation performance using mean Intersection over Union (mloU).</p>",
            "id": 64,
            "page": 6,
            "text": "Implementation details: We used the mmsegmentation codebase and train on a server with 8 Tesla\nV100. We pre-train the encoder on the Imagenet-1K dataset and randomly initialize the decoder.\nDuring training, we applied data augmentation through random resize with ratio 0.5-2.0, random\nhorizontal flipping, and random cropping to 512 x 512, 1024x 1024, 512 x 512 for ADE20K,\nCityscapes and COCO-Stuff, respectively. Following [9] we set crop size to 640 x 640 on ADE20K\nfor our largest model B5. We trained the models using Adam W optimizer for 160K iterations on\nADE20K, Cityscapes, and 80K iterations on COCO-Stuff. Exceptionally, for the ablation studies, we\ntrained the models for 40K iterations. We used a batch size of 16 for ADE20K and COCO-Stuff, and\na batch size of 8 for Cityscapes. The learning rate was set to an initial value of 0.00006 and then used\na \"poly\" LR schedule with factor 1.0 by default. For simplicity, we did not adopt widely-used tricks\nsuch as OHEM, auxiliary losses or class balance loss. During evaluation, we rescale the short side\nof the image to training cropping size and keep the aspect ratio for ADE20K and COCO-Stuff. For\nCityscapes, we do inference using sliding window test by cropping 1024 x 1024 windows. We report\nsemantic segmentation performance using mean Intersection over Union (mloU)."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1966
                },
                {
                    "x": 843,
                    "y": 1966
                },
                {
                    "x": 843,
                    "y": 2012
                },
                {
                    "x": 443,
                    "y": 2012
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:16px'>4.2 Ablation Studies</p>",
            "id": 65,
            "page": 6,
            "text": "4.2 Ablation Studies"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2053
                },
                {
                    "x": 2109,
                    "y": 2053
                },
                {
                    "x": 2109,
                    "y": 2560
                },
                {
                    "x": 442,
                    "y": 2560
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:16px'>Influence of the size of model. We first analyze the effect of increasing the size of the encoder on<br>the performance and model efficiency. Figure 1 shows the performance VS. model efficiency for<br>ADE20K as a function of the encoder size and, Table 1a summarizes the results for the three datasets.<br>The first thing to observe here is the size of the decoder compared to the encoder. As shown, for<br>the lightweight model, the decoder has only 0.4M parameters. For MiT-B5 encoder, the decoder<br>only takes up to 4% of the total number of parameters in the model. In terms of performance, we<br>can observe that, overall, increasing the size of the encoder yields consistent improvements on all<br>the datasets. Our lightweight model, SegFormer-B0, is compact and efficient while maintaining a<br>competitive performance, showing that our method is very convenient for real-time applications. On<br>the other hand, our SegFormer-B5, the largest model, achieves state-of-the-art results on all three<br>datasets, showing the potential of our Transformer encoder.</p>",
            "id": 66,
            "page": 6,
            "text": "Influence of the size of model. We first analyze the effect of increasing the size of the encoder on\nthe performance and model efficiency. Figure 1 shows the performance VS. model efficiency for\nADE20K as a function of the encoder size and, Table 1a summarizes the results for the three datasets.\nThe first thing to observe here is the size of the decoder compared to the encoder. As shown, for\nthe lightweight model, the decoder has only 0.4M parameters. For MiT-B5 encoder, the decoder\nonly takes up to 4% of the total number of parameters in the model. In terms of performance, we\ncan observe that, overall, increasing the size of the encoder yields consistent improvements on all\nthe datasets. Our lightweight model, SegFormer-B0, is compact and efficient while maintaining a\ncompetitive performance, showing that our method is very convenient for real-time applications. On\nthe other hand, our SegFormer-B5, the largest model, achieves state-of-the-art results on all three\ndatasets, showing the potential of our Transformer encoder."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2577
                },
                {
                    "x": 2109,
                    "y": 2577
                },
                {
                    "x": 2109,
                    "y": 2901
                },
                {
                    "x": 442,
                    "y": 2901
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:14px'>Influence of C, the MLP decoder channel dimension. We now analyze the influence of the channel<br>dimension C in the MLP decoder, see Section 3.2. In Table 1b we show performance, flops, and<br>parameters as a function of this dimension. We can observe that setting C = 256 provides a<br>very competitive performance and computational cost. The performance increases as C increases;<br>however, it leads to larger and less efficient models. Interestingly, this performance plateaus for<br>channel dimensions wider than 768. Given these results, we choose C = 256 for our real-time<br>models SegFormer-B0, B1 and C = 768 for the rest.</p>",
            "id": 67,
            "page": 6,
            "text": "Influence of C, the MLP decoder channel dimension. We now analyze the influence of the channel\ndimension C in the MLP decoder, see Section 3.2. In Table 1b we show performance, flops, and\nparameters as a function of this dimension. We can observe that setting C = 256 provides a\nvery competitive performance and computational cost. The performance increases as C increases;\nhowever, it leads to larger and less efficient models. Interestingly, this performance plateaus for\nchannel dimensions wider than 768. Given these results, we choose C = 256 for our real-time\nmodels SegFormer-B0, B1 and C = 768 for the rest."
        },
        {
            "bounding_box": [
                {
                    "x": 498,
                    "y": 2965
                },
                {
                    "x": 1262,
                    "y": 2965
                },
                {
                    "x": 1262,
                    "y": 3012
                },
                {
                    "x": 498,
                    "y": 3012
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:22px'>https://ginub.com/oper-mnianlab/mmegrientation</p>",
            "id": 68,
            "page": 6,
            "text": "https://ginub.com/oper-mnianlab/mmegrientation"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3094
                },
                {
                    "x": 1290,
                    "y": 3094
                },
                {
                    "x": 1290,
                    "y": 3130
                },
                {
                    "x": 1259,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='69' style='font-size:14px'>6</footer>",
            "id": 69,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 580,
                    "y": 370
                },
                {
                    "x": 2026,
                    "y": 370
                },
                {
                    "x": 2026,
                    "y": 451
                },
                {
                    "x": 580,
                    "y": 451
                }
            ],
            "category": "caption",
            "html": "<caption id='70' style='font-size:16px'>Table 1: Ablation studies related to model size, encoder and decoder design.<br>Accuracy, parameters and flops as a function of the model size on the three datasets. \"SS\" and \"MS\" means single/multi-scale</caption>",
            "id": 70,
            "page": 7,
            "text": "Table 1: Ablation studies related to model size, encoder and decoder design.\nAccuracy, parameters and flops as a function of the model size on the three datasets. \"SS\" and \"MS\" means single/multi-scale"
        },
        {
            "bounding_box": [
                {
                    "x": 489,
                    "y": 419
                },
                {
                    "x": 2059,
                    "y": 419
                },
                {
                    "x": 2059,
                    "y": 454
                },
                {
                    "x": 489,
                    "y": 454
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:16px'>(a) test.</p>",
            "id": 71,
            "page": 7,
            "text": "(a) test."
        },
        {
            "bounding_box": [
                {
                    "x": 493,
                    "y": 457
                },
                {
                    "x": 2053,
                    "y": 457
                },
                {
                    "x": 2053,
                    "y": 833
                },
                {
                    "x": 493,
                    "y": 833
                }
            ],
            "category": "table",
            "html": "<br><table id='72' style='font-size:18px'><tr><td rowspan=\"2\">Encoder Model Size</td><td colspan=\"2\">Params</td><td colspan=\"2\">ADE20K</td><td colspan=\"2\">Cityscapes</td><td colspan=\"2\">COCO-Stuff</td></tr><tr><td>Encoder</td><td>Decoder</td><td>Flops ↓</td><td>mloU(SS/MS) ↑</td><td>Flops ↓</td><td>mIoU(SS/MS) ↑</td><td>Flops ↓</td><td>mIoU(SS) ↑</td></tr><tr><td>MiT-B0</td><td>3.4</td><td>0.4</td><td>8.4</td><td>37.4/38.0</td><td>125.5</td><td>76.2 / 78.1</td><td>8.4</td><td>35.6</td></tr><tr><td>MiT-B1</td><td>13.1</td><td>0.6</td><td>15.9</td><td>42.2/ 43.1</td><td>243.7</td><td>78.5 / 80.0</td><td>15.9</td><td>40.2</td></tr><tr><td>MiT-B2</td><td>24.2</td><td>3.3</td><td>62.4</td><td>46.5 / 47.5</td><td>717.1</td><td>81.0 / 82.2</td><td>62.4</td><td>44.6</td></tr><tr><td>MiT-B3</td><td>44.0</td><td>3.3</td><td>79.0</td><td>49.4/ 50.0</td><td>962.9</td><td>81.7 / 83.3</td><td>79.0</td><td>45.5</td></tr><tr><td>MiT-B4</td><td>60.8</td><td>3.3</td><td>95.7</td><td>50.3 /51.1</td><td>1240.6</td><td>82.3 / 83.9</td><td>95.7</td><td>46.5</td></tr><tr><td>MiT-B5</td><td>81.4</td><td>3.3</td><td>183.3</td><td>51.0/ 51.8</td><td>1460.4</td><td>82.4 / 84.0</td><td>111.6</td><td>46.7</td></tr></table>",
            "id": 72,
            "page": 7,
            "text": "Encoder Model Size Params ADE20K Cityscapes COCO-Stuff\n Encoder Decoder Flops ↓ mloU(SS/MS) ↑ Flops ↓ mIoU(SS/MS) ↑ Flops ↓ mIoU(SS) ↑\n MiT-B0 3.4 0.4 8.4 37.4/38.0 125.5 76.2 / 78.1 8.4 35.6\n MiT-B1 13.1 0.6 15.9 42.2/ 43.1 243.7 78.5 / 80.0 15.9 40.2\n MiT-B2 24.2 3.3 62.4 46.5 / 47.5 717.1 81.0 / 82.2 62.4 44.6\n MiT-B3 44.0 3.3 79.0 49.4/ 50.0 962.9 81.7 / 83.3 79.0 45.5\n MiT-B4 60.8 3.3 95.7 50.3 /51.1 1240.6 82.3 / 83.9 95.7 46.5\n MiT-B5 81.4 3.3 183.3 51.0/ 51.8 1460.4 82.4 / 84.0 111.6"
        },
        {
            "bounding_box": [
                {
                    "x": 479,
                    "y": 863
                },
                {
                    "x": 2038,
                    "y": 863
                },
                {
                    "x": 2038,
                    "y": 911
                },
                {
                    "x": 479,
                    "y": 911
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>(b) Accuracy as a function of the MLP (c) Mix-FFN vs. positional encoding (PE) for (d) Accuracy on ADE20K of CNN and</p>",
            "id": 73,
            "page": 7,
            "text": "(b) Accuracy as a function of the MLP (c) Mix-FFN vs. positional encoding (PE) for (d) Accuracy on ADE20K of CNN and"
        },
        {
            "bounding_box": [
                {
                    "x": 481,
                    "y": 897
                },
                {
                    "x": 2043,
                    "y": 897
                },
                {
                    "x": 2043,
                    "y": 940
                },
                {
                    "x": 481,
                    "y": 940
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:14px'>dimension C in the decoder on ADE20K. different test resolution on Cityscapes. Transformer encoder with MLP decoder.</p>",
            "id": 74,
            "page": 7,
            "text": "dimension C in the decoder on ADE20K. different test resolution on Cityscapes. Transformer encoder with MLP decoder."
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 943
                },
                {
                    "x": 2036,
                    "y": 943
                },
                {
                    "x": 2036,
                    "y": 1215
                },
                {
                    "x": 486,
                    "y": 1215
                }
            ],
            "category": "table",
            "html": "<br><table id='75' style='font-size:14px'><tr><td rowspan=\"2\">C</td><td rowspan=\"2\">Flops ↓</td><td rowspan=\"2\">Params ↓</td><td rowspan=\"2\">mIoU ↑</td><td rowspan=\"2\">Inf Res</td><td rowspan=\"2\">Enc Type</td><td rowspan=\"2\">mIoU ↑</td><td colspan=\"4\">\"S4\" means stage-4 feature.</td></tr><tr><td>Encoder</td><td>Flops ↓</td><td>Params ↓</td><td>mloU ↑</td></tr><tr><td>256</td><td>25.7</td><td>24.7</td><td>44.9</td><td rowspan=\"3\">768x768 1024x2048</td><td rowspan=\"3\">PE PE</td><td rowspan=\"3\">77.3 74.0</td><td>ResNet50 (S1-4)</td><td>69.2</td><td>29.0</td><td>34.7</td></tr><tr><td>512</td><td>39.8</td><td>25.8</td><td>45.0</td><td>ResNet101 (S1-4)</td><td>88.7</td><td>47.9</td><td>38.7</td></tr><tr><td>768</td><td>62.4</td><td>27.5</td><td>45.4</td><td>ResNeXt101 (S1-4)</td><td>127.5</td><td>86.8</td><td>39.8</td></tr><tr><td>1024</td><td>93.6</td><td>29.6</td><td>45.2</td><td rowspan=\"2\">768x768 1024x2048</td><td rowspan=\"2\">Mix-FFN Mix-FFN</td><td rowspan=\"2\">80.5 79.8</td><td>MiT-B2 (S4)</td><td>22.3</td><td>24.7</td><td>43.1</td></tr><tr><td>2048</td><td>304.4</td><td>43.4</td><td>45.6</td><td>MiT-B2 (S1-4) MiT-B3 (S1-4)</td><td>62.4 79.0</td><td>27.7 47.3</td><td>45.4 48.6</td></tr></table>",
            "id": 75,
            "page": 7,
            "text": "C Flops ↓ Params ↓ mIoU ↑ Inf Res Enc Type mIoU ↑ \"S4\" means stage-4 feature.\n Encoder Flops ↓ Params ↓ mloU ↑\n 256 25.7 24.7 44.9 768x768 1024x2048 PE PE 77.3 74.0 ResNet50 (S1-4) 69.2 29.0 34.7\n 512 39.8 25.8 45.0 ResNet101 (S1-4) 88.7 47.9 38.7\n 768 62.4 27.5 45.4 ResNeXt101 (S1-4) 127.5 86.8 39.8\n 1024 93.6 29.6 45.2 768x768 1024x2048 Mix-FFN Mix-FFN 80.5 79.8 MiT-B2 (S4) 22.3 24.7 43.1\n 2048 304.4 43.4 45.6 MiT-B2 (S1-4) MiT-B3 (S1-4) 62.4 79.0 27.7 47.3"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1231
                },
                {
                    "x": 2105,
                    "y": 1231
                },
                {
                    "x": 2105,
                    "y": 1357
                },
                {
                    "x": 442,
                    "y": 1357
                }
            ],
            "category": "caption",
            "html": "<br><caption id='76' style='font-size:20px'>Table 2: Comparison to state of the art methods on ADE20K and Cityscapes. SegFormer has significant<br>advantages on #Params, #Flops, #Speed and #Accuracy. Note that for SegFormer-B0 we scale the short side of<br>image to { 1024, 768, 640, 512} to get speed-accuracy tradeoffs.</caption>",
            "id": 76,
            "page": 7,
            "text": "Table 2: Comparison to state of the art methods on ADE20K and Cityscapes. SegFormer has significant\nadvantages on #Params, #Flops, #Speed and #Accuracy. Note that for SegFormer-B0 we scale the short side of\nimage to { 1024, 768, 640, 512} to get speed-accuracy tradeoffs."
        },
        {
            "bounding_box": [
                {
                    "x": 482,
                    "y": 1364
                },
                {
                    "x": 2057,
                    "y": 1364
                },
                {
                    "x": 2057,
                    "y": 2254
                },
                {
                    "x": 482,
                    "y": 2254
                }
            ],
            "category": "table",
            "html": "<table id='77' style='font-size:14px'><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Method</td><td rowspan=\"2\">Encoder</td><td rowspan=\"2\">Params ↓</td><td colspan=\"3\">ADE20K</td><td colspan=\"3\">Cityscapes</td></tr><tr><td>Flops ↓</td><td>FPS ↑</td><td>mIoU ↑</td><td>Flops ↓</td><td>FPS ↑</td><td>mIoU ↑</td></tr><tr><td rowspan=\"8\">Real-Time</td><td>FCN [1]</td><td>MobileNetV2</td><td>9.8</td><td>39.6</td><td>64.4</td><td>19.7</td><td>317.1</td><td>14.2</td><td>61.5</td></tr><tr><td>ICNet [11]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>30.3</td><td>67.7</td></tr><tr><td>PSPNet [17]</td><td>MobileNetV2</td><td>13.7</td><td>52.9</td><td>57.7</td><td>29.6</td><td>423.4</td><td>11.2</td><td>70.2</td></tr><tr><td>DeepLabV3+ [20]</td><td>MobileNetV2</td><td>15.4</td><td>69.4</td><td>43.1</td><td>34.0</td><td>555.4</td><td>8.4</td><td>75.2</td></tr><tr><td rowspan=\"4\">SegFormer (Ours)</td><td rowspan=\"4\">MiT-B0</td><td rowspan=\"4\">3.8</td><td>8.4</td><td>50.5</td><td>37.4</td><td>125.5</td><td>15.2</td><td>76.2</td></tr><tr><td>-</td><td>、</td><td>-</td><td>51.7</td><td>26.3</td><td>75.3</td></tr><tr><td>-</td><td>-</td><td>-</td><td>31.5</td><td>37.1</td><td>73.7</td></tr><tr><td>-</td><td>-</td><td>-</td><td>17.7</td><td>47.6</td><td>71.9</td></tr><tr><td rowspan=\"13\">Real-Time Non</td><td>FCN [1]</td><td>ResNet-101</td><td>68.6</td><td>275.7</td><td>14.8</td><td>41.4</td><td>2203.3</td><td>1.2</td><td>76.6</td></tr><tr><td>EncNet [24]</td><td>ResNet-101</td><td>55.1</td><td>218.8</td><td>14.9</td><td>44.7</td><td>1748.0</td><td>1.3</td><td>76.9</td></tr><tr><td>PSPNet [17]</td><td>ResNet-101</td><td>68.1</td><td>256.4</td><td>15.3</td><td>44.4</td><td>2048.9</td><td>1.2</td><td>78.5</td></tr><tr><td>CCNet [41]</td><td>ResNet-101</td><td>68.9</td><td>278.4</td><td>14.1</td><td>45.2</td><td>2224.8</td><td>1.0</td><td>80.2</td></tr><tr><td>DeeplabV3+ [20]</td><td>ResNet-101</td><td>62.7</td><td>255.1</td><td>14.1</td><td>44.1</td><td>2032.3</td><td>1.2</td><td>80.9</td></tr><tr><td>OCRNet [23]</td><td>HRNet-W48</td><td>70.5</td><td>164.8</td><td>17.0</td><td>45.6</td><td>1296.8</td><td>4.2</td><td>81.1</td></tr><tr><td>GSCNN [35]</td><td>WideResNet38</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>80.8</td></tr><tr><td>Axial-DeepLab [74]</td><td>AxialResNet-XL</td><td>-</td><td>-</td><td>-</td><td>-</td><td>2446.8</td><td>-</td><td>81.1</td></tr><tr><td>Dynamic Routing [75]</td><td>Dynamic-L33-PSP</td><td>-</td><td>-</td><td>-</td><td>-</td><td>270.0</td><td>-</td><td>80.7</td></tr><tr><td>Auto-Deeplab [50] SETR</td><td>NAS-F48-ASPP</td><td>-</td><td>-</td><td>-</td><td>44.0</td><td>695.0</td><td>-</td><td>80.3</td></tr><tr><td>[7]</td><td>ViT-Large</td><td>318.3</td><td>-</td><td>5.4</td><td>50.2</td><td>-</td><td>0.5</td><td>82.2</td></tr><tr><td>SegFormer (Ours)</td><td>MiT-B4</td><td>64.1</td><td>95.7</td><td>15.4</td><td>51.1</td><td>1240.6</td><td>3.0</td><td>83.8</td></tr><tr><td>SegFormer (Ours)</td><td>MiT-B5</td><td>84.7</td><td>183.3</td><td>9.8</td><td>51.8</td><td>1447.6</td><td>2.5</td><td>84.0</td></tr></table>",
            "id": 77,
            "page": 7,
            "text": "Method Encoder Params ↓ ADE20K Cityscapes\n Flops ↓ FPS ↑ mIoU ↑ Flops ↓ FPS ↑ mIoU ↑\n Real-Time FCN [1] MobileNetV2 9.8 39.6 64.4 19.7 317.1 14.2 61.5\n ICNet [11] - - - - - - 30.3 67.7\n PSPNet [17] MobileNetV2 13.7 52.9 57.7 29.6 423.4 11.2 70.2\n DeepLabV3+ [20] MobileNetV2 15.4 69.4 43.1 34.0 555.4 8.4 75.2\n SegFormer (Ours) MiT-B0 3.8 8.4 50.5 37.4 125.5 15.2 76.2\n - 、 - 51.7 26.3 75.3\n - - - 31.5 37.1 73.7\n - - - 17.7 47.6 71.9\n Real-Time Non FCN [1] ResNet-101 68.6 275.7 14.8 41.4 2203.3 1.2 76.6\n EncNet [24] ResNet-101 55.1 218.8 14.9 44.7 1748.0 1.3 76.9\n PSPNet [17] ResNet-101 68.1 256.4 15.3 44.4 2048.9 1.2 78.5\n CCNet [41] ResNet-101 68.9 278.4 14.1 45.2 2224.8 1.0 80.2\n DeeplabV3+ [20] ResNet-101 62.7 255.1 14.1 44.1 2032.3 1.2 80.9\n OCRNet [23] HRNet-W48 70.5 164.8 17.0 45.6 1296.8 4.2 81.1\n GSCNN [35] WideResNet38 - - - - - - 80.8\n Axial-DeepLab [74] AxialResNet-XL - - - - 2446.8 - 81.1\n Dynamic Routing [75] Dynamic-L33-PSP - - - - 270.0 - 80.7\n Auto-Deeplab [50] SETR NAS-F48-ASPP - - - 44.0 695.0 - 80.3\n [7] ViT-Large 318.3 - 5.4 50.2 - 0.5 82.2\n SegFormer (Ours) MiT-B4 64.1 95.7 15.4 51.1 1240.6 3.0 83.8\n SegFormer (Ours) MiT-B5 84.7 183.3 9.8 51.8 1447.6 2.5"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2280
                },
                {
                    "x": 2107,
                    "y": 2280
                },
                {
                    "x": 2107,
                    "y": 2512
                },
                {
                    "x": 441,
                    "y": 2512
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:22px'>Mix-FFN VS. Positional Encoder (PE). In this experiment, we analyze the effect of removing the<br>positional encoding in the Transformer encoder in favor of using the proposed Mix-FFN. To this<br>end, we train Transformer encoders with a positional encoding (PE) and the proposed Mix-FFN<br>and perform inference on Cityscapes with two different image resolutions: 768 x768 using a sliding<br>window, and 1024x 2048 using the whole image.</p>",
            "id": 78,
            "page": 7,
            "text": "Mix-FFN VS. Positional Encoder (PE). In this experiment, we analyze the effect of removing the\npositional encoding in the Transformer encoder in favor of using the proposed Mix-FFN. To this\nend, we train Transformer encoders with a positional encoding (PE) and the proposed Mix-FFN\nand perform inference on Cityscapes with two different image resolutions: 768 x768 using a sliding\nwindow, and 1024x 2048 using the whole image."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2533
                },
                {
                    "x": 2107,
                    "y": 2533
                },
                {
                    "x": 2107,
                    "y": 2808
                },
                {
                    "x": 441,
                    "y": 2808
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:22px'>Table 1c shows the results for this experiment. As shown, for a given resolution, our approach using<br>Mix-FFN clearly outperforms using a positional encoding. Moreover, our approach is less sensitive<br>to differences in the test resolution: the accuracy drops 3.3% when using a positional encoding with a<br>lower resolution. In contrast, when we use the proposed Mix-FFN the performance drop is reduced<br>to only 0.7%. From these results, we can conclude using the proposed Mix-FFN produces better and<br>more robust encoders than those using positional encoding.</p>",
            "id": 79,
            "page": 7,
            "text": "Table 1c shows the results for this experiment. As shown, for a given resolution, our approach using\nMix-FFN clearly outperforms using a positional encoding. Moreover, our approach is less sensitive\nto differences in the test resolution: the accuracy drops 3.3% when using a positional encoding with a\nlower resolution. In contrast, when we use the proposed Mix-FFN the performance drop is reduced\nto only 0.7%. From these results, we can conclude using the proposed Mix-FFN produces better and\nmore robust encoders than those using positional encoding."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2828
                },
                {
                    "x": 2108,
                    "y": 2828
                },
                {
                    "x": 2108,
                    "y": 3012
                },
                {
                    "x": 442,
                    "y": 3012
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:22px'>Effective receptive field evaluation. In Section 3.2, we argued that our MLP decoder benefits<br>from Transformers having a larger effective receptive field compared to other CNN models. To<br>quantify this effect, in this experiment, we compare the performance of our MLP-decoder when<br>used with CNN-based encoders such as ResNet or ResNeXt. As shown in Table 1d, coupling our</p>",
            "id": 80,
            "page": 7,
            "text": "Effective receptive field evaluation. In Section 3.2, we argued that our MLP decoder benefits\nfrom Transformers having a larger effective receptive field compared to other CNN models. To\nquantify this effect, in this experiment, we compare the performance of our MLP-decoder when\nused with CNN-based encoders such as ResNet or ResNeXt. As shown in Table 1d, coupling our"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3094
                },
                {
                    "x": 1288,
                    "y": 3094
                },
                {
                    "x": 1288,
                    "y": 3127
                },
                {
                    "x": 1261,
                    "y": 3127
                }
            ],
            "category": "footer",
            "html": "<footer id='81' style='font-size:20px'>7</footer>",
            "id": 81,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 305
                },
                {
                    "x": 2108,
                    "y": 305
                },
                {
                    "x": 2108,
                    "y": 582
                },
                {
                    "x": 443,
                    "y": 582
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:22px'>MLP-decoder with a CNN-based encoder yields a significantly lower accuracy compared to coupling<br>it with the proposed Transformer encoder. Intuitively, as a CNN has a smaller receptive field than the<br>Transformer (see the analysis in Section 3.2), the MLP-decoder is not enough for global reasoning.<br>In contrast, coupling our Transformer encoder with the MLP decoder leads to the best performance.<br>Moreover, for Transformer encoder, it is necessary to combine low-level local features and high-level<br>non-local features instead of only high-level feature.</p>",
            "id": 82,
            "page": 8,
            "text": "MLP-decoder with a CNN-based encoder yields a significantly lower accuracy compared to coupling\nit with the proposed Transformer encoder. Intuitively, as a CNN has a smaller receptive field than the\nTransformer (see the analysis in Section 3.2), the MLP-decoder is not enough for global reasoning.\nIn contrast, coupling our Transformer encoder with the MLP decoder leads to the best performance.\nMoreover, for Transformer encoder, it is necessary to combine low-level local features and high-level\nnon-local features instead of only high-level feature."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 641
                },
                {
                    "x": 1240,
                    "y": 641
                },
                {
                    "x": 1240,
                    "y": 687
                },
                {
                    "x": 444,
                    "y": 687
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:20px'>4.3 Comparison to state of the art methods</p>",
            "id": 83,
            "page": 8,
            "text": "4.3 Comparison to state of the art methods"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 727
                },
                {
                    "x": 2105,
                    "y": 727
                },
                {
                    "x": 2105,
                    "y": 814
                },
                {
                    "x": 443,
                    "y": 814
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:22px'>We now compare our results with existing approaches on the ADE20K [72], Cityscapes [71] and<br>COCO-Stuff [73] datasets.</p>",
            "id": 84,
            "page": 8,
            "text": "We now compare our results with existing approaches on the ADE20K [72], Cityscapes [71] and\nCOCO-Stuff [73] datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 817
                },
                {
                    "x": 2108,
                    "y": 817
                },
                {
                    "x": 2108,
                    "y": 1045
                },
                {
                    "x": 443,
                    "y": 1045
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:20px'>ADE20K and Cityscapes: Table 2 summarizes our results including parameters, FLOPS, latency,<br>and accuracy for ADE20K and Cityscapes. In the top part of the table, we report real-time approaches<br>where we include state-of-the-art methods and our results using the MiT-B0 lightweight encoder. In<br>the bottom part, we focus on performance and report the results of our approach and related works<br>using stronger encoders.</p>",
            "id": 85,
            "page": 8,
            "text": "ADE20K and Cityscapes: Table 2 summarizes our results including parameters, FLOPS, latency,\nand accuracy for ADE20K and Cityscapes. In the top part of the table, we report real-time approaches\nwhere we include state-of-the-art methods and our results using the MiT-B0 lightweight encoder. In\nthe bottom part, we focus on performance and report the results of our approach and related works\nusing stronger encoders."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1066
                },
                {
                    "x": 2107,
                    "y": 1066
                },
                {
                    "x": 2107,
                    "y": 1343
                },
                {
                    "x": 442,
                    "y": 1343
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:22px'>As shown, on ADE20K, SegFormer-B0 yields 37.4% mIoU using only 3.8M parameters and 8.4G<br>FLOPs, outperforming all other real-time counterparts in terms of parameters, flops, and latency. For<br>instance, compared to DeeplabV3+ (MobileNetV2), SegFormer-B0 is 7.4 FPS, which is faster and<br>keeps 3.4% better mIoU. Moreover, SegFormer-B5 outperforms all other approaches, including the<br>previous best SETR, and establishes a new state-of-the-art of 51.8%, which is 1.6% mIoU better than<br>SETR while being significantly more efficient.</p>",
            "id": 86,
            "page": 8,
            "text": "As shown, on ADE20K, SegFormer-B0 yields 37.4% mIoU using only 3.8M parameters and 8.4G\nFLOPs, outperforming all other real-time counterparts in terms of parameters, flops, and latency. For\ninstance, compared to DeeplabV3+ (MobileNetV2), SegFormer-B0 is 7.4 FPS, which is faster and\nkeeps 3.4% better mIoU. Moreover, SegFormer-B5 outperforms all other approaches, including the\nprevious best SETR, and establishes a new state-of-the-art of 51.8%, which is 1.6% mIoU better than\nSETR while being significantly more efficient."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1365
                },
                {
                    "x": 1242,
                    "y": 1365
                },
                {
                    "x": 1242,
                    "y": 1954
                },
                {
                    "x": 443,
                    "y": 1954
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:22px'>As also shown in Table 2, our results also hold<br>on Cityscapes. SegFormer-B0 yields 15.2 FPS<br>and 76.2% mIoU (the shorter side of input im-<br>age being 1024), which represents a 1.3% mIoU<br>improvement and a 2x speedup compared to<br>DeeplabV3+. Moreover, with the shorter side<br>of input image being 512, SegFormer-B0 runs<br>at 47.6 FPS and yields 71.9% mIoU, which is<br>17.3 FPS faster and 4.2% better than ICNet.<br>SegFormer-B5 archives the best IoU of 84.0%,<br>outperforming all existing methods by at least<br>1.8% mloU, and it runs 5 x faster and 4 x<br>smaller than SETR [7].</p>",
            "id": 87,
            "page": 8,
            "text": "As also shown in Table 2, our results also hold\non Cityscapes. SegFormer-B0 yields 15.2 FPS\nand 76.2% mIoU (the shorter side of input im-\nage being 1024), which represents a 1.3% mIoU\nimprovement and a 2x speedup compared to\nDeeplabV3+. Moreover, with the shorter side\nof input image being 512, SegFormer-B0 runs\nat 47.6 FPS and yields 71.9% mIoU, which is\n17.3 FPS faster and 4.2% better than ICNet.\nSegFormer-B5 archives the best IoU of 84.0%,\noutperforming all existing methods by at least\n1.8% mloU, and it runs 5 x faster and 4 x\nsmaller than SETR [7]."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1977
                },
                {
                    "x": 2107,
                    "y": 1977
                },
                {
                    "x": 2107,
                    "y": 2297
                },
                {
                    "x": 443,
                    "y": 2297
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:22px'>On Cityscapes test set, we follow the common setting [20] and merge the validation images to the<br>train set and report results using Imagenet-1K pre-training and also using Mapillary Vistas [76].<br>As reported in Table 3, using only Cityscapes fine data and Imagenet-1K pre-training, our method<br>achieves 82.2% mloU outperforming all other methods including SETR, which uses ImageNet-22K<br>pre-training and the additional Cityscapes coarse data. Using Mapillary pre-training, our sets a<br>new state-of-the-art result of 83.1% mIoU. Figure 4 shows qualitative results on Cityscapes, where<br>SegFormer provides better details than SETR and smoother predictions than Deeplab V3+.</p>",
            "id": 88,
            "page": 8,
            "text": "On Cityscapes test set, we follow the common setting [20] and merge the validation images to the\ntrain set and report results using Imagenet-1K pre-training and also using Mapillary Vistas [76].\nAs reported in Table 3, using only Cityscapes fine data and Imagenet-1K pre-training, our method\nachieves 82.2% mloU outperforming all other methods including SETR, which uses ImageNet-22K\npre-training and the additional Cityscapes coarse data. Using Mapillary pre-training, our sets a\nnew state-of-the-art result of 83.1% mIoU. Figure 4 shows qualitative results on Cityscapes, where\nSegFormer provides better details than SETR and smoother predictions than Deeplab V3+."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1371
                },
                {
                    "x": 2105,
                    "y": 1371
                },
                {
                    "x": 2105,
                    "y": 1504
                },
                {
                    "x": 1270,
                    "y": 1504
                }
            ],
            "category": "caption",
            "html": "<br><caption id='89' style='font-size:14px'>Table 3: Comparison to state of the art methods on Cityscapes<br>test set. IM-1K, IM-22K, Coarse and MV refer to the ImageNet-1K,<br>ImageNet-22K, Cityscapes coarse set and Mapillary Vistas. SegFormer<br>outperforms the compared methods with equal or less extra data.</caption>",
            "id": 89,
            "page": 8,
            "text": "Table 3: Comparison to state of the art methods on Cityscapes\ntest set. IM-1K, IM-22K, Coarse and MV refer to the ImageNet-1K,\nImageNet-22K, Cityscapes coarse set and Mapillary Vistas. SegFormer\noutperforms the compared methods with equal or less extra data."
        },
        {
            "bounding_box": [
                {
                    "x": 1287,
                    "y": 1532
                },
                {
                    "x": 2085,
                    "y": 1532
                },
                {
                    "x": 2085,
                    "y": 1888
                },
                {
                    "x": 1287,
                    "y": 1888
                }
            ],
            "category": "table",
            "html": "<table id='90' style='font-size:14px'><tr><td>Method</td><td>Encoder</td><td>Extra Data</td><td>mloU</td></tr><tr><td>PSPNet [17]</td><td>ResNet-101</td><td>IM-1K</td><td>78.4</td></tr><tr><td>PSANet[43]</td><td>ResNet-101</td><td>IM-1K</td><td>80.1</td></tr><tr><td>CCNet [41]</td><td>ResNet-101</td><td>IM-1K</td><td>81.9</td></tr><tr><td>OCNet [21]</td><td>ResNet-101</td><td>IM-1K</td><td>80.1</td></tr><tr><td>Axial-DeepLab [74]</td><td>AxiaiResNet-XL</td><td>IM-1K</td><td>79.9</td></tr><tr><td>SETR [7]</td><td>ViT</td><td>IM-22K</td><td>81.0</td></tr><tr><td>SETR [7]</td><td>ViT</td><td>IM-22K, Coarse</td><td>81.6</td></tr><tr><td>SegFormer</td><td>MiT-B5</td><td>IM-1K</td><td>82.2</td></tr><tr><td>SegFormer</td><td>MiT-B5</td><td>IM-1K, MV</td><td>83.1</td></tr></table>",
            "id": 90,
            "page": 8,
            "text": "Method Encoder Extra Data mloU\n PSPNet [17] ResNet-101 IM-1K 78.4\n PSANet[43] ResNet-101 IM-1K 80.1\n CCNet [41] ResNet-101 IM-1K 81.9\n OCNet [21] ResNet-101 IM-1K 80.1\n Axial-DeepLab [74] AxiaiResNet-XL IM-1K 79.9\n SETR [7] ViT IM-22K 81.0\n SETR [7] ViT IM-22K, Coarse 81.6\n SegFormer MiT-B5 IM-1K 82.2\n SegFormer MiT-B5 IM-1K, MV"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2320
                },
                {
                    "x": 1379,
                    "y": 2320
                },
                {
                    "x": 1379,
                    "y": 2676
                },
                {
                    "x": 442,
                    "y": 2676
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:18px'>COCO-Stuff. Finally, we evaluate SegFormer on the<br>full COCO-Stuff dataset. For comparison, as exist-<br>ing methods do not provide results on this dataset,<br>we reproduce the most representative methods such<br>as DeeplabV3+, OCRNet, and SETR. In this case, the<br>flops on this dataset are the same as those reported for<br>ADE20K. As shown in Table 4, SegFormer-B5 reaches<br>46.7% mIoU with only 84.7M parameters, which is 0.9%</p>",
            "id": 91,
            "page": 8,
            "text": "COCO-Stuff. Finally, we evaluate SegFormer on the\nfull COCO-Stuff dataset. For comparison, as exist-\ning methods do not provide results on this dataset,\nwe reproduce the most representative methods such\nas DeeplabV3+, OCRNet, and SETR. In this case, the\nflops on this dataset are the same as those reported for\nADE20K. As shown in Table 4, SegFormer-B5 reaches\n46.7% mIoU with only 84.7M parameters, which is 0.9%"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2684
                },
                {
                    "x": 2104,
                    "y": 2684
                },
                {
                    "x": 2104,
                    "y": 2774
                },
                {
                    "x": 443,
                    "y": 2774
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:20px'>better and 4x smaller than SETR. In summary, these results demonstrate the superiority of SegFormer<br>in semantic segmentation in terms of accuracy, computation cost, and model size.</p>",
            "id": 92,
            "page": 8,
            "text": "better and 4x smaller than SETR. In summary, these results demonstrate the superiority of SegFormer\nin semantic segmentation in terms of accuracy, computation cost, and model size."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2834
                },
                {
                    "x": 1152,
                    "y": 2834
                },
                {
                    "x": 1152,
                    "y": 2882
                },
                {
                    "x": 444,
                    "y": 2882
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:22px'>4.4 Robustness to natural corruptions</p>",
            "id": 93,
            "page": 8,
            "text": "4.4 Robustness to natural corruptions"
        },
        {
            "bounding_box": [
                {
                    "x": 1403,
                    "y": 2326
                },
                {
                    "x": 2105,
                    "y": 2326
                },
                {
                    "x": 2105,
                    "y": 2391
                },
                {
                    "x": 1403,
                    "y": 2391
                }
            ],
            "category": "caption",
            "html": "<br><caption id='94' style='font-size:14px'>Table 4: Results on COCO-Stuff full dataset containing<br>all 164K images from COCO 2017 and covers 172 classes.</caption>",
            "id": 94,
            "page": 8,
            "text": "Table 4: Results on COCO-Stuff full dataset containing\nall 164K images from COCO 2017 and covers 172 classes."
        },
        {
            "bounding_box": [
                {
                    "x": 1414,
                    "y": 2421
                },
                {
                    "x": 2091,
                    "y": 2421
                },
                {
                    "x": 2091,
                    "y": 2643
                },
                {
                    "x": 1414,
                    "y": 2643
                }
            ],
            "category": "table",
            "html": "<table id='95' style='font-size:16px'><tr><td>Method</td><td>Encoder</td><td>Params</td><td>mloU</td></tr><tr><td>DeeplabV3+ [20]</td><td>ResNet50</td><td>43.7</td><td>38.4</td></tr><tr><td>OCRNet [23]</td><td>HRNet-W48</td><td>70.5</td><td>42.3</td></tr><tr><td>SETR [7]</td><td>ViT</td><td>305.7</td><td>45.8</td></tr><tr><td>SegFormer</td><td>MiT-B5</td><td>84.7</td><td>46.7</td></tr></table>",
            "id": 95,
            "page": 8,
            "text": "Method Encoder Params mloU\n DeeplabV3+ [20] ResNet50 43.7 38.4\n OCRNet [23] HRNet-W48 70.5 42.3\n SETR [7] ViT 305.7 45.8\n SegFormer MiT-B5 84.7"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2921
                },
                {
                    "x": 2106,
                    "y": 2921
                },
                {
                    "x": 2106,
                    "y": 3013
                },
                {
                    "x": 443,
                    "y": 3013
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:20px'>Model robustness is important for many safety-critical tasks such as autonomous driving [77]. In this<br>experiment, we evaluate the robustness of SegFormer to common corruptions and perturbations. To</p>",
            "id": 96,
            "page": 8,
            "text": "Model robustness is important for many safety-critical tasks such as autonomous driving [77]. In this\nexperiment, we evaluate the robustness of SegFormer to common corruptions and perturbations. To"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3095
                },
                {
                    "x": 1288,
                    "y": 3095
                },
                {
                    "x": 1288,
                    "y": 3130
                },
                {
                    "x": 1260,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='97' style='font-size:18px'>8</footer>",
            "id": 97,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 455,
                    "y": 307
                },
                {
                    "x": 2084,
                    "y": 307
                },
                {
                    "x": 2084,
                    "y": 573
                },
                {
                    "x": 455,
                    "y": 573
                }
            ],
            "category": "figure",
            "html": "<figure><img id='98' style='font-size:14px' alt=\"SETR SegFormer DeepLabv3+ SegFormer\n내역\" data-coord=\"top-left:(455,307); bottom-right:(2084,573)\" /></figure>",
            "id": 98,
            "page": 9,
            "text": "SETR SegFormer DeepLabv3+ SegFormer\n내역"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 591
                },
                {
                    "x": 2107,
                    "y": 591
                },
                {
                    "x": 2107,
                    "y": 718
                },
                {
                    "x": 442,
                    "y": 718
                }
            ],
            "category": "caption",
            "html": "<br><caption id='99' style='font-size:16px'>Figure 4: Qualitative results on Cityscapes. Compared to SETR, our SegFormer predicts masks with substan-<br>tially finer details near object boundaries. Compared to DeeplabV3+, SegFormer reduces long-range errors as<br>highlighted in red. Best viewed in screen.</caption>",
            "id": 99,
            "page": 9,
            "text": "Figure 4: Qualitative results on Cityscapes. Compared to SETR, our SegFormer predicts masks with substan-\ntially finer details near object boundaries. Compared to DeeplabV3+, SegFormer reduces long-range errors as\nhighlighted in red. Best viewed in screen."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 768
                },
                {
                    "x": 2106,
                    "y": 768
                },
                {
                    "x": 2106,
                    "y": 951
                },
                {
                    "x": 443,
                    "y": 951
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:18px'>this end, we follow [77] and generate Cityscapes-C, which expands the Cityscapes validation set with<br>16 types of algorithmically generated corruptions from noise, blur, weather and digital categories. We<br>compare our method to variants of DeeplabV3+ and other methods as reported in [77]. The results<br>for this experiment are summarized in Table 5.</p>",
            "id": 100,
            "page": 9,
            "text": "this end, we follow [77] and generate Cityscapes-C, which expands the Cityscapes validation set with\n16 types of algorithmically generated corruptions from noise, blur, weather and digital categories. We\ncompare our method to variants of DeeplabV3+ and other methods as reported in [77]. The results\nfor this experiment are summarized in Table 5."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 975
                },
                {
                    "x": 2107,
                    "y": 975
                },
                {
                    "x": 2107,
                    "y": 1110
                },
                {
                    "x": 442,
                    "y": 1110
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:16px'>Our method significantly outperforms previous methods, yielding a relative improvement of up to<br>588% on Gaussian Noise and up to 295% on snow weather. The results indicate the strong robustness<br>of SegFormer, which we envision to benefit safety-critical applications where robustness is important.</p>",
            "id": 101,
            "page": 9,
            "text": "Our method significantly outperforms previous methods, yielding a relative improvement of up to\n588% on Gaussian Noise and up to 295% on snow weather. The results indicate the strong robustness\nof SegFormer, which we envision to benefit safety-critical applications where robustness is important."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1132
                },
                {
                    "x": 2108,
                    "y": 1132
                },
                {
                    "x": 2108,
                    "y": 1215
                },
                {
                    "x": 444,
                    "y": 1215
                }
            ],
            "category": "caption",
            "html": "<br><caption id='102' style='font-size:14px'>Table 5: Main results on Cityscapes-C. \"DLv3+\", \"MBv2\", \"R\" and \"X\" refer to DeepLabv3+, MobileNetv2,<br>ResNet and Xception. The mIoUs of compared methods are reported from [77].</caption>",
            "id": 102,
            "page": 9,
            "text": "Table 5: Main results on Cityscapes-C. \"DLv3+\", \"MBv2\", \"R\" and \"X\" refer to DeepLabv3+, MobileNetv2,\nResNet and Xception. The mIoUs of compared methods are reported from [77]."
        },
        {
            "bounding_box": [
                {
                    "x": 457,
                    "y": 1229
                },
                {
                    "x": 2100,
                    "y": 1229
                },
                {
                    "x": 2100,
                    "y": 1778
                },
                {
                    "x": 457,
                    "y": 1778
                }
            ],
            "category": "table",
            "html": "<br><table id='103' style='font-size:14px'><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Clean</td><td colspan=\"4\">Blur</td><td colspan=\"4\">Noise</td><td colspan=\"4\">Digital</td><td colspan=\"4\">Weather</td></tr><tr><td>Motion</td><td>Defoc</td><td>Glass</td><td>Gauss</td><td>Gauss</td><td>Impul</td><td>Shot</td><td>Speck</td><td>Bright</td><td>Contr</td><td>Satur</td><td>JPEG</td><td>Snow</td><td>Spatt</td><td>Fog</td><td>Frost</td></tr><tr><td>DLv3+ (MBv2)</td><td>72.0</td><td>53.5</td><td>49.0</td><td>45.3</td><td>49.1</td><td>6.4</td><td>7.0</td><td>6.6</td><td>16.6</td><td>51.7</td><td>46.7</td><td>32.4</td><td>27.2</td><td>13.7</td><td>38.9</td><td>47.4</td><td>17.3</td></tr><tr><td>DLv3+ (R50)</td><td>76.6</td><td>58.5</td><td>56.6</td><td>47.2</td><td>57.7</td><td>6.5</td><td>7.2</td><td>10.0</td><td>31.1</td><td>58.2</td><td>54.7</td><td>41.3</td><td>27.4</td><td>12.0</td><td>42.0</td><td>55.9</td><td>22.8</td></tr><tr><td>DLv3+ (R101)</td><td>77.1</td><td>59.1</td><td>56.3</td><td>47.7</td><td>57.3</td><td>13.2</td><td>13.9</td><td>16.3</td><td>36.9</td><td>59.2</td><td>54.5</td><td>41.5</td><td>37.4</td><td>11.9</td><td>47.8</td><td>55.1</td><td>22.7</td></tr><tr><td>DLv3+ (X41)</td><td>77.8</td><td>61.6</td><td>54.9</td><td>51.0</td><td>54.7</td><td>17.0</td><td>17.3</td><td>21.6</td><td>43.7</td><td>63.6</td><td>56.9</td><td>51.7</td><td>38.5</td><td>18.2</td><td>46.6</td><td>57.6</td><td>20.6</td></tr><tr><td>DLv3+ (X65)</td><td>78.4</td><td>63.9</td><td>59.1</td><td>52.8</td><td>59.2</td><td>15.0</td><td>10.6</td><td>19.8</td><td>42.4</td><td>65.9</td><td>59.1</td><td>46.1</td><td>31.4</td><td>19.3</td><td>50.7</td><td>63.6</td><td>23.8</td></tr><tr><td>DLv3+ (X71)</td><td>78.6</td><td>64.1</td><td>60.9</td><td>52.0</td><td>60.4</td><td>14.9</td><td>10.8</td><td>19.4</td><td>41.2</td><td>68.0</td><td>58.7</td><td>47.1</td><td>40.2</td><td>18.8</td><td>50.4</td><td>64.1</td><td>20.2</td></tr><tr><td>ICNet</td><td>65.9</td><td>45.8</td><td>44.6</td><td>47.4</td><td>44.7</td><td>8.4</td><td>8.4</td><td>10.6</td><td>27.9</td><td>41.0</td><td>33.1</td><td>27.5</td><td>34.0</td><td>6.3</td><td>30.5</td><td>27.3</td><td>11.0</td></tr><tr><td>FCN8s</td><td>66.7</td><td>42.7</td><td>31.1</td><td>37.0</td><td>34.1</td><td>6.7</td><td>5.7</td><td>7.8</td><td>24.9</td><td>53.3</td><td>39.0</td><td>36.0</td><td>21.2</td><td>11.3</td><td>31.6</td><td>37.6</td><td>19.7</td></tr><tr><td>DilatedNet</td><td>68.6</td><td>44.4</td><td>36.3</td><td>32.5</td><td>38.4</td><td>15.6</td><td>14.0</td><td>18.4</td><td>32.7</td><td>52.7</td><td>32.6</td><td>38.1</td><td>29.1</td><td>12.5</td><td>32.3</td><td>34.7</td><td>19.2</td></tr><tr><td>ResNet-38</td><td>77.5</td><td>54.6</td><td>45.1</td><td>43.3</td><td>47.2</td><td>13.7</td><td>16.0</td><td>18.2</td><td>38.3</td><td>60.0</td><td>50.6</td><td>46.9</td><td>14.7</td><td>13.5</td><td>45.9</td><td>52.9</td><td>22.2</td></tr><tr><td>PSPNet</td><td>78.8</td><td>59.8</td><td>53.2</td><td>44.4</td><td>53.9</td><td>11.0</td><td>15.4</td><td>15.4</td><td>34.2</td><td>60.4</td><td>51.8</td><td>30.6</td><td>21.4</td><td>8.4</td><td>42.7</td><td>34.4</td><td>16.2</td></tr><tr><td>GSCNN</td><td>80.9</td><td>58.9</td><td>58.4</td><td>41.9</td><td>60.1</td><td>5.5</td><td>2.6</td><td>6.8</td><td>24.7</td><td>75.9</td><td>61.9</td><td>70.7</td><td>12.0</td><td>12.4</td><td>47.3</td><td>67.9</td><td>32.6</td></tr><tr><td>SegFormer-B5</td><td>82.4</td><td>69.1</td><td>68.6</td><td>64.1</td><td>69.8</td><td>57.8</td><td>63.4</td><td>52.3</td><td>72.8</td><td>81.0</td><td>77.7</td><td>80.1</td><td>58.8</td><td>40.7</td><td>68.4</td><td>78.5</td><td>49.9</td></tr></table>",
            "id": 103,
            "page": 9,
            "text": "Method Clean Blur Noise Digital Weather\n Motion Defoc Glass Gauss Gauss Impul Shot Speck Bright Contr Satur JPEG Snow Spatt Fog Frost\n DLv3+ (MBv2) 72.0 53.5 49.0 45.3 49.1 6.4 7.0 6.6 16.6 51.7 46.7 32.4 27.2 13.7 38.9 47.4 17.3\n DLv3+ (R50) 76.6 58.5 56.6 47.2 57.7 6.5 7.2 10.0 31.1 58.2 54.7 41.3 27.4 12.0 42.0 55.9 22.8\n DLv3+ (R101) 77.1 59.1 56.3 47.7 57.3 13.2 13.9 16.3 36.9 59.2 54.5 41.5 37.4 11.9 47.8 55.1 22.7\n DLv3+ (X41) 77.8 61.6 54.9 51.0 54.7 17.0 17.3 21.6 43.7 63.6 56.9 51.7 38.5 18.2 46.6 57.6 20.6\n DLv3+ (X65) 78.4 63.9 59.1 52.8 59.2 15.0 10.6 19.8 42.4 65.9 59.1 46.1 31.4 19.3 50.7 63.6 23.8\n DLv3+ (X71) 78.6 64.1 60.9 52.0 60.4 14.9 10.8 19.4 41.2 68.0 58.7 47.1 40.2 18.8 50.4 64.1 20.2\n ICNet 65.9 45.8 44.6 47.4 44.7 8.4 8.4 10.6 27.9 41.0 33.1 27.5 34.0 6.3 30.5 27.3 11.0\n FCN8s 66.7 42.7 31.1 37.0 34.1 6.7 5.7 7.8 24.9 53.3 39.0 36.0 21.2 11.3 31.6 37.6 19.7\n DilatedNet 68.6 44.4 36.3 32.5 38.4 15.6 14.0 18.4 32.7 52.7 32.6 38.1 29.1 12.5 32.3 34.7 19.2\n ResNet-38 77.5 54.6 45.1 43.3 47.2 13.7 16.0 18.2 38.3 60.0 50.6 46.9 14.7 13.5 45.9 52.9 22.2\n PSPNet 78.8 59.8 53.2 44.4 53.9 11.0 15.4 15.4 34.2 60.4 51.8 30.6 21.4 8.4 42.7 34.4 16.2\n GSCNN 80.9 58.9 58.4 41.9 60.1 5.5 2.6 6.8 24.7 75.9 61.9 70.7 12.0 12.4 47.3 67.9 32.6\n SegFormer-B5 82.4 69.1 68.6 64.1 69.8 57.8 63.4 52.3 72.8 81.0 77.7 80.1 58.8 40.7 68.4 78.5"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1859
                },
                {
                    "x": 767,
                    "y": 1859
                },
                {
                    "x": 767,
                    "y": 1912
                },
                {
                    "x": 443,
                    "y": 1912
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:20px'>5 Conclusion</p>",
            "id": 104,
            "page": 9,
            "text": "5 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1961
                },
                {
                    "x": 2108,
                    "y": 1961
                },
                {
                    "x": 2108,
                    "y": 2328
                },
                {
                    "x": 442,
                    "y": 2328
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:16px'>In this paper, we present SegFormer, a simple, clean yet powerful semantic segmentation method<br>which contains a positional-encoding-free, hierarchical Transformer encoder and a lightweight All-<br>MLP decoder. It avoids common complex designs in previous methods, leading to both high efficiency<br>and performance. SegFormer not only achieves new state of the art results on common datasets,<br>but also shows strong zero-shot robustness. We hope our method can serve as a solid baseline for<br>semantic segmentation and motivate further research. One limitation is that although our smallest<br>3.7M parameters model is smaller than the known CNN's model, it is unclear whether it can work<br>well in a chip of edge device with only 100k memory. We leave it for future work.</p>",
            "id": 105,
            "page": 9,
            "text": "In this paper, we present SegFormer, a simple, clean yet powerful semantic segmentation method\nwhich contains a positional-encoding-free, hierarchical Transformer encoder and a lightweight All-\nMLP decoder. It avoids common complex designs in previous methods, leading to both high efficiency\nand performance. SegFormer not only achieves new state of the art results on common datasets,\nbut also shows strong zero-shot robustness. We hope our method can serve as a solid baseline for\nsemantic segmentation and motivate further research. One limitation is that although our smallest\n3.7M parameters model is smaller than the known CNN's model, it is unclear whether it can work\nwell in a chip of edge device with only 100k memory. We leave it for future work."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2395
                },
                {
                    "x": 846,
                    "y": 2395
                },
                {
                    "x": 846,
                    "y": 2450
                },
                {
                    "x": 445,
                    "y": 2450
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:22px'>Acknowledgement</p>",
            "id": 106,
            "page": 9,
            "text": "Acknowledgement"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2498
                },
                {
                    "x": 2105,
                    "y": 2498
                },
                {
                    "x": 2105,
                    "y": 2591
                },
                {
                    "x": 443,
                    "y": 2591
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:18px'>We thank Ding Liang, Zhe Chen and Yaojun Liu for insightful discussion without which this paper<br>would not be possible.</p>",
            "id": 107,
            "page": 9,
            "text": "We thank Ding Liang, Zhe Chen and Yaojun Liu for insightful discussion without which this paper\nwould not be possible."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2660
                },
                {
                    "x": 987,
                    "y": 2660
                },
                {
                    "x": 987,
                    "y": 2713
                },
                {
                    "x": 444,
                    "y": 2713
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:20px'>A Details of MiT Series</p>",
            "id": 108,
            "page": 9,
            "text": "A Details of MiT Series"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2761
                },
                {
                    "x": 2106,
                    "y": 2761
                },
                {
                    "x": 2106,
                    "y": 2855
                },
                {
                    "x": 441,
                    "y": 2855
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:16px'>In this section, we list some important hyper-parameters of our Mix Transformer (MiT) encoder. By<br>changing these parameters, we can easily scale up our encoder from B0 to B5.</p>",
            "id": 109,
            "page": 9,
            "text": "In this section, we list some important hyper-parameters of our Mix Transformer (MiT) encoder. By\nchanging these parameters, we can easily scale up our encoder from B0 to B5."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2879
                },
                {
                    "x": 1567,
                    "y": 2879
                },
                {
                    "x": 1567,
                    "y": 2922
                },
                {
                    "x": 446,
                    "y": 2922
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:16px'>In summary, the hyper-parameters of our MiT are listed as follows:</p>",
            "id": 110,
            "page": 9,
            "text": "In summary, the hyper-parameters of our MiT are listed as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 558,
                    "y": 2964
                },
                {
                    "x": 1692,
                    "y": 2964
                },
                {
                    "x": 1692,
                    "y": 3012
                },
                {
                    "x": 558,
                    "y": 3012
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>· Ki: the patch size of the overlapping patch embedding in Stage i;</p>",
            "id": 111,
            "page": 9,
            "text": "· Ki: the patch size of the overlapping patch embedding in Stage i;"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3092
                },
                {
                    "x": 1290,
                    "y": 3092
                },
                {
                    "x": 1290,
                    "y": 3128
                },
                {
                    "x": 1259,
                    "y": 3128
                }
            ],
            "category": "footer",
            "html": "<footer id='112' style='font-size:16px'>9</footer>",
            "id": 112,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 563,
                    "y": 301
                },
                {
                    "x": 1729,
                    "y": 301
                },
                {
                    "x": 1729,
                    "y": 767
                },
                {
                    "x": 563,
                    "y": 767
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:14px'>· Si: the stride of the overlapping patch embedding in Stage i;<br>· Pi: the padding size of the overlapping patch embedding in Stage i;<br>· Ci: the channel number of the output of Stage 2;<br>· Li: the number of encoder layers in Stage i;<br>· Ri: the reduction ratio of the Efficient Self-Attention in Stage i;<br>· Ni: the head number of the Efficient Self-Attention in Stage i;<br>· Ei: the expansion ratio of the feed-forward layer [78] in Stage i;</p>",
            "id": 113,
            "page": 10,
            "text": "· Si: the stride of the overlapping patch embedding in Stage i;\n· Pi: the padding size of the overlapping patch embedding in Stage i;\n· Ci: the channel number of the output of Stage 2;\n· Li: the number of encoder layers in Stage i;\n· Ri: the reduction ratio of the Efficient Self-Attention in Stage i;\n· Ni: the head number of the Efficient Self-Attention in Stage i;\n· Ei: the expansion ratio of the feed-forward layer [78] in Stage i;"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 803
                },
                {
                    "x": 2109,
                    "y": 803
                },
                {
                    "x": 2109,
                    "y": 944
                },
                {
                    "x": 442,
                    "y": 944
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:14px'>Table 6 shows the detailed information of our MiT series. To facilitate efficient discussion, we assign<br>the code name B0 to B5 for MiT encoder, where B0 is the smallest model designed for real-time,<br>while B5 is the largest model designed for high performance.</p>",
            "id": 114,
            "page": 10,
            "text": "Table 6 shows the detailed information of our MiT series. To facilitate efficient discussion, we assign\nthe code name B0 to B5 for MiT encoder, where B0 is the smallest model designed for real-time,\nwhile B5 is the largest model designed for high performance."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1007
                },
                {
                    "x": 1520,
                    "y": 1007
                },
                {
                    "x": 1520,
                    "y": 1061
                },
                {
                    "x": 443,
                    "y": 1061
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:20px'>B More Qualitative Results on Mask Predictions</p>",
            "id": 115,
            "page": 10,
            "text": "B More Qualitative Results on Mask Predictions"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1110
                },
                {
                    "x": 2106,
                    "y": 1110
                },
                {
                    "x": 2106,
                    "y": 1201
                },
                {
                    "x": 441,
                    "y": 1201
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:14px'>In Figure 5, we present more qualitative results on Cityscapes, ADE20K and COCO-Stuff, compared<br>with SETR and DeepLabV3+.</p>",
            "id": 116,
            "page": 10,
            "text": "In Figure 5, we present more qualitative results on Cityscapes, ADE20K and COCO-Stuff, compared\nwith SETR and DeepLabV3+."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1224
                },
                {
                    "x": 2107,
                    "y": 1224
                },
                {
                    "x": 2107,
                    "y": 1451
                },
                {
                    "x": 441,
                    "y": 1451
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:16px'>Compared to SETR, our SegFormer predicts masks with significantly finer details near object<br>boundaries because our Transformer encoder can capture much higher resolution features than<br>SETR, which preserves more detailed texture information. Compared to DeepLabV3+, SegFormer<br>reduces long-range errors benefit from the larger effective receptive field of Transformer encoder than<br>ConvNet.</p>",
            "id": 117,
            "page": 10,
            "text": "Compared to SETR, our SegFormer predicts masks with significantly finer details near object\nboundaries because our Transformer encoder can capture much higher resolution features than\nSETR, which preserves more detailed texture information. Compared to DeepLabV3+, SegFormer\nreduces long-range errors benefit from the larger effective receptive field of Transformer encoder than\nConvNet."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1518
                },
                {
                    "x": 1548,
                    "y": 1518
                },
                {
                    "x": 1548,
                    "y": 1572
                },
                {
                    "x": 443,
                    "y": 1572
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:20px'>C More Visualization on Effective Receptive Field</p>",
            "id": 118,
            "page": 10,
            "text": "C More Visualization on Effective Receptive Field"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1620
                },
                {
                    "x": 2107,
                    "y": 1620
                },
                {
                    "x": 2107,
                    "y": 1851
                },
                {
                    "x": 442,
                    "y": 1851
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:14px'>In Figure 6, we select some representative images and effective receptive field (ERF) of DeepLabV3+<br>and SegFormer. Beyond larger ERF, the ERF of SegFormer is more sensitive to the context of<br>the image. We see SegFormer's ERF learned the pattern of roads, cars, and buildings, while<br>DeepLabV3+'s ERF shows a relatively fixed pattern. The results also indicate that our Transformer<br>encoder has a stronger feature extraction ability than ConvNets.</p>",
            "id": 119,
            "page": 10,
            "text": "In Figure 6, we select some representative images and effective receptive field (ERF) of DeepLabV3+\nand SegFormer. Beyond larger ERF, the ERF of SegFormer is more sensitive to the context of\nthe image. We see SegFormer's ERF learned the pattern of roads, cars, and buildings, while\nDeepLabV3+'s ERF shows a relatively fixed pattern. The results also indicate that our Transformer\nencoder has a stronger feature extraction ability than ConvNets."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1915
                },
                {
                    "x": 1959,
                    "y": 1915
                },
                {
                    "x": 1959,
                    "y": 1970
                },
                {
                    "x": 443,
                    "y": 1970
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:22px'>D More Comparison of Deeplab V3+ and SegFormer on Cityscapes-C</p>",
            "id": 120,
            "page": 10,
            "text": "D More Comparison of Deeplab V3+ and SegFormer on Cityscapes-C"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2015
                },
                {
                    "x": 2108,
                    "y": 2015
                },
                {
                    "x": 2108,
                    "y": 2154
                },
                {
                    "x": 441,
                    "y": 2154
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:14px'>In this section, we detailed show the zero-shot robustness compared with SegFormer and DeepLabV3+.<br>Following [77], we test 3 severities for 4 kinds of \"Noise\" and 5 severities for the rest 12 kinds of<br>corruptions and perturbations.</p>",
            "id": 121,
            "page": 10,
            "text": "In this section, we detailed show the zero-shot robustness compared with SegFormer and DeepLabV3+.\nFollowing [77], we test 3 severities for 4 kinds of \"Noise\" and 5 severities for the rest 12 kinds of\ncorruptions and perturbations."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2176
                },
                {
                    "x": 2108,
                    "y": 2176
                },
                {
                    "x": 2108,
                    "y": 2360
                },
                {
                    "x": 442,
                    "y": 2360
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:18px'>As shown in Figure 7, with severity increase, DeepLabV3+ shows a considerable performance<br>degradation. In contrast, the performance of SegFormer is relatively stable. Moreover, SegFormer<br>has significant advantages over DeepLabV3+ on all corruptions/perturbations and all severities,<br>demonstrating excellent zero-shot robustness.</p>",
            "id": 122,
            "page": 10,
            "text": "As shown in Figure 7, with severity increase, DeepLabV3+ shows a considerable performance\ndegradation. In contrast, the performance of SegFormer is relatively stable. Moreover, SegFormer\nhas significant advantages over DeepLabV3+ on all corruptions/perturbations and all severities,\ndemonstrating excellent zero-shot robustness."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='123' style='font-size:14px'>10</footer>",
            "id": 123,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 524,
                    "y": 524
                },
                {
                    "x": 2028,
                    "y": 524
                },
                {
                    "x": 2028,
                    "y": 1700
                },
                {
                    "x": 524,
                    "y": 1700
                }
            ],
            "category": "table",
            "html": "<table id='124' style='font-size:14px'><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Output Size</td><td rowspan=\"2\">Layer Name</td><td colspan=\"6\">Mix Transformer</td></tr><tr><td>B0</td><td>B1</td><td>B2</td><td>B3</td><td>B4</td><td>B5</td></tr><tr><td rowspan=\"3\">Stage 1</td><td rowspan=\"3\">W H4 x 4</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"6\">K1 = 7; S1 = 4; P1 = 3</td></tr><tr><td>C1 = 32</td><td colspan=\"5\">C1 = 64</td></tr><tr><td>Transformer Encoder</td><td>R1 = 8 N1 = 1 E1 = 8 L1 = 2</td><td>R1 = 8 N1 = 1 E1 = 8 L1 = 2</td><td>R1 = 8 N1 = 1 E1 = 8 L1 = 3</td><td>R1 = 8 N1 = 1 E1 = 8 L1 = 3</td><td>R1 = 8 N1 = 1 E1 = 8 L1 = 3</td><td>R1 = 8 N1 = 1 E1 = 4 L1 = 3</td></tr><tr><td rowspan=\"3\">Stage 2</td><td rowspan=\"3\">W 풍 x 8</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"6\">K2 = 3; S2 = 2; P2 = 1</td></tr><tr><td>C2 = 64</td><td colspan=\"5\">C2 = 128</td></tr><tr><td>Transformer Encoder</td><td>R2 = 4 N2 = 2 E2 = 8 L2 = 2</td><td>R2 = 4 N2 = 2 E2 = 8 L2 = 2</td><td>R2 = 4 N2 = 2 E2 = 8 L2 = 3</td><td>R2 = 4 N2 = 2 E2 = 8 L2 = 3</td><td>R2 = 4 N2 = 2 E2 = 8 L2 = 8</td><td>R2 = 4 N2 = 2 E2 = 4 L2 = 6</td></tr><tr><td rowspan=\"3\">Stage 3</td><td rowspan=\"3\">H W x 16 16</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"6\">K3 = 3; S3 = 2; P3 = 1</td></tr><tr><td>C3 = 160</td><td colspan=\"5\">C3 = 320</td></tr><tr><td>Transformer Encoder</td><td>R3 = 2 N3 = 5 E3 = 4 L3 = 2</td><td>R3 = 2 N3 = 5 E3 = 4 L3 = 2</td><td>R3 = 2 N3 = 5 E3 = 4 L3 = 6</td><td>R3 = 2 N3 = 5 E3 = 4 L3 = 18</td><td>R3 = 2 N3 = 5 E3 = 4 L3 = 27</td><td>R3 = 2 N3 = 5 E3 = 4 L3 = 40</td></tr><tr><td rowspan=\"3\">Stage 4</td><td rowspan=\"3\">H W x 32 32</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"6\">K4 = 3; S4 = 2; P4 = 1</td></tr><tr><td>C4 = 256</td><td colspan=\"5\">C4 = 512</td></tr><tr><td>Transformer Encoder</td><td>R4 = 1 N4 = 8 E4 = 4 L4 = 2</td><td>R4 = 1 N4 = 8 E4 = 4 L4 = 2</td><td>R4 = 1 N4 = 8 E4 = 4 L4 = 3</td><td>R4 = 1 N4 = 8 E4 = 4 L4 = 3</td><td>R4 = 1 N4 = 8 E4 = 4 L4 = 3</td><td>R4 = 1 N4 = 8 E4 = 4 L4 = 3</td></tr></table>",
            "id": 124,
            "page": 11,
            "text": "Output Size Layer Name Mix Transformer\n B0 B1 B2 B3 B4 B5\n Stage 1 W H4 x 4 Overlapping Patch Embedding K1 = 7; S1 = 4; P1 = 3\n C1 = 32 C1 = 64\n Transformer Encoder R1 = 8 N1 = 1 E1 = 8 L1 = 2 R1 = 8 N1 = 1 E1 = 8 L1 = 2 R1 = 8 N1 = 1 E1 = 8 L1 = 3 R1 = 8 N1 = 1 E1 = 8 L1 = 3 R1 = 8 N1 = 1 E1 = 8 L1 = 3 R1 = 8 N1 = 1 E1 = 4 L1 = 3\n Stage 2 W 풍 x 8 Overlapping Patch Embedding K2 = 3; S2 = 2; P2 = 1\n C2 = 64 C2 = 128\n Transformer Encoder R2 = 4 N2 = 2 E2 = 8 L2 = 2 R2 = 4 N2 = 2 E2 = 8 L2 = 2 R2 = 4 N2 = 2 E2 = 8 L2 = 3 R2 = 4 N2 = 2 E2 = 8 L2 = 3 R2 = 4 N2 = 2 E2 = 8 L2 = 8 R2 = 4 N2 = 2 E2 = 4 L2 = 6\n Stage 3 H W x 16 16 Overlapping Patch Embedding K3 = 3; S3 = 2; P3 = 1\n C3 = 160 C3 = 320\n Transformer Encoder R3 = 2 N3 = 5 E3 = 4 L3 = 2 R3 = 2 N3 = 5 E3 = 4 L3 = 2 R3 = 2 N3 = 5 E3 = 4 L3 = 6 R3 = 2 N3 = 5 E3 = 4 L3 = 18 R3 = 2 N3 = 5 E3 = 4 L3 = 27 R3 = 2 N3 = 5 E3 = 4 L3 = 40\n Stage 4 H W x 32 32 Overlapping Patch Embedding K4 = 3; S4 = 2; P4 = 1\n C4 = 256 C4 = 512\n Transformer Encoder R4 = 1 N4 = 8 E4 = 4 L4 = 2 R4 = 1 N4 = 8 E4 = 4 L4 = 2 R4 = 1 N4 = 8 E4 = 4 L4 = 3 R4 = 1 N4 = 8 E4 = 4 L4 = 3 R4 = 1 N4 = 8 E4 = 4 L4 = 3"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1699
                },
                {
                    "x": 2106,
                    "y": 1699
                },
                {
                    "x": 2106,
                    "y": 1840
                },
                {
                    "x": 442,
                    "y": 1840
                }
            ],
            "category": "caption",
            "html": "<br><caption id='125' style='font-size:22px'>Table 6: Detailed settings of MiT series. Our design follows the principles of ResNet [12]. (1) the<br>channel dimension increase while the spatial resolution shrink with the layer goes deeper. (2) Stage 3<br>is assigned to most of the computation cost.</caption>",
            "id": 125,
            "page": 11,
            "text": "Table 6: Detailed settings of MiT series. Our design follows the principles of ResNet [12]. (1) the\nchannel dimension increase while the spatial resolution shrink with the layer goes deeper. (2) Stage 3\nis assigned to most of the computation cost."
        },
        {
            "bounding_box": [
                {
                    "x": 877,
                    "y": 2360
                },
                {
                    "x": 1669,
                    "y": 2360
                },
                {
                    "x": 1669,
                    "y": 2696
                },
                {
                    "x": 877,
                    "y": 2696
                }
            ],
            "category": "table",
            "html": "<table id='126' style='font-size:18px'><tr><td>Method</td><td>GFLOPs</td><td>Params (M)</td><td>Top 1</td></tr><tr><td>MiT-B0</td><td>0.6</td><td>3.7</td><td>70.5</td></tr><tr><td>MiT-B1</td><td>2.1</td><td>14.0</td><td>78.7</td></tr><tr><td>MiT-B2</td><td>4.0</td><td>25.4</td><td>81.6</td></tr><tr><td>MiT-B3</td><td>6.9</td><td>45.2</td><td>83.1</td></tr><tr><td>MiT-B4</td><td>10.1</td><td>62.6</td><td>83.6</td></tr><tr><td>MiT-B5</td><td>11.8</td><td>82.0</td><td>83.8</td></tr></table>",
            "id": 126,
            "page": 11,
            "text": "Method GFLOPs Params (M) Top 1\n MiT-B0 0.6 3.7 70.5\n MiT-B1 2.1 14.0 78.7\n MiT-B2 4.0 25.4 81.6\n MiT-B3 6.9 45.2 83.1\n MiT-B4 10.1 62.6 83.6\n MiT-B5 11.8 82.0"
        },
        {
            "bounding_box": [
                {
                    "x": 984,
                    "y": 2694
                },
                {
                    "x": 1567,
                    "y": 2694
                },
                {
                    "x": 1567,
                    "y": 2734
                },
                {
                    "x": 984,
                    "y": 2734
                }
            ],
            "category": "caption",
            "html": "<br><caption id='127' style='font-size:20px'>Table 7: Mix Transformer Encoder</caption>",
            "id": 127,
            "page": 11,
            "text": "Table 7: Mix Transformer Encoder"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3093
                },
                {
                    "x": 1295,
                    "y": 3093
                },
                {
                    "x": 1295,
                    "y": 3131
                },
                {
                    "x": 1253,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='128' style='font-size:16px'>11</footer>",
            "id": 128,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 465,
                    "y": 1018
                },
                {
                    "x": 2071,
                    "y": 1018
                },
                {
                    "x": 2071,
                    "y": 2188
                },
                {
                    "x": 465,
                    "y": 2188
                }
            ],
            "category": "figure",
            "html": "<figure><img id='129' style='font-size:20px' alt=\"SegFormer SETR DeepLabV3+\" data-coord=\"top-left:(465,1018); bottom-right:(2071,2188)\" /></figure>",
            "id": 129,
            "page": 12,
            "text": "SegFormer SETR DeepLabV3+"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2222
                },
                {
                    "x": 2107,
                    "y": 2222
                },
                {
                    "x": 2107,
                    "y": 2307
                },
                {
                    "x": 443,
                    "y": 2307
                }
            ],
            "category": "caption",
            "html": "<caption id='130' style='font-size:14px'>Figure 5: Qualitative results on Cityscapes, ADE20K and COCO-Stuff. First row: Cityscapes. Second row:<br>ADE20K. Third row: COCO-Stuff. Zoom in for best view.</caption>",
            "id": 130,
            "page": 12,
            "text": "Figure 5: Qualitative results on Cityscapes, ADE20K and COCO-Stuff. First row: Cityscapes. Second row:\nADE20K. Third row: COCO-Stuff. Zoom in for best view."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3129
                },
                {
                    "x": 1253,
                    "y": 3129
                }
            ],
            "category": "footer",
            "html": "<footer id='131' style='font-size:16px'>12</footer>",
            "id": 131,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 458,
                    "y": 754
                },
                {
                    "x": 2108,
                    "y": 754
                },
                {
                    "x": 2108,
                    "y": 2519
                },
                {
                    "x": 458,
                    "y": 2519
                }
            ],
            "category": "figure",
            "html": "<figure><img id='132' style='font-size:14px' alt=\"Stage-1 Stage-2 Stage-3 Stage-4 Head\nDeepLabv3+\nSegFormer\nDeepLabv3+\nSegFormer\nDeepLabv3+\nSegFormer\" data-coord=\"top-left:(458,754); bottom-right:(2108,2519)\" /></figure>",
            "id": 132,
            "page": 13,
            "text": "Stage-1 Stage-2 Stage-3 Stage-4 Head\nDeepLabv3+\nSegFormer\nDeepLabv3+\nSegFormer\nDeepLabv3+\nSegFormer"
        },
        {
            "bounding_box": [
                {
                    "x": 480,
                    "y": 2519
                },
                {
                    "x": 2062,
                    "y": 2519
                },
                {
                    "x": 2062,
                    "y": 2558
                },
                {
                    "x": 480,
                    "y": 2558
                }
            ],
            "category": "caption",
            "html": "<br><caption id='133' style='font-size:16px'>Figure 6: Effective Receptive Field on Cityscapes. ERFs of the four stages and the decoder heads of both architectures are visualized.</caption>",
            "id": 133,
            "page": 13,
            "text": "Figure 6: Effective Receptive Field on Cityscapes. ERFs of the four stages and the decoder heads of both architectures are visualized."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3091
                },
                {
                    "x": 1298,
                    "y": 3091
                },
                {
                    "x": 1298,
                    "y": 3130
                },
                {
                    "x": 1253,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='134' style='font-size:20px'>13</footer>",
            "id": 134,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 448,
                    "y": 810
                },
                {
                    "x": 2104,
                    "y": 810
                },
                {
                    "x": 2104,
                    "y": 2388
                },
                {
                    "x": 448,
                    "y": 2388
                }
            ],
            "category": "figure",
            "html": "<figure><img id='135' style='font-size:14px' alt=\"Gaussian Noise Shot Noise Impluse Noise Speckle Noise\n80.0 80.0 80.0 100.0\n80.0\n60.0 60.0 60.0\n60.0\n40.0 40.0 40.0\n40.0\n20.0 20.0 20.0\n20.0\n0.0 0.0 0.0 0.0\n1 2 3 1 2 3 1 2 3 1 2 3\nMotion blur Defocus Blur Glass Blur Gaussian Blur\n90.0 80.0 80.0 100.0\n80.0\n60.0\n70.0 60.0\n60.0\n40.0\n40.0\n50.0 40.0\n20.0\n20.0\n30.0 20.0 0.0 0.0\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nForst\nSnow Spatter Fog\n80.0 100.0 84.0 80.0\n80.0\n60.0 78.0 60.0\n60.0\n40.0 72.0 40.0\n40.0\n20.0 66.0 20.0\n20.0\n0.0 0.0 60.0 0.0\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nBrightness Contrast Saturate JPEG_compression\n86.0 100.0 100.0 80.0\n80.0\n82.0 60.0\n80.0\n60.0\n78.0 40.0\n40.0\n60.0\n74.0 20.0\n20.0\n70.0 0.0 40.0 0.0\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\" data-coord=\"top-left:(448,810); bottom-right:(2104,2388)\" /></figure>",
            "id": 135,
            "page": 14,
            "text": "Gaussian Noise Shot Noise Impluse Noise Speckle Noise\n80.0 80.0 80.0 100.0\n80.0\n60.0 60.0 60.0\n60.0\n40.0 40.0 40.0\n40.0\n20.0 20.0 20.0\n20.0\n0.0 0.0 0.0 0.0\n1 2 3 1 2 3 1 2 3 1 2 3\nMotion blur Defocus Blur Glass Blur Gaussian Blur\n90.0 80.0 80.0 100.0\n80.0\n60.0\n70.0 60.0\n60.0\n40.0\n40.0\n50.0 40.0\n20.0\n20.0\n30.0 20.0 0.0 0.0\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nForst\nSnow Spatter Fog\n80.0 100.0 84.0 80.0\n80.0\n60.0 78.0 60.0\n60.0\n40.0 72.0 40.0\n40.0\n20.0 66.0 20.0\n20.0\n0.0 0.0 60.0 0.0\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nBrightness Contrast Saturate JPEG_compression\n86.0 100.0 100.0 80.0\n80.0\n82.0 60.0\n80.0\n60.0\n78.0 40.0\n40.0\n60.0\n74.0 20.0\n20.0\n70.0 0.0 40.0 0.0\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2428
                },
                {
                    "x": 2106,
                    "y": 2428
                },
                {
                    "x": 2106,
                    "y": 2535
                },
                {
                    "x": 440,
                    "y": 2535
                }
            ],
            "category": "caption",
            "html": "<caption id='136' style='font-size:16px'>Figure 7: Comparison of zero shot robustness on Cityscapes-C between SegFormer and DeepLabV3+. Blue line is SegFormer and orange<br>line is DeepLabV3+. X-Axis means corrupt severity and Y-Axis is mIoU. Following[77], we test 3 severities for \"Noise\" and 5 severities for<br>the rest.</caption>",
            "id": 136,
            "page": 14,
            "text": "Figure 7: Comparison of zero shot robustness on Cityscapes-C between SegFormer and DeepLabV3+. Blue line is SegFormer and orange\nline is DeepLabV3+. X-Axis means corrupt severity and Y-Axis is mIoU. Following[77], we test 3 severities for \"Noise\" and 5 severities for\nthe rest."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='137' style='font-size:20px'>14</footer>",
            "id": 137,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 301
                },
                {
                    "x": 688,
                    "y": 301
                },
                {
                    "x": 688,
                    "y": 352
                },
                {
                    "x": 444,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:20px'>References</p>",
            "id": 138,
            "page": 15,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 450,
                    "y": 371
                },
                {
                    "x": 2119,
                    "y": 371
                },
                {
                    "x": 2119,
                    "y": 3019
                },
                {
                    "x": 450,
                    "y": 3019
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:14px'>[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-<br>tion. In CVPR, 2015. 1, 2, 7<br>[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic<br>image segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015. 1, 2<br>[3] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas<br>Mueller, R Manmatha, et al. ResNest: Split-attention networks. arXiv, 2020. 1<br>[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:<br>Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs.<br>TPAMI, 2017. 2<br>[5] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016. 2,<br>5<br>[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas<br>Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth<br>16x16 words: Transformers for image recognition at scale. arXiv, 2020. 2, 3<br>[7] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng<br>Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence<br>perspective with transformers. CVPR, 2021. 2. 3, 5 , 7, 8<br>[8] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and<br>Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.<br>arXiv, 2021. 2, 3, 4<br>[9] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin<br>transformer: Hierarchical vision transformer using shifted windows. arXiv, 2021. 2, 3, 6<br>[10] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua<br>Shen. Twins: Revisiting spatial attention design in vision transformers. arXiv, 2021. 2, 3<br>[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic<br>segmentation on high-resolution images. In ECCV, 2018. 2, 7<br>[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.<br>In CVPR, 2016. 2, 11<br>[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional<br>neural networks. NeurIPS, 2012.<br>[14] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-<br>tion. arXiv, 2014.<br>[15] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In CVPR, 2019.<br>[16] Wenhai Wang, Xiang Li, Tong Lu, and Jian Yang. Mixed link networks. In IJCAI, 2018. 2<br>[17] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing<br>network. In CVPR, 2017. 2, 7, 8<br>[18] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. Denseaspp for semantic segmentation in<br>street scenes. In CVPR, 2018. 5<br>[19] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters-improve semantic<br>segmentation by global convolutional network. In CVPR, 2017. 2, 5<br>[20] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder<br>with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 2, 5, 7, 8<br>[21] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene parsing. arXiv, 2018. 2, 8<br>[22] Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context prior for<br>scene segmentation. In CVPR, 2020.</p>",
            "id": 139,
            "page": 15,
            "text": "[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In CVPR, 2015. 1, 2, 7\n[2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic\nimage segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015. 1, 2\n[3] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas\nMueller, R Manmatha, et al. ResNest: Split-attention networks. arXiv, 2020. 1\n[4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs.\nTPAMI, 2017. 2\n[5] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016. 2,\n5\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv, 2020. 2, 3\n[7] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. CVPR, 2021. 2. 3, 5 , 7, 8\n[8] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv, 2021. 2, 3, 4\n[9] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv, 2021. 2, 3, 6\n[10] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua\nShen. Twins: Revisiting spatial attention design in vision transformers. arXiv, 2021. 2, 3\n[11] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic\nsegmentation on high-resolution images. In ECCV, 2018. 2, 7\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016. 2, 11\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional\nneural networks. NeurIPS, 2012.\n[14] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. arXiv, 2014.\n[15] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In CVPR, 2019.\n[16] Wenhai Wang, Xiang Li, Tong Lu, and Jian Yang. Mixed link networks. In IJCAI, 2018. 2\n[17] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\nnetwork. In CVPR, 2017. 2, 7, 8\n[18] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. Denseaspp for semantic segmentation in\nstreet scenes. In CVPR, 2018. 5\n[19] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters-improve semantic\nsegmentation by global convolutional network. In CVPR, 2017. 2, 5\n[20] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In ECCV, 2018. 2, 5, 7, 8\n[21] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene parsing. arXiv, 2018. 2, 8\n[22] Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context prior for\nscene segmentation. In CVPR, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='140' style='font-size:16px'>15</footer>",
            "id": 140,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 296
                },
                {
                    "x": 2118,
                    "y": 296
                },
                {
                    "x": 2118,
                    "y": 3013
                },
                {
                    "x": 444,
                    "y": 3013
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>[23] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation.<br>arXiv, 2019. 7, 8<br>[24] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit<br>Agrawal. Context encoding for semantic segmentation. In CVPR, 2018. 7<br>[25] Yizhou Zhou, Xiaoyan Sun, Zheng-Jun Zha, and Wenjun Zeng. Context-reinforced semantic segmentation.<br>In CVPR, 2019.<br>[26] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for<br>high-resolution semantic segmentation. In CVPR, 2017.<br>[27] Rudra PK Poudel, Ujwal Bonde, Stephan Liwicki, and Christopher Zach. Contextnet: Exploring context<br>and detail for semantic segmentation in real-time. arXiv, 2018.<br>[28] Tianyi Wu, Sheng Tang, Rui Zhang, and Yongdong Zhang. Cgnet: A light-weight context guided network<br>for semantic segmentation. arXiv, 2018.<br>[29] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for<br>semantic segmentation. In CVPR, 2019. 3<br>[30] Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, and Gang Wang. Boundary-aware<br>feature propagation for scene segmentation. In ICCV, 2019. 3<br>[31] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Semantic segmentation with boundary neural fields.<br>In CVPR, 2016.<br>[32] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, and Yunhai<br>Tong. Improving semantic segmentation via decoupled body and edge supervision. arxiv, 2020.<br>[33] Yuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang. Segfix: Model-agnostic boundary refinement for<br>segmentation. In ECCV, 2020.<br>[34] Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, and Long<br>Quan. Joint semantic segmentation and boundary detection using iterative pyramid contexts. In CVPR,<br>2020.<br>[35] Towaki Takikawa, David Acuna, Varun Jampani, and Sanja Fidler. Gated-scnn: Gated shape cnns for<br>semantic segmentation. In ICCV, 2019. 7<br>[36] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Learning a discrimina-<br>tive feature network for semantic segmentation. In CVPR, 2018.<br>[37] Liang-Chieh Chen, Jonathan T Barron, George Papandreou, Kevin Murphy, and Alan L Yuille. Semantic<br>image segmentation with task-specific edge detection using cnns and a discriminatively trained domain<br>transform. In CVPR, 2016. 3<br>[38] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention<br>network for scene segmentation. In CVPR, 2019. 3<br>[39] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,<br>2018.<br>[40] Zilong Zhong, Zhong Qiu Lin, Rene Bidart, Xiaodan Hu, Ibrahim Ben Daya, Zhifeng Li, Wei-Shi Zheng,<br>Jonathan Li, and Alexander Wong. Squeeze-and-attention networks for semantic segmentation. In CVPR,<br>2020.<br>[41] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:<br>Criss-cross attention for semantic segmentation. In ICCV, 2019. 7, 8<br>[42] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang. Pyramid attention network for semantic segmenta-<br>tion. arXiv, 2018.<br>[43] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet:<br>Point-wise spatial attention network for scene parsing. In ECCV, 2018. 8<br>[44] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-maximization<br>attention networks for semantic segmentation. In ICCV, 2019.</p>",
            "id": 141,
            "page": 16,
            "text": "[23] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation.\narXiv, 2019. 7, 8\n[24] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit\nAgrawal. Context encoding for semantic segmentation. In CVPR, 2018. 7\n[25] Yizhou Zhou, Xiaoyan Sun, Zheng-Jun Zha, and Wenjun Zeng. Context-reinforced semantic segmentation.\nIn CVPR, 2019.\n[26] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for\nhigh-resolution semantic segmentation. In CVPR, 2017.\n[27] Rudra PK Poudel, Ujwal Bonde, Stephan Liwicki, and Christopher Zach. Contextnet: Exploring context\nand detail for semantic segmentation in real-time. arXiv, 2018.\n[28] Tianyi Wu, Sheng Tang, Rui Zhang, and Yongdong Zhang. Cgnet: A light-weight context guided network\nfor semantic segmentation. arXiv, 2018.\n[29] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for\nsemantic segmentation. In CVPR, 2019. 3\n[30] Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann, and Gang Wang. Boundary-aware\nfeature propagation for scene segmentation. In ICCV, 2019. 3\n[31] Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Semantic segmentation with boundary neural fields.\nIn CVPR, 2016.\n[32] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, and Yunhai\nTong. Improving semantic segmentation via decoupled body and edge supervision. arxiv, 2020.\n[33] Yuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang. Segfix: Model-agnostic boundary refinement for\nsegmentation. In ECCV, 2020.\n[34] Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, and Long\nQuan. Joint semantic segmentation and boundary detection using iterative pyramid contexts. In CVPR,\n2020.\n[35] Towaki Takikawa, David Acuna, Varun Jampani, and Sanja Fidler. Gated-scnn: Gated shape cnns for\nsemantic segmentation. In ICCV, 2019. 7\n[36] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Learning a discrimina-\ntive feature network for semantic segmentation. In CVPR, 2018.\n[37] Liang-Chieh Chen, Jonathan T Barron, George Papandreou, Kevin Murphy, and Alan L Yuille. Semantic\nimage segmentation with task-specific edge detection using cnns and a discriminatively trained domain\ntransform. In CVPR, 2016. 3\n[38] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention\nnetwork for scene segmentation. In CVPR, 2019. 3\n[39] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,\n2018.\n[40] Zilong Zhong, Zhong Qiu Lin, Rene Bidart, Xiaodan Hu, Ibrahim Ben Daya, Zhifeng Li, Wei-Shi Zheng,\nJonathan Li, and Alexander Wong. Squeeze-and-attention networks for semantic segmentation. In CVPR,\n2020.\n[41] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In ICCV, 2019. 7, 8\n[42] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang. Pyramid attention network for semantic segmenta-\ntion. arXiv, 2018.\n[43] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet:\nPoint-wise spatial attention network for scene parsing. In ECCV, 2018. 8\n[44] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-maximization\nattention networks for semantic segmentation. In ICCV, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1253,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='142' style='font-size:14px'>16</footer>",
            "id": 142,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 293
                },
                {
                    "x": 2119,
                    "y": 293
                },
                {
                    "x": 2119,
                    "y": 3001
                },
                {
                    "x": 443,
                    "y": 3001
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:14px'>[45] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-<br>excitation networks and beyond. In ICCVW, 2019.<br>[46] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Segmenting<br>transparent object in the wild with transformer. IJCAI, 2021. 3<br>[47] Albert Shaw, Daniel Hunter, Forrest Landola, and Sammy Sidhu. Squeezenas: Fast neural architecture<br>search for faster semantic segmentation. In ICCVW, 2019. 3<br>[48] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang Wang. Fasterseg:<br>Searching for faster real-time semantic segmentation. arXiv, 2019.<br>[49] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning<br>dynamic routing for semantic segmentation. In CVPR, 2020.<br>[50] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei.<br>Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In CPVR, 2019. 7<br>[51] Vladimir Nekrasov, Hao Chen, Chunhua Shen, and Ian Reid. Fast neural architecture search of compact<br>semantic segmentation models via auxiliary cells. In CVPR, 2019. 3<br>[52] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey<br>Zagoruyko. End-to-End object detection with transformers. In ECCV, 2020. 3<br>[53] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and<br>Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv, 2021.<br>3<br>[54] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.<br>Conditional positional encodings for vision transformers. arXiv, 2021. 3, 4<br>[55] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.<br>arXiv, 2021. 3<br>[56] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer<br>for image classification. arXiv, 2021. 3<br>[57] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to<br>vision transformers. arXiv, 2021. 3<br>[58] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:<br>Introducing convolutions to vision transformers. arXiv, 2021. 3<br>[59] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers.<br>arXiv, 2021. 3<br>[60] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herve Jegou, and Matthijs<br>Douze. Levit: a vision transformer in convnet's clothing for faster inference. arXiv, 2021. 3<br>[61] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu<br>Wang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. arXiv, 2020. 3<br>[62] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-<br>object tracking with transformers. arXiv, 2021. 3<br>[63] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,<br>Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv, 2020. 3<br>[64] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang. Transreid: Transformer-based<br>object re-identification. arXiv, 2021. 3<br>[65] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. Colorization transformer. arXiv, 2021. 3<br>[66] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herve Jegou. Training vision transformers for<br>image retrieval. arXiv, 2021. 3<br>[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish<br>Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from<br>natural language supervision. arXiv, 2021. 3</p>",
            "id": 143,
            "page": 17,
            "text": "[45] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-\nexcitation networks and beyond. In ICCVW, 2019.\n[46] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Segmenting\ntransparent object in the wild with transformer. IJCAI, 2021. 3\n[47] Albert Shaw, Daniel Hunter, Forrest Landola, and Sammy Sidhu. Squeezenas: Fast neural architecture\nsearch for faster semantic segmentation. In ICCVW, 2019. 3\n[48] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang Wang. Fasterseg:\nSearching for faster real-time semantic segmentation. arXiv, 2019.\n[49] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning\ndynamic routing for semantic segmentation. In CVPR, 2020.\n[50] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei.\nAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In CPVR, 2019. 7\n[51] Vladimir Nekrasov, Hao Chen, Chunhua Shen, and Ian Reid. Fast neural architecture search of compact\nsemantic segmentation models via auxiliary cells. In CVPR, 2019. 3\n[52] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-End object detection with transformers. In ECCV, 2020. 3\n[53] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv, 2021.\n3\n[54] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nConditional positional encodings for vision transformers. arXiv, 2021. 3, 4\n[55] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv, 2021. 3\n[56] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classification. arXiv, 2021. 3\n[57] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to\nvision transformers. arXiv, 2021. 3\n[58] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv, 2021. 3\n[59] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers.\narXiv, 2021. 3\n[60] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herve Jegou, and Matthijs\nDouze. Levit: a vision transformer in convnet's clothing for faster inference. arXiv, 2021. 3\n[61] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu\nWang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. arXiv, 2020. 3\n[62] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-\nobject tracking with transformers. arXiv, 2021. 3\n[63] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv, 2020. 3\n[64] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang. Transreid: Transformer-based\nobject re-identification. arXiv, 2021. 3\n[65] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. Colorization transformer. arXiv, 2021. 3\n[66] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herve Jegou. Training vision transformers for\nimage retrieval. arXiv, 2021. 3\n[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. arXiv, 2021. 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='144' style='font-size:18px'>17</footer>",
            "id": 144,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 297
                },
                {
                    "x": 2116,
                    "y": 297
                },
                {
                    "x": 2116,
                    "y": 1609
                },
                {
                    "x": 439,
                    "y": 1609
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:14px'>[68] Ronghang Hu and Amanpreet Singh. Transformer is all you need: Multimodal multitask learning with a<br>unified transformer. arXiv, 2021. 3<br>[69] Md Amirul Islam, Sen Jia, and Neil DB Bruce. How much position information do convolutional neural<br>networks encode? arXiv, 2020. 4<br>[70] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in<br>deep convolutional neural networks. arXiv, 2017. 5<br>[71] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,<br>Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.<br>In CVPR, 2016. 6, 8<br>[72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing<br>through ade20k dataset. In CVPR, 2017. 6, 8<br>[73] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In<br>CVPR, 2018. 6, 8<br>[74] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen. Axial-<br>deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020. 7, 8<br>[75] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning<br>dynamic routing for semantic segmentation. In CVPR, 2020. 7<br>[76] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Buld, and Peter Kontschieder. The mapillary vistas<br>dataset for semantic understanding of street scenes. In ICCV, 2017. 8<br>[77] Christoph Kamann and Carsten Rother. Benchmarking the robustness of semantic segmentation models.<br>In CVPR, 2020. 8, 9, 10, 14<br>[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz<br>Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 10</p>",
            "id": 145,
            "page": 18,
            "text": "[68] Ronghang Hu and Amanpreet Singh. Transformer is all you need: Multimodal multitask learning with a\nunified transformer. arXiv, 2021. 3\n[69] Md Amirul Islam, Sen Jia, and Neil DB Bruce. How much position information do convolutional neural\nnetworks encode? arXiv, 2020. 4\n[70] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in\ndeep convolutional neural networks. arXiv, 2017. 5\n[71] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.\nIn CVPR, 2016. 6, 8\n[72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017. 6, 8\n[73] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In\nCVPR, 2018. 6, 8\n[74] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen. Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020. 7, 8\n[75] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning\ndynamic routing for semantic segmentation. In CVPR, 2020. 7\n[76] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Buld, and Peter Kontschieder. The mapillary vistas\ndataset for semantic understanding of street scenes. In ICCV, 2017. 8\n[77] Christoph Kamann and Carsten Rother. Benchmarking the robustness of semantic segmentation models.\nIn CVPR, 2020. 8, 9, 10, 14\n[78] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 10"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='146' style='font-size:14px'>18</footer>",
            "id": 146,
            "page": 18,
            "text": "18"
        }
    ]
}