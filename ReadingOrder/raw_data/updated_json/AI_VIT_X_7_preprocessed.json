{
    "id": "32b9cd34-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2102.09672v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 579,
                    "y": 367
                },
                {
                    "x": 1907,
                    "y": 367
                },
                {
                    "x": 1907,
                    "y": 444
                },
                {
                    "x": 579,
                    "y": 444
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Improved Denoising Diffusion Probabilistic Models</p>",
            "id": 0,
            "page": 1,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 912,
                    "y": 585
                },
                {
                    "x": 1565,
                    "y": 585
                },
                {
                    "x": 1565,
                    "y": 641
                },
                {
                    "x": 912,
                    "y": 641
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:14px'>Alex Nichol * 1 Prafulla Dhariwal * 1</p>",
            "id": 1,
            "page": 1,
            "text": "Alex Nichol * 1 Prafulla Dhariwal * 1"
        },
        {
            "bounding_box": [
                {
                    "x": 618,
                    "y": 722
                },
                {
                    "x": 816,
                    "y": 722
                },
                {
                    "x": 816,
                    "y": 776
                },
                {
                    "x": 618,
                    "y": 776
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:20px'>Abstract</p>",
            "id": 2,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 308,
                    "y": 801
                },
                {
                    "x": 1137,
                    "y": 801
                },
                {
                    "x": 1137,
                    "y": 1711
                },
                {
                    "x": 308,
                    "y": 1711
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:18px'>Denoising diffusion probabilistic models (DDPM)<br>are a class of generative models which have re-<br>cently been shown to produce excellent sam-<br>ples. We show that with a few simple modifi-<br>cations, DDPMs can also achieve competitive log-<br>likelihoods while maintaining high sample quality.<br>Additionally, we find that learning variances of<br>the reverse diffusion process allows sampling with<br>an order of magnitude fewer forward passes with<br>a negligible difference in sample quality, which<br>is important for the practical deployment of these<br>models. We additionally use precision and re-<br>call to compare how well DDPMs and GANs<br>cover the target distribution. Finally, we show<br>that the sample quality and likelihood of these<br>models scale smoothly with model capacity and<br>training compute, making them easily scalable.<br>We release our code at https : / / github · com/</p>",
            "id": 3,
            "page": 1,
            "text": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive loglikelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https : / / github · com/"
        },
        {
            "bounding_box": [
                {
                    "x": 309,
                    "y": 1709
                },
                {
                    "x": 889,
                    "y": 1709
                },
                {
                    "x": 889,
                    "y": 1747
                },
                {
                    "x": 309,
                    "y": 1747
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:14px'>openai / improved-diffusion.</p>",
            "id": 4,
            "page": 1,
            "text": "openai / improved-diffusion."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1849
                },
                {
                    "x": 553,
                    "y": 1849
                },
                {
                    "x": 553,
                    "y": 1905
                },
                {
                    "x": 226,
                    "y": 1905
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1936
                },
                {
                    "x": 1217,
                    "y": 1936
                },
                {
                    "x": 1217,
                    "y": 2837
                },
                {
                    "x": 221,
                    "y": 2837
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Sohl-Dickstein et al. (2015) introduced diffusion probabilis-<br>tic models, a class of generative models which match a<br>data distribution by learning to reverse a gradual, multi-step<br>noising process. More recently, Ho et al. (2020) showed<br>an equivalence between denoising diffusion probabilistic<br>models (DDPM) and score based generative models (Song<br>& Ermon, 2019; 2020), which learn a gradient of the log-<br>density of the data distribution using denoising score match-<br>ing (Hyv�rinen, 2005). It has recently been shown that this<br>class of models can produce high-quality images (Ho et al.,<br>2020; Song & Ermon, 2020; Jolicoeur-Martineau et al.,<br>2020) and audio (Chen et al., 2020b; Kong et al., 2020),<br>but it has yet to be shown that DDPMs can achieve log-<br>likelihoods competitive with other likelihood-based models<br>such as autoregressive models (van den Oord et al., 2016c)<br>and VAEs (Kingma & Welling, 2013). This raises various<br>questions, such as whether DDPMs are capable of capturing<br>all the modes of a distribution. Furthermore, while Ho et al.</p>",
            "id": 6,
            "page": 1,
            "text": "Sohl-Dickstein  (2015) introduced diffusion probabilistic models, a class of generative models which match a data distribution by learning to reverse a gradual, multi-step noising process. More recently, Ho  (2020) showed an equivalence between denoising diffusion probabilistic models (DDPM) and score based generative models (Song & Ermon, 2019; 2020), which learn a gradient of the logdensity of the data distribution using denoising score matching (Hyv�rinen, 2005). It has recently been shown that this class of models can produce high-quality images (Ho , 2020; Song & Ermon, 2020; Jolicoeur-Martineau , 2020) and audio (Chen , 2020b; Kong , 2020), but it has yet to be shown that DDPMs can achieve loglikelihoods competitive with other likelihood-based models such as autoregressive models (van den Oord , 2016c) and VAEs (Kingma & Welling, 2013). This raises various questions, such as whether DDPMs are capable of capturing all the modes of a distribution. Furthermore, while Ho "
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2860
                },
                {
                    "x": 1216,
                    "y": 2860
                },
                {
                    "x": 1216,
                    "y": 2953
                },
                {
                    "x": 222,
                    "y": 2953
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>Equal contribution 1OpenAI, San Francisco, USA. Correspon-<br>dence to: <alex@openai.com>, <prafulla@openai.com>.</p>",
            "id": 7,
            "page": 1,
            "text": "Equal contribution 1OpenAI, San Francisco, USA. Correspondence to: <alex@openai.com>, <prafulla@openai.com>."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 726
                },
                {
                    "x": 2266,
                    "y": 726
                },
                {
                    "x": 2266,
                    "y": 1078
                },
                {
                    "x": 1273,
                    "y": 1078
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>(2020) showed extremely good results on the CIFAR-10<br>(Krizhevsky, 2009) and LSUN (Yu et al., 2015) datasets, it<br>is unclear how well DDPMs scale to datasets with higher di-<br>versity such as ImageNet. Finally, while Chen et al. (2020b)<br>found that DDPMs can efficiently generate audio using a<br>small number of sampling steps, it has yet to be shown that<br>the same is true for images.</p>",
            "id": 8,
            "page": 1,
            "text": "(2020) showed extremely good results on the CIFAR-10 (Krizhevsky, 2009) and LSUN (Yu , 2015) datasets, it is unclear how well DDPMs scale to datasets with higher diversity such as ImageNet. Finally, while Chen  (2020b) found that DDPMs can efficiently generate audio using a small number of sampling steps, it has yet to be shown that the same is true for images."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1100
                },
                {
                    "x": 2265,
                    "y": 1100
                },
                {
                    "x": 2265,
                    "y": 1451
                },
                {
                    "x": 1274,
                    "y": 1451
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>In this paper, we show that DDPMs can achieve log-<br>likelihoods competitive with other likelihood-based models,<br>even on high-diversity datasets like ImageNet. To more<br>tightly optimise the variational lower-bound (VLB), we<br>learn the reverse process variances using a simple reparame-<br>terization and a hybrid learning objective that combines the<br>VLB with the simplified objective from Ho et al. (2020).</p>",
            "id": 9,
            "page": 1,
            "text": "In this paper, we show that DDPMs can achieve loglikelihoods competitive with other likelihood-based models, even on high-diversity datasets like ImageNet. To more tightly optimise the variational lower-bound (VLB), we learn the reverse process variances using a simple reparameterization and a hybrid learning objective that combines the VLB with the simplified objective from Ho  (2020)."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1473
                },
                {
                    "x": 2265,
                    "y": 1473
                },
                {
                    "x": 2265,
                    "y": 1824
                },
                {
                    "x": 1274,
                    "y": 1824
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>We find surprisingly that, with our hybrid objective, our<br>models obtain better log-likelihoods than those obtained<br>by optimizing the log-likelihood directly, and discover that<br>the latter objective has much more gradient noise during<br>training. We show that a simple importance sampling tech-<br>nique reduces this noise and allows us to achieve better<br>log-likelihoods than with the hybrid objective.</p>",
            "id": 10,
            "page": 1,
            "text": "We find surprisingly that, with our hybrid objective, our models obtain better log-likelihoods than those obtained by optimizing the log-likelihood directly, and discover that the latter objective has much more gradient noise during training. We show that a simple importance sampling technique reduces this noise and allows us to achieve better log-likelihoods than with the hybrid objective."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1849
                },
                {
                    "x": 2266,
                    "y": 1849
                },
                {
                    "x": 2266,
                    "y": 2348
                },
                {
                    "x": 1272,
                    "y": 2348
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:18px'>After incorporating learned variances into our model, we<br>surprisingly discovered that we could sample in fewer steps<br>from our models with very little change in sample quality.<br>While DDPM (Ho et al., 2020) requires hundreds of for-<br>ward passes to produce good samples, we can achieve good<br>samples with as few as 50 forward passes, thus speeding<br>up sampling for use in practical applications. In parallel to<br>our work, Song et al. (2020a) develops a different approach<br>to fast sampling, and we compare against their approach,<br>DDIM, in our experiments.</p>",
            "id": 11,
            "page": 1,
            "text": "After incorporating learned variances into our model, we surprisingly discovered that we could sample in fewer steps from our models with very little change in sample quality. While DDPM (Ho , 2020) requires hundreds of forward passes to produce good samples, we can achieve good samples with as few as 50 forward passes, thus speeding up sampling for use in practical applications. In parallel to our work, Song  (2020a) develops a different approach to fast sampling, and we compare against their approach, DDIM, in our experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2369
                },
                {
                    "x": 2264,
                    "y": 2369
                },
                {
                    "x": 2264,
                    "y": 2722
                },
                {
                    "x": 1273,
                    "y": 2722
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:16px'>While likelihood is a good metric to compare against other<br>likelihood-based models, we also wanted to compare the<br>distribution coverage of these models with GANs. We use<br>the improved precision and recall metrics (Kynkaanniemi<br>et al., 2019) and discover that diffusion models achieve<br>much higher recall for similar FID, suggesting that they do<br>indeed cover a much larger portion of the target distribution.</p>",
            "id": 12,
            "page": 1,
            "text": "While likelihood is a good metric to compare against other likelihood-based models, we also wanted to compare the distribution coverage of these models with GANs. We use the improved precision and recall metrics (Kynkaanniemi , 2019) and discover that diffusion models achieve much higher recall for similar FID, suggesting that they do indeed cover a much larger portion of the target distribution."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2745
                },
                {
                    "x": 2266,
                    "y": 2745
                },
                {
                    "x": 2266,
                    "y": 2946
                },
                {
                    "x": 1273,
                    "y": 2946
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:16px'>Finally, since we expect machine learning models to con-<br>sume more computational resources in the future, we evalu-<br>ate the performance of these models as we increase model<br>size and training compute. Similar to (Henighan et al.,</p>",
            "id": 13,
            "page": 1,
            "text": "Finally, since we expect machine learning models to consume more computational resources in the future, we evaluate the performance of these models as we increase model size and training compute. Similar to (Henighan ,"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 893
                },
                {
                    "x": 150,
                    "y": 893
                },
                {
                    "x": 150,
                    "y": 2329
                },
                {
                    "x": 58,
                    "y": 2329
                }
            ],
            "category": "footer",
            "html": "<br><footer id='14' style='font-size:22px'>2021<br>Feb<br>18<br>[cs.LG]<br>arxiv:2.2017.77.12</footer>",
            "id": 14,
            "page": 1,
            "text": "2021 Feb 18 [cs.LG] arxiv:2.2017.77.12"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 195
                },
                {
                    "x": 1053,
                    "y": 195
                },
                {
                    "x": 1053,
                    "y": 238
                },
                {
                    "x": 225,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='15' style='font-size:16px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 15,
            "page": 2,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2230,
                    "y": 195
                },
                {
                    "x": 2258,
                    "y": 195
                },
                {
                    "x": 2258,
                    "y": 234
                },
                {
                    "x": 2230,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<br><header id='16' style='font-size:14px'>2</header>",
            "id": 16,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 286
                },
                {
                    "x": 1214,
                    "y": 286
                },
                {
                    "x": 1214,
                    "y": 383
                },
                {
                    "x": 225,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:16px'>2020), we observe trends that suggest predictable improve-<br>ments in performance as we increase training compute.</p>",
            "id": 17,
            "page": 2,
            "text": "2020), we observe trends that suggest predictable improvements in performance as we increase training compute."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 447
                },
                {
                    "x": 1154,
                    "y": 447
                },
                {
                    "x": 1154,
                    "y": 502
                },
                {
                    "x": 223,
                    "y": 502
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>2. Denoising Diffusion Probabilistic Models</p>",
            "id": 18,
            "page": 2,
            "text": "2. Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 533
                },
                {
                    "x": 1213,
                    "y": 533
                },
                {
                    "x": 1213,
                    "y": 783
                },
                {
                    "x": 224,
                    "y": 783
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:14px'>We briefly review the formulation of DDPMs from Ho et al.<br>(2020). This formulation makes various simplifying assump-<br>tions, such as a fixed noising process q which adds diagonal<br>Gaussian noise at each timestep. For a more general deriva-<br>tion, see Sohl-Dickstein et al. (2015).</p>",
            "id": 19,
            "page": 2,
            "text": "We briefly review the formulation of DDPMs from Ho  (2020). This formulation makes various simplifying assumptions, such as a fixed noising process q which adds diagonal Gaussian noise at each timestep. For a more general derivation, see Sohl-Dickstein  (2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 838
                },
                {
                    "x": 499,
                    "y": 838
                },
                {
                    "x": 499,
                    "y": 885
                },
                {
                    "x": 223,
                    "y": 885
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:14px'>2.1. Definitions</p>",
            "id": 20,
            "page": 2,
            "text": "2.1. Definitions"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 916
                },
                {
                    "x": 1213,
                    "y": 916
                },
                {
                    "x": 1213,
                    "y": 1113
                },
                {
                    "x": 224,
                    "y": 1113
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:14px'>Given a data distribution xo ~ q(xo), we define a forward<br>noising process q which produces latents X1 through XT by<br>adding Gaussian noise at time t with variance Bt E (0,1) as<br>follows:</p>",
            "id": 21,
            "page": 2,
            "text": "Given a data distribution xo ~ q(xo), we define a forward noising process q which produces latents X1 through XT by adding Gaussian noise at time t with variance Bt E (0,1) as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1416
                },
                {
                    "x": 1213,
                    "y": 1416
                },
                {
                    "x": 1213,
                    "y": 1765
                },
                {
                    "x": 224,
                    "y": 1765
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:14px'>Given sufficiently large T and a well behaved schedule of<br>Bt, the latent XT is nearly an isotropic Gaussian distribution.<br>Thus, if we know the exact reverse distribution q(xt-1|xt),<br>we can sample XT ~ N(0, I) and run the process in reverse<br>to get a sample from q(xo). However, since q(xt-1|xt)<br>depends on the entire data distribution, we approximate it<br>using a neural network as follows:</p>",
            "id": 22,
            "page": 2,
            "text": "Given sufficiently large T and a well behaved schedule of Bt, the latent XT is nearly an isotropic Gaussian distribution. Thus, if we know the exact reverse distribution q(xt-1|xt), we can sample XT ~ N(0, I) and run the process in reverse to get a sample from q(xo). However, since q(xt-1|xt) depends on the entire data distribution, we approximate it using a neural network as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1924
                },
                {
                    "x": 1212,
                    "y": 1924
                },
                {
                    "x": 1212,
                    "y": 2070
                },
                {
                    "x": 225,
                    "y": 2070
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:14px'>The combination of q and p is a variational auto-encoder<br>(Kingma & Welling, 2013), and we can write the variational<br>lower bound (VLB) as follows:</p>",
            "id": 23,
            "page": 2,
            "text": "The combination of q and p is a variational auto-encoder (Kingma & Welling, 2013), and we can write the variational lower bound (VLB) as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2421
                },
                {
                    "x": 1212,
                    "y": 2421
                },
                {
                    "x": 1212,
                    "y": 2869
                },
                {
                    "x": 224,
                    "y": 2869
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:16px'>Aside from Lo, each term of Equation 4 is a KL divergence<br>between two Gaussians, and can thus be evaluated in closed<br>form. To evaluate Lo for images, we assume that each color<br>component is divided into 256 bins, and we compute the<br>probability of pe(xo|x1) landing in the correct bin (which is<br>tractable using the CDF of the Gaussian distribution). Also<br>note that while LT does not depend on 0, it will be close to<br>zero if the forward noising process adequately destroys the<br>data distribution SO that q(xT|xo) 21 N(0,I).</p>",
            "id": 24,
            "page": 2,
            "text": "Aside from Lo, each term of Equation 4 is a KL divergence between two Gaussians, and can thus be evaluated in closed form. To evaluate Lo for images, we assume that each color component is divided into 256 bins, and we compute the probability of pe(xo|x1) landing in the correct bin (which is tractable using the CDF of the Gaussian distribution). Also note that while LT does not depend on 0, it will be close to zero if the forward noising process adequately destroys the data distribution SO that q(xT|xo) 21 N(0,I)."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2895
                },
                {
                    "x": 1211,
                    "y": 2895
                },
                {
                    "x": 1211,
                    "y": 2992
                },
                {
                    "x": 224,
                    "y": 2992
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>As noted in (Ho et al., 2020), the noising process defined<br>in Equation 2 allows us to sample an arbitrary step of the</p>",
            "id": 25,
            "page": 2,
            "text": "As noted in (Ho , 2020), the noising process defined in Equation 2 allows us to sample an arbitrary step of the"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 286
                },
                {
                    "x": 2261,
                    "y": 286
                },
                {
                    "x": 2261,
                    "y": 386
                },
                {
                    "x": 1274,
                    "y": 386
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:14px'>noised latents directly conditioned on the input xo. With<br>at := 1 - Bt and at := II's=o �s, we can write the marginal</p>",
            "id": 26,
            "page": 2,
            "text": "noised latents directly conditioned on the input xo. With at := 1 - Bt and at := II's=o �s, we can write the marginal"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 578
                },
                {
                    "x": 2259,
                    "y": 578
                },
                {
                    "x": 2259,
                    "y": 727
                },
                {
                    "x": 1274,
                    "y": 727
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:14px'>where E ~ N(0,I). Here, 1 - �t tells us the variance of the<br>noise for an arbitrary timestep, and we could equivalently<br>use this to define the noise schedule instead of Bt.</p>",
            "id": 27,
            "page": 2,
            "text": "where E ~ N(0,I). Here, 1 - �t tells us the variance of the noise for an arbitrary timestep, and we could equivalently use this to define the noise schedule instead of Bt."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 756
                },
                {
                    "x": 2263,
                    "y": 756
                },
                {
                    "x": 2263,
                    "y": 900
                },
                {
                    "x": 1275,
                    "y": 900
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:16px'>Using Bayes theorem, one can calculate the posterior<br>q(xt-1|xt, xo) in terms of Bt and ut(xt, xo) which are de-<br>fined as follows:</p>",
            "id": 28,
            "page": 2,
            "text": "Using Bayes theorem, one can calculate the posterior q(xt-1|xt, xo) in terms of Bt and ut(xt, xo) which are defined as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1322
                },
                {
                    "x": 1712,
                    "y": 1322
                },
                {
                    "x": 1712,
                    "y": 1368
                },
                {
                    "x": 1274,
                    "y": 1368
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:16px'>2.2. Training in Practice</p>",
            "id": 29,
            "page": 2,
            "text": "2.2. Training in Practice"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1401
                },
                {
                    "x": 2262,
                    "y": 1401
                },
                {
                    "x": 2262,
                    "y": 1747
                },
                {
                    "x": 1272,
                    "y": 1747
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>The objective in Equation 4 is a sum of independent terms<br>Lt-1, and Equation 9 provides an efficient way to sample<br>from an arbitrary step of the forward noising process and<br>estimate Lt-1 using the posterior (Equation 12) and prior<br>(Equation 3). We can thus randomly sample t and use the<br>expectation Et,xo,e [Lt-1] to estimate Lvlb. Ho et al. (2020)<br>uniformly sample t for each image in each mini-batch.</p>",
            "id": 30,
            "page": 2,
            "text": "The objective in Equation 4 is a sum of independent terms Lt-1, and Equation 9 provides an efficient way to sample from an arbitrary step of the forward noising process and estimate Lt-1 using the posterior (Equation 12) and prior (Equation 3). We can thus randomly sample t and use the expectation Et,xo,e [Lt-1] to estimate Lvlb. Ho  (2020) uniformly sample t for each image in each mini-batch."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1773
                },
                {
                    "x": 2263,
                    "y": 1773
                },
                {
                    "x": 2263,
                    "y": 2070
                },
                {
                    "x": 1273,
                    "y": 2070
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:16px'>There are many different ways to parameterize mo(xt, t) in<br>the prior. The most obvious option is to predict mo(xt, t)<br>directly with a neural network. Alternatively, the network<br>could predict xo, and this output could be used in Equation<br>11 to produce mu (xt, t). The network could also predict the<br>noise E and use Equations 9 and 11 to derive</p>",
            "id": 31,
            "page": 2,
            "text": "There are many different ways to parameterize mo(xt, t) in the prior. The most obvious option is to predict mo(xt, t) directly with a neural network. Alternatively, the network could predict xo, and this output could be used in Equation 11 to produce mu (xt, t). The network could also predict the noise E and use Equations 9 and 11 to derive"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2247
                },
                {
                    "x": 2267,
                    "y": 2247
                },
                {
                    "x": 2267,
                    "y": 2345
                },
                {
                    "x": 1273,
                    "y": 2345
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:14px'>Ho et al. (2020) found that predicting E worked best, es-<br>pecially when combined with a reweighted loss function:</p>",
            "id": 32,
            "page": 2,
            "text": "Ho  (2020) found that predicting E worked best, especially when combined with a reweighted loss function:"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2471
                },
                {
                    "x": 2261,
                    "y": 2471
                },
                {
                    "x": 2261,
                    "y": 2766
                },
                {
                    "x": 1272,
                    "y": 2766
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>This objective can be seen as a reweighted form of Lvlb<br>(without the terms affecting �o). The authors found that<br>optimizing this reweighted objective resulted in much better<br>sample quality than optimizing Lvlb directly, and explain<br>this by drawing a connection to generative score matching<br>(Song & Ermon, 2019; 2020).</p>",
            "id": 33,
            "page": 2,
            "text": "This objective can be seen as a reweighted form of Lvlb (without the terms affecting �o). The authors found that optimizing this reweighted objective resulted in much better sample quality than optimizing Lvlb directly, and explain this by drawing a connection to generative score matching (Song & Ermon, 2019; 2020)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2794
                },
                {
                    "x": 2262,
                    "y": 2794
                },
                {
                    "x": 2262,
                    "y": 2994
                },
                {
                    "x": 1272,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>One subtlety is that Lsimple provides no learning signal for<br>E�(xt, t). This is irrelevant, however, since Ho et al. (2020)<br>achieved their best results by fixing the variance to o2I<br>rather than learning it. They found that they achieve similar</p>",
            "id": 34,
            "page": 2,
            "text": "One subtlety is that Lsimple provides no learning signal for E�(xt, t). This is irrelevant, however, since Ho  (2020) achieved their best results by fixing the variance to o2I rather than learning it. They found that they achieve similar"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 225,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='35' style='font-size:18px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 35,
            "page": 3,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2230,
                    "y": 193
                },
                {
                    "x": 2258,
                    "y": 193
                },
                {
                    "x": 2258,
                    "y": 234
                },
                {
                    "x": 2230,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<br><header id='36' style='font-size:16px'>3</header>",
            "id": 36,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 278
                },
                {
                    "x": 1216,
                    "y": 278
                },
                {
                    "x": 1216,
                    "y": 486
                },
                {
                    "x": 223,
                    "y": 486
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:18px'>sample quality using either 02 = Bt or o2 Bt, which are<br>the upper and lower bounds on the variance given by q(xo)<br>being either isotropic Gaussian noise or a delta function,<br>respectively.</p>",
            "id": 37,
            "page": 3,
            "text": "sample quality using either 02 = Bt or o2 Bt, which are the upper and lower bounds on the variance given by q(xo) being either isotropic Gaussian noise or a delta function, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 545
                },
                {
                    "x": 912,
                    "y": 545
                },
                {
                    "x": 912,
                    "y": 602
                },
                {
                    "x": 224,
                    "y": 602
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:22px'>3. Improving the Log-likelihood</p>",
            "id": 38,
            "page": 3,
            "text": "3. Improving the Log-likelihood"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 630
                },
                {
                    "x": 1218,
                    "y": 630
                },
                {
                    "x": 1218,
                    "y": 1581
                },
                {
                    "x": 224,
                    "y": 1581
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>While Ho et al. (2020) found that DDPMs can generate high-<br>fidelity samples according to FID (Heusel et al., 2017) and<br>Inception Score (Salimans et al., 2016), they were unable to<br>achieve competitive log-likelihoods with these models. Log-<br>likelihood is a widely used metric in generative modeling,<br>and it is generally believed that optimizing log-likelihood<br>forces generative models to capture all of the modes of<br>the data distribution (Razavi et al., 2019). Additionally,<br>recent work (Henighan et al., 2020) has shown that small<br>improvements in log-likelihood can have a dramatic impact<br>on sample quality and learnt feature representations. Thus, it<br>is important to explore why DDPMs seem to perform poorly<br>on this metric, since this may suggest a fundamental short-<br>coming such as bad mode coverage. This section explores<br>several modifications to the algorithm described in Section<br>2 that, when combined, allow DDPMs to achieve much bet-<br>ter log-likelihoods on image datasets, suggesting that these<br>models enjoy the same benefits as other likelihood-based<br>generative models.</p>",
            "id": 39,
            "page": 3,
            "text": "While Ho  (2020) found that DDPMs can generate highfidelity samples according to FID (Heusel , 2017) and Inception Score (Salimans , 2016), they were unable to achieve competitive log-likelihoods with these models. Loglikelihood is a widely used metric in generative modeling, and it is generally believed that optimizing log-likelihood forces generative models to capture all of the modes of the data distribution (Razavi , 2019). Additionally, recent work (Henighan , 2020) has shown that small improvements in log-likelihood can have a dramatic impact on sample quality and learnt feature representations. Thus, it is important to explore why DDPMs seem to perform poorly on this metric, since this may suggest a fundamental shortcoming such as bad mode coverage. This section explores several modifications to the algorithm described in Section 2 that, when combined, allow DDPMs to achieve much better log-likelihoods on image datasets, suggesting that these models enjoy the same benefits as other likelihood-based generative models."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1603
                },
                {
                    "x": 1216,
                    "y": 1603
                },
                {
                    "x": 1216,
                    "y": 2253
                },
                {
                    "x": 223,
                    "y": 2253
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:16px'>To study the effects of different modifications, we train<br>fixed model architectures with fixed hyperparameters on<br>the ImageNet 64 x 64 (van den Oord et al., 2016b) and<br>CIFAR-10 (Krizhevsky, 2009) datasets. While CIFAR-10<br>has seen more usage for this class of models, we chose<br>to study ImageNet 64 x 64 as well because it provides a<br>good trade-off between diversity and resolution, allowing us<br>to train models quickly without worrying about overfitting.<br>Additionally, ImageNet 64 x 64 has been studied extensively<br>in the context of generative modeling (van den Oord et al.,<br>2016c; Menick & Kalchbrenner, 2018; Child et al., 2019;<br>Roy et al., 2020), allowing us to compare DDPMs directly<br>to many other generative models.</p>",
            "id": 40,
            "page": 3,
            "text": "To study the effects of different modifications, we train fixed model architectures with fixed hyperparameters on the ImageNet 64 x 64 (van den Oord , 2016b) and CIFAR-10 (Krizhevsky, 2009) datasets. While CIFAR-10 has seen more usage for this class of models, we chose to study ImageNet 64 x 64 as well because it provides a good trade-off between diversity and resolution, allowing us to train models quickly without worrying about overfitting. Additionally, ImageNet 64 x 64 has been studied extensively in the context of generative modeling (van den Oord , 2016c; Menick & Kalchbrenner, 2018; Child , 2019; Roy , 2020), allowing us to compare DDPMs directly to many other generative models."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2277
                },
                {
                    "x": 1215,
                    "y": 2277
                },
                {
                    "x": 1215,
                    "y": 2677
                },
                {
                    "x": 223,
                    "y": 2677
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>The setup from Ho et al. (2020) (optimizing Lsimple while<br>setting 02 = Bt and T = 1000) achieves a log-likelihood<br>of 3.99 (bits/dim) on ImageNet 64 x 64 after 200K training<br>iterations. We found in early experiments that we could<br>get a boost in log-likelihood by increasing T from 1000 to<br>4000; with this change, the log-likelihood improves to 3.77.<br>For the remainder of this section, we use T = 4000, but we<br>explore this choice in Section 4.</p>",
            "id": 41,
            "page": 3,
            "text": "The setup from Ho  (2020) (optimizing Lsimple while setting 02 = Bt and T = 1000) achieves a log-likelihood of 3.99 (bits/dim) on ImageNet 64 x 64 after 200K training iterations. We found in early experiments that we could get a boost in log-likelihood by increasing T from 1000 to 4000; with this change, the log-likelihood improves to 3.77. For the remainder of this section, we use T = 4000, but we explore this choice in Section 4."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2727
                },
                {
                    "x": 635,
                    "y": 2727
                },
                {
                    "x": 635,
                    "y": 2783
                },
                {
                    "x": 224,
                    "y": 2783
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:20px'>3.1. Learning E0(xt, t)</p>",
            "id": 42,
            "page": 3,
            "text": "3.1. Learning E0(xt, t)"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2806
                },
                {
                    "x": 1213,
                    "y": 2806
                },
                {
                    "x": 1213,
                    "y": 2963
                },
                {
                    "x": 223,
                    "y": 2963
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:20px'>In Ho et al. (2020), the authors set Eo(xt, t) = 021, where<br>Ot is not learned. Oddly, they found that fixing o2 to Bt<br>yielded roughly the same sample quality as fixing it to Bt.</p>",
            "id": 43,
            "page": 3,
            "text": "In Ho  (2020), the authors set Eo(xt, t) = 021, where Ot is not learned. Oddly, they found that fixing o2 to Bt yielded roughly the same sample quality as fixing it to Bt."
        },
        {
            "bounding_box": [
                {
                    "x": 1378,
                    "y": 286
                },
                {
                    "x": 2159,
                    "y": 286
                },
                {
                    "x": 2159,
                    "y": 769
                },
                {
                    "x": 1378,
                    "y": 769
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='44' style='font-size:14px' alt=\"10°\n6 x 10-1\nBtlBt\n4 x 10-1\nT = 100 steps\n3 x 10-1\nT = 1000 steps\nT = 10000 steps\n0.0 0.2 0.4 0.6 0.8 1.0\ndiffusion step (t/T)\" data-coord=\"top-left:(1378,286); bottom-right:(2159,769)\" /></figure>",
            "id": 44,
            "page": 3,
            "text": "10° 6 x 10-1 BtlBt 4 x 10-1 T = 100 steps 3 x 10-1 T = 1000 steps T = 10000 steps 0.0 0.2 0.4 0.6 0.8 1.0 diffusion step (t/T)"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 823
                },
                {
                    "x": 2262,
                    "y": 823
                },
                {
                    "x": 2262,
                    "y": 916
                },
                {
                    "x": 1274,
                    "y": 916
                }
            ],
            "category": "caption",
            "html": "<caption id='45' style='font-size:16px'>Figure 1. The ratio Bt /Bt for every diffusion step for diffusion<br>processes of different lengths.</caption>",
            "id": 45,
            "page": 3,
            "text": "Figure 1. The ratio Bt /Bt for every diffusion step for diffusion processes of different lengths."
        },
        {
            "bounding_box": [
                {
                    "x": 1380,
                    "y": 987
                },
                {
                    "x": 2154,
                    "y": 987
                },
                {
                    "x": 2154,
                    "y": 1503
                },
                {
                    "x": 1380,
                    "y": 1503
                }
            ],
            "category": "figure",
            "html": "<figure><img id='46' style='font-size:14px' alt=\"10°\n10-1\n10-2\n(bits) 10-3\nterm\n10-4\nloss\n10-5\n10-6\n10-7\n0 500 1000 1500 2000 2500 3000 3500 4000\ndiffusion step\" data-coord=\"top-left:(1380,987); bottom-right:(2154,1503)\" /></figure>",
            "id": 46,
            "page": 3,
            "text": "10° 10-1 10-2 (bits) 10-3 term 10-4 loss 10-5 10-6 10-7 0 500 1000 1500 2000 2500 3000 3500 4000 diffusion step"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1559
                },
                {
                    "x": 2262,
                    "y": 1559
                },
                {
                    "x": 2262,
                    "y": 1649
                },
                {
                    "x": 1274,
                    "y": 1649
                }
            ],
            "category": "caption",
            "html": "<caption id='47' style='font-size:14px'>Figure 2. Terms of the VLB VS diffusion step. The first few terms<br>contribute most to NLL.</caption>",
            "id": 47,
            "page": 3,
            "text": "Figure 2. Terms of the VLB VS diffusion step. The first few terms contribute most to NLL."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1695
                },
                {
                    "x": 2265,
                    "y": 1695
                },
                {
                    "x": 2265,
                    "y": 2295
                },
                {
                    "x": 1270,
                    "y": 2295
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:16px'>Considering that Bt and Bt represent two opposite extremes,<br>itis reasonable to ask why this choice doesn't affect samples.<br>One clue is given by Figure 1, which shows that Bt and Bt<br>are almost equal except near t = 0, i.e. where the model<br>is dealing with imperceptible details. Furthermore, as we<br>increase the number of diffusion steps, Bt and Bt seem to<br>remain close to one another for more of the diffusion process.<br>This suggests that, in the limit of infinite diffusion steps,<br>the choice of �t might not matter at all for sample quality.<br>In other words, as we add more diffusion steps, the model<br>mean mo(xt, t) determines the distribution much more than<br>20 (xt, t).</p>",
            "id": 48,
            "page": 3,
            "text": "Considering that Bt and Bt represent two opposite extremes, itis reasonable to ask why this choice doesn't affect samples. One clue is given by Figure 1, which shows that Bt and Bt are almost equal except near t = 0, i.e. where the model is dealing with imperceptible details. Furthermore, as we increase the number of diffusion steps, Bt and Bt seem to remain close to one another for more of the diffusion process. This suggests that, in the limit of infinite diffusion steps, the choice of �t might not matter at all for sample quality. In other words, as we add more diffusion steps, the model mean mo(xt, t) determines the distribution much more than 20 (xt, t)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2320
                },
                {
                    "x": 2265,
                    "y": 2320
                },
                {
                    "x": 2265,
                    "y": 2719
                },
                {
                    "x": 1272,
                    "y": 2719
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:16px'>While the above argument suggests that fixing Ot is a reason-<br>able choice for the sake of sample quality, it says nothing<br>about log-likelihood. In fact, Figure 2 shows that the first<br>few steps of the diffusion process contribute the most to<br>the variational lower bound. Thus, it seems likely that we<br>could improve log-likelihood by using a better choice of<br>E�(xt, t). To achieve this, we must learn E�(xt, t) without<br>the instabilities encountered by Ho et al. (2020).</p>",
            "id": 49,
            "page": 3,
            "text": "While the above argument suggests that fixing Ot is a reasonable choice for the sake of sample quality, it says nothing about log-likelihood. In fact, Figure 2 shows that the first few steps of the diffusion process contribute the most to the variational lower bound. Thus, it seems likely that we could improve log-likelihood by using a better choice of E�(xt, t). To achieve this, we must learn E�(xt, t) without the instabilities encountered by Ho  (2020)."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2742
                },
                {
                    "x": 2265,
                    "y": 2742
                },
                {
                    "x": 2265,
                    "y": 2994
                },
                {
                    "x": 1274,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>Since Figure 1 shows that the reasonable range for E�(xt, t)<br>is very small, it would be hard for a neural network to predict<br>20 (xt, t) directly, even in the log domain, as observed by<br>Ho et al. (2020). Instead, we found it better to parameterize<br>the variance as an interpolation between Bt and Bt in the</p>",
            "id": 50,
            "page": 3,
            "text": "Since Figure 1 shows that the reasonable range for E�(xt, t) is very small, it would be hard for a neural network to predict 20 (xt, t) directly, even in the log domain, as observed by Ho  (2020). Instead, we found it better to parameterize the variance as an interpolation between Bt and Bt in the"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='51' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 51,
            "page": 4,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2230,
                    "y": 196
                },
                {
                    "x": 2257,
                    "y": 196
                },
                {
                    "x": 2257,
                    "y": 231
                },
                {
                    "x": 2230,
                    "y": 231
                }
            ],
            "category": "header",
            "html": "<br><header id='52' style='font-size:16px'>4</header>",
            "id": 52,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 276
                },
                {
                    "x": 1211,
                    "y": 276
                },
                {
                    "x": 1211,
                    "y": 456
                },
                {
                    "x": 224,
                    "y": 456
                }
            ],
            "category": "figure",
            "html": "<figure><img id='53' alt=\"\" data-coord=\"top-left:(224,276); bottom-right:(1211,456)\" /></figure>",
            "id": 53,
            "page": 4,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 509
                },
                {
                    "x": 1215,
                    "y": 509
                },
                {
                    "x": 1215,
                    "y": 696
                },
                {
                    "x": 224,
                    "y": 696
                }
            ],
            "category": "caption",
            "html": "<caption id='54' style='font-size:16px'>Figure 3. Latent samples from linear (top) and cosine (bottom)<br>schedules respectively at linearly spaced values of t from 0 to T.<br>The latents in the last quarter of the linear schedule are almost<br>purely noise, whereas the cosine schedule adds noise more slowly</caption>",
            "id": 54,
            "page": 4,
            "text": "Figure 3. Latent samples from linear (top) and cosine (bottom) schedules respectively at linearly spaced values of t from 0 to T. The latents in the last quarter of the linear schedule are almost purely noise, whereas the cosine schedule adds noise more slowly"
        },
        {
            "bounding_box": [
                {
                    "x": 325,
                    "y": 742
                },
                {
                    "x": 1113,
                    "y": 742
                },
                {
                    "x": 1113,
                    "y": 1275
                },
                {
                    "x": 325,
                    "y": 1275
                }
            ],
            "category": "figure",
            "html": "<figure><img id='55' style='font-size:14px' alt=\"cosine schedule\n60 linear schedule\n50\nFID 40\n30\n20\n0.0 0.1 0.2 0.3 0.4 0.5\nfraction of reverse diffusion process skipped\" data-coord=\"top-left:(325,742); bottom-right:(1113,1275)\" /></figure>",
            "id": 55,
            "page": 4,
            "text": "cosine schedule 60 linear schedule 50 FID 40 30 20 0.0 0.1 0.2 0.3 0.4 0.5 fraction of reverse diffusion process skipped"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1330
                },
                {
                    "x": 1214,
                    "y": 1330
                },
                {
                    "x": 1214,
                    "y": 1422
                },
                {
                    "x": 225,
                    "y": 1422
                }
            ],
            "category": "caption",
            "html": "<caption id='56' style='font-size:16px'>Figure 4. FID when skipping a prefix of the reverse diffusion<br>process on ImageNet 64 x 64.</caption>",
            "id": 56,
            "page": 4,
            "text": "Figure 4. FID when skipping a prefix of the reverse diffusion process on ImageNet 64 x 64."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1571
                },
                {
                    "x": 1213,
                    "y": 1571
                },
                {
                    "x": 1213,
                    "y": 1718
                },
                {
                    "x": 224,
                    "y": 1718
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>log domain. In particular, our model outputs a vector v<br>containing one component per dimension, and we turn this<br>output into variances as follows:</p>",
            "id": 57,
            "page": 4,
            "text": "log domain. In particular, our model outputs a vector v containing one component per dimension, and we turn this output into variances as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1878
                },
                {
                    "x": 1213,
                    "y": 1878
                },
                {
                    "x": 1213,
                    "y": 2126
                },
                {
                    "x": 223,
                    "y": 2126
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>We did not apply any constraints on v, theoretically allowing<br>the model to predict variances outside of the interpolated<br>range. However, we did not observe the network doing<br>this in practice, suggesting that the bounds for Eo(xt, t) are<br>indeed expressive enough.</p>",
            "id": 58,
            "page": 4,
            "text": "We did not apply any constraints on v, theoretically allowing the model to predict variances outside of the interpolated range. However, we did not observe the network doing this in practice, suggesting that the bounds for Eo(xt, t) are indeed expressive enough."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2153
                },
                {
                    "x": 1211,
                    "y": 2153
                },
                {
                    "x": 1211,
                    "y": 2251
                },
                {
                    "x": 224,
                    "y": 2251
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:20px'>Since Lsimple doesn't depend on Eo(xt, t), we define a new<br>hybrid objective:</p>",
            "id": 59,
            "page": 4,
            "text": "Since Lsimple doesn't depend on Eo(xt, t), we define a new hybrid objective:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2411
                },
                {
                    "x": 1214,
                    "y": 2411
                },
                {
                    "x": 1214,
                    "y": 2662
                },
                {
                    "x": 223,
                    "y": 2662
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:18px'>For our experiments, we set 入 = 0.001 to prevent Lvlb from<br>overwhelming Lsimple. Along this same line of reasoning,<br>we also apply a stop-gradient to the mo(xt, t) output for the<br>Lvlb term. This way, Lvlb can guide E�(xt, t) while Lsimple<br>is still the main source of influence over mo(xt, t).</p>",
            "id": 60,
            "page": 4,
            "text": "For our experiments, we set 入 = 0.001 to prevent Lvlb from overwhelming Lsimple. Along this same line of reasoning, we also apply a stop-gradient to the mo(xt, t) output for the Lvlb term. This way, Lvlb can guide E�(xt, t) while Lsimple is still the main source of influence over mo(xt, t)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2714
                },
                {
                    "x": 840,
                    "y": 2714
                },
                {
                    "x": 840,
                    "y": 2764
                },
                {
                    "x": 223,
                    "y": 2764
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:20px'>3.2. Improving the Noise Schedule</p>",
            "id": 61,
            "page": 4,
            "text": "3.2. Improving the Noise Schedule"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2793
                },
                {
                    "x": 1214,
                    "y": 2793
                },
                {
                    "x": 1214,
                    "y": 2994
                },
                {
                    "x": 224,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>We found that while the linear noise schedule used in Ho<br>et al. (2020) worked well for high resolution images, it was<br>sub-optimal for images of resolution 64 x 64 and 32 x 32.<br>In particular, the end of the forward noising process is too</p>",
            "id": 62,
            "page": 4,
            "text": "We found that while the linear noise schedule used in Ho  (2020) worked well for high resolution images, it was sub-optimal for images of resolution 64 x 64 and 32 x 32. In particular, the end of the forward noising process is too"
        },
        {
            "bounding_box": [
                {
                    "x": 1384,
                    "y": 282
                },
                {
                    "x": 2157,
                    "y": 282
                },
                {
                    "x": 2157,
                    "y": 803
                },
                {
                    "x": 1384,
                    "y": 803
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='63' style='font-size:14px' alt=\"1.0 linear\ncosine\n0.8\n0.6\na\n0.4\n0.2\n0.0\n0.0 0.2 0.4 0.6 0.8 1.0\ndiffusion step (t/T)\" data-coord=\"top-left:(1384,282); bottom-right:(2157,803)\" /></figure>",
            "id": 63,
            "page": 4,
            "text": "1.0 linear cosine 0.8 0.6 a 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 diffusion step (t/T)"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 862
                },
                {
                    "x": 2262,
                    "y": 862
                },
                {
                    "x": 2262,
                    "y": 955
                },
                {
                    "x": 1275,
                    "y": 955
                }
            ],
            "category": "caption",
            "html": "<caption id='64' style='font-size:16px'>Figure 5. �t throughout diffusion in the linear schedule and our<br>proposed cosine schedule.</caption>",
            "id": 64,
            "page": 4,
            "text": "Figure 5. �t throughout diffusion in the linear schedule and our proposed cosine schedule."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1046
                },
                {
                    "x": 2264,
                    "y": 1046
                },
                {
                    "x": 2264,
                    "y": 1341
                },
                {
                    "x": 1272,
                    "y": 1341
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:16px'>noisy, and SO doesn't contribute very much to sample quality.<br>This can be seen visually in Figure 3. The result of this<br>effect is studied in Figure 4, where we see that a model<br>trained with the linear schedule does not get much worse (as<br>measured by FID) when we skip up to 20% of the reverse<br>diffusion process.</p>",
            "id": 65,
            "page": 4,
            "text": "noisy, and SO doesn't contribute very much to sample quality. This can be seen visually in Figure 3. The result of this effect is studied in Figure 4, where we see that a model trained with the linear schedule does not get much worse (as measured by FID) when we skip up to 20% of the reverse diffusion process."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1370
                },
                {
                    "x": 2261,
                    "y": 1370
                },
                {
                    "x": 2261,
                    "y": 1465
                },
                {
                    "x": 1275,
                    "y": 1465
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:16px'>To address this problem, we construct a different noise<br>schedule in terms of at:</p>",
            "id": 66,
            "page": 4,
            "text": "To address this problem, we construct a different noise schedule in terms of at:"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1660
                },
                {
                    "x": 2263,
                    "y": 1660
                },
                {
                    "x": 2263,
                    "y": 1862
                },
                {
                    "x": 1273,
                    "y": 1862
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:16px'>To go from this definition to variances Bt, we note that<br>�t_ In practice, we clip Bt to be no larger than<br>Bt = 1 -<br>·<br>�t-1<br>0.999 to prevent singularities at the end of the diffusion<br>process near t = T.</p>",
            "id": 67,
            "page": 4,
            "text": "To go from this definition to variances Bt, we note that �t_ In practice, we clip Bt to be no larger than Bt = 1 · �t-1 0.999 to prevent singularities at the end of the diffusion process near t = T."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1890
                },
                {
                    "x": 2264,
                    "y": 1890
                },
                {
                    "x": 2264,
                    "y": 2238
                },
                {
                    "x": 1272,
                    "y": 2238
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>Our cosine schedule is designed to have a linear drop-off of<br>�t in the middle of the process, while changing very little<br>near the extremes of t = 0 and t = T to prevent abrupt<br>changes in noise level. Figure 5 shows how at progresses<br>for both schedules. We can see that the linear schedule from<br>Ho et al. (2020) falls towards zero much faster, destroying<br>information more quickly than necessary.</p>",
            "id": 68,
            "page": 4,
            "text": "Our cosine schedule is designed to have a linear drop-off of �t in the middle of the process, while changing very little near the extremes of t = 0 and t = T to prevent abrupt changes in noise level. Figure 5 shows how at progresses for both schedules. We can see that the linear schedule from Ho  (2020) falls towards zero much faster, destroying information more quickly than necessary."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2262
                },
                {
                    "x": 2264,
                    "y": 2262
                },
                {
                    "x": 2264,
                    "y": 2761
                },
                {
                    "x": 1271,
                    "y": 2761
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>We use a small offset s to prevent Bt from being too small<br>near t = 0, since we found that having tiny amounts of<br>noise at the beginning of the process made it hard for the<br>network to predict E accurately enough. In particular, we<br>selected s such that VB0 was slightly smaller than the pixel<br>bin size 1/127.5, which gives s = 0.008. We chose to<br>use cos2 in particular because it is a common mathematical<br>function with the shape we were looking for. This choice<br>was arbitrary, and we expect that many other functions with<br>similar shapes would work as well.</p>",
            "id": 69,
            "page": 4,
            "text": "We use a small offset s to prevent Bt from being too small near t = 0, since we found that having tiny amounts of noise at the beginning of the process made it hard for the network to predict E accurately enough. In particular, we selected s such that VB0 was slightly smaller than the pixel bin size 1/127.5, which gives s = 0.008. We chose to use cos2 in particular because it is a common mathematical function with the shape we were looking for. This choice was arbitrary, and we expect that many other functions with similar shapes would work as well."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2815
                },
                {
                    "x": 1805,
                    "y": 2815
                },
                {
                    "x": 1805,
                    "y": 2863
                },
                {
                    "x": 1274,
                    "y": 2863
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:20px'>3.3. Reducing Gradient Noise</p>",
            "id": 70,
            "page": 4,
            "text": "3.3. Reducing Gradient Noise"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2893
                },
                {
                    "x": 2268,
                    "y": 2893
                },
                {
                    "x": 2268,
                    "y": 2994
                },
                {
                    "x": 1274,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:22px'>We expected to achieve the best log-likelihoods by optimiz-<br>ing Lvlb directly, rather than by optimizing Lhybrid. However,</p>",
            "id": 71,
            "page": 4,
            "text": "We expected to achieve the best log-likelihoods by optimizing Lvlb directly, rather than by optimizing Lhybrid. However,"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='72' style='font-size:22px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 72,
            "page": 5,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2231,
                    "y": 195
                },
                {
                    "x": 2258,
                    "y": 195
                },
                {
                    "x": 2258,
                    "y": 231
                },
                {
                    "x": 2231,
                    "y": 231
                }
            ],
            "category": "header",
            "html": "<br><header id='73' style='font-size:16px'>5</header>",
            "id": 73,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 283
                },
                {
                    "x": 1111,
                    "y": 283
                },
                {
                    "x": 1111,
                    "y": 791
                },
                {
                    "x": 328,
                    "y": 791
                }
            ],
            "category": "figure",
            "html": "<figure><img id='74' style='font-size:14px' alt=\"5.00\nLvlb\n4.75 Lhybrid\nLvlb (resampled)\n(bits/dim)\n4.50\n4.25\nloss\n4.00\nlog\n3.75\ntraining\n3.50\n3.25\n3.00\n0 50 100 150 200 250\ntraining iterations (1e3)\" data-coord=\"top-left:(328,283); bottom-right:(1111,791)\" /></figure>",
            "id": 74,
            "page": 5,
            "text": "5.00 Lvlb 4.75 Lhybrid Lvlb (resampled) (bits/dim) 4.50 4.25 loss 4.00 log 3.75 training 3.50 3.25 3.00 0 50 100 150 200 250 training iterations (1e3)"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 850
                },
                {
                    "x": 1211,
                    "y": 850
                },
                {
                    "x": 1211,
                    "y": 943
                },
                {
                    "x": 225,
                    "y": 943
                }
            ],
            "category": "caption",
            "html": "<caption id='75' style='font-size:14px'>Figure 6. Learning curves comparing the log-likelihoods achieved<br>by different objectives on ImageNet 64 x 64.</caption>",
            "id": 75,
            "page": 5,
            "text": "Figure 6. Learning curves comparing the log-likelihoods achieved by different objectives on ImageNet 64 x 64."
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 995
                },
                {
                    "x": 1113,
                    "y": 995
                },
                {
                    "x": 1113,
                    "y": 1509
                },
                {
                    "x": 330,
                    "y": 1509
                }
            ],
            "category": "figure",
            "html": "<figure><img id='76' style='font-size:14px' alt=\"105\n104\nscale\n103\nnoise\n102\ngradient\n101\n10°\nLvlb\nLhybrid\n10-1\n0 25 50 75 100 125 150 175 200\ntraining step (1e3)\" data-coord=\"top-left:(330,995); bottom-right:(1113,1509)\" /></figure>",
            "id": 76,
            "page": 5,
            "text": "105 104 scale 103 noise 102 gradient 101 10° Lvlb Lhybrid 10-1 0 25 50 75 100 125 150 175 200 training step (1e3)"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1569
                },
                {
                    "x": 1212,
                    "y": 1569
                },
                {
                    "x": 1212,
                    "y": 1659
                },
                {
                    "x": 224,
                    "y": 1659
                }
            ],
            "category": "caption",
            "html": "<caption id='77' style='font-size:14px'>Figure 7. Gradient noise scales for the Lvlb and Lhybrid objectives<br>on ImageNet 64 x 64.</caption>",
            "id": 77,
            "page": 5,
            "text": "Figure 7. Gradient noise scales for the Lvlb and Lhybrid objectives on ImageNet 64 x 64."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1756
                },
                {
                    "x": 1215,
                    "y": 1756
                },
                {
                    "x": 1215,
                    "y": 2052
                },
                {
                    "x": 223,
                    "y": 2052
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>we were surprised to find that Lvlb was actually quite diffi-<br>cult to optimize in practice, at least on the diverse ImageNet<br>64 x 64 dataset. Figure 6 shows the learning curves for both<br>Lvlb and Lhybrid. Both curves are noisy, but the hybrid objec-<br>tive clearly achieves better log-likelihoods on the training<br>set given the same amount of training time.</p>",
            "id": 78,
            "page": 5,
            "text": "we were surprised to find that Lvlb was actually quite difficult to optimize in practice, at least on the diverse ImageNet 64 x 64 dataset. Figure 6 shows the learning curves for both Lvlb and Lhybrid. Both curves are noisy, but the hybrid objective clearly achieves better log-likelihoods on the training set given the same amount of training time."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2076
                },
                {
                    "x": 1215,
                    "y": 2076
                },
                {
                    "x": 1215,
                    "y": 2376
                },
                {
                    "x": 223,
                    "y": 2376
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:18px'>We hypothesized that the gradient of Lvlb was much noisier<br>than that of Lhybrid. We confirmed this by evaluating the<br>gradient noise scales (McCandlish et al., 2018) for models<br>trained with both objectives, as shown in Figure 7. Thus,<br>we sought out a way to reduce the variance of Lvlb in order<br>to optimize directly for log-likelihood.</p>",
            "id": 79,
            "page": 5,
            "text": "We hypothesized that the gradient of Lvlb was much noisier than that of Lhybrid. We confirmed this by evaluating the gradient noise scales (McCandlish , 2018) for models trained with both objectives, as shown in Figure 7. Thus, we sought out a way to reduce the variance of Lvlb in order to optimize directly for log-likelihood."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2400
                },
                {
                    "x": 1215,
                    "y": 2400
                },
                {
                    "x": 1215,
                    "y": 2601
                },
                {
                    "x": 224,
                    "y": 2601
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:22px'>Noting that different terms of Lvlb have greatly different<br>magnitudes (Figure 2), we hypothesized that sampling t<br>uniformly causes unnecessary noise in the Lvlb objective.<br>To address this, we employ importance sampling:</p>",
            "id": 80,
            "page": 5,
            "text": "Noting that different terms of Lvlb have greatly different magnitudes (Figure 2), we hypothesized that sampling t uniformly causes unnecessary noise in the Lvlb objective. To address this, we employ importance sampling:"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2793
                },
                {
                    "x": 1212,
                    "y": 2793
                },
                {
                    "x": 1212,
                    "y": 2996
                },
                {
                    "x": 224,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:20px'>Since E[L2] is unknown beforehand and may change<br>throughout training, we maintain a history of the previous<br>10 values for each loss term, and update this dynamically<br>during training. At the beginning of training, we sample t</p>",
            "id": 81,
            "page": 5,
            "text": "Since E[L2] is unknown beforehand and may change throughout training, we maintain a history of the previous 10 values for each loss term, and update this dynamically during training. At the beginning of training, we sample t"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 311
                },
                {
                    "x": 2241,
                    "y": 311
                },
                {
                    "x": 2241,
                    "y": 357
                },
                {
                    "x": 1274,
                    "y": 357
                }
            ],
            "category": "caption",
            "html": "<br><caption id='82' style='font-size:14px'>Table 1. Ablating schedule and objective on ImageNet 64 x 64.</caption>",
            "id": 82,
            "page": 5,
            "text": "Table 1. Ablating schedule and objective on ImageNet 64 x 64."
        },
        {
            "bounding_box": [
                {
                    "x": 1335,
                    "y": 397
                },
                {
                    "x": 2193,
                    "y": 397
                },
                {
                    "x": 2193,
                    "y": 869
                },
                {
                    "x": 1335,
                    "y": 869
                }
            ],
            "category": "table",
            "html": "<table id='83' style='font-size:14px'><tr><td>Iters</td><td>T</td><td>Schedule</td><td>Objective</td><td>NLL</td><td>FID</td></tr><tr><td>200K</td><td>1K</td><td>linear</td><td>Lsimple</td><td>3.99</td><td>32.5</td></tr><tr><td>200K</td><td>4K</td><td>linear</td><td>Lsimple</td><td>3.77</td><td>31.3</td></tr><tr><td>200K</td><td>4K</td><td>linear</td><td>Lhybrid</td><td>3.66</td><td>32.2</td></tr><tr><td>200K</td><td>4K</td><td>cosine</td><td>Lsimple</td><td>3.68</td><td>27.0</td></tr><tr><td>200K</td><td>4K</td><td>cosine</td><td>Lhybrid</td><td>3.62</td><td>28.0</td></tr><tr><td>200K</td><td>4K</td><td>cosine</td><td>Lvlb</td><td>3.57</td><td>56.7</td></tr><tr><td>1.5M</td><td>4K</td><td>cosine</td><td>Lhybrid</td><td>3.57</td><td>19.2</td></tr><tr><td>1.5M</td><td>4K</td><td>cosine</td><td>Lvlb</td><td>3.53</td><td>40.1</td></tr></table>",
            "id": 83,
            "page": 5,
            "text": "Iters T Schedule Objective NLL FID  200K 1K linear Lsimple 3.99 32.5  200K 4K linear Lsimple 3.77 31.3  200K 4K linear Lhybrid 3.66 32.2  200K 4K cosine Lsimple 3.68 27.0  200K 4K cosine Lhybrid 3.62 28.0  200K 4K cosine Lvlb 3.57 56.7  1.5M 4K cosine Lhybrid 3.57 19.2  1.5M 4K cosine Lvlb 3.53"
        },
        {
            "bounding_box": [
                {
                    "x": 1343,
                    "y": 951
                },
                {
                    "x": 2193,
                    "y": 951
                },
                {
                    "x": 2193,
                    "y": 996
                },
                {
                    "x": 1343,
                    "y": 996
                }
            ],
            "category": "caption",
            "html": "<caption id='84' style='font-size:14px'>Table 2. Ablating schedule and objective on CIFAR-10.</caption>",
            "id": 84,
            "page": 5,
            "text": "Table 2. Ablating schedule and objective on CIFAR-10."
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 1034
                },
                {
                    "x": 2204,
                    "y": 1034
                },
                {
                    "x": 2204,
                    "y": 1403
                },
                {
                    "x": 1332,
                    "y": 1403
                }
            ],
            "category": "table",
            "html": "<table id='85' style='font-size:14px'><tr><td>Iters</td><td>T</td><td>Schedule</td><td>Objective</td><td>NLL</td><td>FID</td></tr><tr><td>500K</td><td>1K</td><td>linear</td><td>Lsimple</td><td>3.73</td><td>3.29</td></tr><tr><td>500K</td><td>4K</td><td>linear</td><td>Lsimple</td><td>3.37</td><td>2.90</td></tr><tr><td>500K</td><td>4K</td><td>linear</td><td>Lhybrid</td><td>3.26</td><td>3.07</td></tr><tr><td>500K</td><td>4K</td><td>cosine</td><td>Lsimple</td><td>3.26</td><td>3.05</td></tr><tr><td>500K</td><td>4K</td><td>cosine</td><td>Lhybrid</td><td>3.17</td><td>3.19</td></tr><tr><td>500K</td><td>4K</td><td>cosine</td><td>Lvlb</td><td>2.94</td><td>11.47</td></tr></table>",
            "id": 85,
            "page": 5,
            "text": "Iters T Schedule Objective NLL FID  500K 1K linear Lsimple 3.73 3.29  500K 4K linear Lsimple 3.37 2.90  500K 4K linear Lhybrid 3.26 3.07  500K 4K cosine Lsimple 3.26 3.05  500K 4K cosine Lhybrid 3.17 3.19  500K 4K cosine Lvlb 2.94"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1490
                },
                {
                    "x": 2262,
                    "y": 1490
                },
                {
                    "x": 2262,
                    "y": 1543
                },
                {
                    "x": 1275,
                    "y": 1543
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:16px'>uniformly until we draw 10 samples for every t E [0, T - 1].</p>",
            "id": 86,
            "page": 5,
            "text": "uniformly until we draw 10 samples for every t E [0, T - 1]."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1566
                },
                {
                    "x": 2264,
                    "y": 1566
                },
                {
                    "x": 2264,
                    "y": 1964
                },
                {
                    "x": 1274,
                    "y": 1964
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:20px'>With this importance sampled objective, we are able to<br>achieve our best log-likelihoods by optimizing Lvlb. This<br>can be seen in Figure 6 as the Lvlb (resampled) curve. The<br>figure also shows that the importance sampled objective is<br>considerably less noisy than the original, uniformly sam-<br>pled objective. We found that the importance sampling<br>technique was not helpful when optimizing the less-noisy<br>Lhybrid objective directly.</p>",
            "id": 87,
            "page": 5,
            "text": "With this importance sampled objective, we are able to achieve our best log-likelihoods by optimizing Lvlb. This can be seen in Figure 6 as the Lvlb (resampled) curve. The figure also shows that the importance sampled objective is considerably less noisy than the original, uniformly sampled objective. We found that the importance sampling technique was not helpful when optimizing the less-noisy Lhybrid objective directly."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2019
                },
                {
                    "x": 1747,
                    "y": 2019
                },
                {
                    "x": 1747,
                    "y": 2066
                },
                {
                    "x": 1274,
                    "y": 2066
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:16px'>3.4. Results and Ablations</p>",
            "id": 88,
            "page": 5,
            "text": "3.4. Results and Ablations"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2097
                },
                {
                    "x": 2266,
                    "y": 2097
                },
                {
                    "x": 2266,
                    "y": 2494
                },
                {
                    "x": 1273,
                    "y": 2494
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:16px'>In this section, we ablate the changes we have made to<br>achieve better log-likelihoods. Table 1 summarizes the re-<br>sults of our ablations on ImageNet 64 x 64, and Table 2<br>shows them for CIFAR-10. We also trained our best Ima-<br>geNet 64 x 64 models for 1.5M iterations, and report these<br>results as well. Lvlb and Lhybrid were trained with learned<br>sigmas using the parameterization from Section 3.1. For<br>Lvlb, we used the resampling scheme from Section 3.3.</p>",
            "id": 89,
            "page": 5,
            "text": "In this section, we ablate the changes we have made to achieve better log-likelihoods. Table 1 summarizes the results of our ablations on ImageNet 64 x 64, and Table 2 shows them for CIFAR-10. We also trained our best ImageNet 64 x 64 models for 1.5M iterations, and report these results as well. Lvlb and Lhybrid were trained with learned sigmas using the parameterization from Section 3.1. For Lvlb, we used the resampling scheme from Section 3.3."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2520
                },
                {
                    "x": 2264,
                    "y": 2520
                },
                {
                    "x": 2264,
                    "y": 2820
                },
                {
                    "x": 1274,
                    "y": 2820
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:18px'>Based on our ablations, using Lhybrid and our cosine sched-<br>ule improves log-likelihood while keeping similar FID as<br>the baseline from Ho et al. (2020). Optimizing Lvlb further<br>improves log-likelihood at the cost of a higher FID. We<br>generally prefer to use Lhybrid over Lvlb as it gives a boost<br>in likelihood without sacrificing sample quality.</p>",
            "id": 90,
            "page": 5,
            "text": "Based on our ablations, using Lhybrid and our cosine schedule improves log-likelihood while keeping similar FID as the baseline from Ho  (2020). Optimizing Lvlb further improves log-likelihood at the cost of a higher FID. We generally prefer to use Lhybrid over Lvlb as it gives a boost in likelihood without sacrificing sample quality."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2845
                },
                {
                    "x": 2263,
                    "y": 2845
                },
                {
                    "x": 2263,
                    "y": 2994
                },
                {
                    "x": 1274,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:16px'>In Table 3 we compare our best likelihood models against<br>prior work, showing that these models are competitive with<br>the best conventional methods in terms of log-likelihood.</p>",
            "id": 91,
            "page": 5,
            "text": "In Table 3 we compare our best likelihood models against prior work, showing that these models are competitive with the best conventional methods in terms of log-likelihood."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 194
                },
                {
                    "x": 1054,
                    "y": 194
                },
                {
                    "x": 1054,
                    "y": 238
                },
                {
                    "x": 225,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='92' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 92,
            "page": 6,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 311
                },
                {
                    "x": 1214,
                    "y": 311
                },
                {
                    "x": 1214,
                    "y": 541
                },
                {
                    "x": 223,
                    "y": 541
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:14px'>Table 3. Comparison of DDPMs to other likelihood-based mod-<br>els on CIFAR-10 and Unconditional ImageNet 64 x 64. NLL is<br>reported in bits/dim. On ImageNet 64 x 64, our model is compet-<br>itive with the best convolutional models, but is worse than fully<br>transformer-based architectures.</p>",
            "id": 93,
            "page": 6,
            "text": "Table 3. Comparison of DDPMs to other likelihood-based models on CIFAR-10 and Unconditional ImageNet 64 x 64. NLL is reported in bits/dim. On ImageNet 64 x 64, our model is competitive with the best convolutional models, but is worse than fully transformer-based architectures."
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 621
                },
                {
                    "x": 1242,
                    "y": 621
                },
                {
                    "x": 1242,
                    "y": 1282
                },
                {
                    "x": 227,
                    "y": 1282
                }
            ],
            "category": "table",
            "html": "<table id='94' style='font-size:16px'><tr><td>Model</td><td>ImageNet</td><td>CIFAR</td></tr><tr><td>Glow (Kingma & Dhariwal, 2018)</td><td>3.81</td><td>3.35</td></tr><tr><td>Flow++ (Ho et al., 2019)</td><td>3.69</td><td>3.08</td></tr><tr><td>PixelCNN (van den Oord et al., 2016c)</td><td>3.57</td><td>3.14</td></tr><tr><td>SPN (Menick & Kalchbrenner, 2018)</td><td>3.52</td><td>-</td></tr><tr><td>NVAE (Vahdat & Kautz, 2020)</td><td>-</td><td>2.91</td></tr><tr><td>Very Deep VAE (Child, 2020)</td><td>3.52</td><td>2.87</td></tr><tr><td>PixelSNAIL (Chen et al., 2018)</td><td>3.52</td><td>2.85</td></tr><tr><td>Image Transformer (Parmar et al., 2018)</td><td>3.48</td><td>2.90</td></tr><tr><td>Sparse Transformer (Child et al., 2019)</td><td>3.44</td><td>2.80</td></tr><tr><td>Routing Transformer (Roy et al., 2020)</td><td>3.43</td><td>-</td></tr><tr><td>DDPM (Ho et al., 2020)</td><td>3.77</td><td>3.70</td></tr><tr><td>DDPM (cont flow) (Song et al., 2020b)</td><td>-</td><td>2.99</td></tr><tr><td>Improved DDPM (ours)</td><td>3.53</td><td>2.94</td></tr></table>",
            "id": 94,
            "page": 6,
            "text": "Model ImageNet CIFAR  Glow (Kingma & Dhariwal, 2018) 3.81 3.35  Flow++ (Ho , 2019) 3.69 3.08  PixelCNN (van den Oord , 2016c) 3.57 3.14  SPN (Menick & Kalchbrenner, 2018) 3.52  NVAE (Vahdat & Kautz, 2020) - 2.91  Very Deep VAE (Child, 2020) 3.52 2.87  PixelSNAIL (Chen , 2018) 3.52 2.85  Image Transformer (Parmar , 2018) 3.48 2.90  Sparse Transformer (Child , 2019) 3.44 2.80  Routing Transformer (Roy , 2020) 3.43  DDPM (Ho , 2020) 3.77 3.70  DDPM (cont flow) (Song , 2020b) - 2.99  Improved DDPM (ours) 3.53"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1339
                },
                {
                    "x": 864,
                    "y": 1339
                },
                {
                    "x": 864,
                    "y": 1399
                },
                {
                    "x": 223,
                    "y": 1399
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:22px'>4. Improving Sampling Speed</p>",
            "id": 95,
            "page": 6,
            "text": "4. Improving Sampling Speed"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1426
                },
                {
                    "x": 1215,
                    "y": 1426
                },
                {
                    "x": 1215,
                    "y": 1925
                },
                {
                    "x": 224,
                    "y": 1925
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:18px'>All of our models were trained with 4000 diffusion steps,<br>and thus producing a single sample takes several minutes on<br>a modern GPU. In this section, we explore how performance<br>scales if we reduce the steps used during sampling, and find<br>that our pre-trained Lhybrid models can produce high-quality<br>samples with many fewer diffusion steps than they were<br>trained with (without any fine-tuning). Reducing the steps<br>in this way makes it possible to sample from our models<br>in a number of seconds rather than minutes, and greatly<br>improves the practical applicability of image DDPMs.</p>",
            "id": 96,
            "page": 6,
            "text": "All of our models were trained with 4000 diffusion steps, and thus producing a single sample takes several minutes on a modern GPU. In this section, we explore how performance scales if we reduce the steps used during sampling, and find that our pre-trained Lhybrid models can produce high-quality samples with many fewer diffusion steps than they were trained with (without any fine-tuning). Reducing the steps in this way makes it possible to sample from our models in a number of seconds rather than minutes, and greatly improves the practical applicability of image DDPMs."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1948
                },
                {
                    "x": 1214,
                    "y": 1948
                },
                {
                    "x": 1214,
                    "y": 2346
                },
                {
                    "x": 223,
                    "y": 2346
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:18px'>For a model trained with T diffusion steps, we would<br>typically sample using the same sequence of t values<br>(1, 2, · .., T) as used during training. However, it is also<br>possible to sample using an arbitrary subsequence S of t<br>values. Given the training noise schedule �t, for a given<br>sequence S we can obtain the sampling noise schedule �St,<br>which can be then used to obtain corresponding sampling<br>variances</p>",
            "id": 97,
            "page": 6,
            "text": "For a model trained with T diffusion steps, we would typically sample using the same sequence of t values (1, 2, · .., T) as used during training. However, it is also possible to sample using an arbitrary subsequence S of t values. Given the training noise schedule �t, for a given sequence S we can obtain the sampling noise schedule �St, which can be then used to obtain corresponding sampling variances"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2518
                },
                {
                    "x": 1213,
                    "y": 2518
                },
                {
                    "x": 1213,
                    "y": 2722
                },
                {
                    "x": 223,
                    "y": 2722
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:20px'>Now, since E0(xSt, St) is parameterized as a range between<br>BSt and BSt, it will automatically be rescaled for the shorter<br>diffusion process. We can thus compute p(xSt-1 |xSt ) as<br>N (me(xst, St), 20(xSt, St)).</p>",
            "id": 98,
            "page": 6,
            "text": "Now, since E0(xSt, St) is parameterized as a range between BSt and BSt, it will automatically be rescaled for the shorter diffusion process. We can thus compute p(xSt-1 |xSt ) as N (me(xst, St), 20(xSt, St))."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2744
                },
                {
                    "x": 1214,
                    "y": 2744
                },
                {
                    "x": 1214,
                    "y": 2994
                },
                {
                    "x": 223,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:18px'>To reduce the number of sampling steps from T to K, we<br>use K evenly spaced real numbers between 1 and T (inclu-<br>sive), and then round each resulting number to the nearest<br>integer. In Figure 8, we evaluate FIDs for an Lhybrid model<br>model that were trained with 4000 diffusion<br>and an Lsimple</p>",
            "id": 99,
            "page": 6,
            "text": "To reduce the number of sampling steps from T to K, we use K evenly spaced real numbers between 1 and T (inclusive), and then round each resulting number to the nearest integer. In Figure 8, we evaluate FIDs for an Lhybrid model model that were trained with 4000 diffusion and an Lsimple"
        },
        {
            "bounding_box": [
                {
                    "x": 1382,
                    "y": 284
                },
                {
                    "x": 2158,
                    "y": 284
                },
                {
                    "x": 2158,
                    "y": 1549
                },
                {
                    "x": 1382,
                    "y": 1549
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='100' style='font-size:14px' alt=\"Lsimple (�른 = Bt, mid-training) �simple (DDIM, mid-training)\nLsimple (�른 = Bt, fully trained) Lsimple (DDIM, fully trained)\nLsimple (�른 = Bt, mid-training) Lhybrid (ours, mid-training)\nLhybrid (ours, fully trained)\n�simple (�른 = Bt, fully trained)\n40.0\n35.0\n30.0\nFID\n25.0\n20.0\n15.0\n102 103\nsampling steps\n20.0\n17.5\n15.0\n12.5\n문 10.0\n7.5\n5.0\n2.5\n0.0\n102 103\nsampling steps\" data-coord=\"top-left:(1382,284); bottom-right:(2158,1549)\" /></figure>",
            "id": 100,
            "page": 6,
            "text": "Lsimple (�른 = Bt, mid-training) �simple (DDIM, mid-training) Lsimple (�른 = Bt, fully trained) Lsimple (DDIM, fully trained) Lsimple (�른 = Bt, mid-training) Lhybrid (ours, mid-training) Lhybrid (ours, fully trained) �simple (�른 = Bt, fully trained) 40.0 35.0 30.0 FID 25.0 20.0 15.0 102 103 sampling steps 20.0 17.5 15.0 12.5 문 10.0 7.5 5.0 2.5 0.0 102 103 sampling steps"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1604
                },
                {
                    "x": 2263,
                    "y": 1604
                },
                {
                    "x": 2263,
                    "y": 1740
                },
                {
                    "x": 1273,
                    "y": 1740
                }
            ],
            "category": "caption",
            "html": "<caption id='101' style='font-size:14px'>Figure 8. FID versus number of sampling steps, for models trained<br>on ImageNet 64 x 64 (top) and CIFAR-10 (bottom). All models<br>were trained with 4000 diffusion steps.</caption>",
            "id": 101,
            "page": 6,
            "text": "Figure 8. FID versus number of sampling steps, for models trained on ImageNet 64 x 64 (top) and CIFAR-10 (bottom). All models were trained with 4000 diffusion steps."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1816
                },
                {
                    "x": 2264,
                    "y": 1816
                },
                {
                    "x": 2264,
                    "y": 2419
                },
                {
                    "x": 1271,
                    "y": 2419
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:20px'>steps, using 25, 50, 100, 200, 400, 1000, and 4000 sampling<br>steps. We do this for both a fully-trained checkpoint, and<br>a checkpoint mid-way through training. For CIFAR-10 we<br>used 200K and 500K training iterations, and for ImageNet-<br>64 we used 500K and 1500K training iterations. We find<br>that the Lsimple models with fixed sigmas (with both the<br>larger o2 = Bt and the smaller ot = Bt) suffer much more<br>in sample quality when using a reduced number of sampling<br>steps, whereas our Lhybrid model with learnt sigmas main-<br>tains high sample quality. With this model, 100 sampling<br>steps is sufficient to achieve near-optimal FIDs for our fully<br>trained models.</p>",
            "id": 102,
            "page": 6,
            "text": "steps, using 25, 50, 100, 200, 400, 1000, and 4000 sampling steps. We do this for both a fully-trained checkpoint, and a checkpoint mid-way through training. For CIFAR-10 we used 200K and 500K training iterations, and for ImageNet64 we used 500K and 1500K training iterations. We find that the Lsimple models with fixed sigmas (with both the larger o2 = Bt and the smaller ot = Bt) suffer much more in sample quality when using a reduced number of sampling steps, whereas our Lhybrid model with learnt sigmas maintains high sample quality. With this model, 100 sampling steps is sufficient to achieve near-optimal FIDs for our fully trained models."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2444
                },
                {
                    "x": 2265,
                    "y": 2444
                },
                {
                    "x": 2265,
                    "y": 2995
                },
                {
                    "x": 1271,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:18px'>Parallel to our work, Song et al. (2020a) propose a fast<br>sampling algorithm for DDPMs by producing a new im-<br>plicit model that has the same marginal noise distributions,<br>but deterministically maps noise to images. We include<br>their algorithm, DDIM, in Figure 8, finding that DDIM pro-<br>duces better samples with fewer than 50 sampling steps, but<br>worse samples when using 50 or more steps. Interestingly,<br>DDIM performs worse at the start of training, but closes the<br>gap to other samplers as training continues. We found that<br>our striding technique drastically reduced performance of<br>DDIM, SO our DDIM results instead use the constant strid-</p>",
            "id": 103,
            "page": 6,
            "text": "Parallel to our work, Song  (2020a) propose a fast sampling algorithm for DDPMs by producing a new implicit model that has the same marginal noise distributions, but deterministically maps noise to images. We include their algorithm, DDIM, in Figure 8, finding that DDIM produces better samples with fewer than 50 sampling steps, but worse samples when using 50 or more steps. Interestingly, DDIM performs worse at the start of training, but closes the gap to other samplers as training continues. We found that our striding technique drastically reduced performance of DDIM, SO our DDIM results instead use the constant strid-"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 194
                },
                {
                    "x": 1054,
                    "y": 194
                },
                {
                    "x": 1054,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='104' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 104,
            "page": 7,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2231,
                    "y": 193
                },
                {
                    "x": 2259,
                    "y": 193
                },
                {
                    "x": 2259,
                    "y": 231
                },
                {
                    "x": 2231,
                    "y": 231
                }
            ],
            "category": "header",
            "html": "<br><header id='105' style='font-size:18px'>7</header>",
            "id": 105,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 311
                },
                {
                    "x": 1215,
                    "y": 311
                },
                {
                    "x": 1215,
                    "y": 541
                },
                {
                    "x": 223,
                    "y": 541
                }
            ],
            "category": "caption",
            "html": "<caption id='106' style='font-size:14px'>Table 4. Sample quality comparison on class-conditional Ima-<br>geNet 64 x 64. Precision and recall (Kynkaanniemi et al., 2019)<br>are measured using Inception-V3 features and K = 5. We trained<br>BigGAN-deep for 125K iterations, and did not use truncation for<br>sampling to maximize recall for the GAN.</caption>",
            "id": 106,
            "page": 7,
            "text": "Table 4. Sample quality comparison on class-conditional ImageNet 64 x 64. Precision and recall (Kynkaanniemi , 2019) are measured using Inception-V3 features and K = 5. We trained BigGAN-deep for 125K iterations, and did not use truncation for sampling to maximize recall for the GAN."
        },
        {
            "bounding_box": [
                {
                    "x": 232,
                    "y": 580
                },
                {
                    "x": 1201,
                    "y": 580
                },
                {
                    "x": 1201,
                    "y": 800
                },
                {
                    "x": 232,
                    "y": 800
                }
            ],
            "category": "table",
            "html": "<table id='107' style='font-size:16px'><tr><td>Model</td><td>FID</td><td>Prec.</td><td>Recall</td></tr><tr><td>BigGAN-deep (Brock et al., 2018)</td><td>4.06</td><td>0.86</td><td>0.59</td></tr><tr><td>Improved Diffusion (small)</td><td>6.92</td><td>0.77</td><td>0.72</td></tr><tr><td>Improved Diffusion (large)</td><td>2.92</td><td>0.82</td><td>0.71</td></tr></table>",
            "id": 107,
            "page": 7,
            "text": "Model FID Prec. Recall  BigGAN-deep (Brock , 2018) 4.06 0.86 0.59  Improved Diffusion (small) 6.92 0.77 0.72  Improved Diffusion (large) 2.92 0.82"
        },
        {
            "bounding_box": [
                {
                    "x": 325,
                    "y": 856
                },
                {
                    "x": 1109,
                    "y": 856
                },
                {
                    "x": 1109,
                    "y": 1649
                },
                {
                    "x": 325,
                    "y": 1649
                }
            ],
            "category": "figure",
            "html": "<figure><img id='108' style='font-size:14px' alt=\"现恋爱\n10571787\n'\" data-coord=\"top-left:(325,856); bottom-right:(1109,1649)\" /></figure>",
            "id": 108,
            "page": 7,
            "text": "现恋爱 10571787 \""
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1694
                },
                {
                    "x": 1216,
                    "y": 1694
                },
                {
                    "x": 1216,
                    "y": 1973
                },
                {
                    "x": 223,
                    "y": 1973
                }
            ],
            "category": "caption",
            "html": "<caption id='109' style='font-size:16px'>Figure 9. Class-conditional ImageNet 64 x 64 samples generated<br>using 250 sampling steps from Lhybrid model (FID 2.92). The<br>classes are 9: ostrich, 11: goldfinch, 130: flamingo, 141: redshank,<br>154: pekinese, 157: papillon, 97: drake and 28: spotted salamander.<br>We see that there is a high diversity in each class, suggesting good<br>coverage of the target distribution</caption>",
            "id": 109,
            "page": 7,
            "text": "Figure 9. Class-conditional ImageNet 64 x 64 samples generated using 250 sampling steps from Lhybrid model (FID 2.92). The classes are 9: ostrich, 11: goldfinch, 130: flamingo, 141: redshank, 154: pekinese, 157: papillon, 97: drake and 28: spotted salamander. We see that there is a high diversity in each class, suggesting good coverage of the target distribution"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2080
                },
                {
                    "x": 1215,
                    "y": 2080
                },
                {
                    "x": 1215,
                    "y": 2234
                },
                {
                    "x": 223,
                    "y": 2234
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>ing1 from Song et al. (2020a), wherein the final timestep is<br>T - T / K + 1 rather than T. The other samplers performed<br>slightly better with our striding.</p>",
            "id": 110,
            "page": 7,
            "text": "ing1 from Song  (2020a), wherein the final timestep is T - T / K + 1 rather than T. The other samplers performed slightly better with our striding."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2295
                },
                {
                    "x": 746,
                    "y": 2295
                },
                {
                    "x": 746,
                    "y": 2351
                },
                {
                    "x": 224,
                    "y": 2351
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:20px'>5. Comparison to GANs</p>",
            "id": 111,
            "page": 7,
            "text": "5. Comparison to GANs"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2381
                },
                {
                    "x": 1214,
                    "y": 2381
                },
                {
                    "x": 1214,
                    "y": 2832
                },
                {
                    "x": 224,
                    "y": 2832
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:18px'>While likelihood is a good proxy for mode-coverage, it is<br>difficult to compare to GANs with this metric. Instead, we<br>turn to precision and recall (Kynk��nniemi et al., 2019).<br>Since it is common in the GAN literature to train class-<br>conditional models, we do the same for this experiment.<br>To make our models class-conditional, we inject class in-<br>formation through the same pathway as the timestep t. In<br>particular, we add a class embedding Vi to the timestep<br>embedding et, and pass this embedding to residual blocks</p>",
            "id": 112,
            "page": 7,
            "text": "While likelihood is a good proxy for mode-coverage, it is difficult to compare to GANs with this metric. Instead, we turn to precision and recall (Kynk��nniemi , 2019). Since it is common in the GAN literature to train classconditional models, we do the same for this experiment. To make our models class-conditional, we inject class information through the same pathway as the timestep t. In particular, we add a class embedding Vi to the timestep embedding et, and pass this embedding to residual blocks"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2862
                },
                {
                    "x": 1214,
                    "y": 2862
                },
                {
                    "x": 1214,
                    "y": 2991
                },
                {
                    "x": 224,
                    "y": 2991
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>1We additionally tried the quadratic stride from Song et al.<br>(2020a), but found that it hurt sample quality when combined with<br>our cosine schedule.</p>",
            "id": 113,
            "page": 7,
            "text": "1We additionally tried the quadratic stride from Song  (2020a), but found that it hurt sample quality when combined with our cosine schedule."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 285
                },
                {
                    "x": 2266,
                    "y": 285
                },
                {
                    "x": 2266,
                    "y": 583
                },
                {
                    "x": 1272,
                    "y": 583
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:18px'>throughout the model. We train using the Lhybrid objective<br>and use 250 sampling steps. We train two models: a \"small\"<br>model with 100M parameters for 1.7M training steps, and<br>a larger model with 270 million parameters for 250K it-<br>erations. We train one BigGAN-deep model with 100M<br>parameters across the generator and discriminator.</p>",
            "id": 114,
            "page": 7,
            "text": "throughout the model. We train using the Lhybrid objective and use 250 sampling steps. We train two models: a \"small\" model with 100M parameters for 1.7M training steps, and a larger model with 270 million parameters for 250K iterations. We train one BigGAN-deep model with 100M parameters across the generator and discriminator."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 607
                },
                {
                    "x": 2265,
                    "y": 607
                },
                {
                    "x": 2265,
                    "y": 907
                },
                {
                    "x": 1273,
                    "y": 907
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:18px'>When computing metrics for this task, we generated 50K<br>samples (rather than 10K) to be directly comparable to other<br>works. 2 This is the only ImageNet 64 x 64 FID we report<br>that was computed using 50K samples. For FID, the ref-<br>erence distribution features were computed over the full<br>training set, following (Brock et al., 2018).</p>",
            "id": 115,
            "page": 7,
            "text": "When computing metrics for this task, we generated 50K samples (rather than 10K) to be directly comparable to other works. 2 This is the only ImageNet 64 x 64 FID we report that was computed using 50K samples. For FID, the reference distribution features were computed over the full training set, following (Brock , 2018)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 931
                },
                {
                    "x": 2265,
                    "y": 931
                },
                {
                    "x": 2265,
                    "y": 1232
                },
                {
                    "x": 1272,
                    "y": 1232
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:18px'>Figure 9 shows our samples from the larger model, and<br>Table 4 summarizes our results. We find that BigGAN-<br>deep outperforms our smaller model in terms of FID, but<br>struggles in terms of recall. This suggests that diffusion<br>models are better at covering the modes of the distribution<br>than comparable GANs.</p>",
            "id": 116,
            "page": 7,
            "text": "Figure 9 shows our samples from the larger model, and Table 4 summarizes our results. We find that BigGANdeep outperforms our smaller model in terms of FID, but struggles in terms of recall. This suggests that diffusion models are better at covering the modes of the distribution than comparable GANs."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1296
                },
                {
                    "x": 1735,
                    "y": 1296
                },
                {
                    "x": 1735,
                    "y": 1348
                },
                {
                    "x": 1275,
                    "y": 1348
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:22px'>6. Scaling Model Size</p>",
            "id": 117,
            "page": 7,
            "text": "6. Scaling Model Size"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1380
                },
                {
                    "x": 2266,
                    "y": 1380
                },
                {
                    "x": 2266,
                    "y": 1879
                },
                {
                    "x": 1272,
                    "y": 1879
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:18px'>In the previous sections, we showed algorithmic changes<br>that improved log-likelihood and FID without changing the<br>amount of training compute. However, a trend in modern<br>machine learning is that larger models and more training<br>time tend to improve model performance (Kaplan et al.,<br>2020; Chen et al., 2020a; Brown et al., 2020). Given this<br>observation, we investigate how FID and NLL scale as a<br>function of training compute. Our results, while prelimi-<br>nary, suggest that DDPMs improve in a predictable way as<br>training compute increases.</p>",
            "id": 118,
            "page": 7,
            "text": "In the previous sections, we showed algorithmic changes that improved log-likelihood and FID without changing the amount of training compute. However, a trend in modern machine learning is that larger models and more training time tend to improve model performance (Kaplan , 2020; Chen , 2020a; Brown , 2020). Given this observation, we investigate how FID and NLL scale as a function of training compute. Our results, while preliminary, suggest that DDPMs improve in a predictable way as training compute increases."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1903
                },
                {
                    "x": 2266,
                    "y": 1903
                },
                {
                    "x": 2266,
                    "y": 2455
                },
                {
                    "x": 1270,
                    "y": 2455
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>To measure how performance scales with training compute,<br>we train four different models on ImageNet 64 x 64 with<br>the Lhybrid objective described in Section 3.1. To change<br>model capacity, we apply a depth multiplier across all lay-<br>ers, such that the first layer has either 64, 96, 128, or 192<br>channels. Note that our previous experiments used 128<br>channels in the first layer. Since the depth of each layer af-<br>fects the scale of the initial weights, we scale the Adam<br>(Kingma & Ba, 2014) learning rate for each model by<br>1/ Vchannel multiplier, such that the 128 channel model<br>has a learning rate of 0.0001 (as in our other experiments).</p>",
            "id": 119,
            "page": 7,
            "text": "To measure how performance scales with training compute, we train four different models on ImageNet 64 x 64 with the Lhybrid objective described in Section 3.1. To change model capacity, we apply a depth multiplier across all layers, such that the first layer has either 64, 96, 128, or 192 channels. Note that our previous experiments used 128 channels in the first layer. Since the depth of each layer affects the scale of the initial weights, we scale the Adam (Kingma & Ba, 2014) learning rate for each model by 1/ Vchannel multiplier, such that the 128 channel model has a learning rate of 0.0001 (as in our other experiments)."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2475
                },
                {
                    "x": 2266,
                    "y": 2475
                },
                {
                    "x": 2266,
                    "y": 2778
                },
                {
                    "x": 1273,
                    "y": 2778
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:18px'>Figure 10 shows how FID and NLL improve relative to<br>theoretical training compute.3 The FID curve looks approx-<br>imately linear on a log-log plot, suggesting that FID scales<br>according to a power law (plotted as the black dashed line).<br>The NLL curve does not fit a power law as cleanly, suggest-<br>ing that validation NLL scales in a less-favorable manner</p>",
            "id": 120,
            "page": 7,
            "text": "Figure 10 shows how FID and NLL improve relative to theoretical training compute.3 The FID curve looks approximately linear on a log-log plot, suggesting that FID scales according to a power law (plotted as the black dashed line). The NLL curve does not fit a power law as cleanly, suggesting that validation NLL scales in a less-favorable manner"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2806
                },
                {
                    "x": 2265,
                    "y": 2806
                },
                {
                    "x": 2265,
                    "y": 2894
                },
                {
                    "x": 1273,
                    "y": 2894
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:14px'>2We found that using more samples led to a decrease in esti-<br>mated FID of roughly 2 points.</p>",
            "id": 121,
            "page": 7,
            "text": "2We found that using more samples led to a decrease in estimated FID of roughly 2 points."
        },
        {
            "bounding_box": [
                {
                    "x": 1336,
                    "y": 2896
                },
                {
                    "x": 2017,
                    "y": 2896
                },
                {
                    "x": 2017,
                    "y": 2938
                },
                {
                    "x": 1336,
                    "y": 2938
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:16px'>3The x-axis assumes full hardware utilization</p>",
            "id": 122,
            "page": 7,
            "text": "3The x-axis assumes full hardware utilization"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 223,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='123' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 123,
            "page": 8,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2231,
                    "y": 195
                },
                {
                    "x": 2257,
                    "y": 195
                },
                {
                    "x": 2257,
                    "y": 232
                },
                {
                    "x": 2231,
                    "y": 232
                }
            ],
            "category": "header",
            "html": "<br><header id='124' style='font-size:16px'>8</header>",
            "id": 124,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 325,
                    "y": 283
                },
                {
                    "x": 1111,
                    "y": 283
                },
                {
                    "x": 1111,
                    "y": 1339
                },
                {
                    "x": 325,
                    "y": 1339
                }
            ],
            "category": "figure",
            "html": "<figure><img id='125' style='font-size:14px' alt=\"70.0\n64 ch (30M params)\n60.0\n96 ch (68M params)\n50.0 128 ch (120M params)\n192 ch (270M params)\n40.0 4.00 + (2.500e-25*C)^-0.22\n문 30.0\n20.0\n1017 1018 1019 1020\ncompute (FLOPs)\n64 ch (30M params)\n3.80\n96 ch (68M params)\n128 ch (120M params)\n3.75\n192 ch (270M params)\n3.40 + (3.000e-15*C)^-0.17\n(bits/dim)\n3.70\n3.65\nNLL\n3.60\n3.55\n1017 1018 1019 1020\ncompute (FLOPs)\" data-coord=\"top-left:(325,283); bottom-right:(1111,1339)\" /></figure>",
            "id": 125,
            "page": 8,
            "text": "70.0 64 ch (30M params) 60.0 96 ch (68M params) 50.0 128 ch (120M params) 192 ch (270M params) 40.0 4.00 + (2.500e-25*C)^-0.22 문 30.0 20.0 1017 1018 1019 1020 compute (FLOPs) 64 ch (30M params) 3.80 96 ch (68M params) 128 ch (120M params) 3.75 192 ch (270M params) 3.40 + (3.000e-15*C)^-0.17 (bits/dim) 3.70 3.65 NLL 3.60 3.55 1017 1018 1019 1020 compute (FLOPs)"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1392
                },
                {
                    "x": 1215,
                    "y": 1392
                },
                {
                    "x": 1215,
                    "y": 1670
                },
                {
                    "x": 223,
                    "y": 1670
                }
            ],
            "category": "caption",
            "html": "<caption id='126' style='font-size:14px'>Figure 10. FID and validation NLL throughout training on Im-<br>ageNet 64 x 64 for different model sizes. The constant for the<br>FID trend line was approximated using the FID of in-distribution<br>data. For the NLL trend line, the constant was approximated by<br>rounding down the current state-of-the-art NLL (Roy et al., 2020)<br>on this dataset.</caption>",
            "id": 126,
            "page": 8,
            "text": "Figure 10. FID and validation NLL throughout training on ImageNet 64 x 64 for different model sizes. The constant for the FID trend line was approximated using the FID of in-distribution data. For the NLL trend line, the constant was approximated by rounding down the current state-of-the-art NLL (Roy , 2020) on this dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1720
                },
                {
                    "x": 1216,
                    "y": 1720
                },
                {
                    "x": 1216,
                    "y": 2123
                },
                {
                    "x": 223,
                    "y": 2123
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:16px'>than FID. This could be caused by a variety of factors, such<br>as 1) an unexpectedly high irreducible loss (Henighan et al.,<br>2020) for this type of diffusion model, or 2) the model over-<br>fitting to the training distribution. We also note that these<br>models do not achieve optimal log-likelihoods in general<br>because they were trained with our Lhybrid objective and not<br>directly with Lvlb to keep both good log-likelihoods and<br>sample quality.</p>",
            "id": 127,
            "page": 8,
            "text": "than FID. This could be caused by a variety of factors, such as 1) an unexpectedly high irreducible loss (Henighan , 2020) for this type of diffusion model, or 2) the model overfitting to the training distribution. We also note that these models do not achieve optimal log-likelihoods in general because they were trained with our Lhybrid objective and not directly with Lvlb to keep both good log-likelihoods and sample quality."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2182
                },
                {
                    "x": 582,
                    "y": 2182
                },
                {
                    "x": 582,
                    "y": 2238
                },
                {
                    "x": 222,
                    "y": 2238
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:20px'>7. Related Work</p>",
            "id": 128,
            "page": 8,
            "text": "7. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2268
                },
                {
                    "x": 1214,
                    "y": 2268
                },
                {
                    "x": 1214,
                    "y": 2768
                },
                {
                    "x": 224,
                    "y": 2768
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:16px'>Chen et al. (2020b) and Kong et al. (2020) are two recent<br>works that use DDPMs to produce high fidelity audio condi-<br>tioned on mel-spectrograms. Concurrent to our work, Chen<br>et al. (2020b) use a combination of improved schedule and<br>L1 loss to allow sampling with fewer steps with very lit-<br>tle reduction in sample quality. However, compared to our<br>unconditional image generation task, their generative task<br>has a strong input conditioning signal provided by the mel-<br>spectrograms, and we hypothesize that this makes it easier<br>to sample with fewer diffusion steps.</p>",
            "id": 129,
            "page": 8,
            "text": "Chen  (2020b) and Kong  (2020) are two recent works that use DDPMs to produce high fidelity audio conditioned on mel-spectrograms. Concurrent to our work, Chen  (2020b) use a combination of improved schedule and L1 loss to allow sampling with fewer steps with very little reduction in sample quality. However, compared to our unconditional image generation task, their generative task has a strong input conditioning signal provided by the melspectrograms, and we hypothesize that this makes it easier to sample with fewer diffusion steps."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2793
                },
                {
                    "x": 1214,
                    "y": 2793
                },
                {
                    "x": 1214,
                    "y": 2995
                },
                {
                    "x": 223,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:16px'>Jolicoeur-Martineau et al. (2020) explored score matching<br>in the image domain, and constructed an adversarial training<br>objective to produce better xo predictions. However, they<br>found that choosing a better network architecture removed</p>",
            "id": 130,
            "page": 8,
            "text": "Jolicoeur-Martineau  (2020) explored score matching in the image domain, and constructed an adversarial training objective to produce better xo predictions. However, they found that choosing a better network architecture removed"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 285
                },
                {
                    "x": 2265,
                    "y": 285
                },
                {
                    "x": 2265,
                    "y": 433
                },
                {
                    "x": 1271,
                    "y": 433
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:16px'>the need for this adversarial objective, suggesting that the ad-<br>versarial objective is not necessary for powerful generative<br>modeling.</p>",
            "id": 131,
            "page": 8,
            "text": "the need for this adversarial objective, suggesting that the adversarial objective is not necessary for powerful generative modeling."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 458
                },
                {
                    "x": 2266,
                    "y": 458
                },
                {
                    "x": 2266,
                    "y": 1357
                },
                {
                    "x": 1268,
                    "y": 1357
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>Parallel to our work, Song et al. (2020a) and Song et al.<br>(2020b) propose fast sampling algorithms for models trained<br>with the DDPM objective by leveraging different sampling<br>processes. Song et al. (2020a) does this by deriving an im-<br>plicit generative model that has the same marginal noise<br>distributions as DDPMs while deterministically mapping<br>noise to images. Song et al. (2020b) model the diffusion<br>process as the discretization of a continuous SDE, and ob-<br>serve that there exists an ODE that corresponds to sampling<br>from the reverse SDE. By varying the numerical precision<br>of an ODE solver, they can sample with fewer function<br>evaluations. However, they note that this technique obtains<br>worse samples than ancestral sampling when used directly,<br>and only achieves better FID when combined with Langevin<br>corrector steps. This in turn requires hand-tuning of a signal-<br>to-noise ratio for the Langevin steps. Our method allows<br>fast sampling directly from the ancestral process, which<br>removes the need for extra hyperparameters.</p>",
            "id": 132,
            "page": 8,
            "text": "Parallel to our work, Song  (2020a) and Song  (2020b) propose fast sampling algorithms for models trained with the DDPM objective by leveraging different sampling processes. Song  (2020a) does this by deriving an implicit generative model that has the same marginal noise distributions as DDPMs while deterministically mapping noise to images. Song  (2020b) model the diffusion process as the discretization of a continuous SDE, and observe that there exists an ODE that corresponds to sampling from the reverse SDE. By varying the numerical precision of an ODE solver, they can sample with fewer function evaluations. However, they note that this technique obtains worse samples than ancestral sampling when used directly, and only achieves better FID when combined with Langevin corrector steps. This in turn requires hand-tuning of a signalto-noise ratio for the Langevin steps. Our method allows fast sampling directly from the ancestral process, which removes the need for extra hyperparameters."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1381
                },
                {
                    "x": 2264,
                    "y": 1381
                },
                {
                    "x": 2264,
                    "y": 1582
                },
                {
                    "x": 1271,
                    "y": 1582
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:16px'>Also in parallel, Gao et al. (2020) develops a diffusion model<br>with reverse diffusion steps modeled by an energy-based<br>model. A potential implication of this approach is that fewer<br>diffusion steps should be needed to achieve good samples.</p>",
            "id": 133,
            "page": 8,
            "text": "Also in parallel, Gao  (2020) develops a diffusion model with reverse diffusion steps modeled by an energy-based model. A potential implication of this approach is that fewer diffusion steps should be needed to achieve good samples."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1643
                },
                {
                    "x": 1571,
                    "y": 1643
                },
                {
                    "x": 1571,
                    "y": 1696
                },
                {
                    "x": 1274,
                    "y": 1696
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:22px'>8. Conclusion</p>",
            "id": 134,
            "page": 8,
            "text": "8. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1728
                },
                {
                    "x": 2265,
                    "y": 1728
                },
                {
                    "x": 2265,
                    "y": 2128
                },
                {
                    "x": 1273,
                    "y": 2128
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:18px'>We have shown that, with a few modifications, DDPMs can<br>sample much faster and achieve better log-likelihoods with<br>little impact on sample quality. The likelihood is improved<br>by learning 20 using our parameterization and Lhybrid ob-<br>jective. This brings the likelihood of these models much<br>closer to other likelihood-based models. We surprisingly<br>discover that this change also allows sampling from these<br>models with many fewer steps.</p>",
            "id": 135,
            "page": 8,
            "text": "We have shown that, with a few modifications, DDPMs can sample much faster and achieve better log-likelihoods with little impact on sample quality. The likelihood is improved by learning 20 using our parameterization and Lhybrid objective. This brings the likelihood of these models much closer to other likelihood-based models. We surprisingly discover that this change also allows sampling from these models with many fewer steps."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2151
                },
                {
                    "x": 2265,
                    "y": 2151
                },
                {
                    "x": 2265,
                    "y": 2452
                },
                {
                    "x": 1272,
                    "y": 2452
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:16px'>We have also found that DDPMs can match the sample qual-<br>ity of GANs while achieving much better mode coverage<br>as measured by recall. Furthermore, we have investigated<br>how DDPMs scale with the amount of available training<br>compute, and found that more training compute trivially<br>leads to better sample quality and log-likelihood.</p>",
            "id": 136,
            "page": 8,
            "text": "We have also found that DDPMs can match the sample quality of GANs while achieving much better mode coverage as measured by recall. Furthermore, we have investigated how DDPMs scale with the amount of available training compute, and found that more training compute trivially leads to better sample quality and log-likelihood."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2474
                },
                {
                    "x": 2267,
                    "y": 2474
                },
                {
                    "x": 2267,
                    "y": 2824
                },
                {
                    "x": 1272,
                    "y": 2824
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:16px'>The combination of these results makes DDPMs an attrac-<br>tive choice for generative modeling, since they combine<br>good log-likelihoods, high-quality samples, and reasonably<br>fast sampling with a well-grounded, stationary training ob-<br>jective that scales easily with training compute. These re-<br>sults indicate that DDPMs are a promising direction for<br>future research.</p>",
            "id": 137,
            "page": 8,
            "text": "The combination of these results makes DDPMs an attractive choice for generative modeling, since they combine good log-likelihoods, high-quality samples, and reasonably fast sampling with a well-grounded, stationary training objective that scales easily with training compute. These results indicate that DDPMs are a promising direction for future research."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='138' style='font-size:16px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 138,
            "page": 9,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2229,
                    "y": 194
                },
                {
                    "x": 2259,
                    "y": 194
                },
                {
                    "x": 2259,
                    "y": 231
                },
                {
                    "x": 2229,
                    "y": 231
                }
            ],
            "category": "header",
            "html": "<br><header id='139' style='font-size:14px'>9</header>",
            "id": 139,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 278
                },
                {
                    "x": 468,
                    "y": 278
                },
                {
                    "x": 468,
                    "y": 332
                },
                {
                    "x": 226,
                    "y": 332
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:22px'>References</p>",
            "id": 140,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 359
                },
                {
                    "x": 1215,
                    "y": 359
                },
                {
                    "x": 1215,
                    "y": 507
                },
                {
                    "x": 224,
                    "y": 507
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:20px'>Brock, A., Donahue, J., and Simonyan, K. Large scale gan<br>training for high fidelity natural image synthesis. arXiv<br>preprint arXiv:1809.11096, 2018.</p>",
            "id": 141,
            "page": 9,
            "text": "Brock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 551
                },
                {
                    "x": 1220,
                    "y": 551
                },
                {
                    "x": 1220,
                    "y": 951
                },
                {
                    "x": 226,
                    "y": 951
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:20px'>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,<br>J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,<br>Askell, A., Agarwal, S., Herbert- Voss, A., Krueger, G.,<br>Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,<br>J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,<br>Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,<br>S., Radford, A., Sutskever, I., and Amodei, D. Language<br>models are few-shot learners, 2020.</p>",
            "id": 142,
            "page": 9,
            "text": "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert- Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 995
                },
                {
                    "x": 1220,
                    "y": 995
                },
                {
                    "x": 1220,
                    "y": 1245
                },
                {
                    "x": 224,
                    "y": 1245
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:16px'>Chen, M., Radford, A., Child, R., Wu, J., Jun, H.,<br>Dhariwal, P., Luan, D., and Sutskever, I. Genera-<br>tive pretraining from pixels, 2020a. URL https:<br>/ / cdn · openai · com/papers/Generative_<br>Pretraining_from_Pixels_V2 · pdf.</p>",
            "id": 143,
            "page": 9,
            "text": "Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Dhariwal, P., Luan, D., and Sutskever, I. Generative pretraining from pixels, 2020a. URL https: / / cdn · openai · com/papers/Generative_ Pretraining_from_Pixels_V2 · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1287
                },
                {
                    "x": 1216,
                    "y": 1287
                },
                {
                    "x": 1216,
                    "y": 1438
                },
                {
                    "x": 225,
                    "y": 1438
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:20px'>Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and<br>Chan, W. Wavegrad: Estimating gradients for waveform<br>generation, 2020b.</p>",
            "id": 144,
            "page": 9,
            "text": "Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. Wavegrad: Estimating gradients for waveform generation, 2020b."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1482
                },
                {
                    "x": 1217,
                    "y": 1482
                },
                {
                    "x": 1217,
                    "y": 1679
                },
                {
                    "x": 226,
                    "y": 1679
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P.<br>Pixelsnail: An improved autoregressive generative model.<br>In International Conference on Machine Learning, pp.<br>864-872. PMLR, 2018.</p>",
            "id": 145,
            "page": 9,
            "text": "Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P. Pixelsnail: An improved autoregressive generative model. In International Conference on Machine Learning, pp. 864-872. PMLR, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1724
                },
                {
                    "x": 1217,
                    "y": 1724
                },
                {
                    "x": 1217,
                    "y": 1873
                },
                {
                    "x": 224,
                    "y": 1873
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:16px'>Child, R. Very deep vaes generalize autoregressive models<br>and can outperform them on images. arXiv preprint<br>arXiv:2011.10650, 2020.</p>",
            "id": 146,
            "page": 9,
            "text": "Child, R. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv preprint arXiv:2011.10650, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1918
                },
                {
                    "x": 1213,
                    "y": 1918
                },
                {
                    "x": 1213,
                    "y": 2019
                },
                {
                    "x": 225,
                    "y": 2019
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:20px'>Child, R., Gray, S., Radford, A., and Sutskever, I. Generat-<br>ing long sequences with sparse transformers, 2019.</p>",
            "id": 147,
            "page": 9,
            "text": "Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2063
                },
                {
                    "x": 1217,
                    "y": 2063
                },
                {
                    "x": 1217,
                    "y": 2209
                },
                {
                    "x": 225,
                    "y": 2209
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:20px'>Gao, R., Song, Y., Poole, B., Wu, Y. N., and Kingma, D. P.<br>Learning energy-based models by diffusion recovery like-<br>lihood, 2020.</p>",
            "id": 148,
            "page": 9,
            "text": "Gao, R., Song, Y., Poole, B., Wu, Y. N., and Kingma, D. P. Learning energy-based models by diffusion recovery likelihood, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2255
                },
                {
                    "x": 1215,
                    "y": 2255
                },
                {
                    "x": 1215,
                    "y": 2356
                },
                {
                    "x": 222,
                    "y": 2356
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:20px'>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual<br>learning for image recognition, 2015.</p>",
            "id": 149,
            "page": 9,
            "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2399
                },
                {
                    "x": 1217,
                    "y": 2399
                },
                {
                    "x": 1217,
                    "y": 2699
                },
                {
                    "x": 225,
                    "y": 2699
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:20px'>Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C.,<br>Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S.,<br>Hallacy, C., Mann, B., Radford, A., Ramesh, A., Ryder,<br>N., Ziegler, D. M., Schulman, J., Amodei, D., and Mc-<br>Candlish, S. Scaling laws for autoregressive generative<br>modeling, 2020.</p>",
            "id": 150,
            "page": 9,
            "text": "Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., Hallacy, C., Mann, B., Radford, A., Ramesh, A., Ryder, N., Ziegler, D. M., Schulman, J., Amodei, D., and McCandlish, S. Scaling laws for autoregressive generative modeling, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2742
                },
                {
                    "x": 1215,
                    "y": 2742
                },
                {
                    "x": 1215,
                    "y": 2991
                },
                {
                    "x": 225,
                    "y": 2991
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and<br>Hochreiter, S. Gans trained by a two time-scale update<br>rule converge to a local nash equilibrium. Advances in<br>Neural Information Processing Systems 30 (NIPS 2017),<br>2017.</p>",
            "id": 151,
            "page": 9,
            "text": "Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 281
                },
                {
                    "x": 2266,
                    "y": 281
                },
                {
                    "x": 2266,
                    "y": 483
                },
                {
                    "x": 1273,
                    "y": 483
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='152' style='font-size:18px'>Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P.<br>Flow++: Improving flow-based generative models with<br>variational dequantization and architecture design. arXiv<br>preprint arXiv:1902.00275, 2019.</p>",
            "id": 152,
            "page": 9,
            "text": "Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. Flow++: Improving flow-based generative models with variational dequantization and architecture design. arXiv preprint arXiv:1902.00275, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 518
                },
                {
                    "x": 2268,
                    "y": 518
                },
                {
                    "x": 2268,
                    "y": 619
                },
                {
                    "x": 1275,
                    "y": 619
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-<br>bilistic models, 2020.</p>",
            "id": 153,
            "page": 9,
            "text": "Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 657
                },
                {
                    "x": 2263,
                    "y": 657
                },
                {
                    "x": 2263,
                    "y": 806
                },
                {
                    "x": 1276,
                    "y": 806
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:16px'>Hyvarinen, A. Estimation of non-normalized statistical<br>models by score matching. Journal of Machine Learning<br>Research, 6(Apr):695-709, 2005.</p>",
            "id": 154,
            "page": 9,
            "text": "Hyvarinen, A. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(Apr):695-709, 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 843
                },
                {
                    "x": 2264,
                    "y": 843
                },
                {
                    "x": 2264,
                    "y": 993
                },
                {
                    "x": 1276,
                    "y": 993
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:18px'>Jolicoeur-Martineau, A., Piche-Taillefer, R., des Combes,<br>R. T., and Mitliagkas, I. Adversarial score matching and<br>improved sampling for image generation, 2020.</p>",
            "id": 155,
            "page": 9,
            "text": "Jolicoeur-Martineau, A., Piche-Taillefer, R., des Combes, R. T., and Mitliagkas, I. Adversarial score matching and improved sampling for image generation, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1031
                },
                {
                    "x": 2267,
                    "y": 1031
                },
                {
                    "x": 2267,
                    "y": 1226
                },
                {
                    "x": 1275,
                    "y": 1226
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:20px'>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,<br>Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and<br>Amodei, D. Scaling laws for neural language models,<br>2020.</p>",
            "id": 156,
            "page": 9,
            "text": "Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1264
                },
                {
                    "x": 2262,
                    "y": 1264
                },
                {
                    "x": 2262,
                    "y": 1363
                },
                {
                    "x": 1276,
                    "y": 1363
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:16px'>Kingma, D. P. and Ba, J. Adam: A method for stochastic<br>optimization, 2014.</p>",
            "id": 157,
            "page": 9,
            "text": "Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1401
                },
                {
                    "x": 2265,
                    "y": 1401
                },
                {
                    "x": 2265,
                    "y": 1551
                },
                {
                    "x": 1275,
                    "y": 1551
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:16px'>Kingma, D. P. and Dhariwal, P. Glow: Generative flow<br>with invertible 1x1 convolutions. In Advances in neural<br>information processing systems, pp. 10215-10224, 2018.</p>",
            "id": 158,
            "page": 9,
            "text": "Kingma, D. P. and Dhariwal, P. Glow: Generative flow with invertible 1x1 convolutions. In Advances in neural information processing systems, pp. 10215-10224, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1588
                },
                {
                    "x": 2264,
                    "y": 1588
                },
                {
                    "x": 2264,
                    "y": 1688
                },
                {
                    "x": 1276,
                    "y": 1688
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:18px'>Kingma, D. P. and Welling, M. Auto-encoding variational<br>bayes, 2013.</p>",
            "id": 159,
            "page": 9,
            "text": "Kingma, D. P. and Welling, M. Auto-encoding variational bayes, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1725
                },
                {
                    "x": 2267,
                    "y": 1725
                },
                {
                    "x": 2267,
                    "y": 1871
                },
                {
                    "x": 1277,
                    "y": 1871
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:20px'>Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B.<br>Diffwave: A versatile diffusion model for audio synthesis,<br>2020.</p>",
            "id": 160,
            "page": 9,
            "text": "Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. Diffwave: A versatile diffusion model for audio synthesis, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1911
                },
                {
                    "x": 2264,
                    "y": 1911
                },
                {
                    "x": 2264,
                    "y": 2109
                },
                {
                    "x": 1275,
                    "y": 2109
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:14px'>Krizhevsky, A. Learning multiple layers of<br>features from tiny images, 2009. URL<br>http : / / www · CS · toronto · edu/ ~ kriz/<br>learning-features-2009-TR · pdf.</p>",
            "id": 161,
            "page": 9,
            "text": "Krizhevsky, A. Learning multiple layers of features from tiny images, 2009. URL http : / / www · CS · toronto · edu/ ~ kriz/ learning-features-2009-TR · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2147
                },
                {
                    "x": 2266,
                    "y": 2147
                },
                {
                    "x": 2266,
                    "y": 2296
                },
                {
                    "x": 1276,
                    "y": 2296
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:18px'>Kynka�nniemi, T., Karras, T., Laine, S., Lehtinen, J., and<br>Aila, T. Improved precision and recall metric for assess-<br>ing generative models, 2019.</p>",
            "id": 162,
            "page": 9,
            "text": "Kynka�nniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2333
                },
                {
                    "x": 2264,
                    "y": 2333
                },
                {
                    "x": 2264,
                    "y": 2432
                },
                {
                    "x": 1275,
                    "y": 2432
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:18px'>McCandlish, S., Kaplan, J., Amodei, D., and Team, 0. D.<br>An empirical model of large-batch training, 2018.</p>",
            "id": 163,
            "page": 9,
            "text": "McCandlish, S., Kaplan, J., Amodei, D., and Team, 0. D. An empirical model of large-batch training, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2470
                },
                {
                    "x": 2264,
                    "y": 2470
                },
                {
                    "x": 2264,
                    "y": 2619
                },
                {
                    "x": 1276,
                    "y": 2619
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:18px'>Menick, J. and Kalchbrenner, N. Generating high fidelity im-<br>ages with subscale pixel networks and multidimensional<br>upscaling, 2018.</p>",
            "id": 164,
            "page": 9,
            "text": "Menick, J. and Kalchbrenner, N. Generating high fidelity images with subscale pixel networks and multidimensional upscaling, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2658
                },
                {
                    "x": 2267,
                    "y": 2658
                },
                {
                    "x": 2267,
                    "y": 2805
                },
                {
                    "x": 1275,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:18px'>Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer,<br>N., Ku, A., and Tran, D. Image transformer. arXiv<br>preprint arXiv:1802.05751, 2018.</p>",
            "id": 165,
            "page": 9,
            "text": "Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and Tran, D. Image transformer. arXiv preprint arXiv:1802.05751, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2843
                },
                {
                    "x": 2264,
                    "y": 2843
                },
                {
                    "x": 2264,
                    "y": 2992
                },
                {
                    "x": 1276,
                    "y": 2992
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:16px'>Ravuri, S. and Vinyals, 0. Classification accuracy score<br>for conditional generative models. arXiv preprint<br>arXiv:1905.10887, 2019.</p>",
            "id": 166,
            "page": 9,
            "text": "Ravuri, S. and Vinyals, 0. Classification accuracy score for conditional generative models. arXiv preprint arXiv:1905.10887, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='167' style='font-size:16px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 167,
            "page": 10,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2213,
                    "y": 195
                },
                {
                    "x": 2258,
                    "y": 195
                },
                {
                    "x": 2258,
                    "y": 234
                },
                {
                    "x": 2213,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<br><header id='168' style='font-size:14px'>10</header>",
            "id": 168,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 285
                },
                {
                    "x": 1214,
                    "y": 285
                },
                {
                    "x": 1214,
                    "y": 383
                },
                {
                    "x": 224,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:18px'>Razavi, A., van den Oord, A., and Vinyals, 0. Generating<br>diverse high-fidelity images with vq-vae-2, 2019.</p>",
            "id": 169,
            "page": 10,
            "text": "Razavi, A., van den Oord, A., and Vinyals, 0. Generating diverse high-fidelity images with vq-vae-2, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 417
                },
                {
                    "x": 1215,
                    "y": 417
                },
                {
                    "x": 1215,
                    "y": 565
                },
                {
                    "x": 225,
                    "y": 565
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:18px'>Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient<br>content-based sparse attention with routing transformers,<br>2020.</p>",
            "id": 170,
            "page": 10,
            "text": "Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 599
                },
                {
                    "x": 1216,
                    "y": 599
                },
                {
                    "x": 1216,
                    "y": 749
                },
                {
                    "x": 224,
                    "y": 749
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:22px'>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,<br>Radford, A., and Chen, X. Improved techniques for<br>training gans, 2016.</p>",
            "id": 171,
            "page": 10,
            "text": "Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 783
                },
                {
                    "x": 1214,
                    "y": 783
                },
                {
                    "x": 1214,
                    "y": 929
                },
                {
                    "x": 224,
                    "y": 929
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:18px'>Salimans, T., Karpathy, A., Chen, X., and Kingma, D. P. Pix-<br>elcnn++: Improving the pixelcnn with discretized logistic<br>mixture likelihood and other modifications, 2017.</p>",
            "id": 172,
            "page": 10,
            "text": "Salimans, T., Karpathy, A., Chen, X., and Kingma, D. P. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 965
                },
                {
                    "x": 1214,
                    "y": 965
                },
                {
                    "x": 1214,
                    "y": 1113
                },
                {
                    "x": 224,
                    "y": 1113
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:20px'>Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and<br>Ganguli, S. Deep unsupervised learning using nonequi-<br>librium thermodynamics, 2015.</p>",
            "id": 173,
            "page": 10,
            "text": "Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1148
                },
                {
                    "x": 1213,
                    "y": 1148
                },
                {
                    "x": 1213,
                    "y": 1245
                },
                {
                    "x": 223,
                    "y": 1245
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:20px'>Song, J., Meng, C., and Ermon, S. Denoising diffusion<br>implicit models, 2020a.</p>",
            "id": 174,
            "page": 10,
            "text": "Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models, 2020a."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1280
                },
                {
                    "x": 1215,
                    "y": 1280
                },
                {
                    "x": 1215,
                    "y": 1431
                },
                {
                    "x": 225,
                    "y": 1431
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:18px'>Song, Y. and Ermon, S. Generative modeling by estimating<br>gradients of the data distribution. In Advances in Neural<br>Information Processing Systems, pp. 11918-11930, 2019.</p>",
            "id": 175,
            "page": 10,
            "text": "Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pp. 11918-11930, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1463
                },
                {
                    "x": 1217,
                    "y": 1463
                },
                {
                    "x": 1217,
                    "y": 1610
                },
                {
                    "x": 224,
                    "y": 1610
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:18px'>Song, Y. and Ermon, S. Improved techniques for train-<br>ing score-based generative models. arXiv preprint<br>arXiv:2006.09011, 2020.</p>",
            "id": 176,
            "page": 10,
            "text": "Song, Y. and Ermon, S. Improved techniques for training score-based generative models. arXiv preprint arXiv:2006.09011, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1646
                },
                {
                    "x": 1215,
                    "y": 1646
                },
                {
                    "x": 1215,
                    "y": 1795
                },
                {
                    "x": 225,
                    "y": 1795
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:20px'>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-<br>mon, S., and Poole, B. Score-based generative modeling<br>through stochastic differential equations, 2020b.</p>",
            "id": 177,
            "page": 10,
            "text": "Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations, 2020b."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1829
                },
                {
                    "x": 1215,
                    "y": 1829
                },
                {
                    "x": 1215,
                    "y": 1976
                },
                {
                    "x": 225,
                    "y": 1976
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:16px'>Vahdat, A. and Kautz, J. Nvae: A deep hierarchical vari-<br>ational autoencoder. arXiv preprint arXiv:2007.03898,<br>2020.</p>",
            "id": 178,
            "page": 10,
            "text": "Vahdat, A. and Kautz, J. Nvae: A deep hierarchical variational autoencoder. arXiv preprint arXiv:2007.03898, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2011
                },
                {
                    "x": 1214,
                    "y": 2011
                },
                {
                    "x": 1214,
                    "y": 2109
                },
                {
                    "x": 225,
                    "y": 2109
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:16px'>van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K.<br>Pixel recurrent neural networks, 2016a.</p>",
            "id": 179,
            "page": 10,
            "text": "van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks, 2016a."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2146
                },
                {
                    "x": 1216,
                    "y": 2146
                },
                {
                    "x": 1216,
                    "y": 2341
                },
                {
                    "x": 226,
                    "y": 2341
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:16px'>van den Oord, A., Kalchbrenner, N., Vinyals, 0., Espeholt,<br>L., Graves, A., and Kavukcuoglu, K. Conditional image<br>generation with pixelcnn decoders, 2016b. URL http :<br>/ / image-net · org/ small / download · php.</p>",
            "id": 180,
            "page": 10,
            "text": "van den Oord, A., Kalchbrenner, N., Vinyals, 0., Espeholt, L., Graves, A., and Kavukcuoglu, K. Conditional image generation with pixelcnn decoders, 2016b. URL http : / / image-net · org/ small / download · php."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2377
                },
                {
                    "x": 1214,
                    "y": 2377
                },
                {
                    "x": 1214,
                    "y": 2525
                },
                {
                    "x": 225,
                    "y": 2525
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:22px'>van den Oord, A., Kalchbrenner, N., Vinyals, 0., Espeholt,<br>L., Graves, A., and Kavukcuoglu, K. Conditional image<br>generation with pixelcnn decoders, 2016c.</p>",
            "id": 181,
            "page": 10,
            "text": "van den Oord, A., Kalchbrenner, N., Vinyals, 0., Espeholt, L., Graves, A., and Kavukcuoglu, K. Conditional image generation with pixelcnn decoders, 2016c."
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 2560
                },
                {
                    "x": 1214,
                    "y": 2560
                },
                {
                    "x": 1214,
                    "y": 2707
                },
                {
                    "x": 227,
                    "y": 2707
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:18px'>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,<br>L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention<br>is all you need, 2017.</p>",
            "id": 182,
            "page": 10,
            "text": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2739
                },
                {
                    "x": 1214,
                    "y": 2739
                },
                {
                    "x": 1214,
                    "y": 2893
                },
                {
                    "x": 226,
                    "y": 2893
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:20px'>Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and<br>Xiao, J. Lsun: Construction of a large-scale image dataset<br>using deep learning with humans in the loop, 2015.</p>",
            "id": 183,
            "page": 10,
            "text": "Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 193
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='184' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 184,
            "page": 11,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2213,
                    "y": 195
                },
                {
                    "x": 2255,
                    "y": 195
                },
                {
                    "x": 2255,
                    "y": 234
                },
                {
                    "x": 2213,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<br><header id='185' style='font-size:16px'>11</header>",
            "id": 185,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 279
                },
                {
                    "x": 675,
                    "y": 279
                },
                {
                    "x": 675,
                    "y": 336
                },
                {
                    "x": 227,
                    "y": 336
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:22px'>A. Hyperparameters</p>",
            "id": 186,
            "page": 11,
            "text": "A. Hyperparameters"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 364
                },
                {
                    "x": 1218,
                    "y": 364
                },
                {
                    "x": 1218,
                    "y": 1063
                },
                {
                    "x": 223,
                    "y": 1063
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:16px'>For all of our experiments, we use a UNet model architec-<br>ture4 similar to that used by Ho et al. (2020). We changed<br>the attention layers to use multi-head attention (Vaswani<br>et al., 2017), and opted to use four attention heads rather<br>than one (while keeping the same total number of channels).<br>We employ attention not only at the 16x16 resolution, but<br>also at the 8x8 resolution. Additionally, we changed the<br>way the model conditions on t. In particular, instead of com-<br>puting a conditioning vector v and injecting it into hidden<br>state h as GroupNorm(h + v), we compute conditioning<br>vectors w and 6 and inject them into the hidden state as<br>GroupNorm(h)(w + 1) + b. We found in preliminary ex-<br>periments on ImageNet 64 x 64 that these modifications<br>slightly improved FID.</p>",
            "id": 187,
            "page": 11,
            "text": "For all of our experiments, we use a UNet model architecture4 similar to that used by Ho  (2020). We changed the attention layers to use multi-head attention (Vaswani , 2017), and opted to use four attention heads rather than one (while keeping the same total number of channels). We employ attention not only at the 16x16 resolution, but also at the 8x8 resolution. Additionally, we changed the way the model conditions on t. In particular, instead of computing a conditioning vector v and injecting it into hidden state h as GroupNorm(h + v), we compute conditioning vectors w and 6 and inject them into the hidden state as GroupNorm(h)(w + 1) + b. We found in preliminary experiments on ImageNet 64 x 64 that these modifications slightly improved FID."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1085
                },
                {
                    "x": 1216,
                    "y": 1085
                },
                {
                    "x": 1216,
                    "y": 1636
                },
                {
                    "x": 224,
                    "y": 1636
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:16px'>For ImageNet 64 x 64 the architecture we use is described<br>as follows. The downsampling stack performs four steps of<br>downsampling, each with three residual blocks (He et al.,<br>2015). The upsampling stack is setup as a mirror image of<br>the downsampling stack. From highest to lowest resolution,<br>the UNet stages use [C, 2C, 3C, 4C] channels, respectively.<br>In our ImageNet 64 x 64 ablations, we set C = 128, but<br>we experiment with scaling C in a later section. We esti-<br>mate that, with C = 128, our model is comprised of 120M<br>parameters and requires roughly 39 billion FLOPs in the<br>forward pass.</p>",
            "id": 188,
            "page": 11,
            "text": "For ImageNet 64 x 64 the architecture we use is described as follows. The downsampling stack performs four steps of downsampling, each with three residual blocks (He , 2015). The upsampling stack is setup as a mirror image of the downsampling stack. From highest to lowest resolution, the UNet stages use [C, 2C, 3C, 4C] channels, respectively. In our ImageNet 64 x 64 ablations, we set C = 128, but we experiment with scaling C in a later section. We estimate that, with C = 128, our model is comprised of 120M parameters and requires roughly 39 billion FLOPs in the forward pass."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1660
                },
                {
                    "x": 1214,
                    "y": 1660
                },
                {
                    "x": 1214,
                    "y": 1959
                },
                {
                    "x": 224,
                    "y": 1959
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:18px'>For our CIFAR-10 experiments, we use a smaller model with<br>three resblocks per downsampling stage and layer widths<br>[C, 2C, 2C, 2C] with C = 128. We swept over dropout<br>values {0.1, 0.2, 0.3} and found that 0.1 worked best for<br>the linear schedule while 0.3 worked best for our cosine<br>schedule. We expand upon this in Section F.</p>",
            "id": 189,
            "page": 11,
            "text": "For our CIFAR-10 experiments, we use a smaller model with three resblocks per downsampling stage and layer widths [C, 2C, 2C, 2C] with C = 128. We swept over dropout values {0.1, 0.2, 0.3} and found that 0.1 worked best for the linear schedule while 0.3 worked best for our cosine schedule. We expand upon this in Section F."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1984
                },
                {
                    "x": 1216,
                    "y": 1984
                },
                {
                    "x": 1216,
                    "y": 2383
                },
                {
                    "x": 223,
                    "y": 2383
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:16px'>We use Adam (Kingma & Ba, 2014) for all of our experi-<br>ments. For most experiments, we use a batch size of 128,<br>a learning rate of 10-4 and an exponential moving aver-<br>,<br>age (EMA) over model parameters with a rate of 0.9999.<br>For our scaling experiments, we vary the learning rate to<br>accomodate for different model sizes. For our larger class-<br>conditional ImageNet 64 x 64 experiments, we scaled up<br>the batch size to 2048 for faster training on more GPUs.</p>",
            "id": 190,
            "page": 11,
            "text": "We use Adam (Kingma & Ba, 2014) for all of our experiments. For most experiments, we use a batch size of 128, a learning rate of 10-4 and an exponential moving aver, age (EMA) over model parameters with a rate of 0.9999. For our scaling experiments, we vary the learning rate to accomodate for different model sizes. For our larger classconditional ImageNet 64 x 64 experiments, we scaled up the batch size to 2048 for faster training on more GPUs."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2406
                },
                {
                    "x": 1213,
                    "y": 2406
                },
                {
                    "x": 1213,
                    "y": 2605
                },
                {
                    "x": 223,
                    "y": 2605
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:16px'>When using the linear noise schedule from Ho et al. (2020),<br>we linearly interpolate from B1 = 0.0001 /4 to B4000 =<br>0.02/4 to preserve the shape of at for the T = 4000 sched-<br>ule.</p>",
            "id": 191,
            "page": 11,
            "text": "When using the linear noise schedule from Ho  (2020), we linearly interpolate from B1 = 0.0001 /4 to B4000 = 0.02/4 to preserve the shape of at for the T = 4000 schedule."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2631
                },
                {
                    "x": 1213,
                    "y": 2631
                },
                {
                    "x": 1213,
                    "y": 2783
                },
                {
                    "x": 224,
                    "y": 2783
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:20px'>When computing FID we produce 50K samples from our<br>models, except for unconditional ImageNet 64 x 64 where<br>we produce 10K samples. Using only 10K samples biases</p>",
            "id": 192,
            "page": 11,
            "text": "When computing FID we produce 50K samples from our models, except for unconditional ImageNet 64 x 64 where we produce 10K samples. Using only 10K samples biases"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2811
                },
                {
                    "x": 1214,
                    "y": 2811
                },
                {
                    "x": 1214,
                    "y": 2943
                },
                {
                    "x": 224,
                    "y": 2943
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:14px'>4In initial experiments, we found that a ResNet-style architec-<br>ture with no downsampling achieved better log-likelihoods but<br>worse FIDs than the UNet architecture.</p>",
            "id": 193,
            "page": 11,
            "text": "4In initial experiments, we found that a ResNet-style architecture with no downsampling achieved better log-likelihoods but worse FIDs than the UNet architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 282
                },
                {
                    "x": 2263,
                    "y": 282
                },
                {
                    "x": 2263,
                    "y": 884
                },
                {
                    "x": 1271,
                    "y": 884
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='194' style='font-size:18px'>the FID to be higher, but requires much less compute for<br>sampling and helps do large ablations. Since we mainly use<br>FID for relative comparisons on unconditional ImageNet<br>64 x 64, this bias is acceptable. For computing the reference<br>distribution statistics we follow prior work (Ho et al., 2020;<br>Brock et al., 2018) and use the full training set for CIFAR-10<br>and ImageNet, and 50K training samples for LSUN. Note<br>that unconditional ImageNet 64 x 64 models are trained and<br>evaluated using the official ImageNet-64 dataset (van den<br>Oord et al., 2016a), whereas for class conditional ImageNet<br>64 x 64 and 256 x 256 we center crop and area downsample<br>images (Brock et al., 2018).</p>",
            "id": 194,
            "page": 11,
            "text": "the FID to be higher, but requires much less compute for sampling and helps do large ablations. Since we mainly use FID for relative comparisons on unconditional ImageNet 64 x 64, this bias is acceptable. For computing the reference distribution statistics we follow prior work (Ho , 2020; Brock , 2018) and use the full training set for CIFAR-10 and ImageNet, and 50K training samples for LSUN. Note that unconditional ImageNet 64 x 64 models are trained and evaluated using the official ImageNet-64 dataset (van den Oord , 2016a), whereas for class conditional ImageNet 64 x 64 and 256 x 256 we center crop and area downsample images (Brock , 2018)."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 944
                },
                {
                    "x": 2079,
                    "y": 944
                },
                {
                    "x": 2079,
                    "y": 1001
                },
                {
                    "x": 1275,
                    "y": 1001
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:20px'>B. Fast Sampling on LSUN 256 x 256</p>",
            "id": 195,
            "page": 11,
            "text": "B. Fast Sampling on LSUN 256 x 256"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1129
                },
                {
                    "x": 2252,
                    "y": 1129
                },
                {
                    "x": 2252,
                    "y": 1994
                },
                {
                    "x": 1278,
                    "y": 1994
                }
            ],
            "category": "figure",
            "html": "<figure><img id='196' style='font-size:14px' alt=\"�simple (�른 =Bt, batch=64, Ir=2e-5) Lsimple (DDIM, batch=64, Ir=2e-5)\n�simple (�른 =Bt, batch=128, Ir=1e-4) Lsimple (DDIM, batch=128, Ir=1e-4)\nLsimple (여론 =Bt, batch=64, Ir=2e-5) Lhybrid (batch=64, Ir=2e-5)\nLhybrid (batch=128, Ir=1e-4)\nLsimple (�른 = Bt, batch=128, Ir=1e-4)\n20.0\n17.5\n15.0\n12.5\n문 10.0\n7.5\n5.0\n2.5\n0.0\n102 103\nsampling steps\" data-coord=\"top-left:(1278,1129); bottom-right:(2252,1994)\" /></figure>",
            "id": 196,
            "page": 11,
            "text": "�simple (�른 =Bt, batch=64, Ir=2e-5) Lsimple (DDIM, batch=64, Ir=2e-5) �simple (�른 =Bt, batch=128, Ir=1e-4) Lsimple (DDIM, batch=128, Ir=1e-4) Lsimple (여론 =Bt, batch=64, Ir=2e-5) Lhybrid (batch=64, Ir=2e-5) Lhybrid (batch=128, Ir=1e-4) Lsimple (�른 = Bt, batch=128, Ir=1e-4) 20.0 17.5 15.0 12.5 문 10.0 7.5 5.0 2.5 0.0 102 103 sampling steps"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2018
                },
                {
                    "x": 2261,
                    "y": 2018
                },
                {
                    "x": 2261,
                    "y": 2109
                },
                {
                    "x": 1274,
                    "y": 2109
                }
            ],
            "category": "caption",
            "html": "<caption id='197' style='font-size:14px'>Figure 11. FID VS. number of sampling steps from an LSUN<br>256 x 256 bedroom model.</caption>",
            "id": 197,
            "page": 11,
            "text": "Figure 11. FID VS. number of sampling steps from an LSUN 256 x 256 bedroom model."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2219
                },
                {
                    "x": 2265,
                    "y": 2219
                },
                {
                    "x": 2265,
                    "y": 2617
                },
                {
                    "x": 1272,
                    "y": 2617
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:16px'>To test the effectiveness of our Lhybrid models on a high-<br>resolution domain, we trained both Lhybrid and Lsimple mod-<br>els on the LSUN bedroom (Yu et al., 2015) dataset. We<br>train two models: one with batch size 64 and learning rate<br>2 x 10-5 as in Ho et al. (2020), and another with a larger<br>batch size 128 and learning rate 10-4. All models were<br>trained with 153.6M examples, which is 2.4M training itera-<br>tions with batch size 64.</p>",
            "id": 198,
            "page": 11,
            "text": "To test the effectiveness of our Lhybrid models on a highresolution domain, we trained both Lhybrid and Lsimple models on the LSUN bedroom (Yu , 2015) dataset. We train two models: one with batch size 64 and learning rate 2 x 10-5 as in Ho  (2020), and another with a larger batch size 128 and learning rate 10-4. All models were trained with 153.6M examples, which is 2.4M training iterations with batch size 64."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2644
                },
                {
                    "x": 2262,
                    "y": 2644
                },
                {
                    "x": 2262,
                    "y": 2993
                },
                {
                    "x": 1271,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:16px'>Our results are displayed in Figure 11. We find that DDIM<br>outperforms our Lhybrid model when using fewer than 50<br>diffusion steps, while our Lhybrid model outperforms DDIM<br>with more than 50 diffusion steps. Interestingly, we note<br>that DDIM benefits from a smaller learning rate and batch<br>size, whereas our method is able to take advantage of a<br>larger learning rate and batch size.</p>",
            "id": 199,
            "page": 11,
            "text": "Our results are displayed in Figure 11. We find that DDIM outperforms our Lhybrid model when using fewer than 50 diffusion steps, while our Lhybrid model outperforms DDIM with more than 50 diffusion steps. Interestingly, we note that DDIM benefits from a smaller learning rate and batch size, whereas our method is able to take advantage of a larger learning rate and batch size."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 195
                },
                {
                    "x": 1054,
                    "y": 195
                },
                {
                    "x": 1054,
                    "y": 237
                },
                {
                    "x": 225,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='200' style='font-size:18px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 200,
            "page": 12,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2214,
                    "y": 194
                },
                {
                    "x": 2258,
                    "y": 194
                },
                {
                    "x": 2258,
                    "y": 234
                },
                {
                    "x": 2214,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<br><header id='201' style='font-size:16px'>12</header>",
            "id": 201,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 228,
                    "y": 280
                },
                {
                    "x": 1132,
                    "y": 280
                },
                {
                    "x": 1132,
                    "y": 332
                },
                {
                    "x": 228,
                    "y": 332
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:22px'>C. Sample Quality on ImageNet 256 x 256</p>",
            "id": 202,
            "page": 12,
            "text": "C. Sample Quality on ImageNet 256 x 256"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 367
                },
                {
                    "x": 1218,
                    "y": 367
                },
                {
                    "x": 1218,
                    "y": 1162
                },
                {
                    "x": 223,
                    "y": 1162
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:18px'>We trained two models on class conditional ImageNet<br>256 x 256. The first is a usual diffusion model that directly<br>models the 256 x 256 images. The second model reduces<br>compute by chaining a pretrained 64 x 64 model p(x64|y)<br>with another upsampling diffusion model p(x256|x64, y) to<br>upsample images to 256 x 256. For the upsampling model,<br>the downsampled image X64 is passed as extra conditioning<br>input to the UNet. This is similar to VQ-VAE-2 (Razavi<br>et al., 2019), which uses two stages of priors at different<br>latent resolutions to more efficiently learn global and local<br>features. The linear schedule worked better for 256 x 256<br>images, SO we used that for these results. Table 5 summa-<br>rizes our results. For VQ-VAE-2, we use the FIDs reported<br>in (Ravuri & Vinyals, 2019). Diffusion models still obtain<br>the best FIDs for a likelihood-based model, and close the<br>gap to GANs considerably.</p>",
            "id": 203,
            "page": 12,
            "text": "We trained two models on class conditional ImageNet 256 x 256. The first is a usual diffusion model that directly models the 256 x 256 images. The second model reduces compute by chaining a pretrained 64 x 64 model p(x64|y) with another upsampling diffusion model p(x256|x64, y) to upsample images to 256 x 256. For the upsampling model, the downsampled image X64 is passed as extra conditioning input to the UNet. This is similar to VQ-VAE-2 (Razavi , 2019), which uses two stages of priors at different latent resolutions to more efficiently learn global and local features. The linear schedule worked better for 256 x 256 images, SO we used that for these results. Table 5 summarizes our results. For VQ-VAE-2, we use the FIDs reported in (Ravuri & Vinyals, 2019). Diffusion models still obtain the best FIDs for a likelihood-based model, and close the gap to GANs considerably."
        },
        {
            "bounding_box": [
                {
                    "x": 267,
                    "y": 1199
                },
                {
                    "x": 1169,
                    "y": 1199
                },
                {
                    "x": 1169,
                    "y": 1511
                },
                {
                    "x": 267,
                    "y": 1511
                }
            ],
            "category": "table",
            "html": "<table id='204' style='font-size:20px'><tr><td>MODEL</td><td>FID</td></tr><tr><td>VQ-VAE-2 ((Razavi et al., 2019), two-stage)</td><td>38.1</td></tr><tr><td>Improved Diffusion (ours, single-stage)</td><td>31.5</td></tr><tr><td>Improved Diffusion (ours, two-stage)</td><td>12.3</td></tr><tr><td>BigGAN (Brock et al., 2018)</td><td>7.7</td></tr><tr><td>BigGAN-deep (Brock et al., 2018)</td><td>7.0</td></tr></table>",
            "id": 204,
            "page": 12,
            "text": "MODEL FID  VQ-VAE-2 ((Razavi , 2019), two-stage) 38.1  Improved Diffusion (ours, single-stage) 31.5  Improved Diffusion (ours, two-stage) 12.3  BigGAN (Brock , 2018) 7.7  BigGAN-deep (Brock , 2018)"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1547
                },
                {
                    "x": 1215,
                    "y": 1547
                },
                {
                    "x": 1215,
                    "y": 1682
                },
                {
                    "x": 226,
                    "y": 1682
                }
            ],
            "category": "caption",
            "html": "<caption id='205' style='font-size:14px'>Table 5. Sample quality comparison on class conditional Ima-<br>geNet 256 x 256. BigGAN FIDs are reported for the truncation<br>that results in the best FID.</caption>",
            "id": 205,
            "page": 12,
            "text": "Table 5. Sample quality comparison on class conditional ImageNet 256 x 256. BigGAN FIDs are reported for the truncation that results in the best FID."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 270
                },
                {
                    "x": 2260,
                    "y": 270
                },
                {
                    "x": 2260,
                    "y": 1257
                },
                {
                    "x": 1273,
                    "y": 1257
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='206' alt=\"\" data-coord=\"top-left:(1273,270); bottom-right:(2260,1257)\" /></figure>",
            "id": 206,
            "page": 12,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1283
                },
                {
                    "x": 2260,
                    "y": 1283
                },
                {
                    "x": 2260,
                    "y": 2271
                },
                {
                    "x": 1276,
                    "y": 2271
                }
            ],
            "category": "figure",
            "html": "<figure><img id='207' style='font-size:14px' alt=\"co.k\nSTA\" data-coord=\"top-left:(1276,1283); bottom-right:(2260,2271)\" /></figure>",
            "id": 207,
            "page": 12,
            "text": "co.k STA"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2315
                },
                {
                    "x": 2265,
                    "y": 2315
                },
                {
                    "x": 2265,
                    "y": 2547
                },
                {
                    "x": 1272,
                    "y": 2547
                }
            ],
            "category": "caption",
            "html": "<caption id='208' style='font-size:14px'>Figure 12. Random samples from two-stage class conditional Im-<br>ageNet 256 x 256 model. On top are random samples from the<br>64 x 64 model (FID 2.92), whereas on bottom are the results after<br>upsampling them to 256 x 256 (FID 12.3). Each model uses 250<br>sampling steps.</caption>",
            "id": 208,
            "page": 12,
            "text": "Figure 12. Random samples from two-stage class conditional ImageNet 256 x 256 model. On top are random samples from the 64 x 64 model (FID 2.92), whereas on bottom are the results after upsampling them to 256 x 256 (FID 12.3). Each model uses 250 sampling steps."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='209' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 209,
            "page": 13,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2214,
                    "y": 194
                },
                {
                    "x": 2257,
                    "y": 194
                },
                {
                    "x": 2257,
                    "y": 234
                },
                {
                    "x": 2214,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<br><header id='210' style='font-size:18px'>13</header>",
            "id": 210,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 278
                },
                {
                    "x": 1031,
                    "y": 278
                },
                {
                    "x": 1031,
                    "y": 336
                },
                {
                    "x": 226,
                    "y": 336
                }
            ],
            "category": "caption",
            "html": "<caption id='211' style='font-size:22px'>D. Combining Lhybrid and Lvlb Models</caption>",
            "id": 211,
            "page": 13,
            "text": "D. Combining Lhybrid and Lvlb Models"
        },
        {
            "bounding_box": [
                {
                    "x": 229,
                    "y": 393
                },
                {
                    "x": 1211,
                    "y": 393
                },
                {
                    "x": 1211,
                    "y": 1031
                },
                {
                    "x": 229,
                    "y": 1031
                }
            ],
            "category": "figure",
            "html": "<figure><img id='212' style='font-size:14px' alt=\"1.100\n1.075\n1.050\nLt(�hybrid)/Lt(�vlb)\n1.025\n1.000\n0.975\n0.950\n0.925\n0.900\n0 500 1000 1500 2000 2500 3000 3500 4000\ndiffusion step (t)\" data-coord=\"top-left:(229,393); bottom-right:(1211,1031)\" /></figure>",
            "id": 212,
            "page": 13,
            "text": "1.100 1.075 1.050 Lt(�hybrid)/Lt(�vlb) 1.025 1.000 0.975 0.950 0.925 0.900 0 500 1000 1500 2000 2500 3000 3500 4000 diffusion step (t)"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1089
                },
                {
                    "x": 1219,
                    "y": 1089
                },
                {
                    "x": 1219,
                    "y": 1231
                },
                {
                    "x": 224,
                    "y": 1231
                }
            ],
            "category": "caption",
            "html": "<caption id='213' style='font-size:16px'>Figure 13. The ratio between VLB terms for each diffusion step of<br>Ohybrid and Ovlb. Values less than 1.0 indicate that Ohybrid is \"better\"<br>than 0vlb for that timestep of the diffusion process.</caption>",
            "id": 213,
            "page": 13,
            "text": "Figure 13. The ratio between VLB terms for each diffusion step of Ohybrid and Ovlb. Values less than 1.0 indicate that Ohybrid is \"better\" than 0vlb for that timestep of the diffusion process."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1292
                },
                {
                    "x": 1213,
                    "y": 1292
                },
                {
                    "x": 1213,
                    "y": 1725
                },
                {
                    "x": 221,
                    "y": 1725
                }
            ],
            "category": "figure",
            "html": "<figure><img id='214' style='font-size:16px' alt=\"때문\n0\nvlb\n0\nhybrid\nensemble\" data-coord=\"top-left:(221,1292); bottom-right:(1213,1725)\" /></figure>",
            "id": 214,
            "page": 13,
            "text": "때문 0 vlb 0 hybrid ensemble"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1814
                },
                {
                    "x": 1215,
                    "y": 1814
                },
                {
                    "x": 1215,
                    "y": 1997
                },
                {
                    "x": 224,
                    "y": 1997
                }
            ],
            "category": "caption",
            "html": "<caption id='215' style='font-size:16px'>Figure 14. Samples from 0vlb and Ohybrid, as well as an ensemble<br>produced by using 0vlb for the first and last 100 diffusion steps. For<br>these samples, the seed was fixed, allowing a direct comparison<br>between models.</caption>",
            "id": 215,
            "page": 13,
            "text": "Figure 14. Samples from 0vlb and Ohybrid, as well as an ensemble produced by using 0vlb for the first and last 100 diffusion steps. For these samples, the seed was fixed, allowing a direct comparison between models."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2035
                },
                {
                    "x": 1215,
                    "y": 2035
                },
                {
                    "x": 1215,
                    "y": 2385
                },
                {
                    "x": 225,
                    "y": 2385
                }
            ],
            "category": "paragraph",
            "html": "<p id='216' style='font-size:18px'>To understand the trade-off between Lhybrid and Lvlb, we<br>show in Figure 13 that the model resulting from Lvlb (re-<br>ferred to as 0vlb) is better at the start and end of the diffusion<br>process, while the model resulting from Lhybrid (referred<br>to as Ohybrid) is better throughout the middle of the diffu-<br>sion process. This suggests that 0vlb is focusing more on<br>imperceptible details, hence the lower sample quality.</p>",
            "id": 216,
            "page": 13,
            "text": "To understand the trade-off between Lhybrid and Lvlb, we show in Figure 13 that the model resulting from Lvlb (referred to as 0vlb) is better at the start and end of the diffusion process, while the model resulting from Lhybrid (referred to as Ohybrid) is better throughout the middle of the diffusion process. This suggests that 0vlb is focusing more on imperceptible details, hence the lower sample quality."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2409
                },
                {
                    "x": 1215,
                    "y": 2409
                },
                {
                    "x": 1215,
                    "y": 2759
                },
                {
                    "x": 223,
                    "y": 2759
                }
            ],
            "category": "paragraph",
            "html": "<p id='217' style='font-size:18px'>Given the above observation, we performed an experiment<br>on ImageNet 64 x 64 to combine the two models by con-<br>structing an ensemble that uses Ohybrid for t E [100, T - 100)<br>and 0vlb elsewhere. We found that this model achieved an<br>FID of 19.9 and an NLL of 3.52 bits/dim. This is only<br>slightly worse than Ohybrid in terms of FID, while being bet-<br>ter than both models in terms of NLL.</p>",
            "id": 217,
            "page": 13,
            "text": "Given the above observation, we performed an experiment on ImageNet 64 x 64 to combine the two models by constructing an ensemble that uses Ohybrid for t E [100, T - 100) and 0vlb elsewhere. We found that this model achieved an FID of 19.9 and an NLL of 3.52 bits/dim. This is only slightly worse than Ohybrid in terms of FID, while being better than both models in terms of NLL."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 277
                },
                {
                    "x": 2242,
                    "y": 277
                },
                {
                    "x": 2242,
                    "y": 336
                },
                {
                    "x": 1276,
                    "y": 336
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='218' style='font-size:20px'>E. Log-likelihood with Fewer Diffusion Steps</p>",
            "id": 218,
            "page": 13,
            "text": "E. Log-likelihood with Fewer Diffusion Steps"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 394
                },
                {
                    "x": 2254,
                    "y": 394
                },
                {
                    "x": 2254,
                    "y": 1949
                },
                {
                    "x": 1279,
                    "y": 1949
                }
            ],
            "category": "figure",
            "html": "<figure><img id='219' style='font-size:14px' alt=\"Lsimple (02 = Bt, mid-training) Lsimple (DDIM, mid-training)\nLsimple (�2 = Bt, fully trained) Lsimple (DDIM, fully trained)\nLsimple (여론 = Bt, mid-training) Lhybrid (ours, mid-training)\nLhybrid (ours, fully trained)\n�simple (�2 II Bt, fully trained)\n4.2\n4.1\n4.0\n(bits/dim)\n3.9\nNLL\n3.8\n3.7\n3.6\n103\nevaluation steps\n4.0\n3.8\n(bits/dim)\n3.6\nNLL\n3.4\n3.2\n3.0\n103\nevaluation steps\" data-coord=\"top-left:(1279,394); bottom-right:(2254,1949)\" /></figure>",
            "id": 219,
            "page": 13,
            "text": "Lsimple (02 = Bt, mid-training) Lsimple (DDIM, mid-training) Lsimple (�2 = Bt, fully trained) Lsimple (DDIM, fully trained) Lsimple (여론 = Bt, mid-training) Lhybrid (ours, mid-training) Lhybrid (ours, fully trained) �simple (�2 II Bt, fully trained) 4.2 4.1 4.0 (bits/dim) 3.9 NLL 3.8 3.7 3.6 103 evaluation steps 4.0 3.8 (bits/dim) 3.6 NLL 3.4 3.2 3.0 103 evaluation steps"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2051
                },
                {
                    "x": 2264,
                    "y": 2051
                },
                {
                    "x": 2264,
                    "y": 2190
                },
                {
                    "x": 1273,
                    "y": 2190
                }
            ],
            "category": "caption",
            "html": "<caption id='220' style='font-size:16px'>Figure 15. NLL versus number of evaluation steps, for models<br>trained on ImageNet 64 x 64 (top) and CIFAR-10 (bottom). All<br>models were trained with 4000 diffusion steps.</caption>",
            "id": 220,
            "page": 13,
            "text": "Figure 15. NLL versus number of evaluation steps, for models trained on ImageNet 64 x 64 (top) and CIFAR-10 (bottom). All models were trained with 4000 diffusion steps."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2237
                },
                {
                    "x": 2264,
                    "y": 2237
                },
                {
                    "x": 2264,
                    "y": 2835
                },
                {
                    "x": 1270,
                    "y": 2835
                }
            ],
            "category": "paragraph",
            "html": "<p id='221' style='font-size:18px'>Figures 15 plots negative log-likelihood as a function of<br>number of sampling steps for both ImageNet 64 x 64 and<br>CIFAR-10. In initial experiments, we found that although<br>constant striding did not significantly affect FID, it dras-<br>tically reduced log-likelihood. To address this, we use a<br>strided subset of timesteps as for FID, but we also include<br>every t from 1 to T/K. This requires T/ K extra evaluation<br>steps, but greatly improves log-likelihood compared to the<br>uniformly strided schedule. We did not attempt to calculate<br>NLL using DDIM, since Song et al. (2020a) does not present<br>NLL results or a simple way of estimating likelihood under<br>DDIM.</p>",
            "id": 221,
            "page": 13,
            "text": "Figures 15 plots negative log-likelihood as a function of number of sampling steps for both ImageNet 64 x 64 and CIFAR-10. In initial experiments, we found that although constant striding did not significantly affect FID, it drastically reduced log-likelihood. To address this, we use a strided subset of timesteps as for FID, but we also include every t from 1 to T/K. This requires T/ K extra evaluation steps, but greatly improves log-likelihood compared to the uniformly strided schedule. We did not attempt to calculate NLL using DDIM, since Song  (2020a) does not present NLL results or a simple way of estimating likelihood under DDIM."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 194
                },
                {
                    "x": 1055,
                    "y": 238
                },
                {
                    "x": 224,
                    "y": 238
                }
            ],
            "category": "header",
            "html": "<header id='222' style='font-size:18px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 222,
            "page": 14,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2213,
                    "y": 194
                },
                {
                    "x": 2258,
                    "y": 194
                },
                {
                    "x": 2258,
                    "y": 234
                },
                {
                    "x": 2213,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<br><header id='223' style='font-size:16px'>14</header>",
            "id": 223,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 278
                },
                {
                    "x": 816,
                    "y": 278
                },
                {
                    "x": 816,
                    "y": 333
                },
                {
                    "x": 226,
                    "y": 333
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:20px'>F. Overfitting on CIFAR-10</p>",
            "id": 224,
            "page": 14,
            "text": "F. Overfitting on CIFAR-10"
        },
        {
            "bounding_box": [
                {
                    "x": 229,
                    "y": 387
                },
                {
                    "x": 1205,
                    "y": 387
                },
                {
                    "x": 1205,
                    "y": 1724
                },
                {
                    "x": 229,
                    "y": 1724
                }
            ],
            "category": "figure",
            "html": "<figure><img id='225' style='font-size:14px' alt=\"linear\n9\ncosine\n8\n7\nFID\n6\n5\n4\n3\n100 200 300 400 500\ntraining iters (thousands)\n3.40\nlinear (test)\nlinear (train)\n3.35 cosine (test)\ncosine (train)\n3.30\n77N 3.25\n3.20\n3.15\n3.10\n100 200 300 400 500\ntraining iters (thousands)\" data-coord=\"top-left:(229,387); bottom-right:(1205,1724)\" /></figure>",
            "id": 225,
            "page": 14,
            "text": "linear 9 cosine 8 7 FID 6 5 4 3 100 200 300 400 500 training iters (thousands) 3.40 linear (test) linear (train) 3.35 cosine (test) cosine (train) 3.30 77N 3.25 3.20 3.15 3.10 100 200 300 400 500 training iters (thousands)"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1823
                },
                {
                    "x": 1215,
                    "y": 1823
                },
                {
                    "x": 1215,
                    "y": 2102
                },
                {
                    "x": 223,
                    "y": 2102
                }
            ],
            "category": "caption",
            "html": "<caption id='226' style='font-size:14px'>Figure 16. FID (top) and NLL (bottom) over the course of training<br>for two CIFAR-10 models, both with dropout 0.1. The model<br>trained with the linear schedule learns more slowly, but does not<br>overfit as quickly. When too much overfitting occurs, we observed<br>overfitting artifacts similar to those from Salimans et al. (2017),<br>which is reflected by increasing FID.</caption>",
            "id": 226,
            "page": 14,
            "text": "Figure 16. FID (top) and NLL (bottom) over the course of training for two CIFAR-10 models, both with dropout 0.1. The model trained with the linear schedule learns more slowly, but does not overfit as quickly. When too much overfitting occurs, we observed overfitting artifacts similar to those from Salimans  (2017), which is reflected by increasing FID."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2148
                },
                {
                    "x": 1216,
                    "y": 2148
                },
                {
                    "x": 1216,
                    "y": 2748
                },
                {
                    "x": 223,
                    "y": 2748
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:16px'>On CIFAR-10, we noticed that all models overfit, but tended<br>to reach similar optimal FID at some point during training.<br>Holding dropout constant, we found that models trained<br>with our cosine schedule tended to reach optimal perfor-<br>mance (and then overfit) more quickly than those trained<br>with the linear schedule (Figure 16). In our experiments, we<br>corrected for this difference by using more dropout for our<br>cosine models than the linear models. We suspect that the<br>overfitting from the cosine schedule is either due to 1 ) less<br>noise in the cosine schedule providing less regularization,<br>or 2) the cosine schedule making optimization, and thus<br>overfitting, easier.</p>",
            "id": 227,
            "page": 14,
            "text": "On CIFAR-10, we noticed that all models overfit, but tended to reach similar optimal FID at some point during training. Holding dropout constant, we found that models trained with our cosine schedule tended to reach optimal performance (and then overfit) more quickly than those trained with the linear schedule (Figure 16). In our experiments, we corrected for this difference by using more dropout for our cosine models than the linear models. We suspect that the overfitting from the cosine schedule is either due to 1 ) less noise in the cosine schedule providing less regularization, or 2) the cosine schedule making optimization, and thus overfitting, easier."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 278
                },
                {
                    "x": 1837,
                    "y": 278
                },
                {
                    "x": 1837,
                    "y": 336
                },
                {
                    "x": 1274,
                    "y": 336
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='228' style='font-size:22px'>G. Early stopping for FID</p>",
            "id": 228,
            "page": 14,
            "text": "G. Early stopping for FID"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 394
                },
                {
                    "x": 2251,
                    "y": 394
                },
                {
                    "x": 2251,
                    "y": 914
                },
                {
                    "x": 1277,
                    "y": 914
                }
            ],
            "category": "figure",
            "html": "<figure><img id='229' style='font-size:14px' alt=\"10\n0.0, 0.99\n0.0, 0.999\n9 0.0, 0.9999\n0.0, 0.99995\n0.0, 0.99999\n8 0.1, 0.99\n0.1, 0.999\nFID\n0.1, 0.9999\n7 0.1, 0.99995\n0.1, 0.99999\n0.3, 0.99\n6 0.3, 0.999\n0.3, 0.9999\n0.3, 0.99995\n5 0.3, 0.99999\n200 400 600 800 1000 1200 1400\noriginal best\ntraining iters (thousands)\" data-coord=\"top-left:(1277,394); bottom-right:(2251,914)\" /></figure>",
            "id": 229,
            "page": 14,
            "text": "10 0.0, 0.99 0.0, 0.999 9 0.0, 0.9999 0.0, 0.99995 0.0, 0.99999 8 0.1, 0.99 0.1, 0.999 FID 0.1, 0.9999 7 0.1, 0.99995 0.1, 0.99999 0.3, 0.99 6 0.3, 0.999 0.3, 0.9999 0.3, 0.99995 5 0.3, 0.99999 200 400 600 800 1000 1200 1400 original best training iters (thousands)"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1021
                },
                {
                    "x": 2262,
                    "y": 1021
                },
                {
                    "x": 2262,
                    "y": 1114
                },
                {
                    "x": 1273,
                    "y": 1114
                }
            ],
            "category": "caption",
            "html": "<caption id='230' style='font-size:16px'>Figure 17. A sweep of dropout and EMA hyperparameters on<br>class conditional ImageNet-64.</caption>",
            "id": 230,
            "page": 14,
            "text": "Figure 17. A sweep of dropout and EMA hyperparameters on class conditional ImageNet-64."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1160
                },
                {
                    "x": 2264,
                    "y": 1160
                },
                {
                    "x": 2264,
                    "y": 1863
                },
                {
                    "x": 1270,
                    "y": 1863
                }
            ],
            "category": "paragraph",
            "html": "<p id='231' style='font-size:16px'>Like on CIFAR-10, we surprisingly observed overfitting<br>on class-conditional ImageNet 64 x 64, despite it being a<br>much larger and more diverse dataset. The main observable<br>result of this overfitting was that FID started becoming<br>worse over the course of training. We initially tried a sweep<br>(Figure 17) over the EMA hyperparameter to make sure it<br>was well tuned, and found that 0.9999 and 0.99995 worked<br>best. We then tried runs with dropout 0.1 and 0.3, and<br>found that models with a small amount of dropout improved<br>the best attainable FID but took longer to get to the same<br>performance and still eventually overfit. We concluded that<br>the best way to train, given what we know, is to early stop<br>and instead increase model size if we want to use additional<br>training compute.</p>",
            "id": 231,
            "page": 14,
            "text": "Like on CIFAR-10, we surprisingly observed overfitting on class-conditional ImageNet 64 x 64, despite it being a much larger and more diverse dataset. The main observable result of this overfitting was that FID started becoming worse over the course of training. We initially tried a sweep (Figure 17) over the EMA hyperparameter to make sure it was well tuned, and found that 0.9999 and 0.99995 worked best. We then tried runs with dropout 0.1 and 0.3, and found that models with a small amount of dropout improved the best attainable FID but took longer to get to the same performance and still eventually overfit. We concluded that the best way to train, given what we know, is to early stop and instead increase model size if we want to use additional training compute."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1923
                },
                {
                    "x": 2035,
                    "y": 1923
                },
                {
                    "x": 2035,
                    "y": 2036
                },
                {
                    "x": 1274,
                    "y": 2036
                }
            ],
            "category": "paragraph",
            "html": "<p id='232' style='font-size:22px'>H. Samples with Varying Steps and<br>Objectives</p>",
            "id": 232,
            "page": 14,
            "text": "H. Samples with Varying Steps and Objectives"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2065
                },
                {
                    "x": 2263,
                    "y": 2065
                },
                {
                    "x": 2263,
                    "y": 2266
                },
                {
                    "x": 1272,
                    "y": 2266
                }
            ],
            "category": "paragraph",
            "html": "<p id='233' style='font-size:18px'>Figures 18 through 23 show unconditional ImageNet 64 x<br>64 samples as we reduce number of sampling steps for<br>an Lhybrid model with 4K diffusion steps trained for 1.5M<br>training iterations.</p>",
            "id": 233,
            "page": 14,
            "text": "Figures 18 through 23 show unconditional ImageNet 64 x 64 samples as we reduce number of sampling steps for an Lhybrid model with 4K diffusion steps trained for 1.5M training iterations."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2292
                },
                {
                    "x": 2265,
                    "y": 2292
                },
                {
                    "x": 2265,
                    "y": 2490
                },
                {
                    "x": 1273,
                    "y": 2490
                }
            ],
            "category": "paragraph",
            "html": "<p id='234' style='font-size:18px'>Figures 24 through 29 show unconditional CIFAR-10 sam-<br>ples as we reduce number of sampling steps for an Lhybrid<br>model with 4K diffusion steps trained for 500K training<br>iterations.</p>",
            "id": 234,
            "page": 14,
            "text": "Figures 24 through 29 show unconditional CIFAR-10 samples as we reduce number of sampling steps for an Lhybrid model with 4K diffusion steps trained for 500K training iterations."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2516
                },
                {
                    "x": 2259,
                    "y": 2516
                },
                {
                    "x": 2259,
                    "y": 2618
                },
                {
                    "x": 1274,
                    "y": 2618
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='235' style='font-size:20px'>Figures 30 and 31 highlight the difference in sample quality<br>between models trained with Lhybrid and Lvlb.</p>",
            "id": 235,
            "page": 14,
            "text": "Figures 30 and 31 highlight the difference in sample quality between models trained with Lhybrid and Lvlb."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 196
                },
                {
                    "x": 1053,
                    "y": 196
                },
                {
                    "x": 1053,
                    "y": 237
                },
                {
                    "x": 225,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='236' style='font-size:22px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 236,
            "page": 15,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2214,
                    "y": 197
                },
                {
                    "x": 2257,
                    "y": 197
                },
                {
                    "x": 2257,
                    "y": 233
                },
                {
                    "x": 2214,
                    "y": 233
                }
            ],
            "category": "header",
            "html": "<br><header id='237' style='font-size:20px'>15</header>",
            "id": 237,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 273
                },
                {
                    "x": 1103,
                    "y": 273
                },
                {
                    "x": 1103,
                    "y": 1044
                },
                {
                    "x": 333,
                    "y": 1044
                }
            ],
            "category": "figure",
            "html": "<figure><img id='238' alt=\"\" data-coord=\"top-left:(333,273); bottom-right:(1103,1044)\" /></figure>",
            "id": 238,
            "page": 15,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 1094
                },
                {
                    "x": 1213,
                    "y": 1094
                },
                {
                    "x": 1213,
                    "y": 1139
                },
                {
                    "x": 227,
                    "y": 1139
                }
            ],
            "category": "caption",
            "html": "<caption id='239' style='font-size:16px'>Figure 18. 50 sampling steps on unconditional ImageNet 64 x 64</caption>",
            "id": 239,
            "page": 15,
            "text": "Figure 18. 50 sampling steps on unconditional ImageNet 64 x 64"
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 1200
                },
                {
                    "x": 1103,
                    "y": 1200
                },
                {
                    "x": 1103,
                    "y": 1974
                },
                {
                    "x": 330,
                    "y": 1974
                }
            ],
            "category": "figure",
            "html": "<figure><img id='240' alt=\"\" data-coord=\"top-left:(330,1200); bottom-right:(1103,1974)\" /></figure>",
            "id": 240,
            "page": 15,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 228,
                    "y": 2024
                },
                {
                    "x": 1211,
                    "y": 2024
                },
                {
                    "x": 1211,
                    "y": 2070
                },
                {
                    "x": 228,
                    "y": 2070
                }
            ],
            "category": "caption",
            "html": "<caption id='241' style='font-size:18px'>Figure 19. 100 sampling steps on unconditional ImageNet 64 x 64</caption>",
            "id": 241,
            "page": 15,
            "text": "Figure 19. 100 sampling steps on unconditional ImageNet 64 x 64"
        },
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 2129
                },
                {
                    "x": 1103,
                    "y": 2129
                },
                {
                    "x": 1103,
                    "y": 2906
                },
                {
                    "x": 333,
                    "y": 2906
                }
            ],
            "category": "figure",
            "html": "<figure><img id='242' alt=\"\" data-coord=\"top-left:(333,2129); bottom-right:(1103,2906)\" /></figure>",
            "id": 242,
            "page": 15,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 2956
                },
                {
                    "x": 1209,
                    "y": 2956
                },
                {
                    "x": 1209,
                    "y": 3001
                },
                {
                    "x": 227,
                    "y": 3001
                }
            ],
            "category": "caption",
            "html": "<caption id='243' style='font-size:16px'>Figure 20. 200 sampling steps on unconditional ImageNet 64 x 64</caption>",
            "id": 243,
            "page": 15,
            "text": "Figure 20. 200 sampling steps on unconditional ImageNet 64 x 64"
        },
        {
            "bounding_box": [
                {
                    "x": 1386,
                    "y": 269
                },
                {
                    "x": 2155,
                    "y": 269
                },
                {
                    "x": 2155,
                    "y": 1048
                },
                {
                    "x": 1386,
                    "y": 1048
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='244' style='font-size:14px' alt=\"어떻게\" data-coord=\"top-left:(1386,269); bottom-right:(2155,1048)\" /></figure>",
            "id": 244,
            "page": 15,
            "text": "어떻게"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1093
                },
                {
                    "x": 2262,
                    "y": 1093
                },
                {
                    "x": 2262,
                    "y": 1141
                },
                {
                    "x": 1275,
                    "y": 1141
                }
            ],
            "category": "caption",
            "html": "<caption id='245' style='font-size:16px'>Figure 21. 400 sampling steps on unconditional ImageNet 64 x 64</caption>",
            "id": 245,
            "page": 15,
            "text": "Figure 21. 400 sampling steps on unconditional ImageNet 64 x 64"
        },
        {
            "bounding_box": [
                {
                    "x": 1386,
                    "y": 1200
                },
                {
                    "x": 2154,
                    "y": 1200
                },
                {
                    "x": 2154,
                    "y": 1976
                },
                {
                    "x": 1386,
                    "y": 1976
                }
            ],
            "category": "figure",
            "html": "<figure><img id='246' alt=\"\" data-coord=\"top-left:(1386,1200); bottom-right:(2154,1976)\" /></figure>",
            "id": 246,
            "page": 15,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2024
                },
                {
                    "x": 2259,
                    "y": 2024
                },
                {
                    "x": 2259,
                    "y": 2071
                },
                {
                    "x": 1277,
                    "y": 2071
                }
            ],
            "category": "caption",
            "html": "<caption id='247' style='font-size:18px'>Figure 22. 1000 sampling steps on unconditional ImageNet 64x 64</caption>",
            "id": 247,
            "page": 15,
            "text": "Figure 22. 1000 sampling steps on unconditional ImageNet 64x 64"
        },
        {
            "bounding_box": [
                {
                    "x": 1387,
                    "y": 2130
                },
                {
                    "x": 2155,
                    "y": 2130
                },
                {
                    "x": 2155,
                    "y": 2907
                },
                {
                    "x": 1387,
                    "y": 2907
                }
            ],
            "category": "figure",
            "html": "<figure><img id='248' style='font-size:14px' alt=\"#com\nINTER\" data-coord=\"top-left:(1387,2130); bottom-right:(2155,2907)\" /></figure>",
            "id": 248,
            "page": 15,
            "text": "#com INTER"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2956
                },
                {
                    "x": 2263,
                    "y": 2956
                },
                {
                    "x": 2263,
                    "y": 3001
                },
                {
                    "x": 1278,
                    "y": 3001
                }
            ],
            "category": "caption",
            "html": "<caption id='249' style='font-size:16px'>Figure 23. 4K sampling steps on unconditional ImageNet 64 x 64.</caption>",
            "id": 249,
            "page": 15,
            "text": "Figure 23. 4K sampling steps on unconditional ImageNet 64 x 64."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 196
                },
                {
                    "x": 1053,
                    "y": 196
                },
                {
                    "x": 1053,
                    "y": 237
                },
                {
                    "x": 225,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='250' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 250,
            "page": 16,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2215,
                    "y": 197
                },
                {
                    "x": 2257,
                    "y": 197
                },
                {
                    "x": 2257,
                    "y": 233
                },
                {
                    "x": 2215,
                    "y": 233
                }
            ],
            "category": "header",
            "html": "<br><header id='251' style='font-size:16px'>16</header>",
            "id": 251,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 335,
                    "y": 273
                },
                {
                    "x": 1103,
                    "y": 273
                },
                {
                    "x": 1103,
                    "y": 1044
                },
                {
                    "x": 335,
                    "y": 1044
                }
            ],
            "category": "figure",
            "html": "<figure><img id='252' alt=\"\" data-coord=\"top-left:(335,273); bottom-right:(1103,1044)\" /></figure>",
            "id": 252,
            "page": 16,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 280,
                    "y": 1095
                },
                {
                    "x": 1159,
                    "y": 1095
                },
                {
                    "x": 1159,
                    "y": 1137
                },
                {
                    "x": 280,
                    "y": 1137
                }
            ],
            "category": "caption",
            "html": "<caption id='253' style='font-size:16px'>Figure 24. 50 sampling steps on unconditional CIFAR-10</caption>",
            "id": 253,
            "page": 16,
            "text": "Figure 24. 50 sampling steps on unconditional CIFAR-10"
        },
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 1200
                },
                {
                    "x": 1104,
                    "y": 1200
                },
                {
                    "x": 1104,
                    "y": 1974
                },
                {
                    "x": 333,
                    "y": 1974
                }
            ],
            "category": "figure",
            "html": "<figure><img id='254' alt=\"\" data-coord=\"top-left:(333,1200); bottom-right:(1104,1974)\" /></figure>",
            "id": 254,
            "page": 16,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 228,
                    "y": 2026
                },
                {
                    "x": 1110,
                    "y": 2026
                },
                {
                    "x": 1110,
                    "y": 2068
                },
                {
                    "x": 228,
                    "y": 2068
                }
            ],
            "category": "caption",
            "html": "<caption id='255' style='font-size:16px'>Figure 25. 100 sampling steps on unconditional CIFAR-10</caption>",
            "id": 255,
            "page": 16,
            "text": "Figure 25. 100 sampling steps on unconditional CIFAR-10"
        },
        {
            "bounding_box": [
                {
                    "x": 334,
                    "y": 2132
                },
                {
                    "x": 1102,
                    "y": 2132
                },
                {
                    "x": 1102,
                    "y": 2906
                },
                {
                    "x": 334,
                    "y": 2906
                }
            ],
            "category": "figure",
            "html": "<figure><img id='256' alt=\"\" data-coord=\"top-left:(334,2132); bottom-right:(1102,2906)\" /></figure>",
            "id": 256,
            "page": 16,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 228,
                    "y": 2957
                },
                {
                    "x": 1109,
                    "y": 2957
                },
                {
                    "x": 1109,
                    "y": 2999
                },
                {
                    "x": 228,
                    "y": 2999
                }
            ],
            "category": "caption",
            "html": "<caption id='257' style='font-size:14px'>Figure 26. 200 sampling steps on unconditional CIFAR-10</caption>",
            "id": 257,
            "page": 16,
            "text": "Figure 26. 200 sampling steps on unconditional CIFAR-10"
        },
        {
            "bounding_box": [
                {
                    "x": 1386,
                    "y": 269
                },
                {
                    "x": 2155,
                    "y": 269
                },
                {
                    "x": 2155,
                    "y": 1045
                },
                {
                    "x": 1386,
                    "y": 1045
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='258' alt=\"\" data-coord=\"top-left:(1386,269); bottom-right:(2155,1045)\" /></figure>",
            "id": 258,
            "page": 16,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1092
                },
                {
                    "x": 2162,
                    "y": 1092
                },
                {
                    "x": 2162,
                    "y": 1140
                },
                {
                    "x": 1276,
                    "y": 1140
                }
            ],
            "category": "caption",
            "html": "<caption id='259' style='font-size:16px'>Figure 27. 400 sampling steps on unconditional CIFAR-10</caption>",
            "id": 259,
            "page": 16,
            "text": "Figure 27. 400 sampling steps on unconditional CIFAR-10"
        },
        {
            "bounding_box": [
                {
                    "x": 1386,
                    "y": 1202
                },
                {
                    "x": 2155,
                    "y": 1202
                },
                {
                    "x": 2155,
                    "y": 1976
                },
                {
                    "x": 1386,
                    "y": 1976
                }
            ],
            "category": "figure",
            "html": "<figure><img id='260' alt=\"\" data-coord=\"top-left:(1386,1202); bottom-right:(2155,1976)\" /></figure>",
            "id": 260,
            "page": 16,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2024
                },
                {
                    "x": 2182,
                    "y": 2024
                },
                {
                    "x": 2182,
                    "y": 2073
                },
                {
                    "x": 1277,
                    "y": 2073
                }
            ],
            "category": "caption",
            "html": "<caption id='261' style='font-size:16px'>Figure 28. 1000 sampling steps on unconditional CIFAR-10</caption>",
            "id": 261,
            "page": 16,
            "text": "Figure 28. 1000 sampling steps on unconditional CIFAR-10"
        },
        {
            "bounding_box": [
                {
                    "x": 1386,
                    "y": 2131
                },
                {
                    "x": 2155,
                    "y": 2131
                },
                {
                    "x": 2155,
                    "y": 2906
                },
                {
                    "x": 1386,
                    "y": 2906
                }
            ],
            "category": "figure",
            "html": "<figure><img id='262' alt=\"\" data-coord=\"top-left:(1386,2131); bottom-right:(2155,2906)\" /></figure>",
            "id": 262,
            "page": 16,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2955
                },
                {
                    "x": 2189,
                    "y": 2955
                },
                {
                    "x": 2189,
                    "y": 3001
                },
                {
                    "x": 1278,
                    "y": 3001
                }
            ],
            "category": "caption",
            "html": "<caption id='263' style='font-size:16px'>Figure 29. 4000 sampling steps on unconditional CIFAR-10</caption>",
            "id": 263,
            "page": 16,
            "text": "Figure 29. 4000 sampling steps on unconditional CIFAR-10"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 196
                },
                {
                    "x": 1053,
                    "y": 196
                },
                {
                    "x": 1053,
                    "y": 237
                },
                {
                    "x": 226,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='264' style='font-size:20px'>Improved Denoising Diffusion Probabilistic Models</header>",
            "id": 264,
            "page": 17,
            "text": "Improved Denoising Diffusion Probabilistic Models"
        },
        {
            "bounding_box": [
                {
                    "x": 2213,
                    "y": 195
                },
                {
                    "x": 2257,
                    "y": 195
                },
                {
                    "x": 2257,
                    "y": 235
                },
                {
                    "x": 2213,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<br><header id='265' style='font-size:18px'>17</header>",
            "id": 265,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 748
                },
                {
                    "x": 1104,
                    "y": 748
                },
                {
                    "x": 1104,
                    "y": 2327
                },
                {
                    "x": 330,
                    "y": 2327
                }
            ],
            "category": "figure",
            "html": "<figure><img id='266' style='font-size:14px' alt=\"나무진하고\nINTI\nLELD\" data-coord=\"top-left:(330,748); bottom-right:(1104,2327)\" /></figure>",
            "id": 266,
            "page": 17,
            "text": "나무진하고 INTI LELD"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 2373
                },
                {
                    "x": 1212,
                    "y": 2373
                },
                {
                    "x": 1212,
                    "y": 2508
                },
                {
                    "x": 227,
                    "y": 2508
                }
            ],
            "category": "caption",
            "html": "<caption id='267' style='font-size:16px'>Figure 30. Unconditional ImageNet 64 x 64 samples generated<br>from Lhybrid (top) and Lvlb (bottom) models using the exact same<br>random noise. Both models were trained for 1.5M iterations.</caption>",
            "id": 267,
            "page": 17,
            "text": "Figure 30. Unconditional ImageNet 64 x 64 samples generated from Lhybrid (top) and Lvlb (bottom) models using the exact same random noise. Both models were trained for 1.5M iterations."
        },
        {
            "bounding_box": [
                {
                    "x": 1383,
                    "y": 750
                },
                {
                    "x": 2156,
                    "y": 750
                },
                {
                    "x": 2156,
                    "y": 2326
                },
                {
                    "x": 1383,
                    "y": 2326
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='268' alt=\"\" data-coord=\"top-left:(1383,750); bottom-right:(2156,2326)\" /></figure>",
            "id": 268,
            "page": 17,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2371
                },
                {
                    "x": 2260,
                    "y": 2371
                },
                {
                    "x": 2260,
                    "y": 2509
                },
                {
                    "x": 1273,
                    "y": 2509
                }
            ],
            "category": "caption",
            "html": "<caption id='269' style='font-size:16px'>Figure 31. Unconditional CIFAR-10 samples generated from<br>Lhybrid (top) and Lvlb (bottom) models using the exact same random<br>noise. Both models were trained for 500K iterations.</caption>",
            "id": 269,
            "page": 17,
            "text": "Figure 31. Unconditional CIFAR-10 samples generated from Lhybrid (top) and Lvlb (bottom) models using the exact same random noise. Both models were trained for 500K iterations."
        }
    ]
}