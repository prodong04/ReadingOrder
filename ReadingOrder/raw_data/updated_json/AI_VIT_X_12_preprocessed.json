{
    "id": "3294ce4e-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2302.07577v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 389,
                    "y": 437
                },
                {
                    "x": 2086,
                    "y": 437
                },
                {
                    "x": 2086,
                    "y": 504
                },
                {
                    "x": 389,
                    "y": 504
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Efficient Teacher: Semi-Supervised Object Detection for YOLOv5</p>",
            "id": 0,
            "page": 1,
            "text": "Efficient Teacher: Semi-Supervised Object Detection for YOLOv5"
        },
        {
            "bounding_box": [
                {
                    "x": 656,
                    "y": 601
                },
                {
                    "x": 1825,
                    "y": 601
                },
                {
                    "x": 1825,
                    "y": 715
                },
                {
                    "x": 656,
                    "y": 715
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Bowen Xu Mingtao Chen Wenlong Guan Lulu Hu<br>Alibaba Group</p>",
            "id": 1,
            "page": 1,
            "text": "Bowen Xu Mingtao Chen Wenlong Guan Lulu Hu Alibaba Group"
        },
        {
            "bounding_box": [
                {
                    "x": 558,
                    "y": 729
                },
                {
                    "x": 1927,
                    "y": 729
                },
                {
                    "x": 1927,
                    "y": 775
                },
                {
                    "x": 558,
                    "y": 775
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:14px'>{bowen · xbw , ruiyang · cmt , wenl ong · gw1, chudu · hll}@alibaba-inc · com</p>",
            "id": 2,
            "page": 1,
            "text": "{bowen · xbw , ruiyang · cmt , wenl ong · gw1, chudu · hll}@alibaba-inc · com"
        },
        {
            "bounding_box": [
                {
                    "x": 604,
                    "y": 892
                },
                {
                    "x": 797,
                    "y": 892
                },
                {
                    "x": 797,
                    "y": 942
                },
                {
                    "x": 604,
                    "y": 942
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 994
                },
                {
                    "x": 1199,
                    "y": 994
                },
                {
                    "x": 1199,
                    "y": 2443
                },
                {
                    "x": 198,
                    "y": 2443
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>Semi-Supervised Object Detection (SSOD) has been suc-<br>cessful in improving the performance of both R-CNN se-<br>ries and anchor-free detectors. However, one-stage anchor-<br>based detectors lack the structure to generate high-quality<br>or flexible pseudo labels, leading to serious inconsistency<br>problems in SSOD. In this paper, we propose the Efficient<br>Teacher framework for scalable and effective one-stage<br>anchor-based SSOD training, consisting of Dense Detec-<br>tor, Pseudo Label Assigner, and Epoch Adaptor. Dense<br>Detector is a baseline model that extends RetinaNet with<br>dense sampling techniques inspired by YOLOv5. The Ef-<br>ficient Teacher framework introduces a novel pseudo la-<br>bel assignment mechanism, named Pseudo Label Assigner,<br>which makes more refined use of pseudo labels from Dense<br>Detector. Epoch Adaptor is a method that enables a stable<br>and efficient end-to-end semi-supervised training schedule<br>for Dense Detector. The Pseudo Label Assigner prevents<br>the occurrence of bias caused by a large number of low-<br>quality pseudo labels that may interfere with the Dense De-<br>tector during the student-teacher mutual learning mecha-<br>nism, and the Epoch Adaptor utilizes domain and distribu-<br>tion adaptation to allow Dense Detector to learn globally<br>distributed consistent features, making the training inde-<br>pendent of the proportion of labeled data. Our experiments<br>show that the Efficient Teacher framework achieves state-<br>of-the-art results on VOC, COCO-standard, and COCO-<br>additional using fewer FLOPs than previous methods. To<br>the best of our knowledge, this is the first attempt to apply<br>Semi-Supervised Object Detection to YOLOv5.</p>",
            "id": 4,
            "page": 1,
            "text": "Semi-Supervised Object Detection (SSOD) has been successful in improving the performance of both R-CNN series and anchor-free detectors. However, one-stage anchorbased detectors lack the structure to generate high-quality or flexible pseudo labels, leading to serious inconsistency problems in SSOD. In this paper, we propose the Efficient Teacher framework for scalable and effective one-stage anchor-based SSOD training, consisting of Dense Detector, Pseudo Label Assigner, and Epoch Adaptor. Dense Detector is a baseline model that extends RetinaNet with dense sampling techniques inspired by YOLOv5. The Efficient Teacher framework introduces a novel pseudo label assignment mechanism, named Pseudo Label Assigner, which makes more refined use of pseudo labels from Dense Detector. Epoch Adaptor is a method that enables a stable and efficient end-to-end semi-supervised training schedule for Dense Detector. The Pseudo Label Assigner prevents the occurrence of bias caused by a large number of lowquality pseudo labels that may interfere with the Dense Detector during the student-teacher mutual learning mechanism, and the Epoch Adaptor utilizes domain and distribution adaptation to allow Dense Detector to learn globally distributed consistent features, making the training independent of the proportion of labeled data. Our experiments show that the Efficient Teacher framework achieves stateof-the-art results on VOC, COCO-standard, and COCOadditional using fewer FLOPs than previous methods. To the best of our knowledge, this is the first attempt to apply Semi-Supervised Object Detection to YOLOv5."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2542
                },
                {
                    "x": 532,
                    "y": 2542
                },
                {
                    "x": 532,
                    "y": 2593
                },
                {
                    "x": 204,
                    "y": 2593
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2628
                },
                {
                    "x": 1200,
                    "y": 2628
                },
                {
                    "x": 1200,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Object detection [3, 25, 31, 40] has made significant ad-<br>vances in recent years, which follows a traditional super-<br>vised training approach and relies on costly manual an-<br>notation efforts. To mitigate this problem, many semi-<br>supervised techniques [1, 35] are proposed to exploit large<br>amounts of unlabeled data by automatically generating<br>pseudo labels without introducing manual annotation. De-</p>",
            "id": 6,
            "page": 1,
            "text": "Object detection  has made significant advances in recent years, which follows a traditional supervised training approach and relies on costly manual annotation efforts. To mitigate this problem, many semisupervised techniques  are proposed to exploit large amounts of unlabeled data by automatically generating pseudo labels without introducing manual annotation. De-"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 898
                },
                {
                    "x": 2275,
                    "y": 898
                },
                {
                    "x": 2275,
                    "y": 992
                },
                {
                    "x": 1282,
                    "y": 992
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>spite great progress in SSOD [4, 5, 27, 45], there are three<br>key issues that remain challenging:</p>",
            "id": 7,
            "page": 1,
            "text": "spite great progress in SSOD , there are three key issues that remain challenging:"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1009
                },
                {
                    "x": 2277,
                    "y": 1009
                },
                {
                    "x": 2277,
                    "y": 1904
                },
                {
                    "x": 1278,
                    "y": 1904
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>Firstly, few works on one-stage anchor-based SSOD<br>have been reported. Though anchor-free detectors [15, 22,<br>40]have been recently getting more attention in the commu-<br>nity of object detection, one-stage anchor-based detectors<br>[2, 19,22,30, 42], having the advantages of high recall, high<br>numerical stability and fast training speed, are widely used<br>in scenarios with extremely high recall demands. How-<br>ever, most SSOD methods are implemented on a two-stage<br>anchor-based detector such as Faster R-CNN [31] and an<br>one-stage anchor-free detector such as FCOS [40], which<br>output relatively sparse bounding box predictions due to<br>the multi-stage coarse-to-fine prediction mechanism or the<br>anchor-free design of detection head. In contrast, the classic<br>one-stage anchor-based detector generates more dense pre-<br>dictions due to its multiple-anchor mechanism, which leads<br>to positive and negative samples imbalance during super-<br>vised training [46] and poor quality of pseudo labels during<br>semi-supervised training.</p>",
            "id": 8,
            "page": 1,
            "text": "Firstly, few works on one-stage anchor-based SSOD have been reported. Though anchor-free detectors have been recently getting more attention in the community of object detection, one-stage anchor-based detectors , having the advantages of high recall, high numerical stability and fast training speed, are widely used in scenarios with extremely high recall demands. However, most SSOD methods are implemented on a two-stage anchor-based detector such as Faster R-CNN  and an one-stage anchor-free detector such as FCOS , which output relatively sparse bounding box predictions due to the multi-stage coarse-to-fine prediction mechanism or the anchor-free design of detection head. In contrast, the classic one-stage anchor-based detector generates more dense predictions due to its multiple-anchor mechanism, which leads to positive and negative samples imbalance during supervised training  and poor quality of pseudo labels during semi-supervised training."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1919
                },
                {
                    "x": 2277,
                    "y": 1919
                },
                {
                    "x": 2277,
                    "y": 2761
                },
                {
                    "x": 1278,
                    "y": 2761
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>Secondly, current mainstream SSOD approaches, fol-<br>lowing a student-teacher mutual learning manner [27, 45],<br>is difficult for an one-stage anchor-based detector to train<br>due to the serious pseudo label inconsistency problem, that<br>is, throughout the training process, the quantity and qual-<br>ity of pseudo labels generated by the teacher model fluctu-<br>ates greatly and the unqualified pseudo labels can mislead<br>model updates. To alleviate this problem, two-stage meth-<br>ods [4] [27] refine pseudo labels several times more than<br>one-stage methods and anchor-free methods [49] adopt fea-<br>ture maps as soft pseudo labels to avoid bias caused by non<br>maximum suppression. The pseudo label inconsistency is<br>exacerbated in an one-stage anchor-based detector because<br>of its multiple-anchor mechanism mentioned above. The<br>work [47] has reported that the SSOD experimental results<br>of RetinaNet are not as good as those on Faster R-CNN and<br>FCOS.</p>",
            "id": 9,
            "page": 1,
            "text": "Secondly, current mainstream SSOD approaches, following a student-teacher mutual learning manner , is difficult for an one-stage anchor-based detector to train due to the serious pseudo label inconsistency problem, that is, throughout the training process, the quantity and quality of pseudo labels generated by the teacher model fluctuates greatly and the unqualified pseudo labels can mislead model updates. To alleviate this problem, two-stage methods   refine pseudo labels several times more than one-stage methods and anchor-free methods  adopt feature maps as soft pseudo labels to avoid bias caused by non maximum suppression. The pseudo label inconsistency is exacerbated in an one-stage anchor-based detector because of its multiple-anchor mechanism mentioned above. The work  has reported that the SSOD experimental results of RetinaNet are not as good as those on Faster R-CNN and FCOS."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2777
                },
                {
                    "x": 2276,
                    "y": 2777
                },
                {
                    "x": 2276,
                    "y": 2977
                },
                {
                    "x": 1280,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>Thirdly, how to train a SSOD model with both higher<br>accuracy and better efficiency becomes the key issue that<br>restricts the application of SSOD in a wide range of sce-<br>narios. The previous SSOD methods [4, 23, 27, 45, 50] are</p>",
            "id": 10,
            "page": 1,
            "text": "Thirdly, how to train a SSOD model with both higher accuracy and better efficiency becomes the key issue that restricts the application of SSOD in a wide range of scenarios. The previous SSOD methods  are"
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 870
                },
                {
                    "x": 149,
                    "y": 870
                },
                {
                    "x": 149,
                    "y": 2342
                },
                {
                    "x": 64,
                    "y": 2342
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2023<br>Feb<br>16<br>[cs.CV]<br>arXiv:2302.07577v2</footer>",
            "id": 11,
            "page": 1,
            "text": "2023 Feb 16 [cs.CV] arXiv:2302.07577v2"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3057
                },
                {
                    "x": 1251,
                    "y": 3057
                },
                {
                    "x": 1251,
                    "y": 3093
                },
                {
                    "x": 1225,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='12' style='font-size:16px'>1</footer>",
            "id": 12,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 305
                },
                {
                    "x": 2197,
                    "y": 305
                },
                {
                    "x": 2197,
                    "y": 1294
                },
                {
                    "x": 286,
                    "y": 1294
                }
            ],
            "category": "figure",
            "html": "<figure><img id='13' style='font-size:14px' alt=\"Teacher\nMosaic\nPseudo\nAug Dense Detector\nLabel Assigner\n|Strong Aug\nUnlabeled Data\nEpoch Reliable Pseudo Label Uncertain Pseudo Label\nEMA Update\nAdaptor\nDense Detector\nMosaic Lu 'u\ncls reg obj\n+ Lu\n+\nAug\nLcls + L reg + L obj + Lda\nLabeled Data Student\" data-coord=\"top-left:(286,305); bottom-right:(2197,1294)\" /></figure>",
            "id": 13,
            "page": 2,
            "text": "Teacher Mosaic Pseudo Aug Dense Detector Label Assigner |Strong Aug Unlabeled Data Epoch Reliable Pseudo Label Uncertain Pseudo Label EMA Update Adaptor Dense Detector Mosaic Lu 'u cls reg obj + Lu + Aug Lcls + L reg + L obj + Lda Labeled Data Student"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1344
                },
                {
                    "x": 2278,
                    "y": 1344
                },
                {
                    "x": 2278,
                    "y": 1528
                },
                {
                    "x": 200,
                    "y": 1528
                }
            ],
            "category": "caption",
            "html": "<caption id='14' style='font-size:16px'>Figure 1. An overview of Efficient Teacher framework. Efficient Teacher proposes three modules to implement a scalable and effective<br>SSOD framework, where Dense Detector improves the quality of pseudo labels with dense input while has better inference efficiency;<br>Pseudo Label Assigner divides pseudo labels into two types to alleviate pseudo labels inconsistency problem; Epoch Adaptor reduces<br>training time and the inconsistency of features.</caption>",
            "id": 14,
            "page": 2,
            "text": "Figure 1. An overview of Efficient Teacher framework. Efficient Teacher proposes three modules to implement a scalable and effective SSOD framework, where Dense Detector improves the quality of pseudo labels with dense input while has better inference efficiency; Pseudo Label Assigner divides pseudo labels into two types to alleviate pseudo labels inconsistency problem; Epoch Adaptor reduces training time and the inconsistency of features."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1619
                },
                {
                    "x": 1198,
                    "y": 1619
                },
                {
                    "x": 1198,
                    "y": 1865
                },
                {
                    "x": 202,
                    "y": 1865
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:22px'>mainly in pursuit of better accuracy, but usually sacrifice<br>training efficiency. Moreover, most previous works only<br>focus on specific detector architecture, but the variety of<br>real-world applications require faster iterative detector de-<br>sign with lower compute resource and higher accuracy.</p>",
            "id": 15,
            "page": 2,
            "text": "mainly in pursuit of better accuracy, but usually sacrifice training efficiency. Moreover, most previous works only focus on specific detector architecture, but the variety of real-world applications require faster iterative detector design with lower compute resource and higher accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1881
                },
                {
                    "x": 1199,
                    "y": 1881
                },
                {
                    "x": 1199,
                    "y": 2981
                },
                {
                    "x": 200,
                    "y": 2981
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:20px'>In this paper, what we pursue is to design a scalable and<br>effective SSOD framework on an one-stage anchor-based<br>detector while considering both inference and training effi-<br>ciency. We add the effective techniques used in the YOLO<br>series [2, 15, 42] to a classical RetinaNet [25] to design a<br>new representative one-stage anchor-based detector base-<br>line, called Dense Detector. We attempt to transplant a ma-<br>ture SSOD scheme, the Unbiased Teacher [27], to Dense<br>Detector but find only 1.65 AP50:95 improvement compared<br>to the supervised method(shown in Table 5), which con-<br>firms the second problem mentioned above. According to<br>design paradigm of the Dense Detector, we propose the Ef-<br>ficient Teacher framework to overcome these challenges in<br>SSOD. Pseudo Label Assigner(PLA) is introduced to allevi-<br>ate pseudo label inconsistency by exploiting a fine-grained<br>pseudo label assignment strategy on the objectness branch<br>design. By distinguishing the pseudo labels into the reliable<br>and the uncertain regions, different loss calculation methods<br>are used respectively. In addition, we propose Epoch Adap-<br>tor(EA), an end-to-end training strategy for SSOD which<br>consists of a Burn-In stage and a semi-supervised train-<br>ing stage. In the Burn-In stage, only labeled data is used</p>",
            "id": 16,
            "page": 2,
            "text": "In this paper, what we pursue is to design a scalable and effective SSOD framework on an one-stage anchor-based detector while considering both inference and training efficiency. We add the effective techniques used in the YOLO series  to a classical RetinaNet  to design a new representative one-stage anchor-based detector baseline, called Dense Detector. We attempt to transplant a mature SSOD scheme, the Unbiased Teacher , to Dense Detector but find only 1.65 AP50:95 improvement compared to the supervised method(shown in Table 5), which confirms the second problem mentioned above. According to design paradigm of the Dense Detector, we propose the Efficient Teacher framework to overcome these challenges in SSOD. Pseudo Label Assigner(PLA) is introduced to alleviate pseudo label inconsistency by exploiting a fine-grained pseudo label assignment strategy on the objectness branch design. By distinguishing the pseudo labels into the reliable and the uncertain regions, different loss calculation methods are used respectively. In addition, we propose Epoch Adaptor(EA), an end-to-end training strategy for SSOD which consists of a Burn-In stage and a semi-supervised training stage. In the Burn-In stage, only labeled data is used"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1620
                },
                {
                    "x": 2276,
                    "y": 1620
                },
                {
                    "x": 2276,
                    "y": 2113
                },
                {
                    "x": 1280,
                    "y": 2113
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:20px'>for model parameter updating warm-up to obtain a student<br>model with basic pseudo labels generation capability. Spe-<br>cially, the student model performs adversarial learning in<br>the Burn-In stage SO that the feature map of output is con-<br>sistent on both labeled and unlabeled data. In the semi-<br>supervised training stage, EA automatically calculates low<br>and high threshold used by PLA according to the number<br>of the ground truth(GT) after Mosaic augmentation in an<br>epoch, which leads to stable and effective training. The<br>main contributions of this paper are as follows:</p>",
            "id": 17,
            "page": 2,
            "text": "for model parameter updating warm-up to obtain a student model with basic pseudo labels generation capability. Specially, the student model performs adversarial learning in the Burn-In stage SO that the feature map of output is consistent on both labeled and unlabeled data. In the semisupervised training stage, EA automatically calculates low and high threshold used by PLA according to the number of the ground truth(GT) after Mosaic augmentation in an epoch, which leads to stable and effective training. The main contributions of this paper are as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2133
                },
                {
                    "x": 2278,
                    "y": 2133
                },
                {
                    "x": 2278,
                    "y": 2979
                },
                {
                    "x": 1292,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>· We design Dense Detector as a baseline model to com-<br>pare the differences between YOLOv5 and RetinaNet,<br>which leads to a performance improvement of 5.36<br>AP50:95 by utilizing dense sampling.<br>· We propose an effective SSOD training framework<br>called Efficient Teacher, which includes a novel pseudo<br>label assignment mechanism, Pseudo Label Assigner, re-<br>ducing the inconsistency of pseudo labels, and Epoch<br>Adaptor, enabling a fast and efficient end-to-end semi-<br>supervised training schedule.<br>· our experiments demonstrate that utilizing Efficient<br>Teacher on YOLOv5 produces state-of-the-art results on<br>VOC, COCO-standard, and COCO-additional datasets<br>while consuming significantly fewer FLOPs than previ-<br>ous approaches.</p>",
            "id": 18,
            "page": 2,
            "text": "· We design Dense Detector as a baseline model to compare the differences between YOLOv5 and RetinaNet, which leads to a performance improvement of 5.36 AP50:95 by utilizing dense sampling. · We propose an effective SSOD training framework called Efficient Teacher, which includes a novel pseudo label assignment mechanism, Pseudo Label Assigner, reducing the inconsistency of pseudo labels, and Epoch Adaptor, enabling a fast and efficient end-to-end semisupervised training schedule. · our experiments demonstrate that utilizing Efficient Teacher on YOLOv5 produces state-of-the-art results on VOC, COCO-standard, and COCO-additional datasets while consuming significantly fewer FLOPs than previous approaches."
        },
        {
            "bounding_box": [
                {
                    "x": 1224,
                    "y": 3055
                },
                {
                    "x": 1252,
                    "y": 3055
                },
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1224,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='19' style='font-size:18px'>2</footer>",
            "id": 19,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 302
                },
                {
                    "x": 558,
                    "y": 302
                },
                {
                    "x": 558,
                    "y": 351
                },
                {
                    "x": 203,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:20px'>2. Related Work</p>",
            "id": 20,
            "page": 3,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 387
                },
                {
                    "x": 1199,
                    "y": 387
                },
                {
                    "x": 1199,
                    "y": 1434
                },
                {
                    "x": 199,
                    "y": 1434
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:18px'>Semi-supervised Object Detection. Semi-supervised<br>object detection, inherited from the semi-supervised im-<br>age classification methods [1, 33, 35, 39, 43], is divided into<br>consistency-based schemes [18, 37] and pseudo-labeling<br>schemes [27, 36, 45, 50]. The latter has become the cur-<br>rent mainstream approach. STAC [36] exploits weak and<br>strong data augmentation to process unlabeled data respec-<br>tively. Unbiased Teacher [27] follows a stduent-teacher mu-<br>tal learning to generate more accurate pseudo labels.To bal-<br>ance the effect of pseudo labels, Soft Teacher [45] uses the<br>scores of the pseudo labels as the weights for loss calcula-<br>tion. DSL [5] is the first attempt to perform semi-supervised<br>training on an anchor-free detector(FCOS) [40]. To re-<br>lieve inconsistency problems, LabelMatch [4] utilizes label<br>distribution to dynamically determine the filtering thresh-<br>old of different categories of pseudo labels. The methods<br>above have been proven great performance on two-stage<br>and anchor-free detectors, but can not perform well on an<br>one-stage anchor-based detector. Our Efficient Teacher is<br>proposed to bridge the gap between semi-supervised train-<br>ing and one-stage anchor-based detectors.</p>",
            "id": 21,
            "page": 3,
            "text": "Semi-supervised Object Detection. Semi-supervised object detection, inherited from the semi-supervised image classification methods , is divided into consistency-based schemes  and pseudo-labeling schemes . The latter has become the current mainstream approach. STAC  exploits weak and strong data augmentation to process unlabeled data respectively. Unbiased Teacher  follows a stduent-teacher mutal learning to generate more accurate pseudo labels.To balance the effect of pseudo labels, Soft Teacher  uses the scores of the pseudo labels as the weights for loss calculation. DSL  is the first attempt to perform semi-supervised training on an anchor-free detector(FCOS) . To relieve inconsistency problems, LabelMatch  utilizes label distribution to dynamically determine the filtering threshold of different categories of pseudo labels. The methods above have been proven great performance on two-stage and anchor-free detectors, but can not perform well on an one-stage anchor-based detector. Our Efficient Teacher is proposed to bridge the gap between semi-supervised training and one-stage anchor-based detectors."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1439
                },
                {
                    "x": 1198,
                    "y": 1439
                },
                {
                    "x": 1198,
                    "y": 1980
                },
                {
                    "x": 200,
                    "y": 1980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:18px'>Label Assignment. Label assignment is the key com-<br>ponent that determines the performance of an object detec-<br>tor. Many works have been proposed to improve the label<br>assignment mechanism, such as ATSS [46], PAA [21], Au-<br>toAssign [51] and OTA [14]. Some researches [4] [28] have<br>noticed that the default label assignment mechanism using<br>in supervised object detection can not be applied in SSOD<br>directly, which results in performance degradation. In this<br>paper, we propose a novel pseudo label assignment that can<br>adapt to SSOD training for one-stage anchor-based detec-<br>tors.</p>",
            "id": 22,
            "page": 3,
            "text": "Label Assignment. Label assignment is the key component that determines the performance of an object detector. Many works have been proposed to improve the label assignment mechanism, such as ATSS , PAA , AutoAssign  and OTA . Some researches   have noticed that the default label assignment mechanism using in supervised object detection can not be applied in SSOD directly, which results in performance degradation. In this paper, we propose a novel pseudo label assignment that can adapt to SSOD training for one-stage anchor-based detectors."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1989
                },
                {
                    "x": 1198,
                    "y": 1989
                },
                {
                    "x": 1198,
                    "y": 2588
                },
                {
                    "x": 200,
                    "y": 2588
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:18px'>Domain Adaptation in Object Detection. The task of<br>domain-adaptive object detection [7, 10, 24,41], aims to ad-<br>dress the problem of domain shift [8]. The work [13] utilizes<br>adversarial learning by training a discriminator with a gra-<br>dient reverse layer(GRL) to generate domain-invariant fea-<br>ture. The work [10] introduces semi-supervised techniques<br>used in Mean Teacher to alleviate domain bias, which re-<br>veals that domain shift is intrinsically related to incon-<br>sistency of semi-supervised task. This inspires Efficient<br>Teacher to introduce adversarial learning in domain adap-<br>tation to alleviate the pseudo label inconsistency of SSOD<br>training.</p>",
            "id": 23,
            "page": 3,
            "text": "Domain Adaptation in Object Detection. The task of domain-adaptive object detection , aims to address the problem of domain shift . The work  utilizes adversarial learning by training a discriminator with a gradient reverse layer(GRL) to generate domain-invariant feature. The work  introduces semi-supervised techniques used in Mean Teacher to alleviate domain bias, which reveals that domain shift is intrinsically related to inconsistency of semi-supervised task. This inspires Efficient Teacher to introduce adversarial learning in domain adaptation to alleviate the pseudo label inconsistency of SSOD training."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2640
                },
                {
                    "x": 627,
                    "y": 2640
                },
                {
                    "x": 627,
                    "y": 2691
                },
                {
                    "x": 202,
                    "y": 2691
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:22px'>3. Efficient Teacher</p>",
            "id": 24,
            "page": 3,
            "text": "3. Efficient Teacher"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2728
                },
                {
                    "x": 1198,
                    "y": 2728
                },
                {
                    "x": 1198,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>Efficient Teacher is a novel and efficient framework for<br>semi-supervised object detection, which significantly en-<br>hances the performance of one-stage anchor-based detec-<br>tors. The framework is based on a student-teacher mu-<br>tual learning approach, as shown in Figure 1, inspired by</p>",
            "id": 25,
            "page": 3,
            "text": "Efficient Teacher is a novel and efficient framework for semi-supervised object detection, which significantly enhances the performance of one-stage anchor-based detectors. The framework is based on a student-teacher mutual learning approach, as shown in Figure 1, inspired by"
        },
        {
            "bounding_box": [
                {
                    "x": 1289,
                    "y": 295
                },
                {
                    "x": 2275,
                    "y": 295
                },
                {
                    "x": 2275,
                    "y": 642
                },
                {
                    "x": 1289,
                    "y": 642
                }
            ],
            "category": "table",
            "html": "<br><table id='26' style='font-size:14px'><tr><td>Method</td><td>Resolution</td><td>Mosaic</td><td>Param.</td><td>FLOPs</td><td>AP50:95(%)</td></tr><tr><td>Faster R-CNN [31]</td><td>[1333,800]</td><td></td><td>39.8M</td><td>202.31G</td><td>40.3</td></tr><tr><td>FCOS [40]</td><td>[1333,800]</td><td></td><td>32.02M</td><td>200.59G</td><td>38.5</td></tr><tr><td>YOLOv5 w/o</td><td>[640,640]</td><td></td><td>46.56M</td><td>109.59G</td><td>41.2</td></tr><tr><td>YOLOv5 [19]</td><td>[640,640]</td><td>V</td><td>46.56M</td><td>109.59G</td><td>47.87</td></tr><tr><td>YOLOv7 [42]</td><td>[640,640]</td><td></td><td>37.62M</td><td>106.59G</td><td>51.5</td></tr><tr><td>RetinaNet [25]</td><td>[1333,800]</td><td></td><td>37.74M</td><td>239.32G</td><td>39.5</td></tr><tr><td>Dense Detector</td><td>[640,640]</td><td>V</td><td>42.13M</td><td>169.61G</td><td>44.86</td></tr></table>",
            "id": 26,
            "page": 3,
            "text": "Method Resolution Mosaic Param. FLOPs AP50:95(%)  Faster R-CNN    39.8M 202.31G 40.3  FCOS    32.02M 200.59G 38.5  YOLOv5 w/o   46.56M 109.59G 41.2  YOLOv5   V 46.56M 109.59G 47.87  YOLOv7    37.62M 106.59G 51.5  RetinaNet    37.74M 239.32G 39.5  Dense Detector  V 42.13M 169.61G"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 678
                },
                {
                    "x": 2277,
                    "y": 678
                },
                {
                    "x": 2277,
                    "y": 1088
                },
                {
                    "x": 1280,
                    "y": 1088
                }
            ],
            "category": "caption",
            "html": "<caption id='27' style='font-size:14px'>Table 1. Comparison with Faster R-CNN, FCOS, YOLOv5,<br>YOLOv7, RetinaNet and Dense Detector. The top section shows<br>results for object detectors without Mosaic augmentation, the mid-<br>dle section shows results with Mosaic augmentation during train-<br>ing. Dense Detector achieves comparable results to RetinaNet<br>baseline, having lower FLOPs but greatly improved AP50:95.<br>Both Faster R-CNN, FCOS, RetinaNet and Dense Detector uses<br>ResNet-50-FPN as backbone. AP50:95 is reported on COCO val<br>dataset.</caption>",
            "id": 27,
            "page": 3,
            "text": "Table 1. Comparison with Faster R-CNN, FCOS, YOLOv5, YOLOv7, RetinaNet and Dense Detector. The top section shows results for object detectors without Mosaic augmentation, the middle section shows results with Mosaic augmentation during training. Dense Detector achieves comparable results to RetinaNet baseline, having lower FLOPs but greatly improved AP50:95. Both Faster R-CNN, FCOS, RetinaNet and Dense Detector uses ResNet-50-FPN as backbone. AP50:95 is reported on COCO val dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1178
                },
                {
                    "x": 2276,
                    "y": 1178
                },
                {
                    "x": 2276,
                    "y": 1773
                },
                {
                    "x": 1278,
                    "y": 1773
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:18px'>previous works [4, 5, 27, 45]. Our proposed Pseudo La-<br>bel Assigner method divides pseudo labels into reliable and<br>uncertain ones based on their scores, with reliable pseudo<br>labels used for default supervised training, and uncertain<br>ones used to guide the training of the student model with<br>objectness scores. The Epoch Adaptor method is used to<br>speed up convergence by performing domain adaptation be-<br>tween labeled and unlabeled data, and switching the main<br>epoch from labeled to unlabeled data after a burn-in stage.<br>Throughout the training process, the teacher model employs<br>the Exponential Moving Average (EMA) technique for up-<br>dates.</p>",
            "id": 28,
            "page": 3,
            "text": "previous works . Our proposed Pseudo Label Assigner method divides pseudo labels into reliable and uncertain ones based on their scores, with reliable pseudo labels used for default supervised training, and uncertain ones used to guide the training of the student model with objectness scores. The Epoch Adaptor method is used to speed up convergence by performing domain adaptation between labeled and unlabeled data, and switching the main epoch from labeled to unlabeled data after a burn-in stage. Throughout the training process, the teacher model employs the Exponential Moving Average (EMA) technique for updates."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1804
                },
                {
                    "x": 1665,
                    "y": 1804
                },
                {
                    "x": 1665,
                    "y": 1853
                },
                {
                    "x": 1280,
                    "y": 1853
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:18px'>3.1. Dense Detector</p>",
            "id": 29,
            "page": 3,
            "text": "3.1. Dense Detector"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1880
                },
                {
                    "x": 2277,
                    "y": 1880
                },
                {
                    "x": 2277,
                    "y": 2725
                },
                {
                    "x": 1278,
                    "y": 2725
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>The imbalance of positive and negative training assign-<br>ment samples often leads to unqualified pseudo labels in<br>one-stage anchor-based detectors. To address this issue,<br>we introduce the design of dense inputs. YOLOv5 [19] is<br>a widely-used one-stage anchor-based detector in industry<br>due to its friendly-deployed support and fast training speed.<br>Results in Table 1 demonstrate that YOLOv5 w/o outper-<br>forms RetinaNet in terms of performance and computation.<br>Furthermore, with dense image inputs after Mosaic aug-<br>mentation, the AP50:95 of YOLOv5 can be boosted from<br>41.2 to 47.87. YOLOv7 further improves the AP50:95 to<br>51.5 with the help of dense flow of information and gra-<br>dients on the basis of dense inputs. Our study suggests<br>that improvements in the performance of one-stage anchor-<br>based detectors often require dense inputs. Therefore, we<br>propose Dense Detector as a base model for SSOD under<br>dense inputs.</p>",
            "id": 30,
            "page": 3,
            "text": "The imbalance of positive and negative training assignment samples often leads to unqualified pseudo labels in one-stage anchor-based detectors. To address this issue, we introduce the design of dense inputs. YOLOv5  is a widely-used one-stage anchor-based detector in industry due to its friendly-deployed support and fast training speed. Results in Table 1 demonstrate that YOLOv5 w/o outperforms RetinaNet in terms of performance and computation. Furthermore, with dense image inputs after Mosaic augmentation, the AP50:95 of YOLOv5 can be boosted from 41.2 to 47.87. YOLOv7 further improves the AP50:95 to 51.5 with the help of dense flow of information and gradients on the basis of dense inputs. Our study suggests that improvements in the performance of one-stage anchorbased detectors often require dense inputs. Therefore, we propose Dense Detector as a base model for SSOD under dense inputs."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2729
                },
                {
                    "x": 2278,
                    "y": 2729
                },
                {
                    "x": 2278,
                    "y": 2975
                },
                {
                    "x": 1281,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:18px'>Dense Detector is modified from RetinaNet with<br>ResNet-50-FPN backbone while changing the number of<br>FPN output from 5 to 3, eliminating the weight sharing be-<br>tween detection headers and reducing the input resolution<br>from 1333 to 640 for both training and inference. What's</p>",
            "id": 31,
            "page": 3,
            "text": "Dense Detector is modified from RetinaNet with ResNet-50-FPN backbone while changing the number of FPN output from 5 to 3, eliminating the weight sharing between detection headers and reducing the input resolution from 1333 to 640 for both training and inference. What's"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3090
                },
                {
                    "x": 1226,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='32' style='font-size:14px'>3</footer>",
            "id": 32,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 308
                },
                {
                    "x": 1199,
                    "y": 308
                },
                {
                    "x": 1199,
                    "y": 1102
                },
                {
                    "x": 200,
                    "y": 1102
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>more, Dense Detector has two outputs(a classification score<br>and a bounding-box offset) and a third branch that outputs<br>the objectness score. Dense Detetor gained 5.36% AP50:95<br>boost and 30% faster relative to RetinaNet as shown in Ta-<br>ble 1. Specifically, Dense Detector obtains objectness score<br>by calculating the Complete Intersection over Union(CIoU)<br>[48] between the predicted and GT boxes. Objectness out-<br>put directly indicates position response, and reflects the lo-<br>cation quality of the predicted boxes. As illustrated in Fig-<br>ure 1, the pseudo labels in SSOD are the predicted boxes<br>of unlabeled data, the objectness scores of which indicate<br>the location quality of pseudo labels. Thus, compared to<br>RetinaNet with only a classification branch, Dense Detec-<br>tor with an extra objectness branch can indicate the location<br>quality of pseudo labels during SSOD training as shown in<br>Figure2.</p>",
            "id": 33,
            "page": 4,
            "text": "more, Dense Detector has two outputs(a classification score and a bounding-box offset) and a third branch that outputs the objectness score. Dense Detetor gained 5.36% AP50:95 boost and 30% faster relative to RetinaNet as shown in Table 1. Specifically, Dense Detector obtains objectness score by calculating the Complete Intersection over Union(CIoU)  between the predicted and GT boxes. Objectness output directly indicates position response, and reflects the location quality of the predicted boxes. As illustrated in Figure 1, the pseudo labels in SSOD are the predicted boxes of unlabeled data, the objectness scores of which indicate the location quality of pseudo labels. Thus, compared to RetinaNet with only a classification branch, Dense Detector with an extra objectness branch can indicate the location quality of pseudo labels during SSOD training as shown in Figure2."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1107
                },
                {
                    "x": 1200,
                    "y": 1107
                },
                {
                    "x": 1200,
                    "y": 1656
                },
                {
                    "x": 200,
                    "y": 1656
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:18px'>To verify the performance of Dense Detector in SSOD,<br>we apply the classic SSOD method(Unbiased Teacher [27])<br>to the Dense Detector, which contains labeled and unlabeled<br>data, teacher and student model, and a pseudo label filter<br>to select pseudo labels. Furthermore, both labeled and un-<br>labeled data branches adopt loss definition in Equation 2.<br>However, in contrast to Unbiased Teacher on Faster R-CNN<br>in Table 2, the AP50:95 improvement of Unbiased Teacher<br>drops from 7.64 to 4.3 on Dense Detector. This motivated us<br>to develop the following Pseudo Label Assigner that plays<br>a key role in pseudo label assignment.</p>",
            "id": 34,
            "page": 4,
            "text": "To verify the performance of Dense Detector in SSOD, we apply the classic SSOD method(Unbiased Teacher ) to the Dense Detector, which contains labeled and unlabeled data, teacher and student model, and a pseudo label filter to select pseudo labels. Furthermore, both labeled and unlabeled data branches adopt loss definition in Equation 2. However, in contrast to Unbiased Teacher on Faster R-CNN in Table 2, the AP50:95 improvement of Unbiased Teacher drops from 7.64 to 4.3 on Dense Detector. This motivated us to develop the following Pseudo Label Assigner that plays a key role in pseudo label assignment."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1698
                },
                {
                    "x": 736,
                    "y": 1698
                },
                {
                    "x": 736,
                    "y": 1747
                },
                {
                    "x": 201,
                    "y": 1747
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:20px'>3.2. Pseudo Label Assigner</p>",
            "id": 35,
            "page": 4,
            "text": "3.2. Pseudo Label Assigner"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1776
                },
                {
                    "x": 1199,
                    "y": 1776
                },
                {
                    "x": 1199,
                    "y": 2572
                },
                {
                    "x": 200,
                    "y": 2572
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:18px'>The core problem in SSOD is how to assign pseudo la-<br>bels, as sub-optimal assignments can lead to inconsistent<br>pseudo labels and deteriorating performance of the mu-<br>tual learning mechanism. Pseudo Label Filter (Figure 3)<br>is a naive approach that assigns pseudo labels by setting a<br>threshold, with those below the threshold being considered<br>as background. However, this method can result in sub-<br>optimal assignments, as shown in Figure 3: in the top case,<br>the pseudo label with red color is incorrectly assigned due<br>to a threshold value of 0.1, leading to the student learning<br>the wrong label; in the bottom case, the threshold of 0.6 ex-<br>cludes low-scoring pseudo labels but incorrectly treats the<br>pseudo label in red color as background, resulting in de-<br>creased training for this class. These cases highlight the<br>need for more reasonable strategies for pseudo label assign-<br>ment in Dense Detector.</p>",
            "id": 36,
            "page": 4,
            "text": "The core problem in SSOD is how to assign pseudo labels, as sub-optimal assignments can lead to inconsistent pseudo labels and deteriorating performance of the mutual learning mechanism. Pseudo Label Filter (Figure 3) is a naive approach that assigns pseudo labels by setting a threshold, with those below the threshold being considered as background. However, this method can result in suboptimal assignments, as shown in Figure 3: in the top case, the pseudo label with red color is incorrectly assigned due to a threshold value of 0.1, leading to the student learning the wrong label; in the bottom case, the threshold of 0.6 excludes low-scoring pseudo labels but incorrectly treats the pseudo label in red color as background, resulting in decreased training for this class. These cases highlight the need for more reasonable strategies for pseudo label assignment in Dense Detector."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2579
                },
                {
                    "x": 1198,
                    "y": 2579
                },
                {
                    "x": 1198,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:16px'>Proposed in this work, Pseudo Label Assigner (PLA)<br>provides a more refined assignment of the pseudo labels<br>generated by Dense Detector. In PLA, pseudo labels ob-<br>tained after Non-Maximum Suppression (NMS) are sepa-<br>rated into two categories: reliable and uncertain pseudo la-<br>bels. The high and low threshold T1, T2 of the pseudo la-<br>bel score is used to determine two types of pseudo labels.<br>Pseudo labels with scores between T1, T2 are considered un-</p>",
            "id": 37,
            "page": 4,
            "text": "Proposed in this work, Pseudo Label Assigner (PLA) provides a more refined assignment of the pseudo labels generated by Dense Detector. In PLA, pseudo labels obtained after Non-Maximum Suppression (NMS) are separated into two categories: reliable and uncertain pseudo labels. The high and low threshold T1, T2 of the pseudo label score is used to determine two types of pseudo labels. Pseudo labels with scores between T1, T2 are considered un-"
        },
        {
            "bounding_box": [
                {
                    "x": 1360,
                    "y": 294
                },
                {
                    "x": 2268,
                    "y": 294
                },
                {
                    "x": 2268,
                    "y": 1280
                },
                {
                    "x": 1360,
                    "y": 1280
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='38' style='font-size:14px' alt=\"Objectness\nUnlabeled Unlabeled\nPseudo\nData Pseudo Data Label\nLabel\nScore\nClassification Score\nClassification\n(a) RetinaNet (b) Dense Detector\" data-coord=\"top-left:(1360,294); bottom-right:(2268,1280)\" /></figure>",
            "id": 38,
            "page": 4,
            "text": "Objectness Unlabeled Unlabeled Pseudo Data Pseudo Data Label Label Score Classification Score Classification (a) RetinaNet (b) Dense Detector"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1312
                },
                {
                    "x": 2278,
                    "y": 1312
                },
                {
                    "x": 2278,
                    "y": 1678
                },
                {
                    "x": 1279,
                    "y": 1678
                }
            ],
            "category": "caption",
            "html": "<caption id='39' style='font-size:14px'>Figure 2. Comparison of pseudo label score heatmaps from Reti-<br>naNet and Dense Detector. Darker color indicates higher score.<br>(a) RetinaNet produces sparse response due to the calculation of<br>classification scores from pseudo labels generated from unlabeled<br>data of 1333 x 800 input resolution. (b) Dense Detector, with in-<br>put resolution of 640 x 640, uses a weighted pseudo label score<br>based on objectness and classification scores, resulting in a more<br>robust and dense response..</caption>",
            "id": 39,
            "page": 4,
            "text": "Figure 2. Comparison of pseudo label score heatmaps from RetinaNet and Dense Detector. Darker color indicates higher score. (a) RetinaNet produces sparse response due to the calculation of classification scores from pseudo labels generated from unlabeled data of 1333 x 800 input resolution. (b) Dense Detector, with input resolution of 640 x 640, uses a weighted pseudo label score based on objectness and classification scores, resulting in a more robust and dense response.."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1757
                },
                {
                    "x": 2278,
                    "y": 1757
                },
                {
                    "x": 2278,
                    "y": 2102
                },
                {
                    "x": 1280,
                    "y": 2102
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:18px'>certain, and ignoring the loss of these labels directly results<br>in improved performance on Dense Detector, as shown in<br>Table 5. In addition to solving the sub-optimal problem<br>caused by Pseudo Label Filter, PLA includes an unsuper-<br>vised loss that efficiently leverages uncertain pseudo labels.<br>The loss of Dense Detector in SSOD is defined as a pair of<br>single labeled image and single unlabeled image:</p>",
            "id": 40,
            "page": 4,
            "text": "certain, and ignoring the loss of these labels directly results in improved performance on Dense Detector, as shown in Table 5. In addition to solving the sub-optimal problem caused by Pseudo Label Filter, PLA includes an unsupervised loss that efficiently leverages uncertain pseudo labels. The loss of Dense Detector in SSOD is defined as a pair of single labeled image and single unlabeled image:"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2203
                },
                {
                    "x": 2277,
                    "y": 2203
                },
                {
                    "x": 2277,
                    "y": 2450
                },
                {
                    "x": 1280,
                    "y": 2450
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>where Ls represents the loss function computed on a labeled<br>image, while Lu represents the loss function computed on<br>an unlabeled image, 入 is used to balance the supervised loss<br>and the semi-supervised loss, which is set to 3.0 in this pa-<br>per. The Ls follows the standard loss function in [19]:</p>",
            "id": 41,
            "page": 4,
            "text": "where Ls represents the loss function computed on a labeled image, while Lu represents the loss function computed on an unlabeled image, 入 is used to balance the supervised loss and the semi-supervised loss, which is set to 3.0 in this paper. The Ls follows the standard loss function in :"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2705
                },
                {
                    "x": 2277,
                    "y": 2705
                },
                {
                    "x": 2277,
                    "y": 2900
                },
                {
                    "x": 1280,
                    "y": 2900
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>where CE indicates cross-entropy loss function, X (h,w) is<br>the output of student model, and Y(h,w) means the sampled<br>results generated by the label assigner of Dense Detector ·<br>The Lu is defined as follows:</p>",
            "id": 42,
            "page": 4,
            "text": "where CE indicates cross-entropy loss function, X (h,w) is the output of student model, and Y(h,w) means the sampled results generated by the label assigner of Dense Detector · The Lu is defined as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3056
                },
                {
                    "x": 1249,
                    "y": 3056
                },
                {
                    "x": 1249,
                    "y": 3088
                },
                {
                    "x": 1225,
                    "y": 3088
                }
            ],
            "category": "footer",
            "html": "<footer id='43' style='font-size:16px'>4</footer>",
            "id": 43,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 339,
                    "y": 297
                },
                {
                    "x": 2137,
                    "y": 297
                },
                {
                    "x": 2137,
                    "y": 1346
                },
                {
                    "x": 339,
                    "y": 1346
                }
            ],
            "category": "figure",
            "html": "<figure><img id='44' style='font-size:14px' alt=\"Pseudo Labels after NMS\nPseudo Label Filter\nThreshold :0.1\nbox box\ncls x y W h obj obj\nscore Person 0.92 score\n0 0.26 0.23 0.33 0.45 0.92 0.96\nReliable Pseudo Label\nLcls + Lreg + Lobi\n0 0.26 0.23 0.33 0.45 0.92 0.97\n0.0 0.97 0.0\n53 0.33 0.48 0.13 0.21 0.54 0.99 0.0 0.97 0.97\nTeacher 0.0 0.0 0.0\nReliable Pseudo Label\nLcls + Lreg + L obj\nDense 0 0.01 0.9 0.01 0.08 0.12 0.36 0.54 0.88\nSkateboard 0.54 53 0.33 0.48 0.13 0.21\nUnlabeled Detector\n0.88 0.0 Wrong Pseudo Label\nData Person 0.12\n0.88 0.88 Lcls + Lreg + Lobi\n0 0.01 0.9 0.01 0.08 0.12 0.66\n0.00.0\n0.66\n0.66 0.66\nPseudo Label Aissgner\nHigh threshold :0.6 box\nobj\nLow threshold:0.1 score\nThreshold:0.6 Reliable Pseudo Label\nreg obj\n+ L u\nPerson 0.92 Person 0.92 0 0.26 0.23 0.33 0.45 0.92 0.97 Lcls Lu\nMissing Pseudo Label\n0.0 0.97 0.0 obj\nL u\n0.0 0.97 0.97 53 0.33 0.48 0.13 0.21 0.54 0.0\n0.0 0.97 0.0 0.0 0.0 0.0\n0.0 0.97 0.97 Skateboard 0.54\n0.0 0.0 0.0\n0.0 0.0\n0.0 0.0\nSkateboard 0.54 0.0 0.0\n0.99 0.0\nPerson 0.12\n0.99 0.99 obj\n0.0 0.0 Reliable Pseudo Label\n0.36\nobj\nreg L 'u\n0 0.26 0.23 0.33 0.45 0.92 0.97 Lcls + L u\n0.36 0.36\nUncertain Pseudo Label with high obj score\nobj\nreg Lu\nLu\n+\n53 0.33 0.48 0.13 0.21 0.54 0.99\nUncertain Pseudo Label\nobj\nLu\n0 0.01 0.9 0.01 0.08 0.12 0.36\" data-coord=\"top-left:(339,297); bottom-right:(2137,1346)\" /></figure>",
            "id": 44,
            "page": 5,
            "text": "Pseudo Labels after NMS Pseudo Label Filter Threshold :0.1 box box cls x y W h obj obj score Person 0.92 score 0 0.26 0.23 0.33 0.45 0.92 0.96 Reliable Pseudo Label Lcls + Lreg + Lobi 0 0.26 0.23 0.33 0.45 0.92 0.97 0.0 0.97 0.0 53 0.33 0.48 0.13 0.21 0.54 0.99 0.0 0.97 0.97 Teacher 0.0 0.0 0.0 Reliable Pseudo Label Lcls + Lreg + L obj Dense 0 0.01 0.9 0.01 0.08 0.12 0.36 0.54 0.88 Skateboard 0.54 53 0.33 0.48 0.13 0.21 Unlabeled Detector 0.88 0.0 Wrong Pseudo Label Data Person 0.12 0.88 0.88 Lcls + Lreg + Lobi 0 0.01 0.9 0.01 0.08 0.12 0.66 0.00.0 0.66 0.66 0.66 Pseudo Label Aissgner High threshold :0.6 box obj Low threshold:0.1 score Threshold:0.6 Reliable Pseudo Label reg obj + L u Person 0.92 Person 0.92 0 0.26 0.23 0.33 0.45 0.92 0.97 Lcls Lu Missing Pseudo Label 0.0 0.97 0.0 obj L u 0.0 0.97 0.97 53 0.33 0.48 0.13 0.21 0.54 0.0 0.0 0.97 0.0 0.0 0.0 0.0 0.0 0.97 0.97 Skateboard 0.54 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Skateboard 0.54 0.0 0.0 0.99 0.0 Person 0.12 0.99 0.99 obj 0.0 0.0 Reliable Pseudo Label 0.36 obj reg L 'u 0 0.26 0.23 0.33 0.45 0.92 0.97 Lcls + L u 0.36 0.36 Uncertain Pseudo Label with high obj score obj reg Lu Lu + 53 0.33 0.48 0.13 0.21 0.54 0.99 Uncertain Pseudo Label obj Lu 0 0.01 0.9 0.01 0.08 0.12 0.36"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1376
                },
                {
                    "x": 2278,
                    "y": 1376
                },
                {
                    "x": 2278,
                    "y": 1654
                },
                {
                    "x": 198,
                    "y": 1654
                }
            ],
            "category": "caption",
            "html": "<caption id='45' style='font-size:14px'>Figure 3. Comparison of the impact of different pseudo label select strategy. In Pseudo Label Filter, a widely-used method in SSOD [27,45],<br>Setting the threshold too low (0.1) can result in the generation of incorrect pseudo labels (indicated by the red line), while a threshold that<br>is too high (0.6) may exclude reliable pseudo labels. This can lead to suboptimal assignments and adversely affect the training of the<br>network. To address this issue, we propose the Pseudo Label Assigner method, which categorizes pseudo labels into reliable and uncertain<br>categories based on high and low thresholds, respectively. The uncertain pseudo labels are assigned soft labels as targets for Lobj to<br>improve the quality of pseudo labels in SSOD.</caption>",
            "id": 45,
            "page": 5,
            "text": "Figure 3. Comparison of the impact of different pseudo label select strategy. In Pseudo Label Filter, a widely-used method in SSOD , Setting the threshold too low (0.1) can result in the generation of incorrect pseudo labels (indicated by the red line), while a threshold that is too high (0.6) may exclude reliable pseudo labels. This can lead to suboptimal assignments and adversely affect the training of the network. To address this issue, we propose the Pseudo Label Assigner method, which categorizes pseudo labels into reliable and uncertain categories based on high and low thresholds, respectively. The uncertain pseudo labels are assigned soft labels as targets for Lobj to improve the quality of pseudo labels in SSOD."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2518
                },
                {
                    "x": 1199,
                    "y": 2518
                },
                {
                    "x": 1199,
                    "y": 2872
                },
                {
                    "x": 200,
                    "y": 2872
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:16px'>where Ycls Yreg Yobj is the classification score, re-<br>(h,w)' (h,w)' (h,w)<br>gression, objectness score of sampled results from PLA at<br>location (h, w) on feature map separately. obj(h,w) is the<br>objectness score of pseudo label at (h, w). P(h,w)<br>is the<br>score of pseudo label at (h, w). 1{} is the indicator func-<br>tion, which outputs 1 if condition {} is satisfied and 0 oth-<br>erwise.</p>",
            "id": 46,
            "page": 5,
            "text": "where Ycls Yreg Yobj is the classification score, re(h,w)' (h,w)' (h,w) gression, objectness score of sampled results from PLA at location (h, w) on feature map separately. obj(h,w) is the objectness score of pseudo label at (h, w). P(h,w) is the score of pseudo label at (h, w). 1{} is the indicator function, which outputs 1 if condition {} is satisfied and 0 otherwise."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2880
                },
                {
                    "x": 1196,
                    "y": 2880
                },
                {
                    "x": 1196,
                    "y": 2973
                },
                {
                    "x": 204,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:16px'>PLA categorizes uncertain pseudo labels into two types:<br>1) those with high classification scores and 2) those with</p>",
            "id": 47,
            "page": 5,
            "text": "PLA categorizes uncertain pseudo labels into two types: 1) those with high classification scores and 2) those with"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1745
                },
                {
                    "x": 2277,
                    "y": 1745
                },
                {
                    "x": 2277,
                    "y": 2552
                },
                {
                    "x": 1276,
                    "y": 2552
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:16px'>high objectness scores, which are handled differently. For<br>the first type, only Lobj is calculated, and the targets of the<br>cross-entropy Y(h, w) obj<br>are replaced with the soft label<br>objh, w, indicating that these pseudo labels are not classi-<br>fied as either background or positive samples. For the sec-<br>ond type, PLA calculates Lreg when the objectness score<br>is greater than 0.99, as these pseudo labels have good re-<br>gression results but insufficient classification scores to de-<br>termine their label category. PLA aims to convert more<br>uncertain pseudo labels into true positives using Lreg, as<br>more than 70% of uncertain pseudo labels are false posi-<br>tives due to inaccurate prediction boxes during SSOD train-<br>ing on COCO. Therefore, PLA suppresses the inconsistency<br>of pseudo labels through a soft label learning mechanism,<br>without affecting the loss of reliable pseudo labels. Further<br>details can be found in the Appendix.</p>",
            "id": 48,
            "page": 5,
            "text": "high objectness scores, which are handled differently. For the first type, only Lobj is calculated, and the targets of the cross-entropy Y(h, w) obj are replaced with the soft label objh, w, indicating that these pseudo labels are not classified as either background or positive samples. For the second type, PLA calculates Lreg when the objectness score is greater than 0.99, as these pseudo labels have good regression results but insufficient classification scores to determine their label category. PLA aims to convert more uncertain pseudo labels into true positives using Lreg, as more than 70% of uncertain pseudo labels are false positives due to inaccurate prediction boxes during SSOD training on COCO. Therefore, PLA suppresses the inconsistency of pseudo labels through a soft label learning mechanism, without affecting the loss of reliable pseudo labels. Further details can be found in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2597
                },
                {
                    "x": 1671,
                    "y": 2597
                },
                {
                    "x": 1671,
                    "y": 2645
                },
                {
                    "x": 1280,
                    "y": 2645
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:20px'>3.3. Epoch Adaptor</p>",
            "id": 49,
            "page": 5,
            "text": "3.3. Epoch Adaptor"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2679
                },
                {
                    "x": 2277,
                    "y": 2679
                },
                {
                    "x": 2277,
                    "y": 2976
                },
                {
                    "x": 1279,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>Although the PLA addresses the issue of pseudo label<br>inconsistency in SSOD, the hyperparameters T1 and T2 are<br>still hand-tuned and influenced by the distribution of la-<br>beled data. Additionally, the training scheme significantly<br>impacts the performance of SSOD method. Current SSOD<br>methods use two types of training scheme: alternating train-</p>",
            "id": 50,
            "page": 5,
            "text": "Although the PLA addresses the issue of pseudo label inconsistency in SSOD, the hyperparameters T1 and T2 are still hand-tuned and influenced by the distribution of labeled data. Additionally, the training scheme significantly impacts the performance of SSOD method. Current SSOD methods use two types of training scheme: alternating train-"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1225,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='51' style='font-size:14px'>5</footer>",
            "id": 51,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 308,
                    "y": 295
                },
                {
                    "x": 2166,
                    "y": 295
                },
                {
                    "x": 2166,
                    "y": 650
                },
                {
                    "x": 308,
                    "y": 650
                }
            ],
            "category": "figure",
            "html": "<figure><img id='52' style='font-size:14px' alt=\"Unlabeled Labeled\nData Data\nLabeled Unlabeled\nDomain Distribution\nData Data\nAdaptation Adaptation\nMain\nLabeled Unlabeled Epoch Main\nLabeled Labeled\nLabeled Unlabeled Main\nData Data Data Data Epoch Epoch\nIter1 Iter2\nData Data\nIter Iter\nSupervised\nSSOD Training Burn-In SSOD Training Burn-In SSOD Training\nTraining\n(a) Alternating Training (b) Joint Training with Burn-In (c) Joint Training with Epoch Adaptor\" data-coord=\"top-left:(308,295); bottom-right:(2166,650)\" /></figure>",
            "id": 52,
            "page": 6,
            "text": "Unlabeled Labeled Data Data Labeled Unlabeled Domain Distribution Data Data Adaptation Adaptation Main Labeled Unlabeled Epoch Main Labeled Labeled Labeled Unlabeled Main Data Data Data Data Epoch Epoch Iter1 Iter2 Data Data Iter Iter Supervised SSOD Training Burn-In SSOD Training Burn-In SSOD Training Training (a) Alternating Training (b) Joint Training with Burn-In (c) Joint Training with Epoch Adaptor"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 688
                },
                {
                    "x": 2276,
                    "y": 688
                },
                {
                    "x": 2276,
                    "y": 827
                },
                {
                    "x": 200,
                    "y": 827
                }
            ],
            "category": "caption",
            "html": "<caption id='53' style='font-size:16px'>Figure 4. Training strategies for Dense Detector: (a) supervised training on labeled data followed by SSOD training on unlabeled data; (b)<br>supervised training on labeled data with additional SSOD training on unlabeled data; (c) end-to-end training on both labeled and unlabeled<br>data with Epoch Adaptor incorporating Domain and Distribution Adaptation for improved convergence and feature distribution.</caption>",
            "id": 53,
            "page": 6,
            "text": "Figure 4. Training strategies for Dense Detector: (a) supervised training on labeled data followed by SSOD training on unlabeled data; (b) supervised training on labeled data with additional SSOD training on unlabeled data; (c) end-to-end training on both labeled and unlabeled data with Epoch Adaptor incorporating Domain and Distribution Adaptation for improved convergence and feature distribution."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 916
                },
                {
                    "x": 1199,
                    "y": 916
                },
                {
                    "x": 1199,
                    "y": 1360
                },
                {
                    "x": 201,
                    "y": 1360
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>ing and joint training with Burn-In. Alternating training is<br>time-consuming, but it allows for the precise calculation of<br>the reliable pseudo label threshold [4]. In contrast, joint<br>training with Burn-In directly adds unlabeled data to the<br>training process and has a faster training speed but reduces<br>SSOD training stability [27, 45] · Soft Teacher [45] sug-<br>gested artificially increasing the ratio of labeled and unla-<br>beled data in a mini-batch (1:4) due to the differences in the<br>number of labeled and unlabeled data in the training set.</p>",
            "id": 54,
            "page": 6,
            "text": "ing and joint training with Burn-In. Alternating training is time-consuming, but it allows for the precise calculation of the reliable pseudo label threshold . In contrast, joint training with Burn-In directly adds unlabeled data to the training process and has a faster training speed but reduces SSOD training stability  · Soft Teacher  suggested artificially increasing the ratio of labeled and unlabeled data in a mini-batch (1:4) due to the differences in the number of labeled and unlabeled data in the training set."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1365
                },
                {
                    "x": 1199,
                    "y": 1365
                },
                {
                    "x": 1199,
                    "y": 1909
                },
                {
                    "x": 201,
                    "y": 1909
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:18px'>Building upon the joint training scheme, we present the<br>Epoch Adaptor(EA) framework that enhances the Burn-In<br>stage's training efficiency and consistency. Moreover, EA<br>dynamically calculate the T1 and T2 parameters of PLA in<br>the SSOD training stage. During the Burn-In stage, EA uti-<br>lizes domain adaptation to allow Dense Detector to train<br>simultaneously on labeled and unlabeled data. Specifically,<br>EA incorporates a 1:1 ratio of labeled and unlabeled data in<br>1:1 ratio and trains a domain adaptation classifier on each<br>pair of labeled and unlabeled data. The domain adaptation<br>loss function as follow:</p>",
            "id": 55,
            "page": 6,
            "text": "Building upon the joint training scheme, we present the Epoch Adaptor(EA) framework that enhances the Burn-In stage's training efficiency and consistency. Moreover, EA dynamically calculate the T1 and T2 parameters of PLA in the SSOD training stage. During the Burn-In stage, EA utilizes domain adaptation to allow Dense Detector to train simultaneously on labeled and unlabeled data. Specifically, EA incorporates a 1:1 ratio of labeled and unlabeled data in 1:1 ratio and trains a domain adaptation classifier on each pair of labeled and unlabeled data. The domain adaptation loss function as follow:"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2069
                },
                {
                    "x": 1199,
                    "y": 2069
                },
                {
                    "x": 1199,
                    "y": 2431
                },
                {
                    "x": 202,
                    "y": 2431
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:16px'>where P(h,w) output of the domain classifier. D = 0 for<br>is the<br>labeled data and D = 1 for unlabeled data. We use the gradi-<br>ent reverse layer (GRL) [13], whereas the ordinary gradient<br>descent is applied for training the domain classifier and the<br>sign of the gradient is reversed when passing through the<br>GRL layer to optimize the base network. In Burn-In stage,<br>the supervised loss in one image can be reformulated as fol-<br>lows:</p>",
            "id": 56,
            "page": 6,
            "text": "where P(h,w) output of the domain classifier. D = 0 for is the labeled data and D = 1 for unlabeled data. We use the gradient reverse layer (GRL) , whereas the ordinary gradient descent is applied for training the domain classifier and the sign of the gradient is reversed when passing through the GRL layer to optimize the base network. In Burn-In stage, the supervised loss in one image can be reformulated as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2643
                },
                {
                    "x": 1199,
                    "y": 2643
                },
                {
                    "x": 1199,
                    "y": 2835
                },
                {
                    "x": 202,
                    "y": 2835
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>where 入 is the hyper-parameter to control the contribution<br>of domain adaptation, which is 0.1 in our experiments. The<br>expression capability of the model is enhanced by allowing<br>the detector to see the unlabeled data in Burn-In.</p>",
            "id": 57,
            "page": 6,
            "text": "where 入 is the hyper-parameter to control the contribution of domain adaptation, which is 0.1 in our experiments. The expression capability of the model is enhanced by allowing the detector to see the unlabeled data in Burn-In."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2839
                },
                {
                    "x": 1198,
                    "y": 2839
                },
                {
                    "x": 1198,
                    "y": 2976
                },
                {
                    "x": 202,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:16px'>During the SSOD training stage, EA applies a strat-<br>egy where the current main epoch switches from the la-<br>beled data to the unlabeled data, as depicted in Figure</p>",
            "id": 58,
            "page": 6,
            "text": "During the SSOD training stage, EA applies a strategy where the current main epoch switches from the labeled data to the unlabeled data, as depicted in Figure"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 914
                },
                {
                    "x": 2277,
                    "y": 914
                },
                {
                    "x": 2277,
                    "y": 1419
                },
                {
                    "x": 1279,
                    "y": 1419
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:16px'>4. To dynamically calculate the T1 and T2 thresholds of<br>PLA in each epoch, we implement a distribution adapta-<br>tion method based on the re-distribution method in Label-<br>Match [4]. This is necessary because Mosaic augmenta-<br>tion increases the number of ground truth (GT) annotations<br>in labeled data, which alters the GT counting setting used<br>in the re-distribution method based on offline annotations<br>in LabelMatch [4]. Specifically, the GT count per image<br>increases from 7.24 to 21.4 during SSOD training on the<br>COCO dataset. The T1 and T2 thresholds at the k-th epoch<br>are determined as follows:</p>",
            "id": 59,
            "page": 6,
            "text": "4. To dynamically calculate the T1 and T2 thresholds of PLA in each epoch, we implement a distribution adaptation method based on the re-distribution method in LabelMatch . This is necessary because Mosaic augmentation increases the number of ground truth (GT) annotations in labeled data, which alters the GT counting setting used in the re-distribution method based on offline annotations in LabelMatch . Specifically, the GT count per image increases from 7.24 to 21.4 during SSOD training on the COCO dataset. The T1 and T2 thresholds at the k-th epoch are determined as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1678
                },
                {
                    "x": 2275,
                    "y": 1678
                },
                {
                    "x": 2275,
                    "y": 2172
                },
                {
                    "x": 1280,
                    "y": 2172
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:18px'>The reliable ratio a is set to 60 for all experiments, and Pk<br>represents the list of pseudo label scores of the c-th class at<br>the k-th epoch. Meanwhile, Nl and Nu denote the number<br>of labeled and unlabeled data, and nk represents the num-<br>ber of c-th class ground truth annotations that are counted<br>by EA at the k-th epoch. By dynamically calculating the ap-<br>propriate thresholds at each epoch, EA enables joint train-<br>ing to be more adaptable to dynamic data distributions. This<br>approach differs from the offline statistical method used by<br>LabelMatch [4].</p>",
            "id": 60,
            "page": 6,
            "text": "The reliable ratio a is set to 60 for all experiments, and Pk represents the list of pseudo label scores of the c-th class at the k-th epoch. Meanwhile, Nl and Nu denote the number of labeled and unlabeled data, and nk represents the number of c-th class ground truth annotations that are counted by EA at the k-th epoch. By dynamically calculating the appropriate thresholds at each epoch, EA enables joint training to be more adaptable to dynamic data distributions. This approach differs from the offline statistical method used by LabelMatch ."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2178
                },
                {
                    "x": 2277,
                    "y": 2178
                },
                {
                    "x": 2277,
                    "y": 2424
                },
                {
                    "x": 1280,
                    "y": 2424
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:18px'>By enabling the SSOD training to obtain higher AP50:95<br>and faster convergence without the need for hand-tuned<br>thresholds, EA effectively reduces the overall training time<br>of joint training scheme. The experimental results demon-<br>strating these effects are presented in Section 4.</p>",
            "id": 61,
            "page": 6,
            "text": "By enabling the SSOD training to obtain higher AP50:95 and faster convergence without the need for hand-tuned thresholds, EA effectively reduces the overall training time of joint training scheme. The experimental results demonstrating these effects are presented in Section 4."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2470
                },
                {
                    "x": 1612,
                    "y": 2470
                },
                {
                    "x": 1612,
                    "y": 2521
                },
                {
                    "x": 1280,
                    "y": 2521
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:22px'>4. Experiments</p>",
            "id": 62,
            "page": 6,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2552
                },
                {
                    "x": 1760,
                    "y": 2552
                },
                {
                    "x": 1760,
                    "y": 2600
                },
                {
                    "x": 1281,
                    "y": 2600
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:20px'>4.1. Experimental Setup</p>",
            "id": 63,
            "page": 6,
            "text": "4.1. Experimental Setup"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2629
                },
                {
                    "x": 2277,
                    "y": 2629
                },
                {
                    "x": 2277,
                    "y": 2975
                },
                {
                    "x": 1279,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>Datasets. We validate our method on MS-COCO [26]<br>and VOC [12] benchmarks: (1) COCO-standard: 1%, 2%,<br>5%, 10% of the images are sampled on COCO as labeled<br>data, and all the remaining data are used as unlabeled data.<br>(2) COCO-additional: train2017 dataset is set as the la-<br>beled dataset and COCO2017-unlabeled is as the unlabeled<br>dataset. (3) VOC: VOC07 trainval data is as the labeled</p>",
            "id": 64,
            "page": 6,
            "text": "Datasets. We validate our method on MS-COCO  and VOC  benchmarks: (1) COCO-standard: 1%, 2%, 5%, 10% of the images are sampled on COCO as labeled data, and all the remaining data are used as unlabeled data. (2) COCO-additional: train2017 dataset is set as the labeled dataset and COCO2017-unlabeled is as the unlabeled dataset. (3) VOC: VOC07 trainval data is as the labeled"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='65' style='font-size:14px'>6</footer>",
            "id": 65,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 294
                },
                {
                    "x": 2268,
                    "y": 294
                },
                {
                    "x": 2268,
                    "y": 1033
                },
                {
                    "x": 207,
                    "y": 1033
                }
            ],
            "category": "table",
            "html": "<table id='66' style='font-size:14px'><tr><td colspan=\"2\">Method</td><td>%1</td><td>%2</td><td>%5</td><td>%10</td><td>FLOPs</td></tr><tr><td>Two-stage anchor-based</td><td>Supervised STAC [36] Instant Teaching [50] Humber teacher [38] Unbiased Teacher [27] Soft Teacher [45] LabelMatch [4] PseCo [23]</td><td>9.05 13.97 土 0.35(+4.92) 18.05 土 0.15 (+9.00) 16.96 土 0.38 (+7.91) 20.75 土 0.12 (+11.70) 20.46 土 0.39 (+11.41) 25.81 土 0.28 (+16.76) 22.43 土 0.36 (+13.38)</td><td>12.70 18.25 土 0.25 (+5.91) 22.45 土 0.15 (+9.75) 21.72 土 0.24 (+9.02) 24.30 土 0.07 (+9.80) - - 27.77 土 0.18 (+15.07)</td><td>18.47 24.38 土 0.12 (+5.91) 26.75 土 0.05 (+8.28) 27.70 土 0.15 (+9.23) 28.27 土 0.11 (+9.80) 30.74 土 0.08 (+12.27) 32.70 土 0.18 (+14.23) 32.50 土 0.08 (+14.03)</td><td>23.86 28.64 土 0.21 (+4.78) 30.40 土 0.05 (+6.54) 31.61 土 0.28 (+7.75) 31.50 士 0.10 (+7.64) 34.04 土 0.14 (+10.18) 35.49 土 0.17 (+11.63) 36.06 土 0.24 (+12.20)</td><td>202.31G 202.31G 202.31G 202.31G 204.13G 202.31G 202.31G 202.31G</td></tr><tr><td>One-stage anchor-free</td><td>Supervised Unbiased Teacher v2 [28] DSL [] Dense Teacher [49]</td><td>9.53 22.71 土 0.42 (+13.18) 22.03 土 0.28 (+12.50) 22.38 土 0.31 (+12.85)</td><td>11.71 26.03 土 0.12 (+14.32) 25.19 土 0.37 (+13.48) 27.20 土 0.20 (+15.49)</td><td>18.74 30.08 土 0.04 (+11.34) 30.87 土 0.24 (+12.13) 33.01 土 0.21 (+14.27)</td><td>23.70 32.61 土 0.03 (+8.91) 36.22 土 0.18 (+12.52) 37.13 土 0.12 (+13.43)</td><td>200.59G 200.59G 200.59G 200.59G</td></tr><tr><td>One-stage anchor-based</td><td>Supervised Unbiased Teacher* [27] Ours Ours t</td><td>11.29 18.81 土 0.28 (+7.52) 20.18 土 0.21 (+8.87) 23.76 土 0.13 (+12.47)</td><td>13.12 22.72 土 0.21 (+9.60) 25.85 土 0.13 (+12.73) 28.70 土 0.14 (+15.58)</td><td>20.28 28.35 土 0.12 (+8.15) 30.41 土 0.08 (+10.13) 34.11 土 0.09 (+13.83)</td><td>26.04 30.34 土 0.09 (+4.30) 33.44 土 0.11 (+7.40) 37.90 土 0.04 (+11.86)</td><td>169.61G 169.61G 169.61G 109.59G</td></tr></table>",
            "id": 66,
            "page": 7,
            "text": "Method %1 %2 %5 %10 FLOPs  Two-stage anchor-based Supervised STAC  Instant Teaching  Humber teacher  Unbiased Teacher  Soft Teacher  LabelMatch  PseCo  9.05 13.97 土 0.35(+4.92) 18.05 土 0.15 (+9.00) 16.96 土 0.38 (+7.91) 20.75 土 0.12 (+11.70) 20.46 土 0.39 (+11.41) 25.81 土 0.28 (+16.76) 22.43 土 0.36 (+13.38) 12.70 18.25 土 0.25 (+5.91) 22.45 土 0.15 (+9.75) 21.72 土 0.24 (+9.02) 24.30 土 0.07 (+9.80) - - 27.77 土 0.18 (+15.07) 18.47 24.38 土 0.12 (+5.91) 26.75 土 0.05 (+8.28) 27.70 土 0.15 (+9.23) 28.27 土 0.11 (+9.80) 30.74 土 0.08 (+12.27) 32.70 土 0.18 (+14.23) 32.50 土 0.08 (+14.03) 23.86 28.64 土 0.21 (+4.78) 30.40 土 0.05 (+6.54) 31.61 土 0.28 (+7.75) 31.50 士 0.10 (+7.64) 34.04 土 0.14 (+10.18) 35.49 土 0.17 (+11.63) 36.06 土 0.24 (+12.20) 202.31G 202.31G 202.31G 202.31G 204.13G 202.31G 202.31G 202.31G  One-stage anchor-free Supervised Unbiased Teacher v2  DSL [] Dense Teacher  9.53 22.71 土 0.42 (+13.18) 22.03 土 0.28 (+12.50) 22.38 土 0.31 (+12.85) 11.71 26.03 土 0.12 (+14.32) 25.19 土 0.37 (+13.48) 27.20 土 0.20 (+15.49) 18.74 30.08 土 0.04 (+11.34) 30.87 土 0.24 (+12.13) 33.01 土 0.21 (+14.27) 23.70 32.61 土 0.03 (+8.91) 36.22 土 0.18 (+12.52) 37.13 土 0.12 (+13.43) 200.59G 200.59G 200.59G 200.59G  One-stage anchor-based Supervised Unbiased Teacher*  Ours Ours t 11.29 18.81 土 0.28 (+7.52) 20.18 土 0.21 (+8.87) 23.76 土 0.13 (+12.47) 13.12 22.72 土 0.21 (+9.60) 25.85 土 0.13 (+12.73) 28.70 土 0.14 (+15.58) 20.28 28.35 土 0.12 (+8.15) 30.41 土 0.08 (+10.13) 34.11 土 0.09 (+13.83) 26.04 30.34 土 0.09 (+4.30) 33.44 土 0.11 (+7.40) 37.90 土 0.04 (+11.86)"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1067
                },
                {
                    "x": 2274,
                    "y": 1067
                },
                {
                    "x": 2274,
                    "y": 1160
                },
                {
                    "x": 204,
                    "y": 1160
                }
            ],
            "category": "caption",
            "html": "<caption id='67' style='font-size:14px'>Table 2. Experimental results on COCO-standard (AP50:95), * means re-implemented results on Dense Detector, 1 means Efficient Teacher<br>with YOLOv51 [19]. All the results are the average of 5 folds.</caption>",
            "id": 67,
            "page": 7,
            "text": "Table 2. Experimental results on COCO-standard (AP50:95), * means re-implemented results on Dense Detector, 1 means Efficient Teacher with YOLOv51 . All the results are the average of 5 folds."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1250
                },
                {
                    "x": 1196,
                    "y": 1250
                },
                {
                    "x": 1196,
                    "y": 1394
                },
                {
                    "x": 203,
                    "y": 1394
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>dataset and VOC12 trainval is used as the unlabeled dataset.<br>We adopt the mean average precision AP50:90 as the evalu-<br>ation metric.</p>",
            "id": 68,
            "page": 7,
            "text": "dataset and VOC12 trainval is used as the unlabeled dataset. We adopt the mean average precision AP50:90 as the evaluation metric."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1403
                },
                {
                    "x": 1200,
                    "y": 1403
                },
                {
                    "x": 1200,
                    "y": 1745
                },
                {
                    "x": 201,
                    "y": 1745
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:18px'>Network.To verify that our proposed method is scalable,<br>we used three Dense Detector architectures:The first one<br>uses ResNet-50-FPN in Dense Detector. The second one<br>replaces the original backbone with CSPNet and the Neck<br>with PAN, which is similar with YOLOv5. The last one<br>follows YOLOv7 to add ELAN module and RepConv [11]<br>into backbone.</p>",
            "id": 69,
            "page": 7,
            "text": "Network.To verify that our proposed method is scalable, we used three Dense Detector architectures:The first one uses ResNet-50-FPN in Dense Detector. The second one replaces the original backbone with CSPNet and the Neck with PAN, which is similar with YOLOv5. The last one follows YOLOv7 to add ELAN module and RepConv  into backbone."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1754
                },
                {
                    "x": 1198,
                    "y": 1754
                },
                {
                    "x": 1198,
                    "y": 2303
                },
                {
                    "x": 201,
                    "y": 2303
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:18px'>Implementation Details. We use 8 NVIDIA-V100<br>GPUs with 16G memory per GPU. We randomly sample<br>32 images from labeled data and 32 images from unlabeled<br>data with ratio 1:1 in each iteration. For training config-<br>urations, the learning rate is 0.01 all the time, the T1 and<br>T2 are calculated by EA. We used both weak and strong<br>data augmentation. Mosaic is used in weak data augmen-<br>tation. In the strong data augmentation, Mosaic, left-right<br>flip, large scale jittering, graying, Gaussian blur, cutout, and<br>color space conversion are selected. The max epoch is 300.<br>Smoothing hyper-parameter in EMA is 0.999.</p>",
            "id": 70,
            "page": 7,
            "text": "Implementation Details. We use 8 NVIDIA-V100 GPUs with 16G memory per GPU. We randomly sample 32 images from labeled data and 32 images from unlabeled data with ratio 1:1 in each iteration. For training configurations, the learning rate is 0.01 all the time, the T1 and T2 are calculated by EA. We used both weak and strong data augmentation. Mosaic is used in weak data augmentation. In the strong data augmentation, Mosaic, left-right flip, large scale jittering, graying, Gaussian blur, cutout, and color space conversion are selected. The max epoch is 300. Smoothing hyper-parameter in EMA is 0.999."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2345
                },
                {
                    "x": 434,
                    "y": 2345
                },
                {
                    "x": 434,
                    "y": 2391
                },
                {
                    "x": 204,
                    "y": 2391
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:22px'>4.2. Results</p>",
            "id": 71,
            "page": 7,
            "text": "4.2. Results"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2426
                },
                {
                    "x": 1199,
                    "y": 2426
                },
                {
                    "x": 1199,
                    "y": 2871
                },
                {
                    "x": 201,
                    "y": 2871
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>COCO-standard. In Table 2, we validate our proposed<br>method on COCO-standard and the performance of Effi-<br>cient Teacher is better than Unbiased Teacher on Dense De-<br>tector. Moreover, when applying standard YOLOv51 Back-<br>bone, our method achieves state-of-the-art results on labeled<br>data with 2%, 5% ,10% coefficients. Comparing to previ-<br>ous state-of-the-arts, Efficient Teacher is the second highest<br>in 1% labeled setting, after LabelMatch [4], but greatly im-<br>proved both in 5% and 10% COCO using fewer FLOPs.</p>",
            "id": 72,
            "page": 7,
            "text": "COCO-standard. In Table 2, we validate our proposed method on COCO-standard and the performance of Efficient Teacher is better than Unbiased Teacher on Dense Detector. Moreover, when applying standard YOLOv51 Backbone, our method achieves state-of-the-art results on labeled data with 2%, 5% ,10% coefficients. Comparing to previous state-of-the-arts, Efficient Teacher is the second highest in 1% labeled setting, after LabelMatch , but greatly improved both in 5% and 10% COCO using fewer FLOPs."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2879
                },
                {
                    "x": 1196,
                    "y": 2879
                },
                {
                    "x": 1196,
                    "y": 2974
                },
                {
                    "x": 203,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:18px'>COCO-additional. Results in Table 3 show our pro-<br>posed method on COCO-additional, the gain effect of Effi-</p>",
            "id": 73,
            "page": 7,
            "text": "COCO-additional. Results in Table 3 show our proposed method on COCO-additional, the gain effect of Effi-"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1251
                },
                {
                    "x": 2277,
                    "y": 1251
                },
                {
                    "x": 2277,
                    "y": 1392
                },
                {
                    "x": 1281,
                    "y": 1392
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:18px'>cient Teacher is better than Unbiased Teacher, which shows<br>1.01 increase on AP50:95. Backbone in all experiments is<br>YOLOv51.</p>",
            "id": 74,
            "page": 7,
            "text": "cient Teacher is better than Unbiased Teacher, which shows 1.01 increase on AP50:95. Backbone in all experiments is YOLOv51."
        },
        {
            "bounding_box": [
                {
                    "x": 1434,
                    "y": 1440
                },
                {
                    "x": 2123,
                    "y": 1440
                },
                {
                    "x": 2123,
                    "y": 1691
                },
                {
                    "x": 1434,
                    "y": 1691
                }
            ],
            "category": "table",
            "html": "<table id='75' style='font-size:22px'><tr><td>Method</td><td>AP50:95</td></tr><tr><td>Supervised 1</td><td>47.87</td></tr><tr><td>Unbiased Teacher [27] t</td><td>48.48(+0.61)</td></tr><tr><td>Ours 1</td><td>48.88(+1.01)</td></tr></table>",
            "id": 75,
            "page": 7,
            "text": "Method AP50:95  Supervised 1 47.87  Unbiased Teacher  t 48.48(+0.61)  Ours 1"
        },
        {
            "bounding_box": [
                {
                    "x": 1392,
                    "y": 1730
                },
                {
                    "x": 2164,
                    "y": 1730
                },
                {
                    "x": 2164,
                    "y": 1772
                },
                {
                    "x": 1392,
                    "y": 1772
                }
            ],
            "category": "caption",
            "html": "<caption id='76' style='font-size:14px'>Table 3. Experimental results on COCO-additional.</caption>",
            "id": 76,
            "page": 7,
            "text": "Table 3. Experimental results on COCO-additional."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1853
                },
                {
                    "x": 2277,
                    "y": 1853
                },
                {
                    "x": 2277,
                    "y": 2049
                },
                {
                    "x": 1279,
                    "y": 2049
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:18px'>PASCAL-VOC. Table 4 shows the results of experi-<br>ments conducted on VOC are convincing. Our method<br>achieves 58.30 on AP50:95, which shows more accurate<br>bounding box regression results.</p>",
            "id": 77,
            "page": 7,
            "text": "PASCAL-VOC. Table 4 shows the results of experiments conducted on VOC are convincing. Our method achieves 58.30 on AP50:95, which shows more accurate bounding box regression results."
        },
        {
            "bounding_box": [
                {
                    "x": 1409,
                    "y": 2100
                },
                {
                    "x": 2148,
                    "y": 2100
                },
                {
                    "x": 2148,
                    "y": 2557
                },
                {
                    "x": 1409,
                    "y": 2557
                }
            ],
            "category": "table",
            "html": "<table id='78' style='font-size:20px'><tr><td>Method</td><td>AP50:95</td><td>AP50</td></tr><tr><td>STAC [36]</td><td>44.64</td><td>77.45</td></tr><tr><td>Instant Teacher [50]</td><td>50.00</td><td>79.20</td></tr><tr><td>Unbiased Teacher [27]</td><td>48.69</td><td>77.37</td></tr><tr><td>Dense Teacher [49]</td><td>55.87</td><td>79.89</td></tr><tr><td>DSL [5]</td><td>56.80</td><td>80.70</td></tr><tr><td>Unbiased Teacher v2 [28]</td><td>56.87</td><td>81.29</td></tr><tr><td>Ours T</td><td>58.30</td><td>81.60</td></tr></table>",
            "id": 78,
            "page": 7,
            "text": "Method AP50:95 AP50  STAC  44.64 77.45  Instant Teacher  50.00 79.20  Unbiased Teacher  48.69 77.37  Dense Teacher  55.87 79.89  DSL  56.80 80.70  Unbiased Teacher v2  56.87 81.29  Ours T 58.30"
        },
        {
            "bounding_box": [
                {
                    "x": 1410,
                    "y": 2591
                },
                {
                    "x": 2146,
                    "y": 2591
                },
                {
                    "x": 2146,
                    "y": 2632
                },
                {
                    "x": 1410,
                    "y": 2632
                }
            ],
            "category": "caption",
            "html": "<caption id='79' style='font-size:16px'>Table 4. Experimental results on PASCAL- VOC.</caption>",
            "id": 79,
            "page": 7,
            "text": "Table 4. Experimental results on PASCAL- VOC."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2745
                },
                {
                    "x": 1694,
                    "y": 2745
                },
                {
                    "x": 1694,
                    "y": 2795
                },
                {
                    "x": 1281,
                    "y": 2795
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:22px'>4.3. Ablation Studies</p>",
            "id": 80,
            "page": 7,
            "text": "4.3. Ablation Studies"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2829
                },
                {
                    "x": 2277,
                    "y": 2829
                },
                {
                    "x": 2277,
                    "y": 2972
                },
                {
                    "x": 1283,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:18px'>In ablation studies, we conducted experiments using the<br>10% COCO-standard dataset(one of 5 folds). The backbone<br>is YOLOv51.</p>",
            "id": 81,
            "page": 7,
            "text": "In ablation studies, we conducted experiments using the 10% COCO-standard dataset(one of 5 folds). The backbone is YOLOv51."
        },
        {
            "bounding_box": [
                {
                    "x": 1224,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1224,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='82' style='font-size:16px'>7</footer>",
            "id": 82,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 304
                },
                {
                    "x": 1200,
                    "y": 304
                },
                {
                    "x": 1200,
                    "y": 1000
                },
                {
                    "x": 201,
                    "y": 1000
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:18px'>Effect of Pseudo Label Assigner. The impact of the<br>proposed Pseudo Label Assigner is presented in Table 5.<br>We observe that applying the Unbiased Teacher method to<br>the Dense detector with a threshold of 0.3 for pseudo la-<br>bel generation only leads to a modest AP50:95 improvement<br>of 1.65, which is considerably lower than the 7.6 AP50:95<br>gain achieved by the Unbiased Teacher [27] applied to the<br>Faster R-CNN. When neglecting the uncertain pseudo la-<br>bels, the AP50:95 further increases to 35.2. However, by<br>utilizing the Pseudo Label Assigner to handle the uncertain<br>pseudo labels, we obtain a significant improvement of 7.45<br>in AP50:95, resulting in a final performance of 37.90, which<br>is comparable to that of the Unbiased Teacher applied to the<br>Faster R-CNN.</p>",
            "id": 83,
            "page": 8,
            "text": "Effect of Pseudo Label Assigner. The impact of the proposed Pseudo Label Assigner is presented in Table 5. We observe that applying the Unbiased Teacher method to the Dense detector with a threshold of 0.3 for pseudo label generation only leads to a modest AP50:95 improvement of 1.65, which is considerably lower than the 7.6 AP50:95 gain achieved by the Unbiased Teacher  applied to the Faster R-CNN. When neglecting the uncertain pseudo labels, the AP50:95 further increases to 35.2. However, by utilizing the Pseudo Label Assigner to handle the uncertain pseudo labels, we obtain a significant improvement of 7.45 in AP50:95, resulting in a final performance of 37.90, which is comparable to that of the Unbiased Teacher applied to the Faster R-CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 212,
                    "y": 1038
                },
                {
                    "x": 1195,
                    "y": 1038
                },
                {
                    "x": 1195,
                    "y": 1288
                },
                {
                    "x": 212,
                    "y": 1288
                }
            ],
            "category": "table",
            "html": "<table id='84' style='font-size:14px'><tr><td>Method</td><td>AP50:95</td><td>AP50</td></tr><tr><td>Supervised</td><td>30.45</td><td>44.65</td></tr><tr><td>Unbiased Teacher [27]</td><td>32.10 (+1.65)</td><td>47.30 (+2.65)</td></tr><tr><td>Ignore uncertain pseudo label [5]</td><td>35.20 (+4.75)</td><td>52.00 (+7.35)</td></tr><tr><td>Pseudo Label Assigner</td><td>37.90 (+7.45)</td><td>54.19 (+9.54)</td></tr></table>",
            "id": 84,
            "page": 8,
            "text": "Method AP50:95 AP50  Supervised 30.45 44.65  Unbiased Teacher  32.10 (+1.65) 47.30 (+2.65)  Ignore uncertain pseudo label  35.20 (+4.75) 52.00 (+7.35)  Pseudo Label Assigner 37.90 (+7.45)"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1331
                },
                {
                    "x": 1196,
                    "y": 1331
                },
                {
                    "x": 1196,
                    "y": 1416
                },
                {
                    "x": 204,
                    "y": 1416
                }
            ],
            "category": "caption",
            "html": "<caption id='85' style='font-size:14px'>Table 5. Ablation study about different pseudo label assignment<br>methods.</caption>",
            "id": 85,
            "page": 8,
            "text": "Table 5. Ablation study about different pseudo label assignment methods."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1484
                },
                {
                    "x": 1200,
                    "y": 1484
                },
                {
                    "x": 1200,
                    "y": 2129
                },
                {
                    "x": 202,
                    "y": 2129
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>Effect of dynamic threshold. We evaluate the impact<br>of varying the threshold value T2 in the Pseudo Label As-<br>signer method on the COCO 10% standard task. As shown<br>in Table 6, increasing the value of T2 results in a decreas-<br>ing trend for AP50:95, which is indicative of fewer reliable<br>pseudo labels and more uncertain ones. This highlights the<br>importance of having an appropriate balance between re-<br>liable and uncertain pseudo labels, as the decrease in the<br>number of reliable pseudo labels can negatively impact the<br>effectiveness of SSOD training. Notably, the use of Epoch<br>Adaptor to dynamically calculate the value of T2 results in<br>the best performance without requiring manual tuning ef-<br>forts.</p>",
            "id": 86,
            "page": 8,
            "text": "Effect of dynamic threshold. We evaluate the impact of varying the threshold value T2 in the Pseudo Label Assigner method on the COCO 10% standard task. As shown in Table 6, increasing the value of T2 results in a decreasing trend for AP50:95, which is indicative of fewer reliable pseudo labels and more uncertain ones. This highlights the importance of having an appropriate balance between reliable and uncertain pseudo labels, as the decrease in the number of reliable pseudo labels can negatively impact the effectiveness of SSOD training. Notably, the use of Epoch Adaptor to dynamically calculate the value of T2 results in the best performance without requiring manual tuning efforts."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 2165
                },
                {
                    "x": 884,
                    "y": 2165
                },
                {
                    "x": 884,
                    "y": 2533
                },
                {
                    "x": 516,
                    "y": 2533
                }
            ],
            "category": "table",
            "html": "<table id='87' style='font-size:18px'><tr><td>T2</td><td>AP50:95</td><td>AP50</td></tr><tr><td>0.4</td><td>37.20</td><td>54.08</td></tr><tr><td>0.5</td><td>37.20</td><td>54.10</td></tr><tr><td>0.6</td><td>36.90</td><td>53.77</td></tr><tr><td>0.7</td><td>35.10</td><td>51.60</td></tr><tr><td>EA</td><td>37.90</td><td>54.80</td></tr></table>",
            "id": 87,
            "page": 8,
            "text": "T2 AP50:95 AP50  0.4 37.20 54.08  0.5 37.20 54.10  0.6 36.90 53.77  0.7 35.10 51.60  EA 37.90"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2575
                },
                {
                    "x": 1196,
                    "y": 2575
                },
                {
                    "x": 1196,
                    "y": 2662
                },
                {
                    "x": 204,
                    "y": 2662
                }
            ],
            "category": "caption",
            "html": "<caption id='88' style='font-size:14px'>Table 6. Ablation studies on dynamic threshold, EA indicates T2<br>is calculated by Epoch Adaptor.</caption>",
            "id": 88,
            "page": 8,
            "text": "Table 6. Ablation studies on dynamic threshold, EA indicates T2 is calculated by Epoch Adaptor."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2728
                },
                {
                    "x": 1199,
                    "y": 2728
                },
                {
                    "x": 1199,
                    "y": 2975
                },
                {
                    "x": 203,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>Effect of the Epoch Adaptor. We conducted an abla-<br>tion study of Epoch Adaptor, as shown in Figure 5. The<br>results demonstrate that after jointly training with Epoch<br>Adaptor, the network achieves superior performance with<br>fewer iterations compared to the other two SSOD train-</p>",
            "id": 89,
            "page": 8,
            "text": "Effect of the Epoch Adaptor. We conducted an ablation study of Epoch Adaptor, as shown in Figure 5. The results demonstrate that after jointly training with Epoch Adaptor, the network achieves superior performance with fewer iterations compared to the other two SSOD train-"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 309
                },
                {
                    "x": 2277,
                    "y": 309
                },
                {
                    "x": 2277,
                    "y": 503
                },
                {
                    "x": 1279,
                    "y": 503
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>ing methods, namely fully supervised and alternating train-<br>ing, which involves training a detector for 70K iterations in<br>a fully supervised manner followed by 240K iterations of<br>semi-supervised training.</p>",
            "id": 90,
            "page": 8,
            "text": "ing methods, namely fully supervised and alternating training, which involves training a detector for 70K iterations in a fully supervised manner followed by 240K iterations of semi-supervised training."
        },
        {
            "bounding_box": [
                {
                    "x": 1300,
                    "y": 623
                },
                {
                    "x": 2186,
                    "y": 623
                },
                {
                    "x": 2186,
                    "y": 1282
                },
                {
                    "x": 1300,
                    "y": 1282
                }
            ],
            "category": "figure",
            "html": "<figure><img id='91' style='font-size:14px' alt=\"0.35\n0.30\n0.25\nmAP 0.20\n0.15\n0.10\n0.05 Joint Training with Epoch Adaptor\nAlternating Training\n0.00 Joint Training with Burn-In\n0 100 200 300\nEpoch\" data-coord=\"top-left:(1300,623); bottom-right:(2186,1282)\" /></figure>",
            "id": 91,
            "page": 8,
            "text": "0.35 0.30 0.25 mAP 0.20 0.15 0.10 0.05 Joint Training with Epoch Adaptor Alternating Training 0.00 Joint Training with Burn-In 0 100 200 300 Epoch"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1330
                },
                {
                    "x": 2276,
                    "y": 1330
                },
                {
                    "x": 2276,
                    "y": 1462
                },
                {
                    "x": 1281,
                    "y": 1462
                }
            ],
            "category": "caption",
            "html": "<caption id='92' style='font-size:14px'>Figure 5. Performance (AP50:95) comparisons of Epoch Adaptor,<br>Alternating Training and Joint Training with Burn-In methods on<br>COCO standard 10%.</caption>",
            "id": 92,
            "page": 8,
            "text": "Figure 5. Performance (AP50:95) comparisons of Epoch Adaptor, Alternating Training and Joint Training with Burn-In methods on COCO standard 10%."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1566
                },
                {
                    "x": 1578,
                    "y": 1566
                },
                {
                    "x": 1578,
                    "y": 1615
                },
                {
                    "x": 1280,
                    "y": 1615
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:20px'>5. Conclusion</p>",
            "id": 93,
            "page": 8,
            "text": "5. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1648
                },
                {
                    "x": 2277,
                    "y": 1648
                },
                {
                    "x": 2277,
                    "y": 2149
                },
                {
                    "x": 1280,
                    "y": 2149
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:18px'>In this paper, we present Efficient Teacher, a method to<br>bridge the gap between SSOD and one-stage anchor-based<br>detectors, by building on the efficient dense input handling<br>of Dense Detector. Our approach introduces the Pseudo<br>Label Assigner to effectively utilize both reliable and<br>uncertain pseudo labels, based on an analysis of their<br>assignment in SSOD. In addition, we introduce Epoch<br>Adaptor, a training scheme that maximizes the efficiency<br>of training and utilization of both labeled and unlabeled<br>data. Our approach achieves state-of-the-art results on<br>VOC, COCO-standard, and COCO-additional datasets.</p>",
            "id": 94,
            "page": 8,
            "text": "In this paper, we present Efficient Teacher, a method to bridge the gap between SSOD and one-stage anchor-based detectors, by building on the efficient dense input handling of Dense Detector. Our approach introduces the Pseudo Label Assigner to effectively utilize both reliable and uncertain pseudo labels, based on an analysis of their assignment in SSOD. In addition, we introduce Epoch Adaptor, a training scheme that maximizes the efficiency of training and utilization of both labeled and unlabeled data. Our approach achieves state-of-the-art results on VOC, COCO-standard, and COCO-additional datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2243
                },
                {
                    "x": 1523,
                    "y": 2243
                },
                {
                    "x": 1523,
                    "y": 2292
                },
                {
                    "x": 1283,
                    "y": 2292
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:22px'>References</p>",
            "id": 95,
            "page": 8,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1298,
                    "y": 2325
                },
                {
                    "x": 2277,
                    "y": 2325
                },
                {
                    "x": 2277,
                    "y": 2972
                },
                {
                    "x": 1298,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:14px'>[1] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas<br>Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A<br>holistic approach to semi-supervised learning. Advances in<br>neural information processing systems, 32, 2019. 1, 3<br>[2] Alexey Bochkovskiy, Chien- Yao Wang, and Hong-<br>Yuan Mark Liao. Yolov4: Optimal speed and accuracy of<br>object detection. arXiv preprint arXiv:2004.10934, 2020. 1,<br>2<br>[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-<br>ing into high quality object detection. In Proceedings of the<br>IEEE conference on computer vision and pattern recogni-<br>tion, pages 6154-6162, 2018. 1<br>[4] Binbin Chen, Weijie Chen, Shicai Yang, Yunyi Xuan, Jie<br>Song, Di Xie, Shiliang Pu, Mingli Song, and Yueting</p>",
            "id": 96,
            "page": 8,
            "text": " David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019. 1, 3  Alexey Bochkovskiy, Chien- Yao Wang, and HongYuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 1, 2  Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154-6162, 2018. 1  Binbin Chen, Weijie Chen, Shicai Yang, Yunyi Xuan, Jie Song, Di Xie, Shiliang Pu, Mingli Song, and Yueting"
        },
        {
            "bounding_box": [
                {
                    "x": 1227,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3090
                },
                {
                    "x": 1227,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='97' style='font-size:16px'>8</footer>",
            "id": 97,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 310
                },
                {
                    "x": 1199,
                    "y": 310
                },
                {
                    "x": 1199,
                    "y": 487
                },
                {
                    "x": 285,
                    "y": 487
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:18px'>Zhuang. Label matching semi-supervised object detection.<br>In Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, pages 14381-14390, 2022.<br>1, 3, 6, 7, 11, 12</p>",
            "id": 98,
            "page": 9,
            "text": "Zhuang. Label matching semi-supervised object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14381-14390, 2022. 1, 3, 6, 7, 11, 12"
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 483
                },
                {
                    "x": 1199,
                    "y": 483
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 208,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:14px'>[5] Binghui Chen, Pengyu Li, Xiang Chen, Biao Wang, Lei<br>Zhang, and Xian-Sheng Hua. Dense learning based semi-<br>supervised object detection. In Proceedings of the IEEE/CVF<br>Conference on Computer Vision and Pattern Recognition,<br>pages 4815-4824, 2022. 1, 3, 7, 8<br>[6] Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, and<br>Qi Dou. Harmonizing transferability and discriminability for<br>adapting object detectors. In Proceedings of the IEEE/CVF<br>Conference on Computer Vision and Pattern Recognition,<br>pages 8869-8878, 2020. 12<br>[7] Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu,<br>Yueting Zhuang, and Wenqi Ren. Self-supervised noisy la-<br>bel learning for source-free unsupervised domain adaptation.<br>arXiv preprint arXiv:2102.11614, 2021. 3<br>[8] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and<br>Luc Van Gool. Domain adaptive faster r-cnn for object de-<br>tection in the wild. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pages 3339-3348,<br>2018. 3<br>[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo<br>Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe<br>Franke, Stefan Roth, and Bernt Schiele. The cityscapes<br>dataset for semantic urban scene understanding. In Proceed-<br>ings of the IEEE conference on computer vision and pattern<br>recognition, pages 3213-3223, 2016. 12<br>[10] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Un-<br>biased mean teacher for cross-domain object detection. In<br>Proceedings of the IEEE/CVF Conference on Computer Vi-<br>sion and Pattern Recognition, pages 4091-4101, 2021. 3,<br>12<br>[11] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han,<br>Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style<br>convnets great again. In Proceedings of the IEEE/CVF Con-<br>ference on Computer Vision and Pattern Recognition, pages<br>13733-13742, 2021. 7<br>[12] Mark Everingham and John Winn. The pascal visual object<br>classes challenge 2012 (voc2012) development kit. Pattern<br>Anal. Stat. Model. Comput. Learn., Tech. Rep, 2007:1-45,<br>2012. 6<br>[13] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain<br>adaptation by backpropagation. In International conference<br>on machine learning, pages 1180-1189. PMLR, 2015. 3, 6<br>[14] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian<br>Sun. Ota: Optimal transport assignment for object detection.<br>In Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, pages 303-312, 2021. 3<br>[15] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian<br>Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint<br>arXiv:2107.08430, 2021. 1, 2<br>[16] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we<br>ready for autonomous driving? the kitti vision benchmark<br>suite. In 2012 IEEE conference on computer vision and pat-<br>tern recognition, pages 3354-3361. IEEE, 2012. 12</p>",
            "id": 99,
            "page": 9,
            "text": " Binghui Chen, Pengyu Li, Xiang Chen, Biao Wang, Lei Zhang, and Xian-Sheng Hua. Dense learning based semisupervised object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4815-4824, 2022. 1, 3, 7, 8  Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, and Qi Dou. Harmonizing transferability and discriminability for adapting object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8869-8878, 2020. 12  Weijie Chen, Luojun Lin, Shicai Yang, Di Xie, Shiliang Pu, Yueting Zhuang, and Wenqi Ren. Self-supervised noisy label learning for source-free unsupervised domain adaptation. arXiv preprint arXiv:2102.11614, 2021. 3  Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object detection in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3339-3348, 2018. 3  Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213-3223, 2016. 12  Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Unbiased mean teacher for cross-domain object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4091-4101, 2021. 3, 12  Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13733-13742, 2021. 7  Mark Everingham and John Winn. The pascal visual object classes challenge 2012 (voc2012) development kit. Pattern Anal. Stat. Model. Comput. Learn., Tech. Rep, 2007:1-45, 2012. 6  Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180-1189. PMLR, 2015. 3, 6  Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun. Ota: Optimal transport assignment for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 303-312, 2021. 3  Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021. 1, 2  Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 3354-3361. IEEE, 2012. 12"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 306
                },
                {
                    "x": 2288,
                    "y": 306
                },
                {
                    "x": 2288,
                    "y": 2977
                },
                {
                    "x": 1277,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:14px'>[17] Mengzhe He, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li,<br>Bo Li, Weihao Gan, Wei Wu, and Yu Qiao. Cross domain<br>object detection by target-perceived dual branch distillation.<br>In Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, pages 9570-9580, 2022. 12<br>[18] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak.<br>Consistency-based semi-supervised learning for object de-<br>tection. Advances in neural information processing systems,<br>32, 2019. 3<br>[19] G Jocher, A Chaurasia, A Stoken, J Borovec, NanoCode012,<br>Y Kwon, TaoXie, J Fang, imyhxy, and K Michael. ultralyt-<br>ics/yolov5: v6. 1-tensorrt, tensorflow edge tpu and openvino<br>export and inference. Zenodo, Feb, 22, 2022. 1, 3, 4, 7<br>[20] Matthew Johnson-Roberson, Charles Barto, Rounak Mehta,<br>Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.<br>Driving in the matrix: Can virtual worlds replace human-<br>generated annotations for real world tasks? arXiv preprint<br>arXiv:1610.01983, 2016. 12<br>[21] Kang Kim and Hee Seok Lee. Probabilistic anchor assign-<br>ment with iou prediction for object detection. In European<br>Conference on Computer Vision, pages 355-371. Springer,<br>2020. 3<br>[22] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei<br>Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,<br>Weiqiang Nie, et al. Yolov6: a single-stage object detec-<br>tion framework for industrial applications. arXiv preprint<br>arXiv:2209.02976, 2022. 1<br>[23] Gang Li, Xiang Li, Yujie Wang, Shanshan Zhang, Yichao<br>Wu, and Ding Liang. Pseco: Pseudo labeling and consis-<br>tency training for semi-supervised object detection. arXiv<br>preprint arXiv:2203.16317, 2022. 1, 7<br>[24] Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan,<br>Shiliang Pu, and Yueting Zhuang. A free lunch for unsuper-<br>vised domain adaptive object detection without source data.<br>In Proceedings of the AAAI Conference on Artificial Intelli-<br>gence, volume 35, pages 8474-8481, 2021. 3<br>[25] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and<br>Piotr Dollar. Focal loss for dense object detection. In Pro-<br>ceedings of the IEEE international conference on computer<br>vision, pages 2980-2988, 2017. 1, 2, 3<br>[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,<br>Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence<br>Zitnick. Microsoft COCO: Common objects in context. In<br>European conference on computer vision, pages 740-755.<br>Springer, 2014. 6<br>[27] Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo,<br>Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and Peter<br>Vajda. Unbiased teacher for semi-supervised object detec-<br>tion. arXiv preprint arXiv:2102.09480, 2021. 1, 2, 3, 4, 5, 6,<br>7, 8, 11<br>[28] Yen-Cheng Liu, Chih-Yao Ma, and Zsolt Kira. Unbiased<br>teacher v2: Semi-supervised object detection for anchor-free<br>and anchor-based detectors. In Proceedings ofthe IEEE/CVF<br>Conference on Computer Vision and Pattern Recognition,<br>pages 9819-9828, 2022. 3, 7<br>[29] Rindra Ramamonjison, Amin Banitalebi-Dehkordi, Xinyu<br>Kang, Xiaolong Bai, and Yong Zhang. Simrod: A simple</p>",
            "id": 100,
            "page": 9,
            "text": " Mengzhe He, Yali Wang, Jiaxi Wu, Yiru Wang, Hanqing Li, Bo Li, Weihao Gan, Wei Wu, and Yu Qiao. Cross domain object detection by target-perceived dual branch distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9570-9580, 2022. 12  Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object detection. Advances in neural information processing systems, 32, 2019. 3  G Jocher, A Chaurasia, A Stoken, J Borovec, NanoCode012, Y Kwon, TaoXie, J Fang, imyhxy, and K Michael. ultralytics/yolov5: v6. 1-tensorrt, tensorflow edge tpu and openvino export and inference. Zenodo, Feb, 22, 2022. 1, 3, 4, 7  Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan. Driving in the matrix: Can virtual worlds replace humangenerated annotations for real world tasks? arXiv preprint arXiv:1610.01983, 2016. 12  Kang Kim and Hee Seok Lee. Probabilistic anchor assignment with iou prediction for object detection. In European Conference on Computer Vision, pages 355-371. Springer, 2020. 3  Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie,  Yolov6: a single-stage object detection framework for industrial applications. arXiv preprint arXiv:2209.02976, 2022. 1  Gang Li, Xiang Li, Yujie Wang, Shanshan Zhang, Yichao Wu, and Ding Liang. Pseco: Pseudo labeling and consistency training for semi-supervised object detection. arXiv preprint arXiv:2203.16317, 2022. 1, 7  Xianfeng Li, Weijie Chen, Di Xie, Shicai Yang, Peng Yuan, Shiliang Pu, and Yueting Zhuang. A free lunch for unsupervised domain adaptive object detection without source data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8474-8481, 2021. 3  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017. 1, 2, 3  Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 6  Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. arXiv preprint arXiv:2102.09480, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 11  Yen-Cheng Liu, Chih-Yao Ma, and Zsolt Kira. Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9819-9828, 2022. 3, 7  Rindra Ramamonjison, Amin Banitalebi-Dehkordi, Xinyu Kang, Xiaolong Bai, and Yong Zhang. Simrod: A simple"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3056
                },
                {
                    "x": 1253,
                    "y": 3056
                },
                {
                    "x": 1253,
                    "y": 3089
                },
                {
                    "x": 1225,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='101' style='font-size:14px'>9</footer>",
            "id": 101,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 313
                },
                {
                    "x": 1198,
                    "y": 313
                },
                {
                    "x": 1198,
                    "y": 441
                },
                {
                    "x": 287,
                    "y": 441
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:14px'>adaptation method for robust object detection. In Proceed-<br>ings of the IEEE/CVF International Conference on Com-<br>puter Vision, pages 3570-3579, 2021. 12</p>",
            "id": 102,
            "page": 10,
            "text": "adaptation method for robust object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3570-3579, 2021. 12"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 436
                },
                {
                    "x": 1200,
                    "y": 436
                },
                {
                    "x": 1200,
                    "y": 2971
                },
                {
                    "x": 201,
                    "y": 2971
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:16px'>[30] Joseph Redmon and Ali Farhadi. Yolov3: An incremental<br>improvement. arXiv preprint arXiv:1804.02767, 2018. 1<br>[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.<br>Faster r-cnn: Towards real-time object detection with region<br>proposal networks. Advances in neural information process-<br>ing systems, 28, 2015. 1, 3<br>[32] Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate<br>Saenko. Strong-weak distribution alignment for adaptive ob-<br>ject detection. In Proceedings of the IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, pages 6956-<br>6965, 2019. 12<br>[33] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.<br>Regularization with stochastic transformations and perturba-<br>tions for deep semi-supervised learning. Advances in neural<br>information processing systems, 29, 2016. 3<br>[34] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Seman-<br>tic foggy scene understanding with synthetic data. Interna-<br>tional Journal of Computer Vision, 126(9):973-992, 2018.<br>12<br>[35] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao<br>Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,<br>Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying<br>semi-supervised learning with consistency and confidence.<br>Advances in neural information processing systems, 33:596-<br>608, 2020. 1, 3<br>[36] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang,<br>Chen- Yu Lee, and Tomas Pfister. A simple semi-supervised<br>learning framework for object detection. arXiv preprint<br>arXiv:2005.04757, 2020. 3, 7<br>[37] Peng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, and Caim-<br>ing Xiong. Proposal learning for semi-supervised object de-<br>tection. In Proceedings of the IEEE/CVF Winter Confer-<br>ence on Applications of Computer Vision, pages 2291-2301,<br>2021. 3<br>[38] Yihe Tang, Weifeng Chen, Yijun Luo, and Yuting Zhang.<br>Humble teachers teach better students for semi-supervised<br>object detection. In Proceedings of the IEEE/CVF Confer-<br>ence on Computer Vision and Pattern Recognition, pages<br>3132-3141, 2021. 7<br>[39] Antti Tarvainen and Harri Valpola. Mean teachers are better<br>role models: Weight-averaged consistency targets improve<br>semi-supervised deep learning results. Advances in neural<br>information processing systems, 30, 2017. 3<br>[40] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:<br>Fully convolutional one-stage object detection. In Proceed-<br>ings of the IEEE/CVF international conference on computer<br>vision, pages 9627-9636, 2019. 1, 3<br>[41] Vibashan Vs, Vikram Gupta, Poojan Oza, Vishwanath A<br>Sindagi, and Vishal M Patel. Mega-cda: Memory guided<br>attention for category-aware unsupervised domain adaptive<br>object detection. In Proceedings of the IEEE/CVF Confer-<br>ence on Computer Vision and Pattern Recognition, pages<br>4516-4526, 2021. 3, 12<br>[42] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-<br>Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets</p>",
            "id": 103,
            "page": 10,
            "text": " Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. 1  Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. 1, 3  Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Strong-weak distribution alignment for adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69566965, 2019. 12  Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. Advances in neural information processing systems, 29, 2016. 3  Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Semantic foggy scene understanding with synthetic data. International Journal of Computer Vision, 126(9):973-992, 2018. 12  Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596608, 2020. 1, 3  Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen- Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020. 3, 7  Peng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, and Caiming Xiong. Proposal learning for semi-supervised object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2291-2301, 2021. 3  Yihe Tang, Weifeng Chen, Yijun Luo, and Yuting Zhang. Humble teachers teach better students for semi-supervised object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3132-3141, 2021. 7  Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017. 3  Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627-9636, 2019. 1, 3  Vibashan Vs, Vikram Gupta, Poojan Oza, Vishwanath A Sindagi, and Vishal M Patel. Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4516-4526, 2021. 3, 12  Chien-Yao Wang, Alexey Bochkovskiy, and HongYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets"
        },
        {
            "bounding_box": [
                {
                    "x": 1364,
                    "y": 312
                },
                {
                    "x": 2276,
                    "y": 312
                },
                {
                    "x": 2276,
                    "y": 395
                },
                {
                    "x": 1364,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='104' style='font-size:14px'>new state-of-the-art for real-time object detectors. arXiv<br>preprint arXiv:2207.02696, 2022. 1, 2, 3</p>",
            "id": 104,
            "page": 10,
            "text": "new state-of-the-art for real-time object detectors. arXiv preprint arXiv:2207.02696, 2022. 1, 2, 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 400
                },
                {
                    "x": 2281,
                    "y": 400
                },
                {
                    "x": 2281,
                    "y": 2399
                },
                {
                    "x": 1279,
                    "y": 2399
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:18px'>[43] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V<br>Le. Self-training with noisy student improves imagenet<br>classification. In Proceedings of the IEEE/CVF conference<br>on computer vision and pattern recognition, pages 10687-<br>10698, 2020. 3<br>[44] Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wen-<br>jun Zhang. Cross-domain detection via graph-induced proto-<br>type alignment. In Proceedings ofthe IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, pages 12355-<br>12364, 2020. 12<br>[45] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan<br>Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-<br>end semi-supervised object detection with soft teacher. In<br>Proceedings of the IEEE/CVF International Conference on<br>Computer Vision, pages 3060-3069, 2021. 1, 3, 5, 6, 7<br>[46] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and<br>Stan Z Li. Bridging the gap between anchor-based and<br>anchor-free detection via adaptive training sample selection.<br>In Proceedings of the IEEE/CVF conference on computer vi-<br>sion and pattern recognition, pages 9759-9768, 2020. 1, 3<br>[47] Yueming Zhang, Xingxu Yao, Chao Liu, Feng Chen, Xiaolin<br>Song, Tengfei Xing, Runbo Hu, Hua Chai, Pengfei Xu, and<br>Guoshan Zhang. S4od: Semi-supervised learning for single-<br>stage object detection. arXiv preprint arXiv:2204.04492,<br>2022. 1<br>[48] Zhaohui Zheng, Ping Wang, Dongwei Ren, Wei Liu, Rong-<br>guang Ye, Qinghua Hu, and Wangmeng Zuo. Enhancing ge-<br>ometric factors in model learning and inference for object<br>detection and instance segmentation. IEEE Transactions on<br>Cybernetics, 2021. 4<br>[49] Hongyu Zhou, Zheng Ge, Songtao Liu, Weixin Mao, Zeming<br>Li, Haiyan Yu, and Jian Sun. Dense teacher: Dense pseudo-<br>labels for semi-supervised object detection. arXiv preprint<br>arXiv:2207.02541, 2022. 1, 7<br>[50] Qiang Zhou, Chaohui Yu, Zhibin Wang, Qi Qian, and Hao<br>Li. Instant-teaching: An end-to-end semi-supervised object<br>detection framework. In Proceedings of the IEEE/CVF Con-<br>ference on Computer Vision and Pattern Recognition, pages<br>4081-4090, 2021. 1, 3, 7<br>[51] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,<br>Songtao Liu, Zeming Li, and Jian Sun. Autoassign: Differ-<br>entiable label assignment for dense object detection. arXiv<br>preprint arXiv:2007.03496, 2020. 3</p>",
            "id": 105,
            "page": 10,
            "text": " Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068710698, 2020. 3  Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wenjun Zhang. Cross-domain detection via graph-induced prototype alignment. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1235512364, 2020. 12  Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-toend semi-supervised object detection with soft teacher. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3060-3069, 2021. 1, 3, 5, 6, 7  Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759-9768, 2020. 1, 3  Yueming Zhang, Xingxu Yao, Chao Liu, Feng Chen, Xiaolin Song, Tengfei Xing, Runbo Hu, Hua Chai, Pengfei Xu, and Guoshan Zhang. S4od: Semi-supervised learning for singlestage object detection. arXiv preprint arXiv:2204.04492, 2022. 1  Zhaohui Zheng, Ping Wang, Dongwei Ren, Wei Liu, Rongguang Ye, Qinghua Hu, and Wangmeng Zuo. Enhancing geometric factors in model learning and inference for object detection and instance segmentation. IEEE Transactions on Cybernetics, 2021. 4  Hongyu Zhou, Zheng Ge, Songtao Liu, Weixin Mao, Zeming Li, Haiyan Yu, and Jian Sun. Dense teacher: Dense pseudolabels for semi-supervised object detection. arXiv preprint arXiv:2207.02541, 2022. 1, 7  Qiang Zhou, Chaohui Yu, Zhibin Wang, Qi Qian, and Hao Li. Instant-teaching: An end-to-end semi-supervised object detection framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4081-4090, 2021. 1, 3, 7  Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong, Songtao Liu, Zeming Li, and Jian Sun. Autoassign: Differentiable label assignment for dense object detection. arXiv preprint arXiv:2007.03496, 2020. 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2444
                },
                {
                    "x": 2088,
                    "y": 2444
                },
                {
                    "x": 2088,
                    "y": 2495
                },
                {
                    "x": 1282,
                    "y": 2495
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:22px'>A. More Analysis of Efficient Teacher</p>",
            "id": 106,
            "page": 10,
            "text": "A. More Analysis of Efficient Teacher"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2528
                },
                {
                    "x": 2276,
                    "y": 2528
                },
                {
                    "x": 2276,
                    "y": 2824
                },
                {
                    "x": 1280,
                    "y": 2824
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:20px'>In this section, we aim to identify real gain of Efficient<br>Teacher and make an in-depth study of it. Efficient Teacher<br>is conceptually simple: it uses Dense Detector to generate<br>high quality pseudo labels, Pseudo Label Assigner to allevi-<br>ate pseudo label inconsistency problem and Epoch Adaptor<br>to count the number of labels per image.</p>",
            "id": 107,
            "page": 10,
            "text": "In this section, we aim to identify real gain of Efficient Teacher and make an in-depth study of it. Efficient Teacher is conceptually simple: it uses Dense Detector to generate high quality pseudo labels, Pseudo Label Assigner to alleviate pseudo label inconsistency problem and Epoch Adaptor to count the number of labels per image."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2830
                },
                {
                    "x": 2277,
                    "y": 2830
                },
                {
                    "x": 2277,
                    "y": 2973
                },
                {
                    "x": 1281,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:16px'>The backbone of Dense Detector in Efficient Teacher<br>adopts CSPNet and PAN and all experiments are on one<br>of five folds COCO standard 10% dataset.</p>",
            "id": 108,
            "page": 10,
            "text": "The backbone of Dense Detector in Efficient Teacher adopts CSPNet and PAN and all experiments are on one of five folds COCO standard 10% dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 1220,
                    "y": 3056
                },
                {
                    "x": 1263,
                    "y": 3056
                },
                {
                    "x": 1263,
                    "y": 3091
                },
                {
                    "x": 1220,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='109' style='font-size:18px'>10</footer>",
            "id": 109,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 306
                },
                {
                    "x": 824,
                    "y": 306
                },
                {
                    "x": 824,
                    "y": 352
                },
                {
                    "x": 205,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:20px'>A.1. Analysis of Dense Detector</p>",
            "id": 110,
            "page": 11,
            "text": "A.1. Analysis of Dense Detector"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 500
                },
                {
                    "x": 1113,
                    "y": 500
                },
                {
                    "x": 1113,
                    "y": 1164
                },
                {
                    "x": 219,
                    "y": 1164
                }
            ],
            "category": "figure",
            "html": "<figure><img id='111' style='font-size:14px' alt=\"0.30\n0.25\n0.20\nmAP 0.15\n0.10\nw/o Mosaic & w/o multi positive sample\n0.05\nw/ Mosaic & w/o multi positive sample\nw/o Mosaic & w/ multi positive sample\n0.00 w/ Mosaic & W/ multi positive sample\n0 100 200 300\nEpoch\" data-coord=\"top-left:(219,500); bottom-right:(1113,1164)\" /></figure>",
            "id": 111,
            "page": 11,
            "text": "0.30 0.25 0.20 mAP 0.15 0.10 w/o Mosaic & w/o multi positive sample 0.05 w/ Mosaic & w/o multi positive sample w/o Mosaic & w/ multi positive sample 0.00 w/ Mosaic & W/ multi positive sample 0 100 200 300 Epoch"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1212
                },
                {
                    "x": 1196,
                    "y": 1212
                },
                {
                    "x": 1196,
                    "y": 1301
                },
                {
                    "x": 204,
                    "y": 1301
                }
            ],
            "category": "caption",
            "html": "<caption id='112' style='font-size:16px'>Figure 6. comparition between performance of different input and<br>sample strategies, the result shows stable rise with dense inputs.</caption>",
            "id": 112,
            "page": 11,
            "text": "Figure 6. comparition between performance of different input and sample strategies, the result shows stable rise with dense inputs."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1362
                },
                {
                    "x": 1199,
                    "y": 1362
                },
                {
                    "x": 1199,
                    "y": 2309
                },
                {
                    "x": 201,
                    "y": 2309
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>As we explain in Section 3.1, the dense inputs of Dense<br>Detector is an engineering technique to balances the im-<br>portance of positive/negative examples. The dense inputs<br>consists of two parts: dense image inputs and dense sam-<br>ple inputs, we design the ablation experiments about dense<br>image inputs(Mosaic augmentation) and dense sample in-<br>puts(multi positive sample) in Dense Detector. The Mosaic<br>augmentation means patch four images into one and multi<br>positive sample extend label assignment sample to adjacent<br>points on feature map. We report the supervised training<br>performance of Dense Detector, in which the higher mAP<br>corresponds the better pseudo label quality in SSOD. It is<br>revealed in Figure 6 that dense image inputs(Mosaic aug-<br>mentation) shows massive improvement on mAP from 23.1<br>to 28.9 and dense sample inputs(multi positive sample) im-<br>proves the mAP from 23.1 to 26.9, further more, with both<br>two techniques the mAP can be boosted from 23.1 to 30.5.<br>The dense inputs of Dense Detector has a positive effect on<br>the efficiency and accuracy of Efficient Teacher.</p>",
            "id": 113,
            "page": 11,
            "text": "As we explain in Section 3.1, the dense inputs of Dense Detector is an engineering technique to balances the importance of positive/negative examples. The dense inputs consists of two parts: dense image inputs and dense sample inputs, we design the ablation experiments about dense image inputs(Mosaic augmentation) and dense sample inputs(multi positive sample) in Dense Detector. The Mosaic augmentation means patch four images into one and multi positive sample extend label assignment sample to adjacent points on feature map. We report the supervised training performance of Dense Detector, in which the higher mAP corresponds the better pseudo label quality in SSOD. It is revealed in Figure 6 that dense image inputs(Mosaic augmentation) shows massive improvement on mAP from 23.1 to 28.9 and dense sample inputs(multi positive sample) improves the mAP from 23.1 to 26.9, further more, with both two techniques the mAP can be boosted from 23.1 to 30.5. The dense inputs of Dense Detector has a positive effect on the efficiency and accuracy of Efficient Teacher."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2350
                },
                {
                    "x": 988,
                    "y": 2350
                },
                {
                    "x": 988,
                    "y": 2399
                },
                {
                    "x": 205,
                    "y": 2399
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:20px'>A.2. Analysis of Pseudo Labels Assigner</p>",
            "id": 114,
            "page": 11,
            "text": "A.2. Analysis of Pseudo Labels Assigner"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2428
                },
                {
                    "x": 1200,
                    "y": 2428
                },
                {
                    "x": 1200,
                    "y": 2978
                },
                {
                    "x": 202,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:18px'>The Pseudo Label Assigner divides pseudo labels into<br>two types: reliable and uncertain ones. In this part, we<br>present more analysis about ratio of true positive and false<br>positive in pseudo labels. Figure 7 shows the average statis-<br>tics of reliable and uncertain pseudo labels in one epoch,<br>the True Positive means the pseudo labels have the same<br>class as ground truth and an IoU overlap greater than 0.5,<br>the Loc False Positive means IoU overlap with ground truth<br>less than 0.5 and Cls False Positive indicates pseudo labels<br>is misclassified. What can be clearly seen in figures is 76%<br>of reliable pseudo labels are True Positive while only 24%</p>",
            "id": 115,
            "page": 11,
            "text": "The Pseudo Label Assigner divides pseudo labels into two types: reliable and uncertain ones. In this part, we present more analysis about ratio of true positive and false positive in pseudo labels. Figure 7 shows the average statistics of reliable and uncertain pseudo labels in one epoch, the True Positive means the pseudo labels have the same class as ground truth and an IoU overlap greater than 0.5, the Loc False Positive means IoU overlap with ground truth less than 0.5 and Cls False Positive indicates pseudo labels is misclassified. What can be clearly seen in figures is 76% of reliable pseudo labels are True Positive while only 24%"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 308
                },
                {
                    "x": 2277,
                    "y": 308
                },
                {
                    "x": 2277,
                    "y": 557
                },
                {
                    "x": 1279,
                    "y": 557
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:18px'>of uncertain pseudo labels comply with the requirements.<br>The misclassified pseudo labels are only small percentage<br>of both reliable and uncertain pseudo labels, meanwhile, the<br>weight of poor location in uncertain pseudo labels reaches<br>70% which motivated us to design Lreg in Section 3.2.</p>",
            "id": 116,
            "page": 11,
            "text": "of uncertain pseudo labels comply with the requirements. The misclassified pseudo labels are only small percentage of both reliable and uncertain pseudo labels, meanwhile, the weight of poor location in uncertain pseudo labels reaches 70% which motivated us to design Lreg in Section 3.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1406,
                    "y": 615
                },
                {
                    "x": 2147,
                    "y": 615
                },
                {
                    "x": 2147,
                    "y": 1710
                },
                {
                    "x": 1406,
                    "y": 1710
                }
            ],
            "category": "figure",
            "html": "<figure><img id='117' style='font-size:14px' alt=\"Cls False Positive\nLoc False Positive\n5%\n19%\n76%\nTrue Positive\n(a) Reliable pseudo labels.\nLoc False Positive\n70%\n24%\nTrue Positive 6%\nCls False Positive\n(b) Uncertain pseudo labels.\" data-coord=\"top-left:(1406,615); bottom-right:(2147,1710)\" /></figure>",
            "id": 117,
            "page": 11,
            "text": "Cls False Positive Loc False Positive 5% 19% 76% True Positive (a) Reliable pseudo labels. Loc False Positive 70% 24% True Positive 6% Cls False Positive (b) Uncertain pseudo labels."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1739
                },
                {
                    "x": 2273,
                    "y": 1739
                },
                {
                    "x": 2273,
                    "y": 1827
                },
                {
                    "x": 1281,
                    "y": 1827
                }
            ],
            "category": "caption",
            "html": "<caption id='118' style='font-size:16px'>Figure 7. Weight of true and false positive compared across reli-<br>able and uncertain pseudo labels in Efficient Teacher.</caption>",
            "id": 118,
            "page": 11,
            "text": "Figure 7. Weight of true and false positive compared across reliable and uncertain pseudo labels in Efficient Teacher."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1913
                },
                {
                    "x": 1905,
                    "y": 1913
                },
                {
                    "x": 1905,
                    "y": 1962
                },
                {
                    "x": 1282,
                    "y": 1962
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:22px'>A.3. Analysis of Epoch Adaptor</p>",
            "id": 119,
            "page": 11,
            "text": "A.3. Analysis of Epoch Adaptor"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1994
                },
                {
                    "x": 2278,
                    "y": 1994
                },
                {
                    "x": 2278,
                    "y": 2739
                },
                {
                    "x": 1277,
                    "y": 2739
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:18px'>We propose Epoch Adaptor to calculate the distribution<br>of ground truth in each epoch due to Mosaic augmentation.<br>We perform the qualitative comparisons between ground<br>truth distribution of the proposed Efficient Teacher and Un-<br>biased Teacher [27] method. The contents of images and<br>instance distribution are shown in Figure 8, in which Unbi-<br>ased Teacher has monotonous and large instance while Ef-<br>ficient Teacher has various and small instance. Moreover,<br>the Mosaic augmentation is executed online which disabled<br>the offline label number calculation in LabelMatch [4]. As<br>discussed in Section 3.3, we observe the pseudo label and<br>ground truth in Efficient Teacher and implement a distribu-<br>tion adaptation method to dynamically calculate number of<br>ground truth per image which determines the T1 and T2 in<br>Pseudo Label Assigner. Details are shown in Figure 9.</p>",
            "id": 120,
            "page": 11,
            "text": "We propose Epoch Adaptor to calculate the distribution of ground truth in each epoch due to Mosaic augmentation. We perform the qualitative comparisons between ground truth distribution of the proposed Efficient Teacher and Unbiased Teacher  method. The contents of images and instance distribution are shown in Figure 8, in which Unbiased Teacher has monotonous and large instance while Efficient Teacher has various and small instance. Moreover, the Mosaic augmentation is executed online which disabled the offline label number calculation in LabelMatch . As discussed in Section 3.3, we observe the pseudo label and ground truth in Efficient Teacher and implement a distribution adaptation method to dynamically calculate number of ground truth per image which determines the T1 and T2 in Pseudo Label Assigner. Details are shown in Figure 9."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2790
                },
                {
                    "x": 1932,
                    "y": 2790
                },
                {
                    "x": 1932,
                    "y": 2845
                },
                {
                    "x": 1282,
                    "y": 2845
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:22px'>B. Experiments on CityScapes</p>",
            "id": 121,
            "page": 11,
            "text": "B. Experiments on CityScapes"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2877
                },
                {
                    "x": 2276,
                    "y": 2877
                },
                {
                    "x": 2276,
                    "y": 2975
                },
                {
                    "x": 1281,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>To explore the flexibility of Efficient Teacher, we ex-<br>tend it to the scenario of domain adaptive object detection</p>",
            "id": 122,
            "page": 11,
            "text": "To explore the flexibility of Efficient Teacher, we extend it to the scenario of domain adaptive object detection"
        },
        {
            "bounding_box": [
                {
                    "x": 1219,
                    "y": 3056
                },
                {
                    "x": 1258,
                    "y": 3056
                },
                {
                    "x": 1258,
                    "y": 3092
                },
                {
                    "x": 1219,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='123' style='font-size:16px'>11</footer>",
            "id": 123,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 290
                },
                {
                    "x": 1198,
                    "y": 290
                },
                {
                    "x": 1198,
                    "y": 1076
                },
                {
                    "x": 203,
                    "y": 1076
                }
            ],
            "category": "figure",
            "html": "<figure><img id='124' style='font-size:14px' alt=\"person 0.30.31.0)rson 0.80.81.0\n\ntie 0.5.0.5.1.0 OUOO\ntraffic light\ntraffic light\ntitrattic light\n0203 1 numbrella 0 --- umbrella\numbrella umbumbumbrumbr\numbrella 03 0309 0.60; in n person 0.3 ( umbrel\nperson 0.3 ■persor umbrellaturymoreunumbrella\n'uir brella\npers ' ibriperson 0.30.30.9 0.8)n 0. 3 U.3 1.0J persor numbrella\nSTACGER n person|handbag 이\nperson ' / ' 10 98 7 1 Imerson 0 n O\npersor\naperson\nOhandba handbag hackpa\nperson 0.5 0. 5 1.0 pe upersipersoperson person\nhandbag\nsurfboard\ntv0.91.01.0\ncouc couc\ntrain 0.8 0 9 1.0.9 1.0.90.91.0 ain train\nAPP \nJG 궁 G\nairplane 0. 9 0.9 1.0\nperson (person 0.50\nairplane\ntraffic light\nwww.urumbrellace.com ymbul umun'\npellhandbagoby mbuur\naimlane\nps\ntruck 0.6 0 Pperson (perso airplane ehan\nhandbag\" data-coord=\"top-left:(203,290); bottom-right:(1198,1076)\" /></figure>",
            "id": 124,
            "page": 12,
            "text": "person 0.30.31.0)rson 0.80.81.0  tie 0.5.0.5.1.0 OUOO traffic light traffic light titrattic light 0203 1 numbrella 0 --- umbrella umbrella umbumbumbrumbr umbrella 03 0309 0.60; in n person 0.3 ( umbrel person 0.3 ■persor umbrellaturymoreunumbrella 'uir brella pers \" ibriperson 0.30.30.9 0.8)n 0. 3 U.3 1.0J persor numbrella STACGER n person|handbag 이 person \" / \" 10 98 7 1 Imerson 0 n O persor aperson Ohandba handbag hackpa person 0.5 0. 5 1.0 pe upersipersoperson person handbag surfboard tv0.91.01.0 couc couc train 0.8 0 9 1.0.9 1.0.90.91.0 ain train APP  JG 궁 G airplane 0. 9 0.9 1.0 person (person 0.50 airplane traffic light www.urumbrellace.com ymbul umun' pellhandbagoby mbuur aimlane ps truck 0.6 0 Pperson (perso airplane ehan handbag"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1111
                },
                {
                    "x": 1198,
                    "y": 1111
                },
                {
                    "x": 1198,
                    "y": 1385
                },
                {
                    "x": 202,
                    "y": 1385
                }
            ],
            "category": "caption",
            "html": "<caption id='125' style='font-size:18px'>Figure 8. The upper section displays the pseudo labels and ground<br>truth obtained using the Unbiased Teacher method. The lower sec-<br>tion presents figures that have been augmented with Mosaic in the<br>Efficient Teacher method. In these figures, the three floating point<br>numbers that appear with the class name represent the pseudo label<br>score, objectness score, and classification score, respectively.</caption>",
            "id": 125,
            "page": 12,
            "text": "Figure 8. The upper section displays the pseudo labels and ground truth obtained using the Unbiased Teacher method. The lower section presents figures that have been augmented with Mosaic in the Efficient Teacher method. In these figures, the three floating point numbers that appear with the class name represent the pseudo label score, objectness score, and classification score, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 1286,
                    "y": 1164
                },
                {
                    "x": 2272,
                    "y": 1164
                },
                {
                    "x": 2272,
                    "y": 1250
                },
                {
                    "x": 1286,
                    "y": 1250
                }
            ],
            "category": "caption",
            "html": "<br><caption id='126' style='font-size:18px'>Table 8. Results of adaptation from normal to foggy weathers.<br>\"Source only\" refers to the model trained by labeled source data.</caption>",
            "id": 126,
            "page": 12,
            "text": "Table 8. Results of adaptation from normal to foggy weathers. \"Source only\" refers to the model trained by labeled source data."
        },
        {
            "bounding_box": [
                {
                    "x": 1288,
                    "y": 297
                },
                {
                    "x": 2274,
                    "y": 297
                },
                {
                    "x": 2274,
                    "y": 455
                },
                {
                    "x": 1288,
                    "y": 455
                }
            ],
            "category": "table",
            "html": "<br><table id='127' style='font-size:16px'><tr><td>Data Split</td><td>Normal→Foggy</td><td>KITTI→CityScapes</td><td>Sim10K→CityScapes</td></tr><tr><td>labeled data</td><td>Cityscapes (train)</td><td>KITTI</td><td>Sim10K</td></tr><tr><td>unlabeled data</td><td>Cityscapes-foggy (train)</td><td>Cityscapes (train)</td><td>Cityscapes (train)</td></tr><tr><td>test data</td><td>Cityscapes-foggy (val)</td><td>Cityscapes (val)</td><td>Cityscapes (val)</td></tr></table>",
            "id": 127,
            "page": 12,
            "text": "Data Split Normal→Foggy KITTI→CityScapes Sim10K→CityScapes  labeled data Cityscapes (train) KITTI Sim10K  unlabeled data Cityscapes-foggy (train) Cityscapes (train) Cityscapes (train)  test data Cityscapes-foggy (val) Cityscapes (val)"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 496
                },
                {
                    "x": 2275,
                    "y": 496
                },
                {
                    "x": 2275,
                    "y": 632
                },
                {
                    "x": 1283,
                    "y": 632
                }
            ],
            "category": "caption",
            "html": "<caption id='128' style='font-size:20px'>Table 7. Three differnt domain shifts in DAOD, which are<br>contructed by four different datasets, including Cityscapes [9],<br>Cityscapes-foggy [34], KITTI [16] and Sim10k [20].</caption>",
            "id": 128,
            "page": 12,
            "text": "Table 7. Three differnt domain shifts in DAOD, which are contructed by four different datasets, including Cityscapes , Cityscapes-foggy , KITTI  and Sim10k ."
        },
        {
            "bounding_box": [
                {
                    "x": 1287,
                    "y": 657
                },
                {
                    "x": 2273,
                    "y": 657
                },
                {
                    "x": 2273,
                    "y": 1124
                },
                {
                    "x": 1287,
                    "y": 1124
                }
            ],
            "category": "table",
            "html": "<table id='129' style='font-size:16px'><tr><td>Method</td><td>truck</td><td>car</td><td>rider</td><td>person</td><td>train</td><td>motor</td><td>bicycle</td><td>snq</td><td>mean</td></tr><tr><td>Source only</td><td>19.2</td><td>47.9</td><td>40.8</td><td>34.8</td><td>7.8</td><td>24.2</td><td>36.0</td><td>36.4</td><td>30.9</td></tr><tr><td>CVPR2020:GPA [44]</td><td>24.7</td><td>54.1</td><td>46.7</td><td>32.9</td><td>41.1</td><td>32.4</td><td>38.7</td><td>45.7</td><td>39.5</td></tr><tr><td>CVPR2020:HTCN [6]</td><td>31.6</td><td>47.9</td><td>47.5</td><td>33.2</td><td>40.9</td><td>32.3</td><td>37.1</td><td>47.4</td><td>39.8</td></tr><tr><td>CVPR2021:MeGA [41]</td><td>25.4</td><td>52.4</td><td>49.0</td><td>37.7</td><td>46.9</td><td>34.5</td><td>39.0</td><td>49.2</td><td>41.8</td></tr><tr><td>CVPR2021:UMT [10]</td><td>34.1</td><td>48.6</td><td>46.7</td><td>33.0</td><td>46.8</td><td>30.4</td><td>37.3</td><td>56.5</td><td>41.7</td></tr><tr><td>CVPR2022:TDD [17]</td><td>35.1</td><td>68.2</td><td>53.7</td><td>50.7</td><td>45.1</td><td>38.9</td><td>49.1</td><td>53.0</td><td>49.2</td></tr><tr><td>CVPR2022:LabelMatch [4]</td><td>42.0</td><td>62.2</td><td>55.4</td><td>45.3</td><td>55.1</td><td>43.5</td><td>51.5</td><td>64.1</td><td>52.4</td></tr><tr><td>Source only</td><td>24.2</td><td>51.9</td><td>49.0</td><td>46.5</td><td>8.5</td><td>30.3</td><td>39.2</td><td>37.2</td><td>35.9</td></tr><tr><td>Efficient Teacher(Ours)</td><td>39.2</td><td>72.9</td><td>59.2</td><td>58.1</td><td>53.0</td><td>46.4</td><td>54.8</td><td>59</td><td>55.3</td></tr></table>",
            "id": 129,
            "page": 12,
            "text": "Method truck car rider person train motor bicycle snq mean  Source only 19.2 47.9 40.8 34.8 7.8 24.2 36.0 36.4 30.9  CVPR2020:GPA  24.7 54.1 46.7 32.9 41.1 32.4 38.7 45.7 39.5  CVPR2020:HTCN  31.6 47.9 47.5 33.2 40.9 32.3 37.1 47.4 39.8  CVPR2021:MeGA  25.4 52.4 49.0 37.7 46.9 34.5 39.0 49.2 41.8  CVPR2021:UMT  34.1 48.6 46.7 33.0 46.8 30.4 37.3 56.5 41.7  CVPR2022:TDD  35.1 68.2 53.7 50.7 45.1 38.9 49.1 53.0 49.2  CVPR2022:LabelMatch  42.0 62.2 55.4 45.3 55.1 43.5 51.5 64.1 52.4  Source only 24.2 51.9 49.0 46.5 8.5 30.3 39.2 37.2 35.9  Efficient Teacher(Ours) 39.2 72.9 59.2 58.1 53.0 46.4 54.8 59"
        },
        {
            "bounding_box": [
                {
                    "x": 255,
                    "y": 1519
                },
                {
                    "x": 1107,
                    "y": 1519
                },
                {
                    "x": 1107,
                    "y": 2170
                },
                {
                    "x": 255,
                    "y": 2170
                }
            ],
            "category": "figure",
            "html": "<figure><img id='130' style='font-size:14px' alt=\"60 58.2\nUnbias Teacher\nEfficeint Teacher\n50\ntruth/image\n40\nground\n30\nof\n21.9 21.7 21.2 21.4\n20.3\nnumber\n20\n10\n7.28 7.25 7.15 7.21 7.04\n2.51\n0\nCOCO 1% COCO 2% COCO 5% COCO 10% CityScapes VOC12\nDataset\" data-coord=\"top-left:(255,1519); bottom-right:(1107,2170)\" /></figure>",
            "id": 130,
            "page": 12,
            "text": "60 58.2 Unbias Teacher Efficeint Teacher 50 truth/image 40 ground 30 of 21.9 21.7 21.2 21.4 20.3 number 20 10 7.28 7.25 7.15 7.21 7.04 2.51 0 COCO 1% COCO 2% COCO 5% COCO 10% CityScapes VOC12 Dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2221
                },
                {
                    "x": 1197,
                    "y": 2221
                },
                {
                    "x": 1197,
                    "y": 2403
                },
                {
                    "x": 202,
                    "y": 2403
                }
            ],
            "category": "caption",
            "html": "<caption id='131' style='font-size:20px'>Figure 9. Number of annotation and labels after Mosaic per im-<br>age among different datasets. When applying Mosaic technique,<br>the number of labels becomes nearly three times the number of<br>annotations, which disrupts the original label distribution.</caption>",
            "id": 131,
            "page": 12,
            "text": "Figure 9. Number of annotation and labels after Mosaic per image among different datasets. When applying Mosaic technique, the number of labels becomes nearly three times the number of annotations, which disrupts the original label distribution."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2499
                },
                {
                    "x": 1198,
                    "y": 2499
                },
                {
                    "x": 1198,
                    "y": 2694
                },
                {
                    "x": 202,
                    "y": 2694
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:22px'>(DAOD) [6, 17] follow the LabelMatch [4]. The experi-<br>ments are mainly to demonstrate the generality of Efficient<br>Teacher framework, which simply treats the source data as<br>labeled data and the target data as unlabeled data.</p>",
            "id": 132,
            "page": 12,
            "text": "(DAOD)  follow the LabelMatch . The experiments are mainly to demonstrate the generality of Efficient Teacher framework, which simply treats the source data as labeled data and the target data as unlabeled data."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2714
                },
                {
                    "x": 1197,
                    "y": 2714
                },
                {
                    "x": 1197,
                    "y": 2857
                },
                {
                    "x": 203,
                    "y": 2857
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:22px'>Dataset. As described in Tab. 7, following the LabelMatch,<br>We evaluate our method on these settings and compare it<br>with the state-of-the-arts.</p>",
            "id": 133,
            "page": 12,
            "text": "Dataset. As described in Tab. 7, following the LabelMatch, We evaluate our method on these settings and compare it with the state-of-the-arts."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2878
                },
                {
                    "x": 1197,
                    "y": 2878
                },
                {
                    "x": 1197,
                    "y": 2976
                },
                {
                    "x": 203,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:22px'>Implementation Details. The implementation is nearly the<br>same as SSOD, and more training hyper-parameters can be</p>",
            "id": 134,
            "page": 12,
            "text": "Implementation Details. The implementation is nearly the same as SSOD, and more training hyper-parameters can be"
        },
        {
            "bounding_box": [
                {
                    "x": 1391,
                    "y": 1304
                },
                {
                    "x": 2177,
                    "y": 1304
                },
                {
                    "x": 2177,
                    "y": 1761
                },
                {
                    "x": 1391,
                    "y": 1761
                }
            ],
            "category": "table",
            "html": "<br><table id='135' style='font-size:20px'><tr><td>Method</td><td>AP50</td><td>network</td></tr><tr><td>Source only</td><td>42.2</td><td>FR+VGG</td></tr><tr><td>CVPR2019:SW-Faster [32]</td><td>37.9</td><td>FR+VGG</td></tr><tr><td>CVPR2020:GPA [44]</td><td>47.9</td><td>FR+R50</td></tr><tr><td>CVPR2021:MeGA [41]</td><td>43.0</td><td>FR+VGG</td></tr><tr><td>CVPR2022:TDD [17]</td><td>47.4</td><td>FR+VGG</td></tr><tr><td>CVPR2022:LabelMatch [4]</td><td>51.0</td><td>FR+VGG</td></tr><tr><td>ICCV2021:SimROD [29]</td><td>47.5</td><td>YOLOv5</td></tr><tr><td>Efficient Teacher (Ours)</td><td>56.4</td><td>YOLOv5</td></tr></table>",
            "id": 135,
            "page": 12,
            "text": "Method AP50 network  Source only 42.2 FR+VGG  CVPR2019:SW-Faster  37.9 FR+VGG  CVPR2020:GPA  47.9 FR+R50  CVPR2021:MeGA  43.0 FR+VGG  CVPR2022:TDD  47.4 FR+VGG  CVPR2022:LabelMatch  51.0 FR+VGG  ICCV2021:SimROD  47.5 YOLOv5  Efficient Teacher (Ours) 56.4"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 1798
                },
                {
                    "x": 2270,
                    "y": 1798
                },
                {
                    "x": 2270,
                    "y": 1880
                },
                {
                    "x": 1283,
                    "y": 1880
                }
            ],
            "category": "caption",
            "html": "<caption id='136' style='font-size:20px'>Table 9. Results of adaptation from KITTI to CityScapes. FR:<br>Faster-RCNN.</caption>",
            "id": 136,
            "page": 12,
            "text": "Table 9. Results of adaptation from KITTI to CityScapes. FR: Faster-RCNN."
        },
        {
            "bounding_box": [
                {
                    "x": 1385,
                    "y": 1940
                },
                {
                    "x": 2177,
                    "y": 1940
                },
                {
                    "x": 2177,
                    "y": 2390
                },
                {
                    "x": 1385,
                    "y": 2390
                }
            ],
            "category": "table",
            "html": "<table id='137' style='font-size:20px'><tr><td>Method</td><td>AP50</td><td>network</td></tr><tr><td>Source only</td><td>36.5</td><td>FR+VGG</td></tr><tr><td>CVPR2019:SW-Faster [32]</td><td>40.7</td><td>FR+VGG</td></tr><tr><td>CVPR2020:GPA [44]</td><td>47.6</td><td>FR+R50</td></tr><tr><td>CVPR2021:MeGA [41]</td><td>44.8</td><td>FR+VGG</td></tr><tr><td>CVPR2022:TDD [17]</td><td>53.4</td><td>FR+VGG</td></tr><tr><td>CVPR2022:LabelMatch [4]</td><td>52.7</td><td>FR+VGG</td></tr><tr><td>ICCV2021:SimROD [29]</td><td>52.1</td><td>YOLOv5</td></tr><tr><td>Efficient Teacher(Ours)</td><td>59.3</td><td>YOLOv5</td></tr></table>",
            "id": 137,
            "page": 12,
            "text": "Method AP50 network  Source only 36.5 FR+VGG  CVPR2019:SW-Faster  40.7 FR+VGG  CVPR2020:GPA  47.6 FR+R50  CVPR2021:MeGA  44.8 FR+VGG  CVPR2022:TDD  53.4 FR+VGG  CVPR2022:LabelMatch  52.7 FR+VGG  ICCV2021:SimROD  52.1 YOLOv5  Efficient Teacher(Ours) 59.3"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 2428
                },
                {
                    "x": 2271,
                    "y": 2428
                },
                {
                    "x": 2271,
                    "y": 2512
                },
                {
                    "x": 1284,
                    "y": 2512
                }
            ],
            "category": "caption",
            "html": "<caption id='138' style='font-size:20px'>Table 10. Results of adaptation from S10K to CityScapes. VGG:<br>VGG-16.</caption>",
            "id": 138,
            "page": 12,
            "text": "Table 10. Results of adaptation from S10K to CityScapes. VGG: VGG-16."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2614
                },
                {
                    "x": 2276,
                    "y": 2614
                },
                {
                    "x": 2276,
                    "y": 2810
                },
                {
                    "x": 1281,
                    "y": 2810
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:22px'>found in Tab. 11. Following previous works, we use AP50<br>as our evaluation metric, moreover, only AP50 on car is re-<br>ported in KITTI to CityScapes and S10K to CityScapes set-<br>ting because of category differences in annotations.</p>",
            "id": 139,
            "page": 12,
            "text": "found in Tab. 11. Following previous works, we use AP50 as our evaluation metric, moreover, only AP50 on car is reported in KITTI to CityScapes and S10K to CityScapes setting because of category differences in annotations."
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2830
                },
                {
                    "x": 2274,
                    "y": 2830
                },
                {
                    "x": 2274,
                    "y": 2974
                },
                {
                    "x": 1283,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:22px'>Results. We validate the DAOD results of Efficient<br>Teacher on CityScapes, Foggy CityScapes, KITTI and<br>S10K datasets. In the experiments, the input resolution is</p>",
            "id": 140,
            "page": 12,
            "text": "Results. We validate the DAOD results of Efficient Teacher on CityScapes, Foggy CityScapes, KITTI and S10K datasets. In the experiments, the input resolution is"
        },
        {
            "bounding_box": [
                {
                    "x": 1221,
                    "y": 3055
                },
                {
                    "x": 1261,
                    "y": 3055
                },
                {
                    "x": 1261,
                    "y": 3090
                },
                {
                    "x": 1221,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='141' style='font-size:20px'>12</footer>",
            "id": 141,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 234,
                    "y": 289
                },
                {
                    "x": 2251,
                    "y": 289
                },
                {
                    "x": 2251,
                    "y": 638
                },
                {
                    "x": 234,
                    "y": 638
                }
            ],
            "category": "figure",
            "html": "<figure><img id='142' style='font-size:14px' alt=\"person 0.91. 0.93 0.86\n100\" data-coord=\"top-left:(234,289); bottom-right:(2251,638)\" /></figure>",
            "id": 142,
            "page": 13,
            "text": "person 0.91. 0.93 0.86 100"
        },
        {
            "bounding_box": [
                {
                    "x": 432,
                    "y": 671
                },
                {
                    "x": 2043,
                    "y": 671
                },
                {
                    "x": 2043,
                    "y": 717
                },
                {
                    "x": 432,
                    "y": 717
                }
            ],
            "category": "caption",
            "html": "<caption id='143' style='font-size:18px'>Figure 10. DAOD Performance of YOLOv5 with Efficient Teacher on Foggy Cityscapes validation dataset.</caption>",
            "id": 143,
            "page": 13,
            "text": "Figure 10. DAOD Performance of YOLOv5 with Efficient Teacher on Foggy Cityscapes validation dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 232,
                    "y": 750
                },
                {
                    "x": 2249,
                    "y": 750
                },
                {
                    "x": 2249,
                    "y": 1097
                },
                {
                    "x": 232,
                    "y": 1097
                }
            ],
            "category": "figure",
            "html": "<figure><img id='144' style='font-size:14px' alt=\"other (other other 0\noth(other 0.87. 86_otrother HU net - 一 [*]\n■■■ ■ ■ ! C ' W 9th0.87her\n186 .8274\nPUP · -\nother other othe othe . 0.83 otrothic ther O.\n- Chan , � # ��\n中 VI MI 예전 IN ¥ 华为\nother Cother 0.�other 0.87rothe i Vi ri JIN �� 19181 ' IMA\nother iother 0.90:15r lother 0.8 reflective_clo\nVICK Pillacify\nothoother 0.83er 0 709 ||||o|heother 0.87,0thel 0.85 reflective_clothes 0.51\n- I 送日常 157\n雾 ■\nreflectiv other 0 .80\n绿色\nYOLOv5 with YOLOv5 with Efficient Teacher.\" data-coord=\"top-left:(232,750); bottom-right:(2249,1097)\" /></figure>",
            "id": 144,
            "page": 13,
            "text": "other (other other 0 oth(other 0.87. 86_otrother HU net - 一 [*] ■■■ ■ ■ ! C \" W 9th0.87her 186 .8274 PUP · other other othe othe . 0.83 otrothic ther O. - Chan , � # �� 中 VI MI 예전 IN ¥ 华为 other Cother 0.�other 0.87rothe i Vi ri JIN �� 19181 \" IMA other iother 0.90:15r lother 0.8 reflective_clo VICK Pillacify othoother 0.83er 0 709 ||||o|heother 0.87,0thel 0.85 reflective_clothes 0.51 - I 送日常 157 雾 ■ reflectiv other 0 .80 绿色 YOLOv5 with YOLOv5 with Efficient Teacher."
        },
        {
            "bounding_box": [
                {
                    "x": 238,
                    "y": 1077
                },
                {
                    "x": 2239,
                    "y": 1077
                },
                {
                    "x": 2239,
                    "y": 1132
                },
                {
                    "x": 238,
                    "y": 1132
                }
            ],
            "category": "caption",
            "html": "<br><caption id='145' style='font-size:16px'>(a) supervised training. (b)<br>(c) YOLOv5 with supervised training. (d) YOLOv5 with Efficient Teacher.</caption>",
            "id": 145,
            "page": 13,
            "text": "(a) supervised training. (b) (c) YOLOv5 with supervised training. (d) YOLOv5 with Efficient Teacher."
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1174
                },
                {
                    "x": 2148,
                    "y": 1174
                },
                {
                    "x": 2148,
                    "y": 1220
                },
                {
                    "x": 328,
                    "y": 1220
                }
            ],
            "category": "caption",
            "html": "<caption id='146' style='font-size:16px'>Figure 11. Comparison between supervised object detection and semi-supervised object detection on customized dataset.</caption>",
            "id": 146,
            "page": 13,
            "text": "Figure 11. Comparison between supervised object detection and semi-supervised object detection on customized dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1294
                },
                {
                    "x": 1199,
                    "y": 1294
                },
                {
                    "x": 1199,
                    "y": 1493
                },
                {
                    "x": 201,
                    "y": 1493
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:18px'>960 and 入 is 1.0. Table 8 shows Efficient Teacher outper-<br>forms all previous state-of-the-art model with a margin of<br>2.9 AP50. Table 9 and Tabel 10 shows our method ,is an<br>effective framework for both SSOD and DAOD.</p>",
            "id": 147,
            "page": 13,
            "text": "960 and 入 is 1.0. Table 8 shows Efficient Teacher outperforms all previous state-of-the-art model with a margin of 2.9 AP50. Table 9 and Tabel 10 shows our method ,is an effective framework for both SSOD and DAOD."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1289
                },
                {
                    "x": 2141,
                    "y": 1289
                },
                {
                    "x": 2141,
                    "y": 1343
                },
                {
                    "x": 1280,
                    "y": 1343
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:22px'>D. Implementation and Training Details</p>",
            "id": 148,
            "page": 13,
            "text": "D. Implementation and Training Details"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1643
                },
                {
                    "x": 1065,
                    "y": 1643
                },
                {
                    "x": 1065,
                    "y": 1696
                },
                {
                    "x": 202,
                    "y": 1696
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:20px'>C. Experiments on Customized Datasets</p>",
            "id": 149,
            "page": 13,
            "text": "C. Experiments on Customized Datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1374
                },
                {
                    "x": 2280,
                    "y": 1374
                },
                {
                    "x": 2280,
                    "y": 1771
                },
                {
                    "x": 1277,
                    "y": 1771
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:18px'>Our Efficient Teacher is based on YOLO-like Dense De-<br>tector, for fair comparison, we refactor the source code<br>of YOLOv5(https : / / github · com / ultralytics/<br>yolov5), adding utilities like unlabeled dataloader, pseudo<br>labels online transformation and configuration system. Our<br>code can train both supervised and semi-supervised object<br>detection through modify a few lines of configuration. The<br>source code will be released soon.</p>",
            "id": 150,
            "page": 13,
            "text": "Our Efficient Teacher is based on YOLO-like Dense Detector, for fair comparison, we refactor the source code of YOLOv5(https : / / github · com / ultralytics/ yolov5), adding utilities like unlabeled dataloader, pseudo labels online transformation and configuration system. Our code can train both supervised and semi-supervised object detection through modify a few lines of configuration. The source code will be released soon."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1760
                },
                {
                    "x": 1198,
                    "y": 1760
                },
                {
                    "x": 1198,
                    "y": 1907
                },
                {
                    "x": 202,
                    "y": 1907
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='151' style='font-size:18px'>We further report the performance of Efficient Teacher<br>on customized dataset to showcase the generality of our<br>framework.</p>",
            "id": 151,
            "page": 13,
            "text": "We further report the performance of Efficient Teacher on customized dataset to showcase the generality of our framework."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1946
                },
                {
                    "x": 1200,
                    "y": 1946
                },
                {
                    "x": 1200,
                    "y": 2342
                },
                {
                    "x": 201,
                    "y": 2342
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>Dataset. The customized dataset contains two types<br>of ground truth: reflective clothes and other, the relective<br>clothes means the annotation of the reflective clothes that<br>people wear and other means normal clothes. We only col-<br>lect 3800 pictures in a few scenes while customer wish the<br>model can be deployed in a lot of scenes, thus we take 3800<br>pictures as labeled data and COCO train dataset as unla-<br>beled data for Efficient Teacher.</p>",
            "id": 152,
            "page": 13,
            "text": "Dataset. The customized dataset contains two types of ground truth: reflective clothes and other, the relective clothes means the annotation of the reflective clothes that people wear and other means normal clothes. We only collect 3800 pictures in a few scenes while customer wish the model can be deployed in a lot of scenes, thus we take 3800 pictures as labeled data and COCO train dataset as unlabeled data for Efficient Teacher."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2377
                },
                {
                    "x": 1199,
                    "y": 2377
                },
                {
                    "x": 1199,
                    "y": 2978
                },
                {
                    "x": 200,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:20px'>Implementation Details. We verify the generality ofEf-<br>ficient Teacher through inference the supervised model and<br>semi-supervised model on COCO val dataset, results are vi-<br>sualized in Figure 11. Obviously, the SSOD on Efficient<br>Teacher shows great performance improvement with fewer<br>classification and localization error. In addition, what is in-<br>teresting in the figures should be the red carp flags are mis-<br>classified as reflective clothes, which is the common phe-<br>nomenon occurs by supervised training in practical appli-<br>cations. Efficient Teacher shows superiority of alleviating<br>the overfitting due to small training dataset and potential to<br>become the standard solution in application scenario.</p>",
            "id": 153,
            "page": 13,
            "text": "Implementation Details. We verify the generality ofEfficient Teacher through inference the supervised model and semi-supervised model on COCO val dataset, results are visualized in Figure 11. Obviously, the SSOD on Efficient Teacher shows great performance improvement with fewer classification and localization error. In addition, what is interesting in the figures should be the red carp flags are misclassified as reflective clothes, which is the common phenomenon occurs by supervised training in practical applications. Efficient Teacher shows superiority of alleviating the overfitting due to small training dataset and potential to become the standard solution in application scenario."
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3095
                },
                {
                    "x": 1217,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='154' style='font-size:18px'>13</footer>",
            "id": 154,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 230,
                    "y": 516
                },
                {
                    "x": 2257,
                    "y": 516
                },
                {
                    "x": 2257,
                    "y": 1122
                },
                {
                    "x": 230,
                    "y": 1122
                }
            ],
            "category": "table",
            "html": "<table id='155' style='font-size:14px'><tr><td>training setting</td><td>COCO-standard</td><td>COCO-additional</td><td>VOC</td><td>DAOD</td><td>Customized</td></tr><tr><td>input resolution</td><td>640</td><td>640</td><td>640</td><td>960</td><td>640</td></tr><tr><td>batch size for labeled data</td><td>32</td><td>32</td><td>32</td><td>16</td><td>32</td></tr><tr><td>batch size for unlabeled data</td><td>32</td><td>32</td><td>32</td><td>16</td><td>32</td></tr><tr><td>learning rate</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td>iterations</td><td>300K</td><td>540K</td><td>72K</td><td>20K</td><td>150K</td></tr><tr><td>unsupervised loss weight 入</td><td>3.0</td><td>2.0</td><td>2.0</td><td>1.0</td><td>3.0</td></tr><tr><td>EMA rate</td><td>0.999</td><td>0.999</td><td>0.999</td><td>0.999</td><td>0.999</td></tr><tr><td>reliable ratio a</td><td>0.5</td><td>0.5</td><td>0.5</td><td>0.5</td><td>0.3</td></tr><tr><td>pseudo label NMS score thresh</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.01</td><td>0.1</td></tr><tr><td>pseudo label NMS IoU thresh</td><td>0.65</td><td>0.65</td><td>0.65</td><td>0.65</td><td>0.65</td></tr><tr><td>multi-scale (strong augmentation)</td><td>(0.1, 1.9)</td><td>(0.1, 1.9)</td><td>(0.1, 1.9)</td><td>(0.5, 1.5)</td><td>(0.5, 1.5)</td></tr><tr><td>test score thresh</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td></tr></table>",
            "id": 155,
            "page": 14,
            "text": "training setting COCO-standard COCO-additional VOC DAOD Customized  input resolution 640 640 640 960 640  batch size for labeled data 32 32 32 16 32  batch size for unlabeled data 32 32 32 16 32  learning rate 0.01 0.01 0.01 0.01 0.01  iterations 300K 540K 72K 20K 150K  unsupervised loss weight 入 3.0 2.0 2.0 1.0 3.0  EMA rate 0.999 0.999 0.999 0.999 0.999  reliable ratio a 0.5 0.5 0.5 0.5 0.3  pseudo label NMS score thresh 0.01 0.01 0.01 0.01 0.1  pseudo label NMS IoU thresh 0.65 0.65 0.65 0.65 0.65  multi-scale (strong augmentation) (0.1, 1.9) (0.1, 1.9) (0.1, 1.9) (0.5, 1.5) (0.5, 1.5)  test score thresh 0.001 0.001 0.001 0.001"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1156
                },
                {
                    "x": 2277,
                    "y": 1156
                },
                {
                    "x": 2277,
                    "y": 1250
                },
                {
                    "x": 201,
                    "y": 1250
                }
            ],
            "category": "caption",
            "html": "<caption id='156' style='font-size:18px'>Table 11. Training settings for different datasets and different tasks. \"Ablation\" means the training setting of the ablation studies in the<br>main body of the paper, which is also used in all SSOD experiments in the Appendix.</caption>",
            "id": 156,
            "page": 14,
            "text": "Table 11. Training settings for different datasets and different tasks. \"Ablation\" means the training setting of the ablation studies in the main body of the paper, which is also used in all SSOD experiments in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 231,
                    "y": 1721
                },
                {
                    "x": 2251,
                    "y": 1721
                },
                {
                    "x": 2251,
                    "y": 2657
                },
                {
                    "x": 231,
                    "y": 2657
                }
            ],
            "category": "table",
            "html": "<table id='157' style='font-size:18px'><tr><td colspan=\"4\">Weak Augmentation</td></tr><tr><td>Process</td><td>Prob</td><td>Parameters</td><td>Descriptions</td></tr><tr><td>Mosaic</td><td>1.0</td><td>None</td><td></td></tr><tr><td colspan=\"4\">Strong Augmentation</td></tr><tr><td>Process</td><td>Prob</td><td>Parameters</td><td>Descriptions</td></tr><tr><td>Mosaic</td><td>1.0</td><td>None</td><td>·</td></tr><tr><td>Horizontal Flip</td><td>0.5</td><td>None</td><td>None</td></tr><tr><td>Multi-Scale</td><td>1.0</td><td>ratio=(0.1, 1.9)</td><td>The short edge of image is random resized from 0.1lshort to 1.9lshort.</td></tr><tr><td>HSV Color Jitter- ing</td><td>1.0</td><td>(brightness, saturation, hue) = (0.4, 0.7, 0.015)</td><td>Brightness factor is chosen uniformly form [0.6, 1.4], satura- tion factor is chosen uniformly from [0.3, 1.7], and hue value is chosen uniformly from [-0.015, 0.015].</td></tr><tr><td>Grayscale</td><td>0.2</td><td>None</td><td>None</td></tr><tr><td>GaussianBlur</td><td>0.5</td><td>(sigma_x, simga_y)=(0.1, 2.0)</td><td>Gaussian filter with Ox = 0.1 and Oy = 2.0 is applied</td></tr><tr><td>CutoutPattern1</td><td>0.7</td><td>scale=(0.05, 0.2), ratio=(0.3, 3.3)</td><td>Randomly selects a rectangle region in an image and erases its pixels.</td></tr><tr><td>CutoutPattern2</td><td>0.7</td><td>scale=(0.02, 0.2), ratio=(0.1, 6.0)</td><td>Randomly selects a rectangle region in an image and erases its pixels.</td></tr><tr><td>CutoutPattern3</td><td>0.7</td><td>scale=(0.02, 0.2), ratio=(0.05, 8.0)</td><td>Randomly selects a rectangle region in an image and erases its pixels.</td></tr></table>",
            "id": 157,
            "page": 14,
            "text": "Weak Augmentation  Process Prob Parameters Descriptions  Mosaic 1.0 None   Strong Augmentation  Process Prob Parameters Descriptions  Mosaic 1.0 None ·  Horizontal Flip 0.5 None None  Multi-Scale 1.0 ratio=(0.1, 1.9) The short edge of image is random resized from 0.1lshort to 1.9lshort.  HSV Color Jitter- ing 1.0 (brightness, saturation, hue) = (0.4, 0.7, 0.015) Brightness factor is chosen uniformly form [0.6, 1.4], satura- tion factor is chosen uniformly from [0.3, 1.7], and hue value is chosen uniformly from [-0.015, 0.015].  Grayscale 0.2 None None  GaussianBlur 0.5 (sigma_x, simga_y)=(0.1, 2.0) Gaussian filter with Ox = 0.1 and Oy = 2.0 is applied  CutoutPattern1 0.7 scale=(0.05, 0.2), ratio=(0.3, 3.3) Randomly selects a rectangle region in an image and erases its pixels.  CutoutPattern2 0.7 scale=(0.02, 0.2), ratio=(0.1, 6.0) Randomly selects a rectangle region in an image and erases its pixels.  CutoutPattern3 0.7 scale=(0.02, 0.2), ratio=(0.05, 8.0)"
        },
        {
            "bounding_box": [
                {
                    "x": 933,
                    "y": 2693
                },
                {
                    "x": 1545,
                    "y": 2693
                },
                {
                    "x": 1545,
                    "y": 2738
                },
                {
                    "x": 933,
                    "y": 2738
                }
            ],
            "category": "caption",
            "html": "<caption id='158' style='font-size:18px'>Table 12. Details of data augmentations.</caption>",
            "id": 158,
            "page": 14,
            "text": "Table 12. Details of data augmentations."
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3052
                },
                {
                    "x": 1264,
                    "y": 3052
                },
                {
                    "x": 1264,
                    "y": 3094
                },
                {
                    "x": 1217,
                    "y": 3094
                }
            ],
            "category": "footer",
            "html": "<footer id='159' style='font-size:18px'>14</footer>",
            "id": 159,
            "page": 14,
            "text": "14"
        }
    ]
}