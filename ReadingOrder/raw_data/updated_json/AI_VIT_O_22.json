{
    "id": "629bf51a-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2101.11986v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 434
                },
                {
                    "x": 2259,
                    "y": 434
                },
                {
                    "x": 2259,
                    "y": 508
                },
                {
                    "x": 224,
                    "y": 508
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Tokens-to- Token ViT: Training Vision Transformers from Scratch on ImageNet</p>",
            "id": 0,
            "page": 1,
            "text": "Tokens-to- Token ViT: Training Vision Transformers from Scratch on ImageNet"
        },
        {
            "bounding_box": [
                {
                    "x": 573,
                    "y": 596
                },
                {
                    "x": 1909,
                    "y": 596
                },
                {
                    "x": 1909,
                    "y": 720
                },
                {
                    "x": 573,
                    "y": 720
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Weihao Yu1, Yujun Shi1<br>Li Yuan1* Yunpeng Chen2, Tao Wang1,3* ,<br>Zihang Jiang1 , Francis E.H. Tay1 , Jiashi Feng1 , Shuicheng Yan1</p>",
            "id": 1,
            "page": 1,
            "text": "Weihao Yu1, Yujun Shi1\nLi Yuan1* Yunpeng Chen2, Tao Wang1,3* ,\nZihang Jiang1 , Francis E.H. Tay1 , Jiashi Feng1 , Shuicheng Yan1"
        },
        {
            "bounding_box": [
                {
                    "x": 311,
                    "y": 749
                },
                {
                    "x": 2171,
                    "y": 749
                },
                {
                    "x": 2171,
                    "y": 864
                },
                {
                    "x": 311,
                    "y": 864
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:14px'>2 YITU Technology Institute of Data Science, National University of Singapore<br>3<br>1 National University of Singapore<br>yuanl i@u · nus · edu, yunpeng · chen@yitu-inc · com, shuicheng · yan@gmail · com</p>",
            "id": 2,
            "page": 1,
            "text": "2 YITU Technology Institute of Data Science, National University of Singapore\n3\n1 National University of Singapore\nyuanl i@u · nus · edu, yunpeng · chen@yitu-inc · com, shuicheng · yan@gmail · com"
        },
        {
            "bounding_box": [
                {
                    "x": 601,
                    "y": 977
                },
                {
                    "x": 800,
                    "y": 977
                },
                {
                    "x": 800,
                    "y": 1032
                },
                {
                    "x": 601,
                    "y": 1032
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1050
                },
                {
                    "x": 1200,
                    "y": 1050
                },
                {
                    "x": 1200,
                    "y": 2602
                },
                {
                    "x": 198,
                    "y": 2602
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:18px'>Transformers, which are popular for language modeling,<br>have been explored for solving vision tasks recently, e.g.,<br>the Vision Transformer (ViT) for image classification. The<br>ViT model splits each image into a sequence of tokens with<br>fixed length and then applies multiple Transformer layers<br>to model their global relation for classification. However,<br>ViT achieves inferior performance to CNNs when trained<br>from scratch on a midsize dataset like ImageNet. We find<br>it is because: 1) the simple tokenization of input images<br>fails to model the important local structure such as edges<br>and lines among neighboring pixels, leading to low train-<br>ing sample efficiency; 2) the redundant attention backbone<br>design of ViT leads to limited feature richness for fixed com-<br>putation budgets and limited training samples. To overcome<br>such limitations, we propose a new Tokens-To-Token Vi-<br>sion Transformer (T2T- ViT), which incorporates 1) a layer-<br>wise Tokens-to-Token (T2T) transformation to progressively<br>structurize the image to tokens by recursively aggregating<br>neighboring Tokens into one Token (Tokens-to-Token), such<br>that local structure represented by surrounding tokens can<br>be modeled and tokens length can be reduced; 2) an ef-<br>ficient backbone with a deep-narrow structure for vision<br>transformer motivated by CNN architecture design after<br>empirical study. Notably, T2T-ViT reduces the parameter<br>count and MACs of vanilla ViT by half, while achieving<br>more than 3.0% improvement when trained from scratch on<br>ImageNet. It also outperforms ResNets and achieves com-<br>parable performance with MobileNets by directly training<br>on ImageNet. For example, T2T-ViT with comparable size<br>to ResNet50 (21.5M parameters) can achieve 83.3% topl<br>accuracy in image resolution 384x384 on ImageNet. 1</p>",
            "id": 4,
            "page": 1,
            "text": "Transformers, which are popular for language modeling,\nhave been explored for solving vision tasks recently, e.g.,\nthe Vision Transformer (ViT) for image classification. The\nViT model splits each image into a sequence of tokens with\nfixed length and then applies multiple Transformer layers\nto model their global relation for classification. However,\nViT achieves inferior performance to CNNs when trained\nfrom scratch on a midsize dataset like ImageNet. We find\nit is because: 1) the simple tokenization of input images\nfails to model the important local structure such as edges\nand lines among neighboring pixels, leading to low train-\ning sample efficiency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for fixed com-\nputation budgets and limited training samples. To overcome\nsuch limitations, we propose a new Tokens-To-Token Vi-\nsion Transformer (T2T- ViT), which incorporates 1) a layer-\nwise Tokens-to-Token (T2T) transformation to progressively\nstructurize the image to tokens by recursively aggregating\nneighboring Tokens into one Token (Tokens-to-Token), such\nthat local structure represented by surrounding tokens can\nbe modeled and tokens length can be reduced; 2) an ef-\nficient backbone with a deep-narrow structure for vision\ntransformer motivated by CNN architecture design after\nempirical study. Notably, T2T-ViT reduces the parameter\ncount and MACs of vanilla ViT by half, while achieving\nmore than 3.0% improvement when trained from scratch on\nImageNet. It also outperforms ResNets and achieves com-\nparable performance with MobileNets by directly training\non ImageNet. For example, T2T-ViT with comparable size\nto ResNet50 (21.5M parameters) can achieve 83.3% topl\naccuracy in image resolution 384x384 on ImageNet. 1"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 2622
                },
                {
                    "x": 532,
                    "y": 2622
                },
                {
                    "x": 532,
                    "y": 2677
                },
                {
                    "x": 206,
                    "y": 2677
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:22px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 1286,
                    "y": 966
                },
                {
                    "x": 2273,
                    "y": 966
                },
                {
                    "x": 2273,
                    "y": 1418
                },
                {
                    "x": 1286,
                    "y": 1418
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='6' style='font-size:14px' alt=\"Accuracy VS. MACs VS. Model size Accuracy VS. Model size\n77\nT2T-ViT T2T-ViT\n76\n82 ViT\n75\n(%) 81 (%)\nAccuracy 74 MobileNetV2\nAccuracy\nResNet\n73\n80\n72\nTop-1\nTop-1\n79\n71\n70\n78\nMobileNetV1\n69\n20M 50M 200M\n77 68\n0 10 20 30 40 50 60 70 2 3 4 5 6 7\nMACs (x109) Model Size (M)\" data-coord=\"top-left:(1286,966); bottom-right:(2273,1418)\" /></figure>",
            "id": 6,
            "page": 1,
            "text": "Accuracy VS. MACs VS. Model size Accuracy VS. Model size\n77\nT2T-ViT T2T-ViT\n76\n82 ViT\n75\n(%) 81 (%)\nAccuracy 74 MobileNetV2\nAccuracy\nResNet\n73\n80\n72\nTop-1\nTop-1\n79\n71\n70\n78\nMobileNetV1\n69\n20M 50M 200M\n77 68\n0 10 20 30 40 50 60 70 2 3 4 5 6 7\nMACs (x109) Model Size (M)"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1421
                },
                {
                    "x": 2279,
                    "y": 1421
                },
                {
                    "x": 2279,
                    "y": 1607
                },
                {
                    "x": 1279,
                    "y": 1607
                }
            ],
            "category": "caption",
            "html": "<br><caption id='7' style='font-size:16px'>Figure 1. Comparison between T2T-ViT with ViT, ResNets and<br>MobileNets when trained from scratch on ImageNet. Left: per-<br>formance curve of MACs VS. top-1 accuracy. Right: performance<br>curve of model size VS. top-1 accuracy.</caption>",
            "id": 7,
            "page": 1,
            "text": "Figure 1. Comparison between T2T-ViT with ViT, ResNets and\nMobileNets when trained from scratch on ImageNet. Left: per-\nformance curve of MACs VS. top-1 accuracy. Right: performance\ncurve of model size VS. top-1 accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2709
                },
                {
                    "x": 1201,
                    "y": 2709
                },
                {
                    "x": 1201,
                    "y": 2859
                },
                {
                    "x": 202,
                    "y": 2859
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:20px'>Self-attention models for language modeling like Trans-<br>formers [37] have been recently applied to vision tasks,<br>including image classification [5, 12, 43], object detec-</p>",
            "id": 8,
            "page": 1,
            "text": "Self-attention models for language modeling like Trans-\nformers [37] have been recently applied to vision tasks,\nincluding image classification [5, 12, 43], object detec-"
        },
        {
            "bounding_box": [
                {
                    "x": 253,
                    "y": 2893
                },
                {
                    "x": 954,
                    "y": 2893
                },
                {
                    "x": 954,
                    "y": 2972
                },
                {
                    "x": 253,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:16px'>* Work done during an internship at Yitu Tech.<br>1Code: https:/grhub.com/yin-opensource/T2T-MT</p>",
            "id": 9,
            "page": 1,
            "text": "* Work done during an internship at Yitu Tech.\n1Code: https:/grhub.com/yin-opensource/T2T-MT"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1628
                },
                {
                    "x": 2278,
                    "y": 1628
                },
                {
                    "x": 2278,
                    "y": 2073
                },
                {
                    "x": 1280,
                    "y": 2073
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>tion [3, 61] and image processing like denoising, super-<br>resolution and deraining [4]. Among them, the Vision<br>Transformer (ViT) [12] is the first full-transformer model<br>that can be directly applied for image classification. In par-<br>ticular, ViT splits each image into 14x 14 or 16 x 16 patches<br>(a.k.a., tokens) with fixed length; then following practice of<br>the transformer for language modeling, ViT applies trans-<br>former layers to model the global relation among these to-<br>kens for classification.</p>",
            "id": 10,
            "page": 1,
            "text": "tion [3, 61] and image processing like denoising, super-\nresolution and deraining [4]. Among them, the Vision\nTransformer (ViT) [12] is the first full-transformer model\nthat can be directly applied for image classification. In par-\nticular, ViT splits each image into 14x 14 or 16 x 16 patches\n(a.k.a., tokens) with fixed length; then following practice of\nthe transformer for language modeling, ViT applies trans-\nformer layers to model the global relation among these to-\nkens for classification."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2077
                },
                {
                    "x": 2277,
                    "y": 2077
                },
                {
                    "x": 2277,
                    "y": 2774
                },
                {
                    "x": 1278,
                    "y": 2774
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>Though ViT proves the full-transformer architecture is<br>promising for vision tasks, its performance is still inferior<br>to that of similar-sized CNN counterparts (e.g. ResNets)<br>when trained from scratch on a midsize dataset (e.g., Im-<br>ageNet). We hypothesize that such performance gap roots<br>in two main limitations of ViT: 1) the straightforward tok-<br>enization of input images by hard split makes ViT unable<br>to model the image local structure like edges and lines,<br>and thus it requires significantly more training samples (like<br>JFT-300M for pretraining) than CNNs for achieving similar<br>performance; 2) the attention backbone of ViT is not well-<br>designed as CNNs for vision tasks, which contains redun-<br>dancy and leads to limited feature richness and difficulties<br>in model training.</p>",
            "id": 11,
            "page": 1,
            "text": "Though ViT proves the full-transformer architecture is\npromising for vision tasks, its performance is still inferior\nto that of similar-sized CNN counterparts (e.g. ResNets)\nwhen trained from scratch on a midsize dataset (e.g., Im-\nageNet). We hypothesize that such performance gap roots\nin two main limitations of ViT: 1) the straightforward tok-\nenization of input images by hard split makes ViT unable\nto model the image local structure like edges and lines,\nand thus it requires significantly more training samples (like\nJFT-300M for pretraining) than CNNs for achieving similar\nperformance; 2) the attention backbone of ViT is not well-\ndesigned as CNNs for vision tasks, which contains redun-\ndancy and leads to limited feature richness and difficulties\nin model training."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2778
                },
                {
                    "x": 2277,
                    "y": 2778
                },
                {
                    "x": 2277,
                    "y": 2977
                },
                {
                    "x": 1280,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:18px'>To verify our hypotheses, we conduct a pilot study to<br>investigate the difference in the learned features of ViT-<br>L/16 [12] and ResNet50 [15] through visualization in Fig. 2.<br>We observe the features of ResNet capture the desired local</p>",
            "id": 12,
            "page": 1,
            "text": "To verify our hypotheses, we conduct a pilot study to\ninvestigate the difference in the learned features of ViT-\nL/16 [12] and ResNet50 [15] through visualization in Fig. 2.\nWe observe the features of ResNet capture the desired local"
        },
        {
            "bounding_box": [
                {
                    "x": 59,
                    "y": 871
                },
                {
                    "x": 149,
                    "y": 871
                },
                {
                    "x": 149,
                    "y": 2329
                },
                {
                    "x": 59,
                    "y": 2329
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2021<br>Nov<br>30<br>[cs.CV]<br>arXiv:2101.11986v3</footer>",
            "id": 13,
            "page": 1,
            "text": "2021\nNov\n30\n[cs.CV]\narXiv:2101.11986v3"
        },
        {
            "bounding_box": [
                {
                    "x": 388,
                    "y": 285
                },
                {
                    "x": 2097,
                    "y": 285
                },
                {
                    "x": 2097,
                    "y": 1080
                },
                {
                    "x": 388,
                    "y": 1080
                }
            ],
            "category": "figure",
            "html": "<figure><img id='14' style='font-size:14px' alt=\"ResNet50: conv1 ResNet50: conv25 ResNet50: conv49\n♩\nViT-L/16: block1 ViT-L/16: block12 ViT-L/16: block24\nT2T-ViT-24: T2T block1 T2T-ViT-24: block12 T2T-ViT-24: block24\" data-coord=\"top-left:(388,285); bottom-right:(2097,1080)\" /></figure>",
            "id": 14,
            "page": 2,
            "text": "ResNet50: conv1 ResNet50: conv25 ResNet50: conv49\n♩\nViT-L/16: block1 ViT-L/16: block12 ViT-L/16: block24\nT2T-ViT-24: T2T block1 T2T-ViT-24: block12 T2T-ViT-24: block24"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1081
                },
                {
                    "x": 2280,
                    "y": 1081
                },
                {
                    "x": 2280,
                    "y": 1266
                },
                {
                    "x": 199,
                    "y": 1266
                }
            ],
            "category": "caption",
            "html": "<br><caption id='15' style='font-size:16px'>Figure 2. Feature visualization of ResNet50, ViT-L/16 [12] and our proposed T2T-ViT-24 trained on ImageNet. Green boxes highlight<br>learned low-level structure features such as edges and lines; red boxes highlight invalid feature maps with zero or too large values. Note the<br>feature maps visualized here for ViT and T2T-ViT are not attention maps, but image features reshaped from tokens. For better visualization,<br>we scale the input image to size 1024 x 1024 or 2048 x 2048.</caption>",
            "id": 15,
            "page": 2,
            "text": "Figure 2. Feature visualization of ResNet50, ViT-L/16 [12] and our proposed T2T-ViT-24 trained on ImageNet. Green boxes highlight\nlearned low-level structure features such as edges and lines; red boxes highlight invalid feature maps with zero or too large values. Note the\nfeature maps visualized here for ViT and T2T-ViT are not attention maps, but image features reshaped from tokens. For better visualization,\nwe scale the input image to size 1024 x 1024 or 2048 x 2048."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1289
                },
                {
                    "x": 1201,
                    "y": 1289
                },
                {
                    "x": 1201,
                    "y": 1834
                },
                {
                    "x": 201,
                    "y": 1834
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:20px'>structure (edges, lines, textures, etc.) progressively from the<br>bottom layer (conv1) to the middle layer (conv25). How-<br>ever, the features of ViT are quite different: the structure<br>information is poorly modeled while the global relations<br>(e.g., the whole dog) are captured by all the attention blocks.<br>These observations indicate that the vanilla ViT ignores the<br>local structure when directly splitting images to tokens with<br>fixed length. Besides, we find many channels in ViT have<br>zero value (highlighted in red in Fig. 2), implying the back-<br>bone of ViT is not efficient as ResNets and offers limited<br>feature richness when training samples are not enough.</p>",
            "id": 16,
            "page": 2,
            "text": "structure (edges, lines, textures, etc.) progressively from the\nbottom layer (conv1) to the middle layer (conv25). How-\never, the features of ViT are quite different: the structure\ninformation is poorly modeled while the global relations\n(e.g., the whole dog) are captured by all the attention blocks.\nThese observations indicate that the vanilla ViT ignores the\nlocal structure when directly splitting images to tokens with\nfixed length. Besides, we find many channels in ViT have\nzero value (highlighted in red in Fig. 2), implying the back-\nbone of ViT is not efficient as ResNets and offers limited\nfeature richness when training samples are not enough."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1832
                },
                {
                    "x": 1201,
                    "y": 1832
                },
                {
                    "x": 1201,
                    "y": 2981
                },
                {
                    "x": 200,
                    "y": 2981
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:20px'>We are then motivated to design a new full-transformer<br>vision model to overcome above limitations. 1) Instead of<br>the naive tokenization used in ViT [12], we propose a pro-<br>gressive tokenization module to aggregate neighboring To-<br>kens to one Token (named Tokens-to-Token module), which<br>can model the local structure information of surrounding<br>tokens and reduce the length of tokens iteratively. Specifi-<br>cally, in each Token-to-Token (T2T) step, the tokens output<br>by a transformer layer are reconstructed as an image (re-<br>structurization) which is then split into tokens with over-<br>lapping (soft split) and finally the surrounding tokens are<br>aggregated together by flattening the split patches. Thus<br>the local structure from surrounding patches is embedded<br>into the tokens to be input into the next transformer layer.<br>By conducting T2T iteratively, the local structure is aggre-<br>gated into tokens and the length of tokens can be reduced<br>by the aggregation process. 2) To find an efficient back-<br>bone for vision transformers, we explore borrowing some<br>architecture designs from CNNs to build transformer lay-<br>ers for improving the feature richness, and we find \"deep-<br>narrow\" architecture design with fewer channels but more<br>layers in ViT brings much better performance at compara-<br>ble model size and MACs (Multi-Adds). Specifically, we</p>",
            "id": 17,
            "page": 2,
            "text": "We are then motivated to design a new full-transformer\nvision model to overcome above limitations. 1) Instead of\nthe naive tokenization used in ViT [12], we propose a pro-\ngressive tokenization module to aggregate neighboring To-\nkens to one Token (named Tokens-to-Token module), which\ncan model the local structure information of surrounding\ntokens and reduce the length of tokens iteratively. Specifi-\ncally, in each Token-to-Token (T2T) step, the tokens output\nby a transformer layer are reconstructed as an image (re-\nstructurization) which is then split into tokens with over-\nlapping (soft split) and finally the surrounding tokens are\naggregated together by flattening the split patches. Thus\nthe local structure from surrounding patches is embedded\ninto the tokens to be input into the next transformer layer.\nBy conducting T2T iteratively, the local structure is aggre-\ngated into tokens and the length of tokens can be reduced\nby the aggregation process. 2) To find an efficient back-\nbone for vision transformers, we explore borrowing some\narchitecture designs from CNNs to build transformer lay-\ners for improving the feature richness, and we find \"deep-\nnarrow\" architecture design with fewer channels but more\nlayers in ViT brings much better performance at compara-\nble model size and MACs (Multi-Adds). Specifically, we"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1291
                },
                {
                    "x": 2280,
                    "y": 1291
                },
                {
                    "x": 2280,
                    "y": 1735
                },
                {
                    "x": 1279,
                    "y": 1735
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:22px'>investigate Wide-ResNets (shallow-wide VS deep-narrow<br>structure) [52], DenseNet (dense connection) [21], ResneXt<br>structure [44], Ghost operation [14, 59] and channel atten-<br>tion [20]. We find among them, deep-narrow structure [52]<br>is the most efficient and effective for ViT, reducing the pa-<br>rameter count and MACs significantly with nearly no degra-<br>dation in performance. This also indicates the architecture<br>engineering of CNNs can benefit the backbone design of<br>vision transformers.</p>",
            "id": 18,
            "page": 2,
            "text": "investigate Wide-ResNets (shallow-wide VS deep-narrow\nstructure) [52], DenseNet (dense connection) [21], ResneXt\nstructure [44], Ghost operation [14, 59] and channel atten-\ntion [20]. We find among them, deep-narrow structure [52]\nis the most efficient and effective for ViT, reducing the pa-\nrameter count and MACs significantly with nearly no degra-\ndation in performance. This also indicates the architecture\nengineering of CNNs can benefit the backbone design of\nvision transformers."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1738
                },
                {
                    "x": 2279,
                    "y": 1738
                },
                {
                    "x": 2279,
                    "y": 2433
                },
                {
                    "x": 1277,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:20px'>Based on the T2T module and deep-narrow backbone ar-<br>chitecture, we develop the Tokens-to-Token Vision Trans-<br>former (T2T-ViT), which significantly boosts the perfor-<br>mance when trained from scratch on ImageNet (Fig. 1), and<br>is more lightweight than the vanilla ViT. As shown in Fig. 1,<br>our T2T-ViT with 21.5M parameters and 4.8G MACs can<br>achieve 81.5% top-1 accuracy on ImageNet, much higher<br>than that of ViT [12] with 48.6M parameters and 10.1G<br>MACs (78.1%). This result is also higher than the popu-<br>lar CNNs of similar size, like ResNet50 with 25.5M param-<br>eters (76%-79%). Besides, we also design lite variants of<br>T2T-ViT by simply adopting fewer layers, which achieve<br>comparable results with MobileNets [17, 32] (Fig. 1).<br>To sum up, our contributions are three-fold:</p>",
            "id": 19,
            "page": 2,
            "text": "Based on the T2T module and deep-narrow backbone ar-\nchitecture, we develop the Tokens-to-Token Vision Trans-\nformer (T2T-ViT), which significantly boosts the perfor-\nmance when trained from scratch on ImageNet (Fig. 1), and\nis more lightweight than the vanilla ViT. As shown in Fig. 1,\nour T2T-ViT with 21.5M parameters and 4.8G MACs can\nachieve 81.5% top-1 accuracy on ImageNet, much higher\nthan that of ViT [12] with 48.6M parameters and 10.1G\nMACs (78.1%). This result is also higher than the popu-\nlar CNNs of similar size, like ResNet50 with 25.5M param-\neters (76%-79%). Besides, we also design lite variants of\nT2T-ViT by simply adopting fewer layers, which achieve\ncomparable results with MobileNets [17, 32] (Fig. 1).\nTo sum up, our contributions are three-fold:"
        },
        {
            "bounding_box": [
                {
                    "x": 1326,
                    "y": 2481
                },
                {
                    "x": 2278,
                    "y": 2481
                },
                {
                    "x": 2278,
                    "y": 2996
                },
                {
                    "x": 1326,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>· For the first time, we show by carefully designing<br>transformers architecture (T2T module and efficient<br>backbone), visual transformers can outperform CNNs<br>at different complexities on ImageNet without pre-<br>training on JFT-300M.<br>· We develop a novel progressive tokenization for ViT<br>and demonstrate its advantage over the simple tok-<br>enization approach by ViT, and we propose a T2T<br>module that can encode the important local structure<br>for each token.</p>",
            "id": 20,
            "page": 2,
            "text": "· For the first time, we show by carefully designing\ntransformers architecture (T2T module and efficient\nbackbone), visual transformers can outperform CNNs\nat different complexities on ImageNet without pre-\ntraining on JFT-300M.\n· We develop a novel progressive tokenization for ViT\nand demonstrate its advantage over the simple tok-\nenization approach by ViT, and we propose a T2T\nmodule that can encode the important local structure\nfor each token."
        },
        {
            "bounding_box": [
                {
                    "x": 250,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 554
                },
                {
                    "x": 250,
                    "y": 554
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:14px'>· We show the architecture engineering of CNNs can<br>benefit the backbone design of ViT to improve the fea-<br>ture richness and reduce redundancy. Through exten-<br>sive experiments, we find deep-narrow architecture de-<br>sign works best for ViT.</p>",
            "id": 21,
            "page": 3,
            "text": "· We show the architecture engineering of CNNs can\nbenefit the backbone design of ViT to improve the fea-\nture richness and reduce redundancy. Through exten-\nsive experiments, we find deep-narrow architecture de-\nsign works best for ViT."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 647
                },
                {
                    "x": 558,
                    "y": 647
                },
                {
                    "x": 558,
                    "y": 697
                },
                {
                    "x": 202,
                    "y": 697
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:20px'>2. Related Work</p>",
            "id": 22,
            "page": 3,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 722
                },
                {
                    "x": 1200,
                    "y": 722
                },
                {
                    "x": 1200,
                    "y": 2076
                },
                {
                    "x": 199,
                    "y": 2076
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:16px'>Transformers in Vision Transformers [37] are the mod-<br>els that entirely rely on the self-attention mechanism<br>to draw global dependencies between input and output,<br>and currently they have dominated natural language mod-<br>elling [10, 30, 2, 46, 29, 23]. A transformer layer usu-<br>ally consists of a multi-head self-attention layer (MSA) and<br>an MLP block. Layernorm (LN) is applied before each<br>layer and residual connections in both the self-attention<br>layer and MLP block. Recent works have explored apply-<br>ing transformers to various vision tasks: image classifica-<br>tion [5, 12], object detection [3, 61, 58, 8, 34], segmen-<br>tation [4, 40], image enhancement [4, 45], image genera-<br>tion [27], video processing [60, 53], and 3D point cloud<br>processing [56]. Among them, the Vision Transformer<br>(ViT) proves that a pure Transformer architecture can also<br>attain state-of-the-art performance on image classification.<br>However, ViT heavily relies on large-scale datasets such as<br>ImageNet-21k and JFT-300M (which is not publically avail-<br>able) for model pretraining, requiring huge computation re-<br>sources. In contrast, our proposed T2T- ViT is more efficient<br>and can be trained on ImageNet without using those large-<br>scale datasets. A recent concurrent work DeiT [36] applies<br>Knowledge Distillation [16, 49] to improve the original ViT<br>by adding a KD token along with the class token, which is<br>orthogonal to our work, as our T2T-ViT focuses on the ar-<br>chitecture design, and our T2T-ViT can achieve higher per-<br>formance than DeiT without CNN as teacher model.</p>",
            "id": 23,
            "page": 3,
            "text": "Transformers in Vision Transformers [37] are the mod-\nels that entirely rely on the self-attention mechanism\nto draw global dependencies between input and output,\nand currently they have dominated natural language mod-\nelling [10, 30, 2, 46, 29, 23]. A transformer layer usu-\nally consists of a multi-head self-attention layer (MSA) and\nan MLP block. Layernorm (LN) is applied before each\nlayer and residual connections in both the self-attention\nlayer and MLP block. Recent works have explored apply-\ning transformers to various vision tasks: image classifica-\ntion [5, 12], object detection [3, 61, 58, 8, 34], segmen-\ntation [4, 40], image enhancement [4, 45], image genera-\ntion [27], video processing [60, 53], and 3D point cloud\nprocessing [56]. Among them, the Vision Transformer\n(ViT) proves that a pure Transformer architecture can also\nattain state-of-the-art performance on image classification.\nHowever, ViT heavily relies on large-scale datasets such as\nImageNet-21k and JFT-300M (which is not publically avail-\nable) for model pretraining, requiring huge computation re-\nsources. In contrast, our proposed T2T- ViT is more efficient\nand can be trained on ImageNet without using those large-\nscale datasets. A recent concurrent work DeiT [36] applies\nKnowledge Distillation [16, 49] to improve the original ViT\nby adding a KD token along with the class token, which is\northogonal to our work, as our T2T-ViT focuses on the ar-\nchitecture design, and our T2T-ViT can achieve higher per-\nformance than DeiT without CNN as teacher model."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2174
                },
                {
                    "x": 1200,
                    "y": 2174
                },
                {
                    "x": 1200,
                    "y": 2977
                },
                {
                    "x": 199,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:16px'>Self-attention in CNNs Self-attention mechanism has<br>been widely applied to CNNs in vision task [38, 57, 19,<br>47, 20, 39, 1, 6, 18, 31, 42, 13, 50, 48]. Among these<br>works, the SE block [20] applies attention to channel di-<br>mensions and non-local networks [39] are designed for cap-<br>turing long-range dependencies via global attention. Com-<br>pared with most of the works exploring global attention on<br>images [1, 42, 13, 39], some works [18, 31] also explore<br>self-attention in a local patch to reduce the memory and<br>computation cost. More recently, SAN [55] investigates<br>both pairwise and patchwise self-attention for image recog-<br>nition, where the patchwise self-attention is a generalization<br>of convolution. In this work, we also replace the T2T mod-<br>ule with multiple convolution layers in experiments and find<br>the convolution layers do not perform better than our de-<br>signed T2T module.</p>",
            "id": 24,
            "page": 3,
            "text": "Self-attention in CNNs Self-attention mechanism has\nbeen widely applied to CNNs in vision task [38, 57, 19,\n47, 20, 39, 1, 6, 18, 31, 42, 13, 50, 48]. Among these\nworks, the SE block [20] applies attention to channel di-\nmensions and non-local networks [39] are designed for cap-\nturing long-range dependencies via global attention. Com-\npared with most of the works exploring global attention on\nimages [1, 42, 13, 39], some works [18, 31] also explore\nself-attention in a local patch to reduce the memory and\ncomputation cost. More recently, SAN [55] investigates\nboth pairwise and patchwise self-attention for image recog-\nnition, where the patchwise self-attention is a generalization\nof convolution. In this work, we also replace the T2T mod-\nule with multiple convolution layers in experiments and find\nthe convolution layers do not perform better than our de-\nsigned T2T module."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 289
                },
                {
                    "x": 2234,
                    "y": 289
                },
                {
                    "x": 2234,
                    "y": 957
                },
                {
                    "x": 1324,
                    "y": 957
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='25' style='font-size:20px' alt=\"T2T process\nTokens to Token\nT2T\nT2T\nTransformer\nTi Transformer Reshape Unfold\nTi+1\nTi\nstep1: re-structurization step2: soft split1 next T2T\" data-coord=\"top-left:(1324,289); bottom-right:(2234,957)\" /></figure>",
            "id": 25,
            "page": 3,
            "text": "T2T process\nTokens to Token\nT2T\nT2T\nTransformer\nTi Transformer Reshape Unfold\nTi+1\nTi\nstep1: re-structurization step2: soft split1 next T2T"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 959
                },
                {
                    "x": 2280,
                    "y": 959
                },
                {
                    "x": 2280,
                    "y": 1285
                },
                {
                    "x": 1278,
                    "y": 1285
                }
            ],
            "category": "caption",
            "html": "<br><caption id='26' style='font-size:14px'>Figure 3. Illustration of T2T process. The tokens Ti are re-<br>structurized as an image Ii after transformation and reshaping;<br>then Ii is split with overlapping to tokens Ti+1 again. Specifically,<br>as shown in the pink panel, the four tokens (1,2,4,5) of the input<br>Ii are concatenated to form one token in Ti+1. The T2T trans-<br>former can be a normal Transformer layer [37] or other efficient<br>transformers like Performer layer [34] at limited GPU memory.</caption>",
            "id": 26,
            "page": 3,
            "text": "Figure 3. Illustration of T2T process. The tokens Ti are re-\nstructurized as an image Ii after transformation and reshaping;\nthen Ii is split with overlapping to tokens Ti+1 again. Specifically,\nas shown in the pink panel, the four tokens (1,2,4,5) of the input\nIi are concatenated to form one token in Ti+1. The T2T trans-\nformer can be a normal Transformer layer [37] or other efficient\ntransformers like Performer layer [34] at limited GPU memory."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1314
                },
                {
                    "x": 1785,
                    "y": 1314
                },
                {
                    "x": 1785,
                    "y": 1368
                },
                {
                    "x": 1282,
                    "y": 1368
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:22px'>3. Tokens-to-Token ViT</p>",
            "id": 27,
            "page": 3,
            "text": "3. Tokens-to-Token ViT"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1401
                },
                {
                    "x": 2279,
                    "y": 1401
                },
                {
                    "x": 2279,
                    "y": 2099
                },
                {
                    "x": 1277,
                    "y": 2099
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>To overcome the limitations of simple tokenization and<br>inefficient backbone of ViT, we propose Tokens-to-Token<br>Vision Transformer (T2T-ViT) which can progressively to-<br>kenize the image to tokens and has an efficient backbone.<br>Hence, T2T-ViT consists of two main components (Fig. 4):<br>1) a layer-wise \"Tokens-to-Token module\" (T2T module)<br>to model the local structure information of the image and<br>reduce the length of tokens progressively; 2) an efficient<br>\"T2T-ViT backbone\" to draw the global attention relation<br>on tokens from the T2T module. We adopt a deep-narrow<br>structure for the backbone to reduce redundancy and im-<br>prove the feature richness after exploring several CNN-<br>based architecture designs. We now explain these compo-<br>nents one by one.</p>",
            "id": 28,
            "page": 3,
            "text": "To overcome the limitations of simple tokenization and\ninefficient backbone of ViT, we propose Tokens-to-Token\nVision Transformer (T2T-ViT) which can progressively to-\nkenize the image to tokens and has an efficient backbone.\nHence, T2T-ViT consists of two main components (Fig. 4):\n1) a layer-wise \"Tokens-to-Token module\" (T2T module)\nto model the local structure information of the image and\nreduce the length of tokens progressively; 2) an efficient\n\"T2T-ViT backbone\" to draw the global attention relation\non tokens from the T2T module. We adopt a deep-narrow\nstructure for the backbone to reduce redundancy and im-\nprove the feature richness after exploring several CNN-\nbased architecture designs. We now explain these compo-\nnents one by one."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2132
                },
                {
                    "x": 2215,
                    "y": 2132
                },
                {
                    "x": 2215,
                    "y": 2184
                },
                {
                    "x": 1280,
                    "y": 2184
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:18px'>3.1. Tokens-to-Token: Progressive Tokenization</p>",
            "id": 29,
            "page": 3,
            "text": "3.1. Tokens-to-Token: Progressive Tokenization"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2211
                },
                {
                    "x": 2277,
                    "y": 2211
                },
                {
                    "x": 2277,
                    "y": 2510
                },
                {
                    "x": 1279,
                    "y": 2510
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:14px'>The Token-to-Token (T2T) module aims to overcome the<br>limitation of simple tokenization in ViT. It progressively<br>structurizes an image to tokens and models the local struc-<br>ture information, and in this way the length of tokens can<br>be reduced iteratively. Each T2T process has two steps: Re-<br>structurization and Soft Split (SS) (Fig. 3).</p>",
            "id": 30,
            "page": 3,
            "text": "The Token-to-Token (T2T) module aims to overcome the\nlimitation of simple tokenization in ViT. It progressively\nstructurizes an image to tokens and models the local struc-\nture information, and in this way the length of tokens can\nbe reduced iteratively. Each T2T process has two steps: Re-\nstructurization and Soft Split (SS) (Fig. 3)."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2538
                },
                {
                    "x": 2278,
                    "y": 2538
                },
                {
                    "x": 2278,
                    "y": 2736
                },
                {
                    "x": 1280,
                    "y": 2736
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:16px'>Re-structurization As shown in Fig. 3, given a sequence<br>of tokens T from the preceding transformer layer, it will<br>be transformed by the self-attention block (the T2T trans-<br>former in Fig. 3):</p>",
            "id": 31,
            "page": 3,
            "text": "Re-structurization As shown in Fig. 3, given a sequence\nof tokens T from the preceding transformer layer, it will\nbe transformed by the self-attention block (the T2T trans-\nformer in Fig. 3):"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2876
                },
                {
                    "x": 2277,
                    "y": 2876
                },
                {
                    "x": 2277,
                    "y": 2979
                },
                {
                    "x": 1281,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>where MSA denotes the multihead self-attention operation<br>with layer normalization and \"MLP\" is the multilayer per-</p>",
            "id": 32,
            "page": 3,
            "text": "where MSA denotes the multihead self-attention operation\nwith layer normalization and \"MLP\" is the multilayer per-"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 310
                },
                {
                    "x": 1199,
                    "y": 310
                },
                {
                    "x": 1199,
                    "y": 453
                },
                {
                    "x": 201,
                    "y": 453
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>ceptron with layer normalization in the standard Trans-<br>former [12]. Then the tokens T' will be reshaped as an<br>image in the spatial dimension,</p>",
            "id": 33,
            "page": 4,
            "text": "ceptron with layer normalization in the standard Trans-\nformer [12]. Then the tokens T' will be reshaped as an\nimage in the spatial dimension,"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 565
                },
                {
                    "x": 1198,
                    "y": 565
                },
                {
                    "x": 1198,
                    "y": 711
                },
                {
                    "x": 201,
                    "y": 711
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:14px'>Here \"Reshape\" re-organizes tokens T' E Rlxc to I E<br>Rhxwxc 1 is the length of T' , h, w, c are height,<br>, where<br>width and channel respectively, and 1 = h x w.</p>",
            "id": 34,
            "page": 4,
            "text": "Here \"Reshape\" re-organizes tokens T' E Rlxc to I E\nRhxwxc 1 is the length of T' , h, w, c are height,\n, where\nwidth and channel respectively, and 1 = h x w."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 732
                },
                {
                    "x": 1198,
                    "y": 732
                },
                {
                    "x": 1198,
                    "y": 1276
                },
                {
                    "x": 202,
                    "y": 1276
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:18px'>Soft Split As shown in Fig. 3, after obtaining the re-<br>structurized image I, we apply the soft split on it to model<br>local structure information and reduce length of tokens.<br>Specifically, to avoid information loss in generating tokens<br>from the re-structurizated image, we split it into patches<br>with overlapping. As such, each patch is correlated with<br>surrounding patches to establish a prior that there should<br>be stronger correlations between surrounding tokens. The<br>tokens in each split patch are concatenated as one token<br>(Tokens-to-Token, Fig. 3), and thus the local information<br>can be aggregated from surrounding pixels and patches.</p>",
            "id": 35,
            "page": 4,
            "text": "Soft Split As shown in Fig. 3, after obtaining the re-\nstructurized image I, we apply the soft split on it to model\nlocal structure information and reduce length of tokens.\nSpecifically, to avoid information loss in generating tokens\nfrom the re-structurizated image, we split it into patches\nwith overlapping. As such, each patch is correlated with\nsurrounding patches to establish a prior that there should\nbe stronger correlations between surrounding tokens. The\ntokens in each split patch are concatenated as one token\n(Tokens-to-Token, Fig. 3), and thus the local information\ncan be aggregated from surrounding pixels and patches."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1285
                },
                {
                    "x": 1200,
                    "y": 1285
                },
                {
                    "x": 1200,
                    "y": 1526
                },
                {
                    "x": 202,
                    "y": 1526
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:18px'>When conducting the soft split, the size of each patch is<br>k x k with s overlapping and p padding on the image, where<br>k - s is similar to the stride in convolution operation. So for<br>the reconstructed image I E Rhxwxc, the length of output<br>tokens To after soft split is</p>",
            "id": 36,
            "page": 4,
            "text": "When conducting the soft split, the size of each patch is\nk x k with s overlapping and p padding on the image, where\nk - s is similar to the stride in convolution operation. So for\nthe reconstructed image I E Rhxwxc, the length of output\ntokens To after soft split is"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1687
                },
                {
                    "x": 1197,
                    "y": 1687
                },
                {
                    "x": 1197,
                    "y": 1837
                },
                {
                    "x": 201,
                    "y": 1837
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>Each split patch has size k x k x c. We flatten all patches in<br>spatial dimensions to tokens T。 E Rloxck2<br>· After the soft<br>split, the output tokens are fed for the next T2T process.</p>",
            "id": 37,
            "page": 4,
            "text": "Each split patch has size k x k x c. We flatten all patches in\nspatial dimensions to tokens T。 E Rloxck2\n· After the soft\nsplit, the output tokens are fed for the next T2T process."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1858
                },
                {
                    "x": 1200,
                    "y": 1858
                },
                {
                    "x": 1200,
                    "y": 2102
                },
                {
                    "x": 201,
                    "y": 2102
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:18px'>T2T module By conducting the above Re-structurization<br>and Soft Split iteratively, the T2T module can progressively<br>reduce the length of tokens and transform the spatial struc-<br>ture of the image. The iterative process in T2T module can<br>be formulated as</p>",
            "id": 38,
            "page": 4,
            "text": "T2T module By conducting the above Re-structurization\nand Soft Split iteratively, the T2T module can progressively\nreduce the length of tokens and transform the spatial struc-\nture of the image. The iterative process in T2T module can\nbe formulated as"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2331
                },
                {
                    "x": 1198,
                    "y": 2331
                },
                {
                    "x": 1198,
                    "y": 2524
                },
                {
                    "x": 201,
                    "y": 2524
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>For the input image Io, we apply a soft split at first to split<br>it to tokens: T1 = SS(Io). After the final iteration, the<br>output tokens Tf of the T2T module has fixed length, SO the<br>backbone of T2T-ViT can model the global relation on Tf.</p>",
            "id": 39,
            "page": 4,
            "text": "For the input image Io, we apply a soft split at first to split\nit to tokens: T1 = SS(Io). After the final iteration, the\noutput tokens Tf of the T2T module has fixed length, SO the\nbackbone of T2T-ViT can model the global relation on Tf."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2529
                },
                {
                    "x": 1199,
                    "y": 2529
                },
                {
                    "x": 1199,
                    "y": 2976
                },
                {
                    "x": 201,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:16px'>Additionally, as the length of tokens in the T2T module<br>is larger than the normal case (16 x 16) in ViT, the MACs<br>and memory usage are huge. To address the limitations, in<br>our T2T module, we set the channel dimension of the T2T<br>layer small (32 or 64) to reduce MACs, and optionally adopt<br>an efficient Transformer such as Performer [7] layer to re-<br>duce memory usage at limited GPU memory. We provide an<br>ablation study on the difference between adopting standard<br>Transformer layer and Performer layer in our experiments.</p>",
            "id": 40,
            "page": 4,
            "text": "Additionally, as the length of tokens in the T2T module\nis larger than the normal case (16 x 16) in ViT, the MACs\nand memory usage are huge. To address the limitations, in\nour T2T module, we set the channel dimension of the T2T\nlayer small (32 or 64) to reduce MACs, and optionally adopt\nan efficient Transformer such as Performer [7] layer to re-\nduce memory usage at limited GPU memory. We provide an\nablation study on the difference between adopting standard\nTransformer layer and Performer layer in our experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 305
                },
                {
                    "x": 1744,
                    "y": 305
                },
                {
                    "x": 1744,
                    "y": 351
                },
                {
                    "x": 1280,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:22px'>3.2. T2T- ViT Backbone</p>",
            "id": 41,
            "page": 4,
            "text": "3.2. T2T- ViT Backbone"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 385
                },
                {
                    "x": 2278,
                    "y": 385
                },
                {
                    "x": 2278,
                    "y": 1029
                },
                {
                    "x": 1278,
                    "y": 1029
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>As many channels in the backbone of vanilla ViT are in-<br>valid (Fig. 2), we plan to find an efficient backbone for our<br>T2T-ViT to reduce the redundancy and improve the feature<br>richness. Thus we explore different architecture designs for<br>ViT and borrow some designs from CNNs to improve the<br>backbone efficiency and enhance the richness of the learned<br>features. As each transformer layer has skip connection as<br>ResNets, a straightforward idea is to apply dense connec-<br>tion as DenseNet [21] to increase the connectivity and fea-<br>ture richness, or apply Wide-ResNets or ResNeXt structure<br>to change the channel dimension and head number in the<br>backbone of ViT. We explore five architecture designs from<br>CNNs to ViT:</p>",
            "id": 42,
            "page": 4,
            "text": "As many channels in the backbone of vanilla ViT are in-\nvalid (Fig. 2), we plan to find an efficient backbone for our\nT2T-ViT to reduce the redundancy and improve the feature\nrichness. Thus we explore different architecture designs for\nViT and borrow some designs from CNNs to improve the\nbackbone efficiency and enhance the richness of the learned\nfeatures. As each transformer layer has skip connection as\nResNets, a straightforward idea is to apply dense connec-\ntion as DenseNet [21] to increase the connectivity and fea-\nture richness, or apply Wide-ResNets or ResNeXt structure\nto change the channel dimension and head number in the\nbackbone of ViT. We explore five architecture designs from\nCNNs to ViT:"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 1076
                },
                {
                    "x": 2279,
                    "y": 1076
                },
                {
                    "x": 2279,
                    "y": 1669
                },
                {
                    "x": 1310,
                    "y": 1669
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:18px'>1. Dense connection as DenseNet [21];<br>2. Deep-narrow VS. shallow-wide structure as in Wide-<br>ResNets [52];<br>3. Channel attention as Squeeze-an-Excitation (SE) Net-<br>works [20];<br>4. More split heads in multi-head attention layer as<br>ResNeXt [44];<br>5. Ghost operations as GhostNet [14].</p>",
            "id": 43,
            "page": 4,
            "text": "1. Dense connection as DenseNet [21];\n2. Deep-narrow VS. shallow-wide structure as in Wide-\nResNets [52];\n3. Channel attention as Squeeze-an-Excitation (SE) Net-\nworks [20];\n4. More split heads in multi-head attention layer as\nResNeXt [44];\n5. Ghost operations as GhostNet [14]."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1711
                },
                {
                    "x": 2277,
                    "y": 1711
                },
                {
                    "x": 2277,
                    "y": 2206
                },
                {
                    "x": 1279,
                    "y": 2206
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:18px'>The details of these structure designs in ViT are given in the<br>appendix. We conduct extensive experiments on the struc-<br>tures transferring in Sec. 4.2. We empirically find that 1)<br>by adopting a deep-narrow structure that simply decreases<br>channel dimensions to reduce the redundancy in channels<br>and increase layer depth to improve feature richness in ViT,<br>both the model size and MACs are decreased but perfor-<br>mance is improved; 2) the channel attention as SE block<br>also improves ViT but is less effective than using the deep-<br>narrow structure.</p>",
            "id": 44,
            "page": 4,
            "text": "The details of these structure designs in ViT are given in the\nappendix. We conduct extensive experiments on the struc-\ntures transferring in Sec. 4.2. We empirically find that 1)\nby adopting a deep-narrow structure that simply decreases\nchannel dimensions to reduce the redundancy in channels\nand increase layer depth to improve feature richness in ViT,\nboth the model size and MACs are decreased but perfor-\nmance is improved; 2) the channel attention as SE block\nalso improves ViT but is less effective than using the deep-\nnarrow structure."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2215
                },
                {
                    "x": 2278,
                    "y": 2215
                },
                {
                    "x": 2278,
                    "y": 2561
                },
                {
                    "x": 1278,
                    "y": 2561
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:16px'>Based on these findings, we design a deep-narrow ar-<br>chitecture for our T2T-ViT backbone. Specifically, it has a<br>small channel number and a hidden dimension d but more<br>layers 6. For tokens with fixed length Tf from the last layer<br>of T2T module, we concatenate a class token to it and then<br>add Sinusoidal Position Embedding (PE) to it, the same as<br>ViT to do classification:</p>",
            "id": 45,
            "page": 4,
            "text": "Based on these findings, we design a deep-narrow ar-\nchitecture for our T2T-ViT backbone. Specifically, it has a\nsmall channel number and a hidden dimension d but more\nlayers 6. For tokens with fixed length Tf from the last layer\nof T2T module, we concatenate a class token to it and then\nadd Sinusoidal Position Embedding (PE) to it, the same as\nViT to do classification:"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2828
                },
                {
                    "x": 2277,
                    "y": 2828
                },
                {
                    "x": 2277,
                    "y": 2975
                },
                {
                    "x": 1279,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:18px'>where Eis Sinusoidal Position Embedding, LN is layer nor-<br>malization, fc is one fully-connected layer for classification<br>and y is the output prediction.</p>",
            "id": 46,
            "page": 4,
            "text": "where Eis Sinusoidal Position Embedding, LN is layer nor-\nmalization, fc is one fully-connected layer for classification\nand y is the output prediction."
        },
        {
            "bounding_box": [
                {
                    "x": 435,
                    "y": 290
                },
                {
                    "x": 2047,
                    "y": 290
                },
                {
                    "x": 2047,
                    "y": 859
                },
                {
                    "x": 435,
                    "y": 859
                }
            ],
            "category": "figure",
            "html": "<figure><img id='47' style='font-size:22px' alt=\"Tokens-to-Token module T2T-ViT Backbone\nImage\n224 x 224\nTransformer\nTransformer\n7 T2T T2T Fixed Tokens\nTransformer\nUnfold Transformer T2T T2T + PE\n:\n7\n+\ncls token layer\nlayer\nTf\nMLP\nclass\nlo Head\nT1\" data-coord=\"top-left:(435,290); bottom-right:(2047,859)\" /></figure>",
            "id": 47,
            "page": 5,
            "text": "Tokens-to-Token module T2T-ViT Backbone\nImage\n224 x 224\nTransformer\nTransformer\n7 T2T T2T Fixed Tokens\nTransformer\nUnfold Transformer T2T T2T + PE\n:\n7\n+\ncls token layer\nlayer\nTf\nMLP\nclass\nlo Head\nT1"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 869
                },
                {
                    "x": 2279,
                    "y": 869
                },
                {
                    "x": 2279,
                    "y": 1055
                },
                {
                    "x": 199,
                    "y": 1055
                }
            ],
            "category": "caption",
            "html": "<br><caption id='48' style='font-size:14px'>Figure 4. The overall network architecture of T2T-ViT. In the T2T module, the input image is first soft split as patches, and then unfolded<br>as a sequence of tokens To. The length of tokens is reduced progressively in the T2T module (we use two iterations here and output Tf).<br>Then the T2T-ViT backbone takes the fixed tokens as input and outputs the predictions. The two T2T blocks are the same as Fig. 3 and PE<br>is Position Embedding.</caption>",
            "id": 48,
            "page": 5,
            "text": "Figure 4. The overall network architecture of T2T-ViT. In the T2T module, the input image is first soft split as patches, and then unfolded\nas a sequence of tokens To. The length of tokens is reduced progressively in the T2T module (we use two iterations here and output Tf).\nThen the T2T-ViT backbone takes the fixed tokens as input and outputs the predictions. The two T2T blocks are the same as Fig. 3 and PE\nis Position Embedding."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1068
                },
                {
                    "x": 2279,
                    "y": 1068
                },
                {
                    "x": 2279,
                    "y": 1253
                },
                {
                    "x": 200,
                    "y": 1253
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:16px'>Table 1. Structure details of T2T-ViT. T2T-ViT-14/19/24 have comparable model size with ResNet50/101/152. T2T-ViT-7/12 have com-<br>parable model size with MobileNetV1/V2. For T2T transformer layer, we adopt Transformer layer for T2T-ViTt-14 and Performer layer<br>for T2T-ViT-14 at limited GPU memory. For ViT, 'S' means Small, 'B' is Base and 'L' is Large. 'ViT-S/16' is a variant from original<br>ViT-B/16 [12] with smaller MLP size and layer depth.</p>",
            "id": 49,
            "page": 5,
            "text": "Table 1. Structure details of T2T-ViT. T2T-ViT-14/19/24 have comparable model size with ResNet50/101/152. T2T-ViT-7/12 have com-\nparable model size with MobileNetV1/V2. For T2T transformer layer, we adopt Transformer layer for T2T-ViTt-14 and Performer layer\nfor T2T-ViT-14 at limited GPU memory. For ViT, 'S' means Small, 'B' is Base and 'L' is Large. 'ViT-S/16' is a variant from original\nViT-B/16 [12] with smaller MLP size and layer depth."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1253
                },
                {
                    "x": 1970,
                    "y": 1253
                },
                {
                    "x": 1970,
                    "y": 1946
                },
                {
                    "x": 510,
                    "y": 1946
                }
            ],
            "category": "table",
            "html": "<br><table id='50' style='font-size:14px'><tr><td rowspan=\"2\">Models</td><td colspan=\"4\">Tokens-to-Token module</td><td colspan=\"3\">T2T-ViT backbone</td><td colspan=\"2\">Model size</td></tr><tr><td>T2T transformer</td><td>Depth</td><td>Hidden dim</td><td>MLP size</td><td>Depth</td><td>Hidden dim</td><td>MLP size</td><td>Params (M)</td><td>MACs (G)</td></tr><tr><td>ViT-S/16 [12]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8</td><td>786</td><td>2358</td><td>48.6</td><td>10.1</td></tr><tr><td>ViT-B/16 [12]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>12</td><td>786</td><td>3072</td><td>86.8</td><td>17.6</td></tr><tr><td>ViT-L/16 [12]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>24</td><td>1024</td><td>4096</td><td>304.3</td><td>63.6</td></tr><tr><td>T2T-ViT-14</td><td>Performer</td><td>2</td><td>64</td><td>64</td><td>14</td><td>384</td><td>1152</td><td>21.5</td><td>4.8</td></tr><tr><td>T2T-ViT-19</td><td>Performer</td><td>2</td><td>64</td><td>64</td><td>19</td><td>448</td><td>1344</td><td>39.2</td><td>8.5</td></tr><tr><td>T2T-ViT-24</td><td>Performer</td><td>2</td><td>64</td><td>64</td><td>24</td><td>512</td><td>1536</td><td>64.1</td><td>13.8</td></tr><tr><td>T2T-ViTt-14</td><td>Transformer</td><td>2</td><td>64</td><td>64</td><td>14</td><td>384</td><td>1152</td><td>21.5</td><td>6.1</td></tr><tr><td>T2T-ViT-7</td><td>Performer</td><td>2</td><td>64</td><td>64</td><td>8</td><td>256</td><td>512</td><td>4.2</td><td>1.1</td></tr><tr><td>T2T-ViT-12</td><td>Performer</td><td>2</td><td>64</td><td>64</td><td>12</td><td>256</td><td>512</td><td>6.8</td><td>1.8</td></tr></table>",
            "id": 50,
            "page": 5,
            "text": "Models Tokens-to-Token module T2T-ViT backbone Model size\n T2T transformer Depth Hidden dim MLP size Depth Hidden dim MLP size Params (M) MACs (G)\n ViT-S/16 [12] - - - - 8 786 2358 48.6 10.1\n ViT-B/16 [12] - - - - 12 786 3072 86.8 17.6\n ViT-L/16 [12] - - - - 24 1024 4096 304.3 63.6\n T2T-ViT-14 Performer 2 64 64 14 384 1152 21.5 4.8\n T2T-ViT-19 Performer 2 64 64 19 448 1344 39.2 8.5\n T2T-ViT-24 Performer 2 64 64 24 512 1536 64.1 13.8\n T2T-ViTt-14 Transformer 2 64 64 14 384 1152 21.5 6.1\n T2T-ViT-7 Performer 2 64 64 8 256 512 4.2 1.1\n T2T-ViT-12 Performer 2 64 64 12 256 512 6.8"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1974
                },
                {
                    "x": 722,
                    "y": 1974
                },
                {
                    "x": 722,
                    "y": 2025
                },
                {
                    "x": 200,
                    "y": 2025
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:20px'>3.3. T2T-ViT Architecture</p>",
            "id": 51,
            "page": 5,
            "text": "3.3. T2T-ViT Architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2061
                },
                {
                    "x": 1201,
                    "y": 2061
                },
                {
                    "x": 1201,
                    "y": 2459
                },
                {
                    "x": 200,
                    "y": 2459
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:16px'>The T2T-ViT has two parts: the Tokens-to-Token (T2T)<br>module and the T2T-ViT backbone (Fig. 4). There are var-<br>ious possible design choices for the T2T module. Here, we<br>set n = 2 as shown in Fig. 4, which means there is n+1 = 3<br>soft split and n = 2 re-structurization in T2T module. The<br>patch size for the three soft splits is P = [7, 3, 3], and the<br>overlapping is S = [3, 1, 1], which reduces size of the input<br>image from 224 x 224 to 14 x 14 according to Eqn. (3).</p>",
            "id": 52,
            "page": 5,
            "text": "The T2T-ViT has two parts: the Tokens-to-Token (T2T)\nmodule and the T2T-ViT backbone (Fig. 4). There are var-\nious possible design choices for the T2T module. Here, we\nset n = 2 as shown in Fig. 4, which means there is n+1 = 3\nsoft split and n = 2 re-structurization in T2T module. The\npatch size for the three soft splits is P = [7, 3, 3], and the\noverlapping is S = [3, 1, 1], which reduces size of the input\nimage from 224 x 224 to 14 x 14 according to Eqn. (3)."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2468
                },
                {
                    "x": 1201,
                    "y": 2468
                },
                {
                    "x": 1201,
                    "y": 2866
                },
                {
                    "x": 200,
                    "y": 2866
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:18px'>The T2T-ViT backbone takes tokens with fixed length<br>from the T2T module as input, the same as ViT; but has<br>a deep-narrow architecture design with smaller hidden di-<br>mensions (256-512) and MLP size (512-1536) than ViT. For<br>example, T2T-ViT-14 has 14 transformer layers in T2T-ViT<br>backbone with 384 hidden dimensions, while ViT-B/16 has<br>12 transformer layers and 768 hidden dimensions, which is<br>3x larger than T2T-ViT-14 in parameters and MACs.</p>",
            "id": 53,
            "page": 5,
            "text": "The T2T-ViT backbone takes tokens with fixed length\nfrom the T2T module as input, the same as ViT; but has\na deep-narrow architecture design with smaller hidden di-\nmensions (256-512) and MLP size (512-1536) than ViT. For\nexample, T2T-ViT-14 has 14 transformer layers in T2T-ViT\nbackbone with 384 hidden dimensions, while ViT-B/16 has\n12 transformer layers and 768 hidden dimensions, which is\n3x larger than T2T-ViT-14 in parameters and MACs."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2877
                },
                {
                    "x": 1198,
                    "y": 2877
                },
                {
                    "x": 1198,
                    "y": 2976
                },
                {
                    "x": 203,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:16px'>To fairly compare with common hand-designed CNNs,<br>we make T2T-ViT models have comparable size with</p>",
            "id": 54,
            "page": 5,
            "text": "To fairly compare with common hand-designed CNNs,\nwe make T2T-ViT models have comparable size with"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1976
                },
                {
                    "x": 2278,
                    "y": 1976
                },
                {
                    "x": 2278,
                    "y": 2477
                },
                {
                    "x": 1279,
                    "y": 2477
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:18px'>ResNets and MobileNets. Specifically, we design three<br>models: T2T- ViT-14, T2T-ViT-19 and T2T-ViT-24 of<br>comparable parameters with ResNet50, ResNet101 and<br>ResNet152 respectively. To compare with small models like<br>MobileNets, we design two lite models: T2T-ViT-7, T2T-<br>ViT-12 with comparable model size with MibileNetV1 and<br>MibileNetV2. The two lite TiT-ViT have no special designs<br>or tricks like efficient convolution [26] and simply reduce<br>the layer depth, hidden dimension, and MLP ratio. The net-<br>work details are summarized in Tab. 1.</p>",
            "id": 55,
            "page": 5,
            "text": "ResNets and MobileNets. Specifically, we design three\nmodels: T2T- ViT-14, T2T-ViT-19 and T2T-ViT-24 of\ncomparable parameters with ResNet50, ResNet101 and\nResNet152 respectively. To compare with small models like\nMobileNets, we design two lite models: T2T-ViT-7, T2T-\nViT-12 with comparable model size with MibileNetV1 and\nMibileNetV2. The two lite TiT-ViT have no special designs\nor tricks like efficient convolution [26] and simply reduce\nthe layer depth, hidden dimension, and MLP ratio. The net-\nwork details are summarized in Tab. 1."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2535
                },
                {
                    "x": 1612,
                    "y": 2535
                },
                {
                    "x": 1612,
                    "y": 2590
                },
                {
                    "x": 1279,
                    "y": 2590
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>4. Experiments</p>",
            "id": 56,
            "page": 5,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2627
                },
                {
                    "x": 2279,
                    "y": 2627
                },
                {
                    "x": 2279,
                    "y": 2978
                },
                {
                    "x": 1280,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>We conduct the following experiments with T2T-ViT for<br>image classification on ImageNet. a) We validate the T2T-<br>ViT by training from scratch on ImageNet and compare it<br>with some common convolutional neural networks such as<br>ResNets and MobileNets of comparable size; we also trans-<br>fer the pretrained T2T-ViT to downstream datasets such<br>as CIFAR10 and CIFAR100 (Sec. 4.1). (b) We compare</p>",
            "id": 57,
            "page": 5,
            "text": "We conduct the following experiments with T2T-ViT for\nimage classification on ImageNet. a) We validate the T2T-\nViT by training from scratch on ImageNet and compare it\nwith some common convolutional neural networks such as\nResNets and MobileNets of comparable size; we also trans-\nfer the pretrained T2T-ViT to downstream datasets such\nas CIFAR10 and CIFAR100 (Sec. 4.1). (b) We compare"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 306
                },
                {
                    "x": 1198,
                    "y": 306
                },
                {
                    "x": 1198,
                    "y": 502
                },
                {
                    "x": 201,
                    "y": 502
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:20px'>five T2T-ViT backbone architecture designs inspired from<br>CNNs (Sec. 4.2). (c) We conduct ablation study to demon-<br>strate effects of the T2T module and the deep-narrow archi-<br>tecture design of T2T-ViT (Sec. 4.3).</p>",
            "id": 58,
            "page": 6,
            "text": "five T2T-ViT backbone architecture designs inspired from\nCNNs (Sec. 4.2). (c) We conduct ablation study to demon-\nstrate effects of the T2T module and the deep-narrow archi-\ntecture design of T2T-ViT (Sec. 4.3)."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 516
                },
                {
                    "x": 722,
                    "y": 516
                },
                {
                    "x": 722,
                    "y": 563
                },
                {
                    "x": 204,
                    "y": 563
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:22px'>4.1. T2T-ViT on ImageNet</p>",
            "id": 59,
            "page": 6,
            "text": "4.1. T2T-ViT on ImageNet"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 593
                },
                {
                    "x": 1199,
                    "y": 593
                },
                {
                    "x": 1199,
                    "y": 1588
                },
                {
                    "x": 200,
                    "y": 1588
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:20px'>All experiments are conducted on ImageNet dataset [9],<br>with around 1.3 million images in training set and 50k im-<br>ages in validation set. We use batch size 512 or 1024 with 8<br>NVIDIA GPUs for training. We adopt Pytorch [28] library<br>and Pytorch image models library (timm) [41] to implement<br>our models and conduct all experiments. For fair compar-<br>isons, we implement the same training scheme for the CNN<br>models, ViT, and our T2T-ViT. Throughout the experiments<br>on ImageNet, we set default image size as 224 x 224 except<br>for some specific cases on 384 x 384, and adopt some com-<br>mon data augmentation methods such as mixup [54] and<br>cutmix [11, 51] for both CNN and ViT&T2T- ViT model<br>training, because ViT models need more training data to<br>reach reasonable performance. We train these models for<br>310 epochs, using AdamW [25] as the optimizer and CO-<br>sine learning rate decay [24]. The details of experiment set-<br>ting are given in appendix. We also use both Transformer<br>layer and Performer layer in T2T module for our models, re-<br>sulting in T2T-ViTt 14/19/24 (Transformer) and T2T-ViT-<br>14/19/24 (Performer).</p>",
            "id": 60,
            "page": 6,
            "text": "All experiments are conducted on ImageNet dataset [9],\nwith around 1.3 million images in training set and 50k im-\nages in validation set. We use batch size 512 or 1024 with 8\nNVIDIA GPUs for training. We adopt Pytorch [28] library\nand Pytorch image models library (timm) [41] to implement\nour models and conduct all experiments. For fair compar-\nisons, we implement the same training scheme for the CNN\nmodels, ViT, and our T2T-ViT. Throughout the experiments\non ImageNet, we set default image size as 224 x 224 except\nfor some specific cases on 384 x 384, and adopt some com-\nmon data augmentation methods such as mixup [54] and\ncutmix [11, 51] for both CNN and ViT&T2T- ViT model\ntraining, because ViT models need more training data to\nreach reasonable performance. We train these models for\n310 epochs, using AdamW [25] as the optimizer and CO-\nsine learning rate decay [24]. The details of experiment set-\nting are given in appendix. We also use both Transformer\nlayer and Performer layer in T2T module for our models, re-\nsulting in T2T-ViTt 14/19/24 (Transformer) and T2T-ViT-\n14/19/24 (Performer)."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1603
                },
                {
                    "x": 1199,
                    "y": 1603
                },
                {
                    "x": 1199,
                    "y": 2399
                },
                {
                    "x": 201,
                    "y": 2399
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:18px'>T2T-ViT vs. ViT We first compare performance of T2T-<br>ViT and ViT on ImageNet. The results are given in Tab. 2.<br>Our T2T-ViT is much smaller than ViT in number of pa-<br>rameters and MACs, yet giving higher performance. For<br>example, the small ViT model ViT-S/16 with 48.6M and<br>10.1G MACs has 78.1% top-1 accuracy when trained from<br>scratch on ImageNet, while our T2T-ViTt-14 with only<br>44.2% parameters and 51.5% MACs achieves more than<br>3.0% improvement (81.5%). If we compare T2T-ViTt-24<br>with ViT-L/16, the former reduces parameters and MACs<br>around 500% but achieves more than 1.0% improvement<br>on ImageNet. Comparing T2T-ViT-14 with DeiT-small and<br>DeiT-small-Distilled, our T2T-ViT can achieve higher accu-<br>racy without large CNN models as teacher to enhance ViT.<br>We also adopt higher image resolution as 384x384 and get<br>83.3% accuracy by our T2T-ViT-14↑384.</p>",
            "id": 61,
            "page": 6,
            "text": "T2T-ViT vs. ViT We first compare performance of T2T-\nViT and ViT on ImageNet. The results are given in Tab. 2.\nOur T2T-ViT is much smaller than ViT in number of pa-\nrameters and MACs, yet giving higher performance. For\nexample, the small ViT model ViT-S/16 with 48.6M and\n10.1G MACs has 78.1% top-1 accuracy when trained from\nscratch on ImageNet, while our T2T-ViTt-14 with only\n44.2% parameters and 51.5% MACs achieves more than\n3.0% improvement (81.5%). If we compare T2T-ViTt-24\nwith ViT-L/16, the former reduces parameters and MACs\naround 500% but achieves more than 1.0% improvement\non ImageNet. Comparing T2T-ViT-14 with DeiT-small and\nDeiT-small-Distilled, our T2T-ViT can achieve higher accu-\nracy without large CNN models as teacher to enhance ViT.\nWe also adopt higher image resolution as 384x384 and get\n83.3% accuracy by our T2T-ViT-14↑384."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2414
                },
                {
                    "x": 1199,
                    "y": 2414
                },
                {
                    "x": 1199,
                    "y": 2861
                },
                {
                    "x": 202,
                    "y": 2861
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:18px'>T2T-ViT VS. ResNet For fair comparisons, we set up<br>three T2T-ViT models that have similar model size and<br>MACs with ResNet50, ResNet101 and ResNet152. The ex-<br>perimental results are given in Tab. 3. The proposed T2T-<br>ViT achieves 1.4%-2.7% performance gain over ResNets<br>with similar model size and MACs. For example, compared<br>with ResNet50 of 25.5M parameters and 4.3G MACs, our<br>T2T-ViT-14 have 21.5M parameters and 4.8G MACs obtain<br>81.5% accuracy on ImageNet.</p>",
            "id": 62,
            "page": 6,
            "text": "T2T-ViT VS. ResNet For fair comparisons, we set up\nthree T2T-ViT models that have similar model size and\nMACs with ResNet50, ResNet101 and ResNet152. The ex-\nperimental results are given in Tab. 3. The proposed T2T-\nViT achieves 1.4%-2.7% performance gain over ResNets\nwith similar model size and MACs. For example, compared\nwith ResNet50 of 25.5M parameters and 4.3G MACs, our\nT2T-ViT-14 have 21.5M parameters and 4.8G MACs obtain\n81.5% accuracy on ImageNet."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2876
                },
                {
                    "x": 1198,
                    "y": 2876
                },
                {
                    "x": 1198,
                    "y": 2975
                },
                {
                    "x": 204,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:18px'>T2T-ViT VS. MobileNets The T2T-ViT-7 and T2T-ViT-<br>12 have similar model size with MobileNetV1 [17] and Mo-</p>",
            "id": 63,
            "page": 6,
            "text": "T2T-ViT VS. MobileNets The T2T-ViT-7 and T2T-ViT-\n12 have similar model size with MobileNetV1 [17] and Mo-"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 293
                },
                {
                    "x": 2275,
                    "y": 293
                },
                {
                    "x": 2275,
                    "y": 379
                },
                {
                    "x": 1282,
                    "y": 379
                }
            ],
            "category": "caption",
            "html": "<br><caption id='64' style='font-size:16px'>Table 2. Comparison between T2T-ViT and ViT by training from<br>scratch on ImageNet.</caption>",
            "id": 64,
            "page": 6,
            "text": "Table 2. Comparison between T2T-ViT and ViT by training from\nscratch on ImageNet."
        },
        {
            "bounding_box": [
                {
                    "x": 1306,
                    "y": 383
                },
                {
                    "x": 2238,
                    "y": 383
                },
                {
                    "x": 2238,
                    "y": 953
                },
                {
                    "x": 1306,
                    "y": 953
                }
            ],
            "category": "table",
            "html": "<br><table id='65' style='font-size:14px'><tr><td>Models</td><td>Top1-Acc (%)</td><td>Params (M)</td><td>MACs (G)</td></tr><tr><td>ViT-S/16 [12]</td><td>78.1</td><td>48.6</td><td>10.1</td></tr><tr><td>DeiT-small [36]</td><td>79.9</td><td>22.1</td><td>4.6</td></tr><tr><td>DeiT-small-Distilled [36]</td><td>81.2</td><td>22.1</td><td>4.7</td></tr><tr><td>T2T-ViT-14</td><td>81.5</td><td>21.5</td><td>4.8</td></tr><tr><td>T2T-ViT-14↑384</td><td>83.3</td><td>21.5</td><td>17.1</td></tr><tr><td>ViT-B/16 [12]</td><td>79.8</td><td>86.4</td><td>17.6</td></tr><tr><td>ViT-L/16 [12]</td><td>81.1</td><td>304.3</td><td>63.6</td></tr><tr><td>T2T-ViT-24</td><td>82.3</td><td>64.1</td><td>13.8</td></tr></table>",
            "id": 65,
            "page": 6,
            "text": "Models Top1-Acc (%) Params (M) MACs (G)\n ViT-S/16 [12] 78.1 48.6 10.1\n DeiT-small [36] 79.9 22.1 4.6\n DeiT-small-Distilled [36] 81.2 22.1 4.7\n T2T-ViT-14 81.5 21.5 4.8\n T2T-ViT-14↑384 83.3 21.5 17.1\n ViT-B/16 [12] 79.8 86.4 17.6\n ViT-L/16 [12] 81.1 304.3 63.6\n T2T-ViT-24 82.3 64.1"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 969
                },
                {
                    "x": 2275,
                    "y": 969
                },
                {
                    "x": 2275,
                    "y": 1151
                },
                {
                    "x": 1281,
                    "y": 1151
                }
            ],
            "category": "caption",
            "html": "<br><caption id='66' style='font-size:14px'>Table 3. Comparison between our T2T-ViT with ResNets on Im-<br>ageNet. T2T-ViTt-14: using Transformer in T2T module. T2T-<br>ViT-14: using Performer in T2T module. * means we train the<br>model with our training scheme for fair comparisons.</caption>",
            "id": 66,
            "page": 6,
            "text": "Table 3. Comparison between our T2T-ViT with ResNets on Im-\nageNet. T2T-ViTt-14: using Transformer in T2T module. T2T-\nViT-14: using Performer in T2T module. * means we train the\nmodel with our training scheme for fair comparisons."
        },
        {
            "bounding_box": [
                {
                    "x": 1366,
                    "y": 1154
                },
                {
                    "x": 2177,
                    "y": 1154
                },
                {
                    "x": 2177,
                    "y": 1930
                },
                {
                    "x": 1366,
                    "y": 1930
                }
            ],
            "category": "table",
            "html": "<br><table id='67' style='font-size:14px'><tr><td>Models</td><td>Top1-Acc (%)</td><td>Params (M)</td><td>MACs (G)</td></tr><tr><td>ResNet50 [15]</td><td>76.2</td><td>25.5</td><td>4.3</td></tr><tr><td>ResNet50*</td><td>79.1</td><td>25.5</td><td>4.3</td></tr><tr><td>T2T-ViT-14</td><td>81.5</td><td>21.5</td><td>4.8</td></tr><tr><td>T2T-ViTt-14</td><td>81.7</td><td>21.5</td><td>6.1</td></tr><tr><td>ResNet101 [15]</td><td>77.4</td><td>44.6</td><td>7.9</td></tr><tr><td>ResNet101*</td><td>79.9</td><td>44.6</td><td>7.9</td></tr><tr><td>T2T-ViT-19</td><td>81.9</td><td>39.2</td><td>8.5</td></tr><tr><td>T2T-ViTt-19</td><td>82.2</td><td>39.2</td><td>9.8</td></tr><tr><td>ResNet152 [15]</td><td>78.3</td><td>60.2</td><td>11.6</td></tr><tr><td>ResNet152*</td><td>80.8</td><td>60.2</td><td>11.6</td></tr><tr><td>T2T-ViT-24</td><td>82.3</td><td>64.1</td><td>13.8</td></tr><tr><td>T2T-ViTt-24</td><td>82.6</td><td>64.1</td><td>15.0</td></tr></table>",
            "id": 67,
            "page": 6,
            "text": "Models Top1-Acc (%) Params (M) MACs (G)\n ResNet50 [15] 76.2 25.5 4.3\n ResNet50* 79.1 25.5 4.3\n T2T-ViT-14 81.5 21.5 4.8\n T2T-ViTt-14 81.7 21.5 6.1\n ResNet101 [15] 77.4 44.6 7.9\n ResNet101* 79.9 44.6 7.9\n T2T-ViT-19 81.9 39.2 8.5\n T2T-ViTt-19 82.2 39.2 9.8\n ResNet152 [15] 78.3 60.2 11.6\n ResNet152* 80.8 60.2 11.6\n T2T-ViT-24 82.3 64.1 13.8\n T2T-ViTt-24 82.6 64.1"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1973
                },
                {
                    "x": 2277,
                    "y": 1973
                },
                {
                    "x": 2277,
                    "y": 2821
                },
                {
                    "x": 1278,
                    "y": 2821
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>bileNetV2 [32], but achieve comparable or higher perfor-<br>mance than MobileNets (Tab. 4). For example, Our T2T-<br>ViT-12 with 6.9M parameters achieves 76.5% top1 accu-<br>racy, which is higher than MobileNetsV21.4cc by 0.9%. But<br>we also note the MACs of our T2T-ViT are still larger than<br>MobileNets because of the dense operations in Transform-<br>ers. However, there are no special operations or tricks like<br>efficient convolution [26, 32] in current T2T-ViT-7 and T2T-<br>ViT-12, and we only reduce model size by reducing the hid-<br>den dimension, MLP ratio and depth of layers, indicating<br>T2T-ViT is also very promising as a lite model. We also ap-<br>ply knowledge distillation on our T2T-ViT as the concurrent<br>work DeiT [36] and find that our T2T-ViT-7 and T2T-ViT-<br>12 can be further improved by distillation. Overall, the ex-<br>perimental results show, our T2T-ViT can achieve superior<br>performance when it has mid-size as ResNets and reason-<br>able results when it has a small model size as MobileNets.</p>",
            "id": 68,
            "page": 6,
            "text": "bileNetV2 [32], but achieve comparable or higher perfor-\nmance than MobileNets (Tab. 4). For example, Our T2T-\nViT-12 with 6.9M parameters achieves 76.5% top1 accu-\nracy, which is higher than MobileNetsV21.4cc by 0.9%. But\nwe also note the MACs of our T2T-ViT are still larger than\nMobileNets because of the dense operations in Transform-\ners. However, there are no special operations or tricks like\nefficient convolution [26, 32] in current T2T-ViT-7 and T2T-\nViT-12, and we only reduce model size by reducing the hid-\nden dimension, MLP ratio and depth of layers, indicating\nT2T-ViT is also very promising as a lite model. We also ap-\nply knowledge distillation on our T2T-ViT as the concurrent\nwork DeiT [36] and find that our T2T-ViT-7 and T2T-ViT-\n12 can be further improved by distillation. Overall, the ex-\nperimental results show, our T2T-ViT can achieve superior\nperformance when it has mid-size as ResNets and reason-\nable results when it has a small model size as MobileNets."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2877
                },
                {
                    "x": 2277,
                    "y": 2877
                },
                {
                    "x": 2277,
                    "y": 2974
                },
                {
                    "x": 1281,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>Transfer learning We transfer our pretrained T2T-ViT to<br>downstream datasets such as CIFAR10 and CIFAR100. We</p>",
            "id": 69,
            "page": 6,
            "text": "Transfer learning We transfer our pretrained T2T-ViT to\ndownstream datasets such as CIFAR10 and CIFAR100. We"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 294
                },
                {
                    "x": 1198,
                    "y": 294
                },
                {
                    "x": 1198,
                    "y": 424
                },
                {
                    "x": 203,
                    "y": 424
                }
            ],
            "category": "caption",
            "html": "<caption id='70' style='font-size:16px'>Table 4. Comparison between our lite T2T-ViT with MobileNets.<br>Models with '-Distilled' are taught by teacher model with the<br>method as DeiT [36].</caption>",
            "id": 70,
            "page": 7,
            "text": "Table 4. Comparison between our lite T2T-ViT with MobileNets.\nModels with '-Distilled' are taught by teacher model with the\nmethod as DeiT [36]."
        },
        {
            "bounding_box": [
                {
                    "x": 230,
                    "y": 427
                },
                {
                    "x": 1162,
                    "y": 427
                },
                {
                    "x": 1162,
                    "y": 993
                },
                {
                    "x": 230,
                    "y": 993
                }
            ],
            "category": "table",
            "html": "<br><table id='71' style='font-size:14px'><tr><td>Models</td><td>Top1-Acc (%)</td><td>Params (M)</td><td>MACs (G)</td></tr><tr><td>MobileNetV1 1.0x*</td><td>70.8</td><td>4.2</td><td>0.6</td></tr><tr><td>T2T-ViT-7</td><td>71.7</td><td>4.3</td><td>1.1</td></tr><tr><td>T2T-ViT-7-Distilled</td><td>73.1</td><td>4.3</td><td>1.1</td></tr><tr><td>MobileNetV2 1.0x*</td><td>72.8</td><td>3.5</td><td>0.3</td></tr><tr><td>MobileNetV2 1.4x*</td><td>75.6</td><td>6.9</td><td>0.6</td></tr><tr><td>MobileNetV3 (Searched)</td><td>75.2</td><td>5.4</td><td>0.2</td></tr><tr><td>T2T-ViT-12</td><td>76.5</td><td>6.9</td><td>1.8</td></tr><tr><td>T2T-ViT-12-Distilled</td><td>77.4</td><td>6.9</td><td>1.9</td></tr></table>",
            "id": 71,
            "page": 7,
            "text": "Models Top1-Acc (%) Params (M) MACs (G)\n MobileNetV1 1.0x* 70.8 4.2 0.6\n T2T-ViT-7 71.7 4.3 1.1\n T2T-ViT-7-Distilled 73.1 4.3 1.1\n MobileNetV2 1.0x* 72.8 3.5 0.3\n MobileNetV2 1.4x* 75.6 6.9 0.6\n MobileNetV3 (Searched) 75.2 5.4 0.2\n T2T-ViT-12 76.5 6.9 1.8\n T2T-ViT-12-Distilled 77.4 6.9"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 1013
                },
                {
                    "x": 1192,
                    "y": 1013
                },
                {
                    "x": 1192,
                    "y": 1098
                },
                {
                    "x": 206,
                    "y": 1098
                }
            ],
            "category": "caption",
            "html": "<br><caption id='72' style='font-size:16px'>Table 5. The results of fine-tuning the pretrained T2T-ViT to down-<br>stream datasets: CIFAR10 and CIFAR100.</caption>",
            "id": 72,
            "page": 7,
            "text": "Table 5. The results of fine-tuning the pretrained T2T-ViT to down-\nstream datasets: CIFAR10 and CIFAR100."
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 1104
                },
                {
                    "x": 1191,
                    "y": 1104
                },
                {
                    "x": 1191,
                    "y": 1341
                },
                {
                    "x": 208,
                    "y": 1341
                }
            ],
            "category": "table",
            "html": "<br><table id='73' style='font-size:14px'><tr><td>Models</td><td>Params (M)</td><td>ImageNet</td><td>CIFAR10</td><td>CIFAR100</td></tr><tr><td>ViT/S-16</td><td>48.6</td><td>78.1</td><td>97.1</td><td>87.1</td></tr><tr><td>T2T-ViT-14</td><td>21.5</td><td>81.5</td><td>97.5</td><td>88.4</td></tr><tr><td>T2T-ViT-19</td><td>39.1</td><td>81.9</td><td>98.3</td><td>89.0</td></tr></table>",
            "id": 73,
            "page": 7,
            "text": "Models Params (M) ImageNet CIFAR10 CIFAR100\n ViT/S-16 48.6 78.1 97.1 87.1\n T2T-ViT-14 21.5 81.5 97.5 88.4\n T2T-ViT-19 39.1 81.9 98.3"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1348
                },
                {
                    "x": 1199,
                    "y": 1348
                },
                {
                    "x": 1199,
                    "y": 1589
                },
                {
                    "x": 203,
                    "y": 1589
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:18px'>finetune the pretrained T2T- ViT-14/19 with 60 epochs by<br>using SGD optimizer and cosine learning rate decay.' The<br>results are given in Tab. 5. We find that our T2T-ViT<br>can achieve higher performance than the original ViT with<br>smaller model sizes on the downstream datasets.</p>",
            "id": 74,
            "page": 7,
            "text": "finetune the pretrained T2T- ViT-14/19 with 60 epochs by\nusing SGD optimizer and cosine learning rate decay.' The\nresults are given in Tab. 5. We find that our T2T-ViT\ncan achieve higher performance than the original ViT with\nsmaller model sizes on the downstream datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1604
                },
                {
                    "x": 642,
                    "y": 1604
                },
                {
                    "x": 642,
                    "y": 1648
                },
                {
                    "x": 204,
                    "y": 1648
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:20px'>4.2. From CNN to ViT</p>",
            "id": 75,
            "page": 7,
            "text": "4.2. From CNN to ViT"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1678
                },
                {
                    "x": 1199,
                    "y": 1678
                },
                {
                    "x": 1199,
                    "y": 2272
                },
                {
                    "x": 202,
                    "y": 2272
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:18px'>To find an efficient backbone for vision transformers,<br>we experimentally apply DenseNet structure, Wide-ResNet<br>structure (wide or narrow channel dimensions), SE block<br>(channel attention), ResNeXt structure (more heads in mul-<br>tihead attention), and Ghost operation from CNN to ViT.<br>The details of these architecture designs are given in the<br>appendix. From experimental results on \"CNN to ViT\" in<br>Tab. 6, we can find both SE (ViT-SE) and Deep-Narrow<br>structure (ViT-DN) benefit the ViT but the most effective<br>structure is deep-narrow structure, which decreases model<br>size and MACs nearly 2x and brings 0.9% improvement on<br>the baseline model ViT-S/16.</p>",
            "id": 76,
            "page": 7,
            "text": "To find an efficient backbone for vision transformers,\nwe experimentally apply DenseNet structure, Wide-ResNet\nstructure (wide or narrow channel dimensions), SE block\n(channel attention), ResNeXt structure (more heads in mul-\ntihead attention), and Ghost operation from CNN to ViT.\nThe details of these architecture designs are given in the\nappendix. From experimental results on \"CNN to ViT\" in\nTab. 6, we can find both SE (ViT-SE) and Deep-Narrow\nstructure (ViT-DN) benefit the ViT but the most effective\nstructure is deep-narrow structure, which decreases model\nsize and MACs nearly 2x and brings 0.9% improvement on\nthe baseline model ViT-S/16."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2279
                },
                {
                    "x": 1199,
                    "y": 2279
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:18px'>We further apply these structures from CNN to our T2T-<br>ViT, and conduct experiments on ImageNet under the same<br>training scheme. We take ResNet50 as the baseline for<br>CNN, ViT-S/16 for ViT, and T2T-ViT-14 for T2T-ViT. All<br>experimental results are given in Tab. 6, and those on CNN<br>and ViT&T2T- ViT are marked with the same colors. We<br>summarize the effects of each CNN-based structure below.<br>Deep-narrow structure benefits ViT: The models ViT-<br>DN (Deep-Narrow) and ViT-SW (Shallow-Wide) in Tab. 6<br>are two opposite designs in channel dimension and layer<br>depth, where ViT-DN has 384 hidden dimensions and 16<br>layers and ViT-SW has 1,024 hidden dimensions and 4 lay-<br>ers. Compared with the baseline model ViT-S/16 with 768<br>hidden dimensions and 8 layers, shallow-wide model ViT-</p>",
            "id": 77,
            "page": 7,
            "text": "We further apply these structures from CNN to our T2T-\nViT, and conduct experiments on ImageNet under the same\ntraining scheme. We take ResNet50 as the baseline for\nCNN, ViT-S/16 for ViT, and T2T-ViT-14 for T2T-ViT. All\nexperimental results are given in Tab. 6, and those on CNN\nand ViT&T2T- ViT are marked with the same colors. We\nsummarize the effects of each CNN-based structure below.\nDeep-narrow structure benefits ViT: The models ViT-\nDN (Deep-Narrow) and ViT-SW (Shallow-Wide) in Tab. 6\nare two opposite designs in channel dimension and layer\ndepth, where ViT-DN has 384 hidden dimensions and 16\nlayers and ViT-SW has 1,024 hidden dimensions and 4 lay-\ners. Compared with the baseline model ViT-S/16 with 768\nhidden dimensions and 8 layers, shallow-wide model ViT-"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 305
                },
                {
                    "x": 2277,
                    "y": 305
                },
                {
                    "x": 2277,
                    "y": 552
                },
                {
                    "x": 1279,
                    "y": 552
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:18px'>SW has 8.2% decrease in performance while ViT-DN with<br>only half of model size and MACs achieve 0.9% increase.<br>These results validate our hypothesis that vanilla ViT with<br>shallow-wide structure is redundant in channel dimensions<br>and limited feature richness with shallow layers.</p>",
            "id": 78,
            "page": 7,
            "text": "SW has 8.2% decrease in performance while ViT-DN with\nonly half of model size and MACs achieve 0.9% increase.\nThese results validate our hypothesis that vanilla ViT with\nshallow-wide structure is redundant in channel dimensions\nand limited feature richness with shallow layers."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 570
                },
                {
                    "x": 2276,
                    "y": 570
                },
                {
                    "x": 2276,
                    "y": 864
                },
                {
                    "x": 1280,
                    "y": 864
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:20px'>Dense connection hurts performance of both ViT and<br>T2T-ViT: Compared with the ResNet50, DenseNet201<br>has smaller parameters and comparable MACs, while it has<br>higher performance. However, the dense connection can<br>hurt performance of ViT-Dense and T2T-ViT-Dense (dark<br>blue rows in Tab. 6).</p>",
            "id": 79,
            "page": 7,
            "text": "Dense connection hurts performance of both ViT and\nT2T-ViT: Compared with the ResNet50, DenseNet201\nhas smaller parameters and comparable MACs, while it has\nhigher performance. However, the dense connection can\nhurt performance of ViT-Dense and T2T-ViT-Dense (dark\nblue rows in Tab. 6)."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 882
                },
                {
                    "x": 2276,
                    "y": 882
                },
                {
                    "x": 2276,
                    "y": 1176
                },
                {
                    "x": 1280,
                    "y": 1176
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:18px'>SE block improves both ViT and T2T-ViT: From red<br>rows in Tab. 6, we can find SENets, ViT-SE and T2T-ViT-<br>SE are higher than the corresponding baseline. The SE<br>module can improve performance on both CNN and ViT,<br>which means applying attention to channels benefits both<br>CNN and ViT models.</p>",
            "id": 80,
            "page": 7,
            "text": "SE block improves both ViT and T2T-ViT: From red\nrows in Tab. 6, we can find SENets, ViT-SE and T2T-ViT-\nSE are higher than the corresponding baseline. The SE\nmodule can improve performance on both CNN and ViT,\nwhich means applying attention to channels benefits both\nCNN and ViT models."
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 1197
                },
                {
                    "x": 2271,
                    "y": 1197
                },
                {
                    "x": 2271,
                    "y": 1240
                },
                {
                    "x": 1284,
                    "y": 1240
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:16px'>ResNeXt structure has few effects on ViT and T2T-ViT:</p>",
            "id": 81,
            "page": 7,
            "text": "ResNeXt structure has few effects on ViT and T2T-ViT:"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1247
                },
                {
                    "x": 2276,
                    "y": 1247
                },
                {
                    "x": 2276,
                    "y": 1539
                },
                {
                    "x": 1280,
                    "y": 1539
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:16px'>ResNeXts adopt multi-head on ResNets, while Transform-<br>ers are also multi-head attention structure. When we adopt<br>more heads like 32, we can find it has few effects on per-<br>formance (red rows in Tab 6). However, adopting a large<br>number of heads makes the GPU memory large, which is<br>thus unnecessary in ViT and T2T-ViT.</p>",
            "id": 82,
            "page": 7,
            "text": "ResNeXts adopt multi-head on ResNets, while Transform-\ners are also multi-head attention structure. When we adopt\nmore heads like 32, we can find it has few effects on per-\nformance (red rows in Tab 6). However, adopting a large\nnumber of heads makes the GPU memory large, which is\nthus unnecessary in ViT and T2T-ViT."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1560
                },
                {
                    "x": 2277,
                    "y": 1560
                },
                {
                    "x": 2277,
                    "y": 1953
                },
                {
                    "x": 1280,
                    "y": 1953
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:18px'>Ghost can further compress model and reduce MACs of<br>T2T-ViT: Comparing experimental results of Ghost op-<br>eration (magenta row in Tab. 6), the accuracy decreases<br>2.9% on ResNet50, 2.0% on T2T-ViT, and 4.4% on ViT.<br>So the Ghost operation can further reduce the parameters<br>and MACs of T2T-ViT with smaller performance degrada-<br>tion than ResNet. But for the original ViT, it would cause<br>more decrease than ResNet.</p>",
            "id": 83,
            "page": 7,
            "text": "Ghost can further compress model and reduce MACs of\nT2T-ViT: Comparing experimental results of Ghost op-\neration (magenta row in Tab. 6), the accuracy decreases\n2.9% on ResNet50, 2.0% on T2T-ViT, and 4.4% on ViT.\nSo the Ghost operation can further reduce the parameters\nand MACs of T2T-ViT with smaller performance degrada-\ntion than ResNet. But for the original ViT, it would cause\nmore decrease than ResNet."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1960
                },
                {
                    "x": 2278,
                    "y": 1960
                },
                {
                    "x": 2278,
                    "y": 2207
                },
                {
                    "x": 1280,
                    "y": 2207
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:18px'>Besides, for all five structures, the T2T-ViT performs<br>better than ViT, which further validates the superiority of<br>our proposed T2T-ViT. And we also wish this study of trans-<br>ferring CNN structure to ViT can motivate the network de-<br>sign of Transformers in vision tasks.</p>",
            "id": 84,
            "page": 7,
            "text": "Besides, for all five structures, the T2T-ViT performs\nbetter than ViT, which further validates the superiority of\nour proposed T2T-ViT. And we also wish this study of trans-\nferring CNN structure to ViT can motivate the network de-\nsign of Transformers in vision tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2247
                },
                {
                    "x": 1654,
                    "y": 2247
                },
                {
                    "x": 1654,
                    "y": 2294
                },
                {
                    "x": 1282,
                    "y": 2294
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:22px'>4.3. Ablation study</p>",
            "id": 85,
            "page": 7,
            "text": "4.3. Ablation study"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2305
                },
                {
                    "x": 2274,
                    "y": 2305
                },
                {
                    "x": 2274,
                    "y": 2402
                },
                {
                    "x": 1281,
                    "y": 2402
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:18px'>To further identify effects of T2T module and deep-<br>narrow structure, we do ablation study on our T2T-ViT.</p>",
            "id": 86,
            "page": 7,
            "text": "To further identify effects of T2T module and deep-\nnarrow structure, we do ablation study on our T2T-ViT."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2427
                },
                {
                    "x": 2277,
                    "y": 2427
                },
                {
                    "x": 2277,
                    "y": 2772
                },
                {
                    "x": 1281,
                    "y": 2772
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:18px'>T2T module To verify the effects of the proposed T2T<br>module, we experimentally compare three different models:<br>T2T- ViT-14, T2T-ViT-14wo T2T, and T2T-ViTt-14, where<br>T2T-ViT-14wo T2T has the same T2T-ViT backbone but<br>without T2T module. We can find with similar model size<br>and MACs, the T2T module can improve model perfor-<br>mance by 2.0%-2.2% on ImageNet.</p>",
            "id": 87,
            "page": 7,
            "text": "T2T module To verify the effects of the proposed T2T\nmodule, we experimentally compare three different models:\nT2T- ViT-14, T2T-ViT-14wo T2T, and T2T-ViTt-14, where\nT2T-ViT-14wo T2T has the same T2T-ViT backbone but\nwithout T2T module. We can find with similar model size\nand MACs, the T2T module can improve model perfor-\nmance by 2.0%-2.2% on ImageNet."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2779
                },
                {
                    "x": 2278,
                    "y": 2779
                },
                {
                    "x": 2278,
                    "y": 2976
                },
                {
                    "x": 1281,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:20px'>As the soft split in T2T module is similar to convolu-<br>tion operation without convolution filters, we also replace<br>the T2T module by 3 convolution layers with kernel size<br>(7,3,3), stride size (4,2,2) respectively. Such a model with</p>",
            "id": 88,
            "page": 7,
            "text": "As the soft split in T2T module is similar to convolu-\ntion operation without convolution filters, we also replace\nthe T2T module by 3 convolution layers with kernel size\n(7,3,3), stride size (4,2,2) respectively. Such a model with"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 290
                },
                {
                    "x": 2278,
                    "y": 290
                },
                {
                    "x": 2278,
                    "y": 428
                },
                {
                    "x": 201,
                    "y": 428
                }
            ],
            "category": "caption",
            "html": "<caption id='89' style='font-size:18px'>Table 6. Transfer of some common designs in CNN to ViT&T2T-ViT, including DenseNet, Wide-ResNet, SE module, ResNeXt, Ghost<br>operation. The same color means the correspond transfer. All models are trained from scratch on ImageNet. * means we reproduce the<br>model with our training scheme for fair comparisons.</caption>",
            "id": 89,
            "page": 8,
            "text": "Table 6. Transfer of some common designs in CNN to ViT&T2T-ViT, including DenseNet, Wide-ResNet, SE module, ResNeXt, Ghost\noperation. The same color means the correspond transfer. All models are trained from scratch on ImageNet. * means we reproduce the\nmodel with our training scheme for fair comparisons."
        },
        {
            "bounding_box": [
                {
                    "x": 473,
                    "y": 428
                },
                {
                    "x": 2000,
                    "y": 428
                },
                {
                    "x": 2000,
                    "y": 1740
                },
                {
                    "x": 473,
                    "y": 1740
                }
            ],
            "category": "table",
            "html": "<br><table id='90' style='font-size:14px'><tr><td>Model Type</td><td>Models</td><td>Top1-Acc (%)</td><td>Params (M)</td><td>MACs (G)</td><td>Depth</td><td>Hidden_dim</td></tr><tr><td rowspan=\"3\">Traditional CNN</td><td>AlexNet [22]</td><td>56.6</td><td>61.1</td><td>0.77</td><td>-</td><td>-</td></tr><tr><td>VGG11 [33]</td><td>69.1</td><td>132.8</td><td>7.7</td><td>11</td><td>-</td></tr><tr><td>Inception v3 [35]</td><td>77.4</td><td>27.2</td><td>5.7</td><td>-</td><td>-</td></tr><tr><td rowspan=\"7\">Skip-connection CNN</td><td>ResNet50 [15]</td><td>76.2</td><td>25.6</td><td>4.3</td><td>50</td><td>-</td></tr><tr><td>ResNet50* (Baseline)</td><td>79.1</td><td>25.6</td><td>4.3</td><td>50</td><td>-</td></tr><tr><td>Wide-ResNet18x1.5*</td><td>78.0 (-1.1)</td><td>26.0</td><td>4.1</td><td>18</td><td>-</td></tr><tr><td>DenseNet201*</td><td>77.5 (-1.6)</td><td>20.1</td><td>4.4</td><td>201</td><td>-</td></tr><tr><td>SENet50*</td><td>80.3 (+1.2)</td><td>28.1</td><td>4.9</td><td>50</td><td>-</td></tr><tr><td>ResNeXt50*</td><td>79.9 (+0.8)</td><td>25.0</td><td>4.3</td><td>50</td><td>-</td></tr><tr><td>ResNet50-Ghost*</td><td>76.2 (-2.9)</td><td>19.9</td><td>3.2</td><td>50</td><td>-</td></tr><tr><td rowspan=\"7\">CNN to ViT</td><td>ViT-S/16 (Baseline)</td><td>78.1</td><td>48.6</td><td>10.1</td><td>8</td><td>768</td></tr><tr><td>ViT-DN</td><td>79.0 (+0.9)</td><td>24.5</td><td>5.5</td><td>16</td><td>384</td></tr><tr><td>ViT-SW</td><td>69.9 (-8.2)</td><td>47.9</td><td>9.9</td><td>4</td><td>1024</td></tr><tr><td>ViT-Dense</td><td>76.8 (-1.3)</td><td>46.7</td><td>9.7</td><td>19</td><td>128-736</td></tr><tr><td>ViT-SE</td><td>78.4 (+0.3)</td><td>49.2</td><td>10.2</td><td>8</td><td>768</td></tr><tr><td>ViT-ResNeXt</td><td>78.0 (-0.1)</td><td>48.6</td><td>10.1</td><td>8</td><td>768</td></tr><tr><td>ViT-Ghost</td><td>73.7 (-4.4)</td><td>32.1</td><td>6.9</td><td>8</td><td>768</td></tr><tr><td rowspan=\"6\">CNN to T2T-ViT</td><td>T2T-ViT-14 (Baseline)</td><td>81.5</td><td>21.5</td><td>4.8</td><td>14</td><td>384</td></tr><tr><td>T2T-ViT-Wide</td><td>77.9 (-3.4)</td><td>25.1</td><td>5.0</td><td>14</td><td>768</td></tr><tr><td>T2T-ViT-Dense</td><td>80.6 (-1.1)</td><td>23.7</td><td>5.5</td><td>19</td><td>128-584</td></tr><tr><td>T2T-ViT-SE</td><td>81.6 (+0.1)</td><td>21.9</td><td>4.9</td><td>14</td><td>384</td></tr><tr><td>T2T-ViT-ResNeXt</td><td>81.5 (+0.0)</td><td>21.5</td><td>4.8</td><td>14</td><td>384</td></tr><tr><td>T2T-ViT-Ghost</td><td>79.5 (-2.0)</td><td>16.3</td><td>3.7</td><td>14</td><td>384</td></tr></table>",
            "id": 90,
            "page": 8,
            "text": "Model Type Models Top1-Acc (%) Params (M) MACs (G) Depth Hidden_dim\n Traditional CNN AlexNet [22] 56.6 61.1 0.77 - -\n VGG11 [33] 69.1 132.8 7.7 11 -\n Inception v3 [35] 77.4 27.2 5.7 - -\n Skip-connection CNN ResNet50 [15] 76.2 25.6 4.3 50 -\n ResNet50* (Baseline) 79.1 25.6 4.3 50 -\n Wide-ResNet18x1.5* 78.0 (-1.1) 26.0 4.1 18 -\n DenseNet201* 77.5 (-1.6) 20.1 4.4 201 -\n SENet50* 80.3 (+1.2) 28.1 4.9 50 -\n ResNeXt50* 79.9 (+0.8) 25.0 4.3 50 -\n ResNet50-Ghost* 76.2 (-2.9) 19.9 3.2 50 -\n CNN to ViT ViT-S/16 (Baseline) 78.1 48.6 10.1 8 768\n ViT-DN 79.0 (+0.9) 24.5 5.5 16 384\n ViT-SW 69.9 (-8.2) 47.9 9.9 4 1024\n ViT-Dense 76.8 (-1.3) 46.7 9.7 19 128-736\n ViT-SE 78.4 (+0.3) 49.2 10.2 8 768\n ViT-ResNeXt 78.0 (-0.1) 48.6 10.1 8 768\n ViT-Ghost 73.7 (-4.4) 32.1 6.9 8 768\n CNN to T2T-ViT T2T-ViT-14 (Baseline) 81.5 21.5 4.8 14 384\n T2T-ViT-Wide 77.9 (-3.4) 25.1 5.0 14 768\n T2T-ViT-Dense 80.6 (-1.1) 23.7 5.5 19 128-584\n T2T-ViT-SE 81.6 (+0.1) 21.9 4.9 14 384\n T2T-ViT-ResNeXt 81.5 (+0.0) 21.5 4.8 14 384\n T2T-ViT-Ghost 79.5 (-2.0) 16.3 3.7 14"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1760
                },
                {
                    "x": 1197,
                    "y": 1760
                },
                {
                    "x": 1197,
                    "y": 1844
                },
                {
                    "x": 203,
                    "y": 1844
                }
            ],
            "category": "caption",
            "html": "<br><caption id='91' style='font-size:18px'>Table 7. Ablation study results on T2T module, Deep-Narrow(DN)<br>structure.</caption>",
            "id": 91,
            "page": 8,
            "text": "Table 7. Ablation study results on T2T module, Deep-Narrow(DN)\nstructure."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1847
                },
                {
                    "x": 1175,
                    "y": 1847
                },
                {
                    "x": 1175,
                    "y": 2307
                },
                {
                    "x": 221,
                    "y": 2307
                }
            ],
            "category": "table",
            "html": "<br><table id='92' style='font-size:16px'><tr><td>Ablation type</td><td>Models</td><td>Top1-Acc (%)</td><td>Params (M)</td><td>MACs (G)</td></tr><tr><td rowspan=\"4\">T2T module</td><td>T2T-ViT-14wo T2T</td><td>79.5</td><td>21.1</td><td>4.2</td></tr><tr><td>T2T-ViT-14</td><td>81.5 (+2.0)</td><td>21.5</td><td>4.8</td></tr><tr><td>T2T-ViTt-14</td><td>81.7 (+2.2)</td><td>21.5</td><td>6.1</td></tr><tr><td>T2T-ViTc-14</td><td>80.8 (+1.3)</td><td>21.3</td><td>4.6</td></tr><tr><td rowspan=\"2\">DN Structure</td><td>T2T-ViT-14</td><td>81.5</td><td>21.5</td><td>4.8</td></tr><tr><td>T2T-ViT-d768-4</td><td>78.8 (-2.7)</td><td>25.0</td><td>5.4</td></tr></table>",
            "id": 92,
            "page": 8,
            "text": "Ablation type Models Top1-Acc (%) Params (M) MACs (G)\n T2T module T2T-ViT-14wo T2T 79.5 21.1 4.2\n T2T-ViT-14 81.5 (+2.0) 21.5 4.8\n T2T-ViTt-14 81.7 (+2.2) 21.5 6.1\n T2T-ViTc-14 80.8 (+1.3) 21.3 4.6\n DN Structure T2T-ViT-14 81.5 21.5 4.8\n T2T-ViT-d768-4 78.8 (-2.7) 25.0"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2328
                },
                {
                    "x": 1199,
                    "y": 2328
                },
                {
                    "x": 1199,
                    "y": 2773
                },
                {
                    "x": 202,
                    "y": 2773
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:20px'>convolution layers to build T2T module is denoted as T2T-<br>ViTc-14. From Tab. 7, we can find the T2T-ViTc-14 is<br>worse than T2T-ViT-14 and T2T-ViTt-14 by 0.5%-1.0% on<br>ImageNet. We also note that the T2T-ViTc-14 is still higher<br>than T2T-ViT-14wo T2T, as the convolution layers in the<br>early stage can also model the structure information. But<br>our designed T2T module is better than the convolution lay-<br>ers as it can model both the global relation and the structure<br>information of the images.</p>",
            "id": 93,
            "page": 8,
            "text": "convolution layers to build T2T module is denoted as T2T-\nViTc-14. From Tab. 7, we can find the T2T-ViTc-14 is\nworse than T2T-ViT-14 and T2T-ViTt-14 by 0.5%-1.0% on\nImageNet. We also note that the T2T-ViTc-14 is still higher\nthan T2T-ViT-14wo T2T, as the convolution layers in the\nearly stage can also model the structure information. But\nour designed T2T module is better than the convolution lay-\ners as it can model both the global relation and the structure\ninformation of the images."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2779
                },
                {
                    "x": 1199,
                    "y": 2779
                },
                {
                    "x": 1199,
                    "y": 2975
                },
                {
                    "x": 202,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:20px'>Deep-narrow structure We use the deep-narrow struc-<br>ture with fewer hidden dimensions but more layers, rather<br>than the shallow-wide one in the original ViT. We com-<br>pare the T2T-ViT-14 and T2T-ViT-d768-4 to verify its ef-</p>",
            "id": 94,
            "page": 8,
            "text": "Deep-narrow structure We use the deep-narrow struc-\nture with fewer hidden dimensions but more layers, rather\nthan the shallow-wide one in the original ViT. We com-\npare the T2T-ViT-14 and T2T-ViT-d768-4 to verify its ef-"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1772
                },
                {
                    "x": 2277,
                    "y": 1772
                },
                {
                    "x": 2277,
                    "y": 2070
                },
                {
                    "x": 1280,
                    "y": 2070
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:20px'>fects. T2T-ViT-d768-4 is a shallow-wide structure with hid-<br>den dimension of 768 and 4 layers, with similar model size<br>and MACs as T2T-ViT-14. From Tab. 7, we can find af-<br>ter changing our deep-narrow to shallow-wide structure, the<br>T2T- ViT-d768-4 has 2.7% decrease in top-1 accuracy, vali-<br>dating deep-narrow structure is crucial for T2T-ViT.</p>",
            "id": 95,
            "page": 8,
            "text": "fects. T2T-ViT-d768-4 is a shallow-wide structure with hid-\nden dimension of 768 and 4 layers, with similar model size\nand MACs as T2T-ViT-14. From Tab. 7, we can find af-\nter changing our deep-narrow to shallow-wide structure, the\nT2T- ViT-d768-4 has 2.7% decrease in top-1 accuracy, vali-\ndating deep-narrow structure is crucial for T2T-ViT."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2111
                },
                {
                    "x": 1578,
                    "y": 2111
                },
                {
                    "x": 1578,
                    "y": 2162
                },
                {
                    "x": 1279,
                    "y": 2162
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:22px'>5. Conclusion</p>",
            "id": 96,
            "page": 8,
            "text": "5. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2178
                },
                {
                    "x": 2278,
                    "y": 2178
                },
                {
                    "x": 2278,
                    "y": 2973
                },
                {
                    "x": 1278,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:20px'>In this work, we propose a new T2T-ViT model that can<br>be trained from scratch on ImageNet and achieve compara-<br>ble or even better performance than CNNs. T2T-ViT effec-<br>tively models the structure information of images and en-<br>hances feature richness, overcoming limitations of ViT. It<br>introduces the novel tokens-to-token (T2T) process to pro-<br>gressively tokenize images to tokens and structurally ag-<br>gregate tokens. We also explore various architecture de-<br>sign choices from CNNs for improving T2T-ViT perfor-<br>mance, and empirically find the deep-narrow architecture<br>performs better than the shallow-wide structure. Our T2T-<br>ViT achieves superior performance to ResNets and compa-<br>rable performance to MobileNets with similar model size<br>when trained from scratch on ImageNet. It paves the way<br>for further developing transformer-based models for vision<br>tasks.</p>",
            "id": 97,
            "page": 8,
            "text": "In this work, we propose a new T2T-ViT model that can\nbe trained from scratch on ImageNet and achieve compara-\nble or even better performance than CNNs. T2T-ViT effec-\ntively models the structure information of images and en-\nhances feature richness, overcoming limitations of ViT. It\nintroduces the novel tokens-to-token (T2T) process to pro-\ngressively tokenize images to tokens and structurally ag-\ngregate tokens. We also explore various architecture de-\nsign choices from CNNs for improving T2T-ViT perfor-\nmance, and empirically find the deep-narrow architecture\nperforms better than the shallow-wide structure. Our T2T-\nViT achieves superior performance to ResNets and compa-\nrable performance to MobileNets with similar model size\nwhen trained from scratch on ImageNet. It paves the way\nfor further developing transformer-based models for vision\ntasks."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 299
                },
                {
                    "x": 446,
                    "y": 299
                },
                {
                    "x": 446,
                    "y": 354
                },
                {
                    "x": 204,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:18px'>References</p>",
            "id": 98,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 213,
                    "y": 372
                },
                {
                    "x": 1200,
                    "y": 372
                },
                {
                    "x": 1200,
                    "y": 2973
                },
                {
                    "x": 213,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:14px'>[1] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le. At-<br>tention augmented convolutional networks. In Proceedings<br>of the IEEE International Conference on Computer Vision,<br>pages 3286-3295, 2019.<br>[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,<br>P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,<br>et al. Language models are few-shot learners. arXiv preprint<br>arXiv:2005.14165, 2020.<br>[3] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,<br>and S. Zagoruyko. End-to-end object detection with trans-<br>formers. arXiv preprint arXiv:2005.12872, 2020.<br>[4] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma,<br>C. Xu, C. Xu, and W. Gao. Pre-trained image processing<br>transformer. arXiv preprint arXiv:2012.00364, 2020.<br>[5] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and<br>I. Sutskever. Generative pretraining from pixels. In Interna-<br>tional Conference on Machine Learning, pages 1691-1703.<br>PMLR, 2020.<br>[6] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng. A^ 2-nets:<br>Double attention networks. In Advances in neural informa-<br>tion processing systems, pages 352-361, 2018.<br>[7] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song,<br>A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin,<br>L. Kaiser, et al. Rethinking attention with performers. arXiv<br>preprint arXiv:2009.14794, 2020.<br>[8] Z. Dai, B. Cai, Y. Lin, and J. Chen. Up-detr: Unsupervised<br>pre-training for object detection with transformers. arXiv<br>preprint arXiv:2011.09094, 2020.<br>[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-<br>Fei. Imagenet: A large-scale hierarchical image database.<br>In 2009 IEEE conference on computer vision and pattern<br>recognition, pages 248-255. Ieee, 2009.<br>[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert:<br>Pre-training of deep bidirectional transformers for language<br>understanding. arXiv preprint arXiv:1810.04805, 2018.<br>[11] T. DeVries and G. W. Taylor. Improved regularization of<br>convolutional neural networks with cutout. arXiv preprint<br>arXiv:1708.04552, 2017.<br>[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,<br>X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,<br>G. Heigold, S. Gelly, et al. An image is worth 16x16 words:<br>Transformers for image recognition at scale. arXiv preprint<br>arXiv:2010.11929, 2020.<br>[13] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu.<br>Dual attention network for scene segmentation. In Proceed-<br>ings ofthe IEEE Conference on Computer Vision and Pattern<br>Recognition, pages 3146-3154, 2019.<br>[14] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu. Ghost-<br>net: More features from cheap operations. In Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, pages 1580-1589, 2020.<br>[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-<br>ing for image recognition. In Proceedings of the IEEE con-<br>ference on computer vision and pattern recognition, pages<br>770-778, 2016.</p>",
            "id": 99,
            "page": 9,
            "text": "[1] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le. At-\ntention augmented convolutional networks. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 3286-3295, 2019.\n[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\net al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[3] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nand S. Zagoruyko. End-to-end object detection with trans-\nformers. arXiv preprint arXiv:2005.12872, 2020.\n[4] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma,\nC. Xu, C. Xu, and W. Gao. Pre-trained image processing\ntransformer. arXiv preprint arXiv:2012.00364, 2020.\n[5] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever. Generative pretraining from pixels. In Interna-\ntional Conference on Machine Learning, pages 1691-1703.\nPMLR, 2020.\n[6] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng. A^ 2-nets:\nDouble attention networks. In Advances in neural informa-\ntion processing systems, pages 352-361, 2018.\n[7] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song,\nA. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin,\nL. Kaiser, et al. Rethinking attention with performers. arXiv\npreprint arXiv:2009.14794, 2020.\n[8] Z. Dai, B. Cai, Y. Lin, and J. Chen. Up-detr: Unsupervised\npre-training for object detection with transformers. arXiv\npreprint arXiv:2011.09094, 2020.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248-255. Ieee, 2009.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805, 2018.\n[11] T. DeVries and G. W. Taylor. Improved regularization of\nconvolutional neural networks with cutout. arXiv preprint\narXiv:1708.04552, 2017.\n[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[13] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu.\nDual attention network for scene segmentation. In Proceed-\nings ofthe IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3146-3154, 2019.\n[14] K. Han, Y. Wang, Q. Tian, J. Guo, C. Xu, and C. Xu. Ghost-\nnet: More features from cheap operations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1580-1589, 2020.\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n770-778, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 295
                },
                {
                    "x": 2287,
                    "y": 295
                },
                {
                    "x": 2287,
                    "y": 2976
                },
                {
                    "x": 1278,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:14px'>[16] G. Hinton, 0. Vinyals, andJ. Dean. Distilling the knowledge<br>in a neural network. arXiv preprint arXiv:1503.02531, 2015.<br>[17] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,<br>T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Effi-<br>cient convolutional neural networks for mobile vision appli-<br>cations. arXiv preprint arXiv:1704.04861, 2017.<br>[18] H. Hu, Z. Zhang, Z. Xie, and S. Lin. Local relation networks<br>for image recognition. In Proceedings of the IEEE Inter-<br>national Conference on Computer Vision, pages 3464-3473,<br>2019.<br>[19] J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi. Gather-<br>excite: Exploiting feature context in convolutional neural<br>networks. Advances in neural information processing sys-<br>tems, 31:9401-9411, 2018.<br>[20] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-<br>works. In Proceedings of the IEEE conference on computer<br>vision and pattern recognition, pages 7132-7141, 2018.<br>[21] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Wein-<br>berger. Densely connected convolutional networks. In Pro-<br>ceedings of the IEEE conference on computer vision and pat-<br>tern recognition, pages 4700-4708, 2017.<br>[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-<br>sification with deep convolutional neural networks. Commu-<br>nications of the ACM, 60(6):84-90, 2017.<br>[23] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, 0. Levy,<br>M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A ro-<br>bustly optimized bert pretraining approach. arXiv preprint<br>arXiv:1907.11692, 2019.<br>[24] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient de-<br>scent with warm restarts. arXiv preprint arXiv:1608.03983,<br>2016.<br>[25] I. Loshchilov and F. Hutter. Decoupled weight decay regu-<br>larization. arXiv preprint arXiv:1711.05101, 2017.<br>[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient<br>estimation of word representations in vector space. arXiv<br>preprint arXiv:1301.3781, 2013.<br>[27] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer,<br>A. Ku, and D. Tran. Image transformer. arXiv preprint<br>arXiv:1802.05751, 2018.<br>[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,<br>G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,<br>et al. Pytorch: An imperative style, high-performance deep<br>learning library. In Advances in neural information process-<br>ing systems, pages 8026-8037, 2019.<br>[29] M. E. Peters, M. Neumann, R. L. Logan IV, R. Schwartz,<br>V. Joshi, S. Singh, and N. A. Smith. Knowledge en-<br>hanced contextual word representations. arXiv preprint<br>arXiv:1909.04164, 2019.<br>[30] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.<br>Improving language understanding by generative pre-<br>training, 2018.<br>[31] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Lev-<br>skaya, and J. Shlens. Stand-alone self-attention in vision<br>models. arXiv preprint arXiv:1906.05909, 2019.<br>[32] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.<br>Chen. Mobilenetv2: Inverted residuals and linear bottle-<br>necks. In Proceedings of the IEEE conference on computer<br>vision and pattern recognition, pages 4510-4520, 2018.</p>",
            "id": 100,
            "page": 9,
            "text": "[16] G. Hinton, 0. Vinyals, andJ. Dean. Distilling the knowledge\nin a neural network. arXiv preprint arXiv:1503.02531, 2015.\n[17] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam. Mobilenets: Effi-\ncient convolutional neural networks for mobile vision appli-\ncations. arXiv preprint arXiv:1704.04861, 2017.\n[18] H. Hu, Z. Zhang, Z. Xie, and S. Lin. Local relation networks\nfor image recognition. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 3464-3473,\n2019.\n[19] J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi. Gather-\nexcite: Exploiting feature context in convolutional neural\nnetworks. Advances in neural information processing sys-\ntems, 31:9401-9411, 2018.\n[20] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132-7141, 2018.\n[21] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Wein-\nberger. Densely connected convolutional networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 4700-4708, 2017.\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-\nsification with deep convolutional neural networks. Commu-\nnications of the ACM, 60(6):84-90, 2017.\n[23] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, 0. Levy,\nM. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[24] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient de-\nscent with warm restarts. arXiv preprint arXiv:1608.03983,\n2016.\n[25] I. Loshchilov and F. Hutter. Decoupled weight decay regu-\nlarization. arXiv preprint arXiv:1711.05101, 2017.\n[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient\nestimation of word representations in vector space. arXiv\npreprint arXiv:1301.3781, 2013.\n[27] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer,\nA. Ku, and D. Tran. Image transformer. arXiv preprint\narXiv:1802.05751, 2018.\n[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\net al. Pytorch: An imperative style, high-performance deep\nlearning library. In Advances in neural information process-\ning systems, pages 8026-8037, 2019.\n[29] M. E. Peters, M. Neumann, R. L. Logan IV, R. Schwartz,\nV. Joshi, S. Singh, and N. A. Smith. Knowledge en-\nhanced contextual word representations. arXiv preprint\narXiv:1909.04164, 2019.\n[30] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\nImproving language understanding by generative pre-\ntraining, 2018.\n[31] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Lev-\nskaya, and J. Shlens. Stand-alone self-attention in vision\nmodels. arXiv preprint arXiv:1906.05909, 2019.\n[32] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.\nChen. Mobilenetv2: Inverted residuals and linear bottle-\nnecks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4510-4520, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 288
                },
                {
                    "x": 1203,
                    "y": 288
                },
                {
                    "x": 1203,
                    "y": 2972
                },
                {
                    "x": 201,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:14px'>[33] K. Simonyan and A. Zisserman. Very deep convolutional<br>networks for large-scale image recognition. arXiv preprint<br>arXiv:1409.1556, 2014.<br>[34] Z. Sun, S. Cao, Y. Yang, and K. Kitani. Rethinking<br>transformer-based set prediction for object detection. arXiv<br>preprint arXiv:2011.10881, 2020.<br>[35] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.<br>Rethinking the inception architecture for computer vision. In<br>Proceedings of the IEEE conference on computer vision and<br>pattern recognition, pages 2818-2826, 2016.<br>[36] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-<br>rolles, and H. Jegou. Training data-efficient image trans-<br>formers & distillation through attention. arXiv preprint<br>arXiv:2012.12877, 2020.<br>[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,<br>A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all<br>you need. Advances in neural information processing sys-<br>tems, 30:5998-6008, 2017.<br>[38] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,<br>X. Wang, and X. Tang. Residual attention network for im-<br>age classification. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pages 3156-3164,<br>2017.<br>[39] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neu-<br>ral networks. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pages 7794-7803,<br>2018.<br>[40] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and<br>H. Xia. End-to-end video instance segmentation with trans-<br>formers. arXiv preprint arXiv:2011.14503, 2020.<br>[41] R. Wightman. Pytorch image models. https : / /github.<br>com/ rwightman/pytorch- image-models, 2019.<br>[42] S. Woo, J. Park, J.-Y. Lee, and I. So Kweon. Cbam: Convo-<br>lutional block attention module. In Proceedings of the Eu-<br>ropean conference on computer vision (ECCV), pages 3-19,<br>2018.<br>[43] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, M. Tomizuka,<br>K. Keutzer, and P. Vajda. Visual transformers: Token-based<br>image representation and processing for computer vision.<br>arXiv preprint arXiv:2006.03677, 2020.<br>[44] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggre-<br>gated residual transformations for deep neural networks. In<br>Proceedings of the IEEE conference on computer vision and<br>pattern recognition, pages 1492-1500, 2017.<br>[45] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning texture<br>transformer network for image super-resolution. In Proceed-<br>ings of the IEEE/CVF Conference on Computer Vision and<br>Pattern Recognition, pages 5791-5800, 2020.<br>[46] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov,<br>and Q. V. Le. Xlnet: Generalized autoregressive pretraining<br>for language understanding. In Advances in neural informa-<br>tion processing systems, pages 5753-5763, 2019.<br>[47] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked at-<br>tention networks for image question answering. In Proceed-<br>ings of the IEEE conference on computer vision and pattern<br>recognition, pages 21-29, 2016.</p>",
            "id": 101,
            "page": 10,
            "text": "[33] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[34] Z. Sun, S. Cao, Y. Yang, and K. Kitani. Rethinking\ntransformer-based set prediction for object detection. arXiv\npreprint arXiv:2011.10881, 2020.\n[35] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.\nRethinking the inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818-2826, 2016.\n[36] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablay-\nrolles, and H. Jegou. Training data-efficient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. Advances in neural information processing sys-\ntems, 30:5998-6008, 2017.\n[38] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang,\nX. Wang, and X. Tang. Residual attention network for im-\nage classification. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3156-3164,\n2017.\n[39] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neu-\nral networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7794-7803,\n2018.\n[40] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and\nH. Xia. End-to-end video instance segmentation with trans-\nformers. arXiv preprint arXiv:2011.14503, 2020.\n[41] R. Wightman. Pytorch image models. https : / /github.\ncom/ rwightman/pytorch- image-models, 2019.\n[42] S. Woo, J. Park, J.-Y. Lee, and I. So Kweon. Cbam: Convo-\nlutional block attention module. In Proceedings of the Eu-\nropean conference on computer vision (ECCV), pages 3-19,\n2018.\n[43] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, M. Tomizuka,\nK. Keutzer, and P. Vajda. Visual transformers: Token-based\nimage representation and processing for computer vision.\narXiv preprint arXiv:2006.03677, 2020.\n[44] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggre-\ngated residual transformations for deep neural networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1492-1500, 2017.\n[45] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning texture\ntransformer network for image super-resolution. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5791-5800, 2020.\n[46] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov,\nand Q. V. Le. Xlnet: Generalized autoregressive pretraining\nfor language understanding. In Advances in neural informa-\ntion processing systems, pages 5753-5763, 2019.\n[47] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked at-\ntention networks for image question answering. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 21-29, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 310
                },
                {
                    "x": 2288,
                    "y": 310
                },
                {
                    "x": 2288,
                    "y": 2692
                },
                {
                    "x": 1276,
                    "y": 2692
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='102' style='font-size:14px'>[48] L. Yuan, S. Chang, Z. Huang, Y. Zhou, Y. Chen, X. Nie, F. E.<br>Tay, J. Feng, and S. Yan. A simple baseline for pose tracking<br>in videos of crowed scenes. In Proceedings of the 28th ACM<br>International Conference on Multimedia, pages 4684-4688,<br>2020.<br>[49] L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng. Revisiting<br>knowledge distillation via label smoothing regularization. In<br>Proceedings of the IEEE/CVF Conference on Computer Vi-<br>sion and Pattern Recognition, pages 3903-3911, 2020.<br>[50] L. Yuan, Y. Zhou, S. Chang, Z. Huang, Y. Chen, X. Nie,<br>T. Wang, J. Feng, and S. Yan. Toward accurate person-level<br>action recognition in videos of crowed scenes. In Proceed-<br>ings of the 28th ACM International Conference on Multime-<br>dia, pages 4694-4698, 2020.<br>[51] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cut-<br>mix: Regularization strategy to train strong classifiers with<br>localizable features. In Proceedings of the IEEE/CVF Inter-<br>national Conference on Computer Vision, pages 6023-6032,<br>2019.<br>[52] S. Zagoruyko and N. Komodakis. Wide residual networks.<br>arXiv preprint arXiv:1605.07146, 2016.<br>[53] Y. Zeng, J. Fu, and H. Chao. Learning joint spatial-temporal<br>transformations for video inpainting. In European Confer-<br>ence on Computer Vision, pages 528-543. Springer, 2020.<br>[54] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz.<br>mixup: Beyond empirical risk minimization. arXiv preprint<br>arXiv:1710.09412, 2017.<br>[55] H. Zhao, J. Jia, and V. Koltun. Exploring self-attention for<br>image recognition. In Proceedings of the IEEE/CVF Con-<br>ference on Computer Vision and Pattern Recognition, pages<br>10076-10085, 2020.<br>[56] H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun. Point trans-<br>former. arXiv preprint arXiv:2012.09164, 2020.<br>[57] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. Change Loy, D. Lin,<br>and J. Jia. Psanet: Point-wise spatial attention network for<br>scene parsing. In Proceedings of the European Conference<br>on Computer Vision (ECCV), pages 267-283, 2018.<br>[58] M. Zheng, P. Gao, X. Wang, H. Li, and H. Dong. End-to-end<br>object detection with adaptive clustering transformer. arXiv<br>preprint arXiv:2011.09315, 2020.<br>[59] D. Zhou, X. Jin, Q. Hou, K. Wang, J. Yang, andJ. Feng. Neu-<br>ral epitome search for architecture-agnostic network com-<br>pression. In International Conference on Learning Repre-<br>sentations, 2019.<br>[60] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong.<br>End-to-end dense video captioning with masked transformer.<br>In Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition, pages 8739-8748, 2018.<br>[61] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. De-<br>formable detr: Deformable transformers for end-to-end ob-<br>ject detection. arXiv preprint arXiv:2010.04159, 2020.</p>",
            "id": 102,
            "page": 10,
            "text": "[48] L. Yuan, S. Chang, Z. Huang, Y. Zhou, Y. Chen, X. Nie, F. E.\nTay, J. Feng, and S. Yan. A simple baseline for pose tracking\nin videos of crowed scenes. In Proceedings of the 28th ACM\nInternational Conference on Multimedia, pages 4684-4688,\n2020.\n[49] L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng. Revisiting\nknowledge distillation via label smoothing regularization. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3903-3911, 2020.\n[50] L. Yuan, Y. Zhou, S. Chang, Z. Huang, Y. Chen, X. Nie,\nT. Wang, J. Feng, and S. Yan. Toward accurate person-level\naction recognition in videos of crowed scenes. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, pages 4694-4698, 2020.\n[51] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cut-\nmix: Regularization strategy to train strong classifiers with\nlocalizable features. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 6023-6032,\n2019.\n[52] S. Zagoruyko and N. Komodakis. Wide residual networks.\narXiv preprint arXiv:1605.07146, 2016.\n[53] Y. Zeng, J. Fu, and H. Chao. Learning joint spatial-temporal\ntransformations for video inpainting. In European Confer-\nence on Computer Vision, pages 528-543. Springer, 2020.\n[54] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz.\nmixup: Beyond empirical risk minimization. arXiv preprint\narXiv:1710.09412, 2017.\n[55] H. Zhao, J. Jia, and V. Koltun. Exploring self-attention for\nimage recognition. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10076-10085, 2020.\n[56] H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun. Point trans-\nformer. arXiv preprint arXiv:2012.09164, 2020.\n[57] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. Change Loy, D. Lin,\nand J. Jia. Psanet: Point-wise spatial attention network for\nscene parsing. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 267-283, 2018.\n[58] M. Zheng, P. Gao, X. Wang, H. Li, and H. Dong. End-to-end\nobject detection with adaptive clustering transformer. arXiv\npreprint arXiv:2011.09315, 2020.\n[59] D. Zhou, X. Jin, Q. Hou, K. Wang, J. Yang, andJ. Feng. Neu-\nral epitome search for architecture-agnostic network com-\npression. In International Conference on Learning Repre-\nsentations, 2019.\n[60] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong.\nEnd-to-end dense video captioning with masked transformer.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8739-8748, 2018.\n[61] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. De-\nformable detr: Deformable transformers for end-to-end ob-\nject detection. arXiv preprint arXiv:2010.04159, 2020."
        }
    ]
}