{
    "id": "32baacf4-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/nature16961.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 154,
                    "y": 147
                },
                {
                    "x": 780,
                    "y": 147
                },
                {
                    "x": 780,
                    "y": 278
                },
                {
                    "x": 154,
                    "y": 278
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:22px'>ARTICLE</header>",
            "id": 0,
            "page": 1,
            "text": "ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 1928,
                    "y": 268
                },
                {
                    "x": 2304,
                    "y": 268
                },
                {
                    "x": 2304,
                    "y": 310
                },
                {
                    "x": 1928,
                    "y": 310
                }
            ],
            "category": "header",
            "html": "<br><header id='1' style='font-size:18px'>doi:10.1038/nature16961</header>",
            "id": 1,
            "page": 1,
            "text": "doi:10.1038/nature16961"
        },
        {
            "bounding_box": [
                {
                    "x": 148,
                    "y": 431
                },
                {
                    "x": 2044,
                    "y": 431
                },
                {
                    "x": 2044,
                    "y": 666
                },
                {
                    "x": 148,
                    "y": 666
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:22px'>Mastering the game of Go with deep<br>neural networks and tree search</p>",
            "id": 2,
            "page": 1,
            "text": "Mastering the game of Go with deep neural networks and tree search"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 688
                },
                {
                    "x": 2147,
                    "y": 688
                },
                {
                    "x": 2147,
                    "y": 870
                },
                {
                    "x": 146,
                    "y": 870
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:20px'>David Silver1*, Aja Huang1*, Chris J. Maddison1, Arthur Guez1, Laurent Sifre1, George van den Driessche1,<br>Julian Schrittwieser1, Ioannis Antonogloul, Veda Panneershelvam1, Marc Lanctot1, Sander Dieleman1, Dominik Grewe1,<br>John Nham2, Nal Kalchbrenner1, Ilya Sutskever2, Timothy Lillicrap1, Madeleine Leach1, Koray Kavukcuoglu1,<br>Thore Graepel1 & Demis Hassabis1</p>",
            "id": 3,
            "page": 1,
            "text": "David Silver1*, Aja Huang1*, Chris J. Maddison1, Arthur Guez1, Laurent Sifre1, George van den Driessche1, Julian Schrittwieser1, Ioannis Antonogloul, Veda Panneershelvam1, Marc Lanctot1, Sander Dieleman1, Dominik Grewe1, John Nham2, Nal Kalchbrenner1, Ilya Sutskever2, Timothy Lillicrap1, Madeleine Leach1, Koray Kavukcuoglu1, Thore Graepel1 & Demis Hassabis1"
        },
        {
            "bounding_box": [
                {
                    "x": 179,
                    "y": 991
                },
                {
                    "x": 2278,
                    "y": 991
                },
                {
                    "x": 2278,
                    "y": 1445
                },
                {
                    "x": 179,
                    "y": 1445
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its<br>enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach<br>to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep<br>neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement<br>learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-<br>of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a<br>new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm,<br>our program AlphaGo achieved a 99. 8% winning rate against other Go programs, and defeated the human European Go<br>champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the<br>full -sized game of Go, a feat previously thought to be at least a decade away.</p>",
            "id": 4,
            "page": 1,
            "text": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of stateof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99. 8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full -sized game of Go, a feat previously thought to be at least a decade away."
        },
        {
            "bounding_box": [
                {
                    "x": 149,
                    "y": 1513
                },
                {
                    "x": 1212,
                    "y": 1513
                },
                {
                    "x": 1212,
                    "y": 2485
                },
                {
                    "x": 149,
                    "y": 2485
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>All games of perfect information have an optimal value function, v* (s),<br>which determines the outcome of the game, from every board position<br>or state S, under perfect play by all players. These games may be solved<br>by recursively computing the optimal value function in a search tree<br>containing approximately bd possible sequences of moves, where b is<br>the game's breadth (number oflegal moves per position) and dis its<br>depth (game length). In large games, such as chess (6� 35, d� 80)1 and<br>especially Go (b U 250, d� 150)1, exhaustive search is infeasible2,3, but<br>the effective search space can be reduced by two general principles.<br>First, the depth of the search may be reduced by position evaluation:<br>truncating the search tree at state s and replacing the subtree below s<br>by an approximate value function v(s) 21 v (s) that predicts the outcome<br>from state s. This approach has led to superhuman performance in<br>chess4, checkers5 and othello6, but it was believed to be intractable in Go<br>due to the complexity of the game7. Second, the breadth of the search<br>may be reduced by sampling actions from a policy p(a|s) that is a prob-<br>ability distribution over possible moves a in position s. For example,<br>Monte Carlo rollouts8 search to maximum depth without branching<br>at all, by sampling long sequences of actions for both players from a<br>policy p. Averaging over such rollouts can provide an effective position<br>evaluation, achieving superhuman performance in backgammon8 and<br>Scrabble', and weak amateur level play in G010</p>",
            "id": 5,
            "page": 1,
            "text": "All games of perfect information have an optimal value function, v* (s), which determines the outcome of the game, from every board position or state S, under perfect play by all players. These games may be solved by recursively computing the optimal value function in a search tree containing approximately bd possible sequences of moves, where b is the game's breadth (number oflegal moves per position) and dis its depth (game length). In large games, such as chess (6� 35, d� 80)1 and especially Go (b U 250, d� 150)1, exhaustive search is infeasible2,3, but the effective search space can be reduced by two general principles. First, the depth of the search may be reduced by position evaluation: truncating the search tree at state s and replacing the subtree below s by an approximate value function v(s) 21 v (s) that predicts the outcome from state s. This approach has led to superhuman performance in chess4, checkers5 and othello6, but it was believed to be intractable in Go due to the complexity of the game7. Second, the breadth of the search may be reduced by sampling actions from a policy p(a|s) that is a probability distribution over possible moves a in position s. For example, Monte Carlo rollouts8 search to maximum depth without branching at all, by sampling long sequences of actions for both players from a policy p. Averaging over such rollouts can provide an effective position evaluation, achieving superhuman performance in backgammon8 and Scrabble', and weak amateur level play in G010"
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 2481
                },
                {
                    "x": 1211,
                    "y": 2481
                },
                {
                    "x": 1211,
                    "y": 3011
                },
                {
                    "x": 147,
                    "y": 3011
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:16px'>Monte Carlo tree search (MCTS) 11,12 uses Monte Carlo rollouts<br>to estimate the value of each state in a search tree. As more simu-<br>lations are executed, the search tree grows larger and the relevant<br>values become more accurate. The policy used to select actions during<br>search is also improved over time, by selecting children with higher<br>values. Asymptotically, this policy converges to optimal play, and the<br>evaluations converge to the optimal value function12. The strongest<br>current Go programs are based on MCTS, enhanced by policies that<br>are trained to predict human expert moves13. These policies are used<br>to narrow the search to a beam of high-probability actions, and to<br>sample actions during rollouts. This approach has achieved strong<br>amateur play13-15. However, prior work has been limited to shallow</p>",
            "id": 6,
            "page": 1,
            "text": "Monte Carlo tree search (MCTS) 11,12 uses Monte Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values. Asymptotically, this policy converges to optimal play, and the evaluations converge to the optimal value function12. The strongest current Go programs are based on MCTS, enhanced by policies that are trained to predict human expert moves13. These policies are used to narrow the search to a beam of high-probability actions, and to sample actions during rollouts. This approach has achieved strong amateur play13-15. However, prior work has been limited to shallow"
        },
        {
            "bounding_box": [
                {
                    "x": 1246,
                    "y": 1516
                },
                {
                    "x": 2308,
                    "y": 1516
                },
                {
                    "x": 2308,
                    "y": 1605
                },
                {
                    "x": 1246,
                    "y": 1605
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:16px'>policies13-15 or value functions16 based on a linear combination of<br>input features.</p>",
            "id": 7,
            "page": 1,
            "text": "policies13-15 or value functions16 based on a linear combination of input features."
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 1607
                },
                {
                    "x": 2309,
                    "y": 1607
                },
                {
                    "x": 2309,
                    "y": 2047
                },
                {
                    "x": 1245,
                    "y": 2047
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:16px'>Recently, deep convolutional neural networks have achieved unprec-<br>edented performance in visual domains: for example, image classifica-<br>tion17, face recognition 18 playing Atari games19. · They use many<br>, and<br>layers of neurons, each arranged in overlapping tiles, to construct<br>increasingly abstract, localized representations of an image20. We<br>employ a similar architecture for the game of Go. We pass in the board<br>position as a 19 x 19 image and use convolutional layers to construct a<br>representation of the position. We use these neural networks to reduce<br>the effective depth and breadth of the search tree: evaluating positions<br>using a value network, and sampling actions using a policy network.</p>",
            "id": 8,
            "page": 1,
            "text": "Recently, deep convolutional neural networks have achieved unprecedented performance in visual domains: for example, image classification17, face recognition 18 playing Atari games19. · They use many , and layers of neurons, each arranged in overlapping tiles, to construct increasingly abstract, localized representations of an image20. We employ a similar architecture for the game of Go. We pass in the board position as a 19 x 19 image and use convolutional layers to construct a representation of the position. We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network."
        },
        {
            "bounding_box": [
                {
                    "x": 1247,
                    "y": 2044
                },
                {
                    "x": 2310,
                    "y": 2044
                },
                {
                    "x": 2310,
                    "y": 2616
                },
                {
                    "x": 1247,
                    "y": 2616
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>We train the neural networks using a pipeline consisting of several<br>stages of machine learning (Fig. 1). We begin by training a supervised<br>learning (SL) policy network Po directly from expert human moves.<br>This provides fast, efficient learning updates with immediate feedback<br>and high-quality gradients. Similar to prior work13,15 we also train a<br>,<br>fast policy P� that can rapidly sample actions during rollouts. Next, we<br>train a reinforcement learning (RL) policy network Pp that improves<br>the SL policy network by optimizing the final outcome of games of self-<br>play. This adjusts the policy towards the correct goal of winning games,<br>rather than maximizing predictive accuracy. Finally, we train a value<br>network vo that predicts the winner of games played by the RL policy<br>network against itself. Our program AlphaGo efficiently combines the<br>policy and value networks with MCTS.</p>",
            "id": 9,
            "page": 1,
            "text": "We train the neural networks using a pipeline consisting of several stages of machine learning (Fig. 1). We begin by training a supervised learning (SL) policy network Po directly from expert human moves. This provides fast, efficient learning updates with immediate feedback and high-quality gradients. Similar to prior work13,15 we also train a , fast policy P� that can rapidly sample actions during rollouts. Next, we train a reinforcement learning (RL) policy network Pp that improves the SL policy network by optimizing the final outcome of games of selfplay. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy. Finally, we train a value network vo that predicts the winner of games played by the RL policy network against itself. Our program AlphaGo efficiently combines the policy and value networks with MCTS."
        },
        {
            "bounding_box": [
                {
                    "x": 1247,
                    "y": 2654
                },
                {
                    "x": 1988,
                    "y": 2654
                },
                {
                    "x": 1988,
                    "y": 2701
                },
                {
                    "x": 1247,
                    "y": 2701
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:20px'>Supervised learning of policy networks</p>",
            "id": 10,
            "page": 1,
            "text": "Supervised learning of policy networks"
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 2700
                },
                {
                    "x": 2308,
                    "y": 2700
                },
                {
                    "x": 2308,
                    "y": 3009
                },
                {
                    "x": 1245,
                    "y": 3009
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>For the first stage of the training pipeline, we build on prior work<br>on predicting expert moves in the game of Go using supervised<br>learning13,21-24 The SL policy network po(a|s) alternates between con-<br>volutional layers with weights �, and rectifier nonlinearities. A final soft-<br>max layer outputs a probability distribution over all legal moves a. The<br>input s to the policy network is a simple representation of the board state<br>(see Extended Data Table 2). The policy network is trained on randomly</p>",
            "id": 11,
            "page": 1,
            "text": "For the first stage of the training pipeline, we build on prior work on predicting expert moves in the game of Go using supervised learning13,21-24 The SL policy network po(a|s) alternates between convolutional layers with weights �, and rectifier nonlinearities. A final softmax layer outputs a probability distribution over all legal moves a. The input s to the policy network is a simple representation of the board state (see Extended Data Table 2). The policy network is trained on randomly"
        },
        {
            "bounding_box": [
                {
                    "x": 148,
                    "y": 3050
                },
                {
                    "x": 1726,
                    "y": 3050
                },
                {
                    "x": 1726,
                    "y": 3120
                },
                {
                    "x": 148,
                    "y": 3120
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:14px'>1Google DeepMind, 5 New Street Square, London EC4A 3TW, UK. 2Google, 1600 Amphitheatre Parkway, Mountain View, California 94043, USA.<br>*These authors contributed equally to this work.</p>",
            "id": 12,
            "page": 1,
            "text": "1Google DeepMind, 5 New Street Square, London EC4A 3TW, UK. 2Google, 1600 Amphitheatre Parkway, Mountain View, California 94043, USA. *These authors contributed equally to this work."
        },
        {
            "bounding_box": [
                {
                    "x": 144,
                    "y": 3166
                },
                {
                    "x": 1572,
                    "y": 3166
                },
                {
                    "x": 1572,
                    "y": 3241
                },
                {
                    "x": 144,
                    "y": 3241
                }
            ],
            "category": "footer",
            "html": "<footer id='13' style='font-size:14px'>4 8 4 I N A T U R E I V 0 L 5 2 9 I 2 8 J A N U A R Y 2 0 1 6<br>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 13,
            "page": 1,
            "text": "4 8 4 I N A T U R E I V 0 L 5 2 9 I 2 8 J A N U A R Y 2 0 1 6 Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1963,
                    "y": 98
                },
                {
                    "x": 2320,
                    "y": 98
                },
                {
                    "x": 2320,
                    "y": 153
                },
                {
                    "x": 1963,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='14' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 14,
            "page": 2,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 262,
                    "y": 221
                },
                {
                    "x": 2229,
                    "y": 221
                },
                {
                    "x": 2229,
                    "y": 939
                },
                {
                    "x": 262,
                    "y": 939
                }
            ],
            "category": "figure",
            "html": "<figure><img id='15' style='font-size:18px' alt=\"a b\nRollout policy SL policy network RL policy network Value network Policy network Value network\nNeural\nP� Po p p Polp (als) vo (s')\nvo\nnetwork\nPolicy gradient\nClassification\nClassification\nSelf Play\nRegression\nData\ns s\nHuman expert positions Self-play positions\" data-coord=\"top-left:(262,221); bottom-right:(2229,939)\" /></figure>",
            "id": 15,
            "page": 2,
            "text": "a b Rollout policy SL policy network RL policy network Value network Policy network Value network Neural P� Po p p Polp (als) vo (s') vo network Policy gradient Classification Classification Self Play Regression Data s s Human expert positions Self-play positions"
        },
        {
            "bounding_box": [
                {
                    "x": 168,
                    "y": 950
                },
                {
                    "x": 1210,
                    "y": 950
                },
                {
                    "x": 1210,
                    "y": 1313
                },
                {
                    "x": 168,
                    "y": 1313
                }
            ],
            "category": "caption",
            "html": "<br><caption id='16' style='font-size:18px'>Figure 1 I Neural network training pipeline and architecture. a, A fast<br>rollout policy P� and supervised learning (SL) policy network Po are<br>trained to predict human expert moves in a data set of positions.<br>A reinforcement learning (RL) policy network Pp is initialized to the SL<br>policy network, and is then improved by policy gradient learning to<br>maximize the outcome (that is, winning more games) against previous<br>versions of the policy network. A new data set is generated by playing<br>games of self-play with the RL policy network. Finally, a value network vo<br>is trained by regression to predict the expected outcome (that is, whether</caption>",
            "id": 16,
            "page": 2,
            "text": "Figure 1 I Neural network training pipeline and architecture. a, A fast rollout policy P� and supervised learning (SL) policy network Po are trained to predict human expert moves in a data set of positions. A reinforcement learning (RL) policy network Pp is initialized to the SL policy network, and is then improved by policy gradient learning to maximize the outcome (that is, winning more games) against previous versions of the policy network. A new data set is generated by playing games of self-play with the RL policy network. Finally, a value network vo is trained by regression to predict the expected outcome (that is, whether"
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 1351
                },
                {
                    "x": 1236,
                    "y": 1351
                },
                {
                    "x": 1236,
                    "y": 1442
                },
                {
                    "x": 169,
                    "y": 1442
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:18px'>sampled state-action pairs (s, a), using stochastic gradient ascent to<br>maximize the likelihood of the human move a selected in state s</p>",
            "id": 17,
            "page": 2,
            "text": "sampled state-action pairs (s, a), using stochastic gradient ascent to maximize the likelihood of the human move a selected in state s"
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 1595
                },
                {
                    "x": 1239,
                    "y": 1595
                },
                {
                    "x": 1239,
                    "y": 2172
                },
                {
                    "x": 169,
                    "y": 2172
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:18px'>We trained a 13-layer policy network, which we call the SL policy<br>network, from 30 million positions from the KGS Go Server. The net-<br>work predicted expert moves on a held out test set with an accuracy of<br>57.0% using all input features, and 55.7% using only raw board posi-<br>tion and move history as inputs, compared to the state-of-the-art from<br>other research groups of 44.4% at date of submission24 (full results in<br>Extended Data Table 3). Small improvements in accuracy led to large<br>improvements in playing strength (Fig. 2a); larger networks achieve<br>better accuracy but are slower to evaluate during search. We also<br>trained a faster but less accurate rollout policy p�(a|s), using a linear<br>softmax of small pattern features (see Extended Data Table 4) with<br>weights �; this achieved an accuracy of 24.2%, using just 2�s to select<br>an action, rather than 3 ms for the policy network.</p>",
            "id": 18,
            "page": 2,
            "text": "We trained a 13-layer policy network, which we call the SL policy network, from 30 million positions from the KGS Go Server. The network predicted expert moves on a held out test set with an accuracy of 57.0% using all input features, and 55.7% using only raw board position and move history as inputs, compared to the state-of-the-art from other research groups of 44.4% at date of submission24 (full results in Extended Data Table 3). Small improvements in accuracy led to large improvements in playing strength (Fig. 2a); larger networks achieve better accuracy but are slower to evaluate during search. We also trained a faster but less accurate rollout policy p�(a|s), using a linear softmax of small pattern features (see Extended Data Table 4) with weights �; this achieved an accuracy of 24.2%, using just 2�s to select an action, rather than 3 ms for the policy network."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 951
                },
                {
                    "x": 2323,
                    "y": 951
                },
                {
                    "x": 2323,
                    "y": 1314
                },
                {
                    "x": 1269,
                    "y": 1314
                }
            ],
            "category": "caption",
            "html": "<br><caption id='19' style='font-size:16px'>the current player wins) in positions from the self-play data set.<br>b, Schematic representation of the neural network architecture used in<br>AlphaGo. The policy network takes a representation of the board position<br>s as its input, passes it through many convolutional layers with parameters<br>0 (SL policy network) or P (RL policy network), and outputs a probability<br>distribution Po (a|s) or pp (a|s) over legal moves a, represented by a<br>probability map over the board. The value network similarly uses many<br>convolutional layers with parameters 0, but outputs a scalar value vo(s')<br>that predicts the expected outcome in position s'.</caption>",
            "id": 19,
            "page": 2,
            "text": "the current player wins) in positions from the self-play data set. b, Schematic representation of the neural network architecture used in AlphaGo. The policy network takes a representation of the board position s as its input, passes it through many convolutional layers with parameters 0 (SL policy network) or P (RL policy network), and outputs a probability distribution Po (a|s) or pp (a|s) over legal moves a, represented by a probability map over the board. The value network similarly uses many convolutional layers with parameters 0, but outputs a scalar value vo(s') that predicts the expected outcome in position s'."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1350
                },
                {
                    "x": 2337,
                    "y": 1350
                },
                {
                    "x": 2337,
                    "y": 1793
                },
                {
                    "x": 1268,
                    "y": 1793
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>and its weights P are initialized to the same values, P = �. We play<br>games between the current policy network Pp and a randomly selected<br>previous iteration of the policy network. Randomizing from a pool<br>of opponents in this way stabilizes training by preventing overfitting<br>to the current policy. We use a reward function r(s) that is zero for all<br>non-terminal time steps t < T. The outcome Zt= 士 r(sT) is the termi-<br>nal reward at the end of the game from the perspective of the current<br>player at time step t: +1 for winning and - 1 for losing. Weights are<br>then updated at each time step t by stochastic gradient ascent in the<br>direction that maximizes expected outcome25</p>",
            "id": 20,
            "page": 2,
            "text": "and its weights P are initialized to the same values, P = �. We play games between the current policy network Pp and a randomly selected previous iteration of the policy network. Randomizing from a pool of opponents in this way stabilizes training by preventing overfitting to the current policy. We use a reward function r(s) that is zero for all non-terminal time steps t < T. The outcome Zt= 士 r(sT) is the terminal reward at the end of the game from the perspective of the current player at time step t: +1 for winning and - 1 for losing. Weights are then updated at each time step t by stochastic gradient ascent in the direction that maximizes expected outcome25"
        },
        {
            "bounding_box": [
                {
                    "x": 172,
                    "y": 2205
                },
                {
                    "x": 985,
                    "y": 2205
                },
                {
                    "x": 985,
                    "y": 2254
                },
                {
                    "x": 172,
                    "y": 2254
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:22px'>Reinforcement learning of policy networks</p>",
            "id": 21,
            "page": 2,
            "text": "Reinforcement learning of policy networks"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 2254
                },
                {
                    "x": 1235,
                    "y": 2254
                },
                {
                    "x": 1235,
                    "y": 2390
                },
                {
                    "x": 170,
                    "y": 2390
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:20px'>The second stage of the training pipeline aims at improving the policy<br>network by policy gradient reinforcement learning (RL)25,26. The RL<br>policy network Pp is identical in structure to the SL policy network,</p>",
            "id": 22,
            "page": 2,
            "text": "The second stage of the training pipeline aims at improving the policy network by policy gradient reinforcement learning (RL)25,26. The RL policy network Pp is identical in structure to the SL policy network,"
        },
        {
            "bounding_box": [
                {
                    "x": 480,
                    "y": 2417
                },
                {
                    "x": 1225,
                    "y": 2417
                },
                {
                    "x": 1225,
                    "y": 2827
                },
                {
                    "x": 480,
                    "y": 2827
                }
            ],
            "category": "figure",
            "html": "<figure><img id='23' style='font-size:14px' alt=\"70\n128 filters\n60 192 filters\n(%)\n256 filters\n50\nrate\n384 filters\n40\nwin\n30\nAlphaGo\n20\n10\n0\n50 51 52 53 54 55 56 57 58 59\nTraining accuracy on KGS dataset (%)\" data-coord=\"top-left:(480,2417); bottom-right:(1225,2827)\" /></figure>",
            "id": 23,
            "page": 2,
            "text": "70 128 filters 60 192 filters (%) 256 filters 50 rate 384 filters 40 win 30 AlphaGo 20 10 0 50 51 52 53 54 55 56 57 58 59 Training accuracy on KGS dataset (%)"
        },
        {
            "bounding_box": [
                {
                    "x": 168,
                    "y": 2839
                },
                {
                    "x": 1221,
                    "y": 2839
                },
                {
                    "x": 1221,
                    "y": 3122
                },
                {
                    "x": 168,
                    "y": 3122
                }
            ],
            "category": "caption",
            "html": "<br><caption id='24' style='font-size:18px'>Figure 2 I Strength and accuracy of policy and value networks.<br>a, Plot showing the playing strength of policy networks as a function<br>of their training accuracy. Policy networks with 128, 192, 256 and 384<br>convolutional filters per layer were evaluated periodically during training;<br>the plot shows the winning rate of AlphaGo using that policy network<br>against the match version of AlphaGo. b, Comparison of evaluation<br>accuracy between the value network and rollouts with different policies.</caption>",
            "id": 24,
            "page": 2,
            "text": "Figure 2 I Strength and accuracy of policy and value networks. a, Plot showing the playing strength of policy networks as a function of their training accuracy. Policy networks with 128, 192, 256 and 384 convolutional filters per layer were evaluated periodically during training; the plot shows the winning rate of AlphaGo using that policy network against the match version of AlphaGo. b, Comparison of evaluation accuracy between the value network and rollouts with different policies."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1987
                },
                {
                    "x": 2337,
                    "y": 1987
                },
                {
                    "x": 2337,
                    "y": 2388
                },
                {
                    "x": 1268,
                    "y": 2388
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:20px'>We evaluated the performance of the RL policy network in game<br>play, sampling each move at ~ pp (·|st) from its output probability<br>distribution over actions. When played head-to-head, the RL policy<br>network won more than 80% of games against the SL policy network.<br>We also tested against the strongest open-source Go program, Pachi14,<br>a sophisticated Monte Carlo search program, ranked at 2 amateur dan<br>on KGS, that executes 100,000 simulations per move. Using no search<br>at all, the RL policy network won 85% of games against Pachi. In com-<br>parison, the previous state-of-the-art, based only on supervised</p>",
            "id": 25,
            "page": 2,
            "text": "We evaluated the performance of the RL policy network in game play, sampling each move at ~ pp (·|st) from its output probability distribution over actions. When played head-to-head, the RL policy network won more than 80% of games against the SL policy network. We also tested against the strongest open-source Go program, Pachi14, a sophisticated Monte Carlo search program, ranked at 2 amateur dan on KGS, that executes 100,000 simulations per move. Using no search at all, the RL policy network won 85% of games against Pachi. In comparison, the previous state-of-the-art, based only on supervised"
        },
        {
            "bounding_box": [
                {
                    "x": 1301,
                    "y": 2416
                },
                {
                    "x": 1331,
                    "y": 2416
                },
                {
                    "x": 1331,
                    "y": 2450
                },
                {
                    "x": 1301,
                    "y": 2450
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:16px'>b</p>",
            "id": 26,
            "page": 2,
            "text": "b"
        },
        {
            "bounding_box": [
                {
                    "x": 1298,
                    "y": 2441
                },
                {
                    "x": 2024,
                    "y": 2441
                },
                {
                    "x": 2024,
                    "y": 2827
                },
                {
                    "x": 1298,
                    "y": 2827
                }
            ],
            "category": "figure",
            "html": "<figure><img id='27' style='font-size:14px' alt=\"0.50\n0.45\nerror\n0.40\ngames\n0.35 ····· Uniform random\nsquared\nexpert\n0.30 rollout policy\n···· Fast rollout policy\n0.25\nMean\nValue network\non\n0.20\nSL policy network\n0.15\nRL policy network\n0.10\n15 45 75 105 135 165 195 225 255 >285\nMove number\" data-coord=\"top-left:(1298,2441); bottom-right:(2024,2827)\" /></figure>",
            "id": 27,
            "page": 2,
            "text": "0.50 0.45 error 0.40 games 0.35 ····· Uniform random squared expert 0.30 rollout policy ···· Fast rollout policy 0.25 Mean Value network on 0.20 SL policy network 0.15 RL policy network 0.10 15 45 75 105 135 165 195 225 255 >285 Move number"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2838
                },
                {
                    "x": 2321,
                    "y": 2838
                },
                {
                    "x": 2321,
                    "y": 3123
                },
                {
                    "x": 1269,
                    "y": 3123
                }
            ],
            "category": "caption",
            "html": "<br><caption id='28' style='font-size:16px'>Positions and outcomes were sampled from human expert games. Each<br>position was evaluated by a single forward pass ofthe value network VO,<br>or by the mean outcome of 100 rollouts, played out using either uniform<br>random rollouts, the fast rollout policy P�, the SL policy network Po or<br>the RL policy network Pp. The mean squared error between the predicted<br>value and the actual game outcome is plotted against the stage of the game<br>(how many moves had been played in the given position).</caption>",
            "id": 28,
            "page": 2,
            "text": "Positions and outcomes were sampled from human expert games. Each position was evaluated by a single forward pass ofthe value network VO, or by the mean outcome of 100 rollouts, played out using either uniform random rollouts, the fast rollout policy P�, the SL policy network Po or the RL policy network Pp. The mean squared error between the predicted value and the actual game outcome is plotted against the stage of the game (how many moves had been played in the given position)."
        },
        {
            "bounding_box": [
                {
                    "x": 1549,
                    "y": 3173
                },
                {
                    "x": 1589,
                    "y": 3173
                },
                {
                    "x": 1589,
                    "y": 3198
                },
                {
                    "x": 1549,
                    "y": 3198
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:14px'>2 8</p>",
            "id": 29,
            "page": 2,
            "text": "2 8"
        },
        {
            "bounding_box": [
                {
                    "x": 911,
                    "y": 3205
                },
                {
                    "x": 1597,
                    "y": 3205
                },
                {
                    "x": 1597,
                    "y": 3242
                },
                {
                    "x": 911,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<br><footer id='30' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 30,
            "page": 2,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1587,
                    "y": 3167
                },
                {
                    "x": 2336,
                    "y": 3167
                },
                {
                    "x": 2336,
                    "y": 3204
                },
                {
                    "x": 1587,
                    "y": 3204
                }
            ],
            "category": "footer",
            "html": "<br><footer id='31' style='font-size:14px'>J A N U A R Y 2 0 1 6 I v 0 L 5 2 9 T N A T U R E  4 8 5</footer>",
            "id": 31,
            "page": 2,
            "text": "J A N U A R Y 2 0 1 6 I v 0 L 5 2 9 T N A T U R E  4 8 5"
        },
        {
            "bounding_box": [
                {
                    "x": 158,
                    "y": 98
                },
                {
                    "x": 516,
                    "y": 98
                },
                {
                    "x": 516,
                    "y": 153
                },
                {
                    "x": 158,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='32' style='font-size:22px'>RESEARCH ARTICLE</header>",
            "id": 32,
            "page": 3,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 259,
                    "y": 209
                },
                {
                    "x": 2182,
                    "y": 209
                },
                {
                    "x": 2182,
                    "y": 847
                },
                {
                    "x": 259,
                    "y": 847
                }
            ],
            "category": "figure",
            "html": "<figure><img id='33' style='font-size:14px' alt=\"a Selection b Expansion c Evaluation d Backup\n□ vo\nQ + u(P) max Q + u(P)\nvo vo\nQ + u(P) Vmax Q + u(P)\nPo vu ) V V\n0 0\n~ p �\nr r )\" data-coord=\"top-left:(259,209); bottom-right:(2182,847)\" /></figure>",
            "id": 33,
            "page": 3,
            "text": "a Selection b Expansion c Evaluation d Backup □ vo Q + u(P) max Q + u(P) vo vo Q + u(P) Vmax Q + u(P) Po vu ) V V 0 0 ~ p � r r )"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 853
                },
                {
                    "x": 1183,
                    "y": 853
                },
                {
                    "x": 1183,
                    "y": 1098
                },
                {
                    "x": 146,
                    "y": 1098
                }
            ],
            "category": "caption",
            "html": "<br><caption id='34' style='font-size:14px'>Figure 3 I Monte Carlo tree search in AlphaGo. a, Each simulation<br>traverses the tree by selecting the edge with maximum action value Q,<br>plus a bonus u(P) that depends on a stored prior probability P for that<br>edge. b, The leaf node may be expanded; the new node is processed once<br>by the policy network Po and the output probabilities are stored as prior<br>probabilities P for each action. c, At the end of a simulation, the leaf node</caption>",
            "id": 34,
            "page": 3,
            "text": "Figure 3 I Monte Carlo tree search in AlphaGo. a, Each simulation traverses the tree by selecting the edge with maximum action value Q, plus a bonus u(P) that depends on a stored prior probability P for that edge. b, The leaf node may be expanded; the new node is processed once by the policy network Po and the output probabilities are stored as prior probabilities P for each action. c, At the end of a simulation, the leaf node"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 1137
                },
                {
                    "x": 1207,
                    "y": 1137
                },
                {
                    "x": 1207,
                    "y": 1229
                },
                {
                    "x": 146,
                    "y": 1229
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>learning of convolutional networks, won 11% of games against Pachi23<br>and 12% against a slightly weaker program, Fuego24.</p>",
            "id": 35,
            "page": 3,
            "text": "learning of convolutional networks, won 11% of games against Pachi23 and 12% against a slightly weaker program, Fuego24."
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 1259
                },
                {
                    "x": 945,
                    "y": 1259
                },
                {
                    "x": 945,
                    "y": 1308
                },
                {
                    "x": 147,
                    "y": 1308
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:20px'>Reinforcement learning of value networks</p>",
            "id": 36,
            "page": 3,
            "text": "Reinforcement learning of value networks"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 1308
                },
                {
                    "x": 1211,
                    "y": 1308
                },
                {
                    "x": 1211,
                    "y": 1441
                },
                {
                    "x": 146,
                    "y": 1441
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:18px'>The final stage of the training pipeline focuses on position evaluation,<br>estimating a value function vP(s) that predicts the outcome from posi-<br>tion s of games played by using policy p for both players28-30</p>",
            "id": 37,
            "page": 3,
            "text": "The final stage of the training pipeline focuses on position evaluation, estimating a value function vP(s) that predicts the outcome from position s of games played by using policy p for both players28-30"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 1558
                },
                {
                    "x": 1211,
                    "y": 1558
                },
                {
                    "x": 1211,
                    "y": 2001
                },
                {
                    "x": 146,
                    "y": 2001
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:16px'>Ideally, we would like to know the optimal value function under<br>perfect play レ* (s); in practice, we instead estimate the value function<br>VPP for our strongest policy, using the RL policy network pp. We approx-<br>imate the value function using a value network vo(s) with weights 0,<br>vo(s) 21 vPp(S) 21 v*(s). This neural network has a similar architecture<br>to the policy network, but outputs a single prediction instead of a prob-<br>ability distribution. We train the weights of the value network by regres-<br>sion on state-outcome pairs (s, z), using stochastic gradient descent to<br>minimize the mean squared error (MSE) between the predicted value<br>vo(s), and the corresponding outcome z</p>",
            "id": 38,
            "page": 3,
            "text": "Ideally, we would like to know the optimal value function under perfect play レ* (s); in practice, we instead estimate the value function VPP for our strongest policy, using the RL policy network pp. We approximate the value function using a value network vo(s) with weights 0, vo(s) 21 vPp(S) 21 v*(s). This neural network has a similar architecture to the policy network, but outputs a single prediction instead of a probability distribution. We train the weights of the value network by regression on state-outcome pairs (s, z), using stochastic gradient descent to minimize the mean squared error (MSE) between the predicted value vo(s), and the corresponding outcome z"
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 859
                },
                {
                    "x": 2273,
                    "y": 859
                },
                {
                    "x": 2273,
                    "y": 1058
                },
                {
                    "x": 1245,
                    "y": 1058
                }
            ],
            "category": "caption",
            "html": "<br><caption id='39' style='font-size:14px'>is evaluated in two ways: using the value network v0; and by running<br>a rollout to the end of the game with the fast rollout policy P�, then<br>computing the winner with function r. d, Action values Q are updated to<br>track the mean value of all evaluations r(·) and vo(·) in the subtree below<br>that action.</caption>",
            "id": 39,
            "page": 3,
            "text": "is evaluated in two ways: using the value network v0; and by running a rollout to the end of the game with the fast rollout policy P�, then computing the winner with function r. d, Action values Q are updated to track the mean value of all evaluations r(·) and vo(·) in the subtree below that action."
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 1138
                },
                {
                    "x": 2310,
                    "y": 1138
                },
                {
                    "x": 2310,
                    "y": 1358
                },
                {
                    "x": 1245,
                    "y": 1358
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:16px'>(s, a) ofthe search tree stores an action value Q(s, a), visit count N(s, a),<br>and prior probability P(s, a). The tree is traversed by simulation (that<br>is, descending the tree in complete games without backup), starting<br>from the root state. At each time step tof each simulation, an action at<br>is selected from state St</p>",
            "id": 40,
            "page": 3,
            "text": "(s, a) ofthe search tree stores an action value Q(s, a), visit count N(s, a), and prior probability P(s, a). The tree is traversed by simulation (that is, descending the tree in complete games without backup), starting from the root state. At each time step tof each simulation, an action at is selected from state St"
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 2163
                },
                {
                    "x": 1212,
                    "y": 2163
                },
                {
                    "x": 1212,
                    "y": 2957
                },
                {
                    "x": 147,
                    "y": 2957
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>The naive approach of predicting game outcomes from data con-<br>sisting of complete games leads to overfitting. The problem is that<br>successive positions are strongly correlated, differing by just one stone,<br>but the regression target is shared for the entire game. When trained<br>on the KGS data set in this way, the value network memorized the<br>game outcomes rather than generalizing to new positions, achieving a<br>minimum MSE of0.37 on the test set, compared to 0.19 on the training<br>set. To mitigate this problem, we generated a new self-play data set<br>consisting of 30 million distinct positions, each sampled from a sepa-<br>rate game. Each game was played between the RL policy network and<br>itself until the game terminated. Training on this data set led to MSEs<br>of0.226 and 0.234 on the training and test set respectively, indicating<br>minimal overfitting. Figure 2b shows the position evaluation accuracy<br>of the value network, compared to Monte Carlo rollouts using the fast<br>rollout policy P�; the value function was consistently more accurate.<br>A single evaluation of vo(s) also approached the accuracy of Monte<br>Carlo rollouts using the RL policy network pp, but using 15,000 times<br>less computation.</p>",
            "id": 41,
            "page": 3,
            "text": "The naive approach of predicting game outcomes from data consisting of complete games leads to overfitting. The problem is that successive positions are strongly correlated, differing by just one stone, but the regression target is shared for the entire game. When trained on the KGS data set in this way, the value network memorized the game outcomes rather than generalizing to new positions, achieving a minimum MSE of0.37 on the test set, compared to 0.19 on the training set. To mitigate this problem, we generated a new self-play data set consisting of 30 million distinct positions, each sampled from a separate game. Each game was played between the RL policy network and itself until the game terminated. Training on this data set led to MSEs of0.226 and 0.234 on the training and test set respectively, indicating minimal overfitting. Figure 2b shows the position evaluation accuracy of the value network, compared to Monte Carlo rollouts using the fast rollout policy P�; the value function was consistently more accurate. A single evaluation of vo(s) also approached the accuracy of Monte Carlo rollouts using the RL policy network pp, but using 15,000 times less computation."
        },
        {
            "bounding_box": [
                {
                    "x": 148,
                    "y": 2987
                },
                {
                    "x": 942,
                    "y": 2987
                },
                {
                    "x": 942,
                    "y": 3034
                },
                {
                    "x": 148,
                    "y": 3034
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:22px'>Searching with policy and value networks</p>",
            "id": 42,
            "page": 3,
            "text": "Searching with policy and value networks"
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 1516
                },
                {
                    "x": 1916,
                    "y": 1516
                },
                {
                    "x": 1916,
                    "y": 1561
                },
                {
                    "x": 1245,
                    "y": 1561
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:14px'>so as to maximize action value plus a bonus</p>",
            "id": 43,
            "page": 3,
            "text": "so as to maximize action value plus a bonus"
        },
        {
            "bounding_box": [
                {
                    "x": 148,
                    "y": 3036
                },
                {
                    "x": 1210,
                    "y": 3036
                },
                {
                    "x": 1210,
                    "y": 3124
                },
                {
                    "x": 148,
                    "y": 3124
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:18px'>AlphaGo combines the policy and value networks in an MCTS algo-<br>rithm (Fig. 3) that selects actions by lookahead search. Each edge</p>",
            "id": 44,
            "page": 3,
            "text": "AlphaGo combines the policy and value networks in an MCTS algorithm (Fig. 3) that selects actions by lookahead search. Each edge"
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 1742
                },
                {
                    "x": 2309,
                    "y": 1742
                },
                {
                    "x": 2309,
                    "y": 2183
                },
                {
                    "x": 1245,
                    "y": 2183
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:16px'>that is proportional to the prior probability but decays with<br>repeated visits to encourage exploration. When the traversal reaches a<br>leafnode SL at step L, the leaf node may be expanded. The leaf position<br>SL is processed just once by the SL policy network Po. The output prob-<br>abilities are stored as prior probabilities P for each legal action a,<br>P(s,a) = Po (a|s). The leaf node is evaluated in two very different ways:<br>first, by the value network v0(SL); and second, by the outcome ZL of a<br>random rollout played out until terminal step T using the fast rollout<br>policy p�; these evaluations are combined, using a mixing parameter<br>入, into a leaf evaluation V(SL)</p>",
            "id": 45,
            "page": 3,
            "text": "that is proportional to the prior probability but decays with repeated visits to encourage exploration. When the traversal reaches a leafnode SL at step L, the leaf node may be expanded. The leaf position SL is processed just once by the SL policy network Po. The output probabilities are stored as prior probabilities P for each legal action a, P(s,a) = Po (a|s). The leaf node is evaluated in two very different ways: first, by the value network v0(SL); and second, by the outcome ZL of a random rollout played out until terminal step T using the fast rollout policy p�; these evaluations are combined, using a mixing parameter 入, into a leaf evaluation V(SL)"
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 2304
                },
                {
                    "x": 2307,
                    "y": 2304
                },
                {
                    "x": 2307,
                    "y": 2441
                },
                {
                    "x": 1245,
                    "y": 2441
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:16px'>At the end of simulation, the action values and visit counts of all<br>traversed edges are updated. Each edge accumulates the visit count and<br>mean evaluation of all simulations passing through that edge</p>",
            "id": 46,
            "page": 3,
            "text": "At the end of simulation, the action values and visit counts of all traversed edges are updated. Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge"
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 2726
                },
                {
                    "x": 2308,
                    "y": 2726
                },
                {
                    "x": 2308,
                    "y": 2903
                },
                {
                    "x": 1245,
                    "y": 2903
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:16px'>wheres'L is the leaf node from the ith simulation, and 1(s, a, i) indicates<br>whether an edge (s, a) was traversed during the ith simulation. Once<br>the search is complete, the algorithm chooses the most visited move<br>from the root position.</p>",
            "id": 47,
            "page": 3,
            "text": "wheres'L is the leaf node from the ith simulation, and 1(s, a, i) indicates whether an edge (s, a) was traversed during the ith simulation. Once the search is complete, the algorithm chooses the most visited move from the root position."
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 2905
                },
                {
                    "x": 2309,
                    "y": 2905
                },
                {
                    "x": 2309,
                    "y": 3126
                },
                {
                    "x": 1245,
                    "y": 3126
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:18px'>Itis worth noting that the SL policy network Po performed better in<br>AlphaGo than the stronger RL policy network pp, presumably because<br>humans select a diverse beam of promising moves, whereas RL opti-<br>mizes for the single best move. However, the value function<br>vo(s) U vPp(s) derived from the stronger RL policy network performed</p>",
            "id": 48,
            "page": 3,
            "text": "Itis worth noting that the SL policy network Po performed better in AlphaGo than the stronger RL policy network pp, presumably because humans select a diverse beam of promising moves, whereas RL optimizes for the single best move. However, the value function vo(s) U vPp(s) derived from the stronger RL policy network performed"
        },
        {
            "bounding_box": [
                {
                    "x": 143,
                    "y": 3167
                },
                {
                    "x": 1572,
                    "y": 3167
                },
                {
                    "x": 1572,
                    "y": 3243
                },
                {
                    "x": 143,
                    "y": 3243
                }
            ],
            "category": "footer",
            "html": "<footer id='49' style='font-size:14px'>4 8 6 I N A T U R E I V 0 L 5 2 9 I 2 8 J A N U A R Y 2 0 1 6<br>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 49,
            "page": 3,
            "text": "4 8 6 I N A T U R E I V 0 L 5 2 9 I 2 8 J A N U A R Y 2 0 1 6 Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1964,
                    "y": 98
                },
                {
                    "x": 2319,
                    "y": 98
                },
                {
                    "x": 2319,
                    "y": 154
                },
                {
                    "x": 1964,
                    "y": 154
                }
            ],
            "category": "header",
            "html": "<header id='50' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 50,
            "page": 4,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 320,
                    "y": 215
                },
                {
                    "x": 2013,
                    "y": 215
                },
                {
                    "x": 2013,
                    "y": 913
                },
                {
                    "x": 320,
                    "y": 913
                }
            ],
            "category": "figure",
            "html": "<figure><img id='51' style='font-size:18px' alt=\"a b C\n3,500 9p Professional\n3,500 3,500\n7p dan\n5p\n3,000- 3,000\n3,000 - 3p (p)\n1p\nI 9d 2,500- 2,500\n2,500 -\nI\nH\n7d dan\nRating\nAmateur\n2,000 工 2,000- 2,000\n工\nI 5d (d)\nElo\n1,500 - 3d 1,500- 1,500\n1d\n1,000 - 1k 1,000- 1,000\nBeginner\n3k kyu\n500 (k)\n500 - 500\n5k\n7k\n0 0\n0 AlphaGo\nAlphaGo\nFan\nFuego GnuGo Value network ● GPUs 8 1 2 4\ndistributed\nCrazy Zen Pachi\nRollouts · · Threads 1 2 4 8 16 32 40 40\nHui\nStone\nPolicy network · ·\nSingle machine\" data-coord=\"top-left:(320,215); bottom-right:(2013,913)\" /></figure>",
            "id": 51,
            "page": 4,
            "text": "a b C 3,500 9p Professional 3,500 3,500 7p dan 5p 3,000- 3,000 3,000 - 3p (p) 1p I 9d 2,500- 2,500 2,500 I H 7d dan Rating Amateur 2,000 工 2,000- 2,000 工 I 5d (d) Elo 1,500 - 3d 1,500- 1,500 1d 1,000 - 1k 1,000- 1,000 Beginner 3k kyu 500 (k) 500 - 500 5k 7k 0 0 0 AlphaGo AlphaGo Fan Fuego GnuGo Value network ● GPUs 8 1 2 4 distributed Crazy Zen Pachi Rollouts · · Threads 1 2 4 8 16 32 40 40 Hui Stone Policy network · · Single machine"
        },
        {
            "bounding_box": [
                {
                    "x": 168,
                    "y": 921
                },
                {
                    "x": 1206,
                    "y": 921
                },
                {
                    "x": 1206,
                    "y": 1287
                },
                {
                    "x": 168,
                    "y": 1287
                }
            ],
            "category": "caption",
            "html": "<br><caption id='52' style='font-size:18px'>Figure 4 I Tournament evaluation of AlphaGo. a, Results of a<br>tournament between different Go programs (see Extended Data Tables<br>6-11). Each program used approximately 5s computation time per move.<br>To provide a greater challenge to AlphaGo, some programs (pale upper<br>bars) were given four handicap stones (that is, free moves at the start of<br>every game) against all opponents. Programs were evaluated on an<br>Elo scale37: a 230 point gap corresponds to a 79% probability of winning,<br>which roughly corresponds to one amateur dan rank advantage on<br>KGS38; an approximate correspondence to human ranks is also shown,</caption>",
            "id": 52,
            "page": 4,
            "text": "Figure 4 I Tournament evaluation of AlphaGo. a, Results of a tournament between different Go programs (see Extended Data Tables 6-11). Each program used approximately 5s computation time per move. To provide a greater challenge to AlphaGo, some programs (pale upper bars) were given four handicap stones (that is, free moves at the start of every game) against all opponents. Programs were evaluated on an Elo scale37: a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur dan rank advantage on KGS38; an approximate correspondence to human ranks is also shown,"
        },
        {
            "bounding_box": [
                {
                    "x": 168,
                    "y": 1331
                },
                {
                    "x": 1236,
                    "y": 1331
                },
                {
                    "x": 1236,
                    "y": 1422
                },
                {
                    "x": 168,
                    "y": 1422
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>better in AlphaGo than a value function vo(s) 2 VPo(s) derived from the<br>SL policy network.</p>",
            "id": 53,
            "page": 4,
            "text": "better in AlphaGo than a value function vo(s) 2 VPo(s) derived from the SL policy network."
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 1421
                },
                {
                    "x": 1239,
                    "y": 1421
                },
                {
                    "x": 1239,
                    "y": 1730
                },
                {
                    "x": 170,
                    "y": 1730
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:20px'>Evaluating policy and value networks requires several orders of<br>magnitude more computation than traditional search heuristics. To<br>efficiently combine MCTS with deep neural networks, AlphaGo uses<br>an asynchronous multi-threaded search that executes simulations on<br>CPUs, and computes policy and value networks in parallel on GPUs.<br>The final version of AlphaGo used 40 search threads, 48 CPUs, and<br>8 GPUs. We also implemented a distributed version of AlphaGo that</p>",
            "id": 54,
            "page": 4,
            "text": "Evaluating policy and value networks requires several orders of magnitude more computation than traditional search heuristics. To efficiently combine MCTS with deep neural networks, AlphaGo uses an asynchronous multi-threaded search that executes simulations on CPUs, and computes policy and value networks in parallel on GPUs. The final version of AlphaGo used 40 search threads, 48 CPUs, and 8 GPUs. We also implemented a distributed version of AlphaGo that"
        },
        {
            "bounding_box": [
                {
                    "x": 2044,
                    "y": 782
                },
                {
                    "x": 2170,
                    "y": 782
                },
                {
                    "x": 2170,
                    "y": 804
                },
                {
                    "x": 2044,
                    "y": 804
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:14px'>12 24 40 64</p>",
            "id": 55,
            "page": 4,
            "text": "12 24 40 64"
        },
        {
            "bounding_box": [
                {
                    "x": 2012,
                    "y": 818
                },
                {
                    "x": 2175,
                    "y": 818
                },
                {
                    "x": 2175,
                    "y": 841
                },
                {
                    "x": 2012,
                    "y": 841
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:14px'>8 64 112176280</p>",
            "id": 56,
            "page": 4,
            "text": "8 64 112176280"
        },
        {
            "bounding_box": [
                {
                    "x": 2048,
                    "y": 884
                },
                {
                    "x": 2167,
                    "y": 884
                },
                {
                    "x": 2167,
                    "y": 911
                },
                {
                    "x": 2048,
                    "y": 911
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>Distributed</p>",
            "id": 57,
            "page": 4,
            "text": "Distributed"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 923
                },
                {
                    "x": 2319,
                    "y": 923
                },
                {
                    "x": 2319,
                    "y": 1247
                },
                {
                    "x": 1268,
                    "y": 1247
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>horizontal lines show KGS ranks achieved online by that program. Games<br>against the human European champion Fan Hui were also included;<br>these games used longer time controls. 95% confidence intervals are<br>shown. b, Performance of AlphaGo, on a single machine, for different<br>combinations of components. The version solely using the policy network<br>does not perform any search. c, Scalability study of MCTS in AlphaGo<br>with search threads and GPUs, using asynchronous search (light blue) or<br>distributed search (dark blue), for 2 s per move.</p>",
            "id": 58,
            "page": 4,
            "text": "horizontal lines show KGS ranks achieved online by that program. Games against the human European champion Fan Hui were also included; these games used longer time controls. 95% confidence intervals are shown. b, Performance of AlphaGo, on a single machine, for different combinations of components. The version solely using the policy network does not perform any search. c, Scalability study of MCTS in AlphaGo with search threads and GPUs, using asynchronous search (light blue) or distributed search (dark blue), for 2 s per move."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1332
                },
                {
                    "x": 2336,
                    "y": 1332
                },
                {
                    "x": 2336,
                    "y": 1466
                },
                {
                    "x": 1268,
                    "y": 1466
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:22px'>exploited multiple machines, 40 search threads, 1,202 CPUs and<br>176 GPUs. The Methods section provides full details ofasynchronous<br>and distributed MCTS.</p>",
            "id": 59,
            "page": 4,
            "text": "exploited multiple machines, 40 search threads, 1,202 CPUs and 176 GPUs. The Methods section provides full details ofasynchronous and distributed MCTS."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1504
                },
                {
                    "x": 2084,
                    "y": 1504
                },
                {
                    "x": 2084,
                    "y": 1554
                },
                {
                    "x": 1270,
                    "y": 1554
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:22px'>Evaluating the playing strength of AlphaGo</p>",
            "id": 60,
            "page": 4,
            "text": "Evaluating the playing strength of AlphaGo"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1554
                },
                {
                    "x": 2336,
                    "y": 1554
                },
                {
                    "x": 2336,
                    "y": 1730
                },
                {
                    "x": 1269,
                    "y": 1730
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:20px'>To evaluate AlphaGo, we ran an internal tournament among variants<br>of AlphaGo and several other Go programs, including the strongest<br>commercial programs Crazy Stone13 and Zen, and the strongest open<br>source programs Pachi14 and Fuego15, All of these programs are based</p>",
            "id": 61,
            "page": 4,
            "text": "To evaluate AlphaGo, we ran an internal tournament among variants of AlphaGo and several other Go programs, including the strongest commercial programs Crazy Stone13 and Zen, and the strongest open source programs Pachi14 and Fuego15, All of these programs are based"
        },
        {
            "bounding_box": [
                {
                    "x": 1039,
                    "y": 1772
                },
                {
                    "x": 1477,
                    "y": 1772
                },
                {
                    "x": 1477,
                    "y": 1807
                },
                {
                    "x": 1039,
                    "y": 1807
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:16px'>b Tree evaluation from value net</p>",
            "id": 62,
            "page": 4,
            "text": "b Tree evaluation from value net"
        },
        {
            "bounding_box": [
                {
                    "x": 1556,
                    "y": 1773
                },
                {
                    "x": 1931,
                    "y": 1773
                },
                {
                    "x": 1931,
                    "y": 1807
                },
                {
                    "x": 1556,
                    "y": 1807
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:16px'>Tree evaluation from rollouts</p>",
            "id": 63,
            "page": 4,
            "text": "Tree evaluation from rollouts"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1788
                },
                {
                    "x": 1970,
                    "y": 1788
                },
                {
                    "x": 1970,
                    "y": 2780
                },
                {
                    "x": 549,
                    "y": 2780
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='64' style='font-size:14px' alt=\"a Value network c\n25\n20\nd Policy network e Percentage of simulations f Principal variation\n11\n2 3 3 13\n19 23\n25\n21\n27\" data-coord=\"top-left:(549,1788); bottom-right:(1970,2780)\" /></figure>",
            "id": 64,
            "page": 4,
            "text": "a Value network c 25 20 d Policy network e Percentage of simulations f Principal variation 11 2 3 3 13 19 23 25 21 27"
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 2798
                },
                {
                    "x": 1182,
                    "y": 2798
                },
                {
                    "x": 1182,
                    "y": 3122
                },
                {
                    "x": 169,
                    "y": 3122
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:18px'>Figure 5 I How AlphaGo (black, to play) selected its move in an<br>informal game against Fan Hui. For each of the following statistics,<br>the location of the maximum value is indicated by an orange circle.<br>a, Evaluation of all successors s' of the root position s, using the value<br>network vo(s'); estimated winning percentages are shown for the top<br>evaluations. b, Action values Q(s, a) for each edge (s, a) in the tree from<br>root position S; averaged over value network evaluations only (入=0).<br>c, Action values Q(s, a), averaged over rollout evaluations only (入=1).</p>",
            "id": 65,
            "page": 4,
            "text": "Figure 5 I How AlphaGo (black, to play) selected its move in an informal game against Fan Hui. For each of the following statistics, the location of the maximum value is indicated by an orange circle. a, Evaluation of all successors s' of the root position s, using the value network vo(s'); estimated winning percentages are shown for the top evaluations. b, Action values Q(s, a) for each edge (s, a) in the tree from root position S; averaged over value network evaluations only (入=0). c, Action values Q(s, a), averaged over rollout evaluations only (入=1)."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2800
                },
                {
                    "x": 2311,
                    "y": 2800
                },
                {
                    "x": 2311,
                    "y": 3124
                },
                {
                    "x": 1269,
                    "y": 3124
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:18px'>d, Move probabilities directly from the SL policy network, Po (a|s);<br>reported as a percentage (if above 0.1%). e, Percentage frequency with<br>which actions were selected from the root during simulations. f, The<br>principal variation (path with maximum visit count) from AlphaGo's<br>search tree. The moves are presented in a numbered sequence. AlphaGo<br>selected the move indicated by the red circle; Fan Hui responded with the<br>move indicated by the white square; in his post-game commentary he<br>preferred the move (labelled 1) predicted by AlphaGo.</p>",
            "id": 66,
            "page": 4,
            "text": "d, Move probabilities directly from the SL policy network, Po (a|s); reported as a percentage (if above 0.1%). e, Percentage frequency with which actions were selected from the root during simulations. f, The principal variation (path with maximum visit count) from AlphaGo's search tree. The moves are presented in a numbered sequence. AlphaGo selected the move indicated by the red circle; Fan Hui responded with the move indicated by the white square; in his post-game commentary he preferred the move (labelled 1) predicted by AlphaGo."
        },
        {
            "bounding_box": [
                {
                    "x": 1549,
                    "y": 3173
                },
                {
                    "x": 1590,
                    "y": 3173
                },
                {
                    "x": 1590,
                    "y": 3198
                },
                {
                    "x": 1549,
                    "y": 3198
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:14px'>2 8</p>",
            "id": 67,
            "page": 4,
            "text": "2 8"
        },
        {
            "bounding_box": [
                {
                    "x": 911,
                    "y": 3205
                },
                {
                    "x": 1597,
                    "y": 3205
                },
                {
                    "x": 1597,
                    "y": 3243
                },
                {
                    "x": 911,
                    "y": 3243
                }
            ],
            "category": "footer",
            "html": "<br><footer id='68' style='font-size:18px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 68,
            "page": 4,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1600,
                    "y": 3169
                },
                {
                    "x": 2337,
                    "y": 3169
                },
                {
                    "x": 2337,
                    "y": 3202
                },
                {
                    "x": 1600,
                    "y": 3202
                }
            ],
            "category": "footer",
            "html": "<br><footer id='69' style='font-size:14px'>J A N U A R Y 2 0 1 6 I V 0 L 5 2 9 I N A T U R E I 4 8 7</footer>",
            "id": 69,
            "page": 4,
            "text": "J A N U A R Y 2 0 1 6 I V 0 L 5 2 9 I N A T U R E I 4 8 7"
        },
        {
            "bounding_box": [
                {
                    "x": 156,
                    "y": 97
                },
                {
                    "x": 516,
                    "y": 97
                },
                {
                    "x": 516,
                    "y": 153
                },
                {
                    "x": 156,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='70' style='font-size:22px'>RESEARCH ARTICLE</header>",
            "id": 70,
            "page": 5,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 263,
                    "y": 225
                },
                {
                    "x": 675,
                    "y": 225
                },
                {
                    "x": 675,
                    "y": 333
                },
                {
                    "x": 263,
                    "y": 333
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>Game 1<br>Fan Hui (Black), AlphaGo (White)<br>AlphaGo wins by 2.5 points</p>",
            "id": 71,
            "page": 5,
            "text": "Game 1 Fan Hui (Black), AlphaGo (White) AlphaGo wins by 2.5 points"
        },
        {
            "bounding_box": [
                {
                    "x": 233,
                    "y": 333
                },
                {
                    "x": 865,
                    "y": 333
                },
                {
                    "x": 865,
                    "y": 1044
                },
                {
                    "x": 233,
                    "y": 1044
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='72' style='font-size:14px' alt=\"203 65\n93 92 201 1 51 57 49 51 164 33\n(156) - 54 205 35 202 34 152 50 10 47 43 45 22 220 4 223 9\n155 1 57 3 (204 85 149 150 48 44 46 8 231 160 161\n229 87 84 186 184 196 236 232 235 5\n(228 39 31 169 271 86 83 58 82 81 (224 79 268 53 99\n(230 1 48 36 37 41 168185181 ⑫⑱ 88 195 89 80 68 32 176 177\n38 40 96 90 167189187 (264) 70 238251 67 63 6 192 175 1 97\n(242) 269 270 145143 146255191 227237253 65 64 178 198\n(194) 193 207 95 139 142(144)261265254,170252 69 66 248 78\n42 267 99 98 (226) 225213209214266 71 72 77 76\n60 56 (240 97 94 256174/13621212/140/260 73 114 75 12\n(208) 52 53 241 91 (100) ⑩ 171 ⑫ 257141219 1⑮ 116 15 16 112)158\n59 54 23 133101 102 258 173262 121263119 110 27 14 11 25 13\n(210 55 (130) 131103104259 217129215/215/218117 62 120 28 18 26 11 1\n211 135 1 132)128 216 137 (206) 233 118 74 13 108 2 249 07 59\n29 134 22 138 30 179 (200 20 105 109 126 17 19\n61 24 166 180 106 124 21 127 239 272\n63 147 162 244 243 23 122 25 246 247\n(234) at 1 79 245 at 122) (250 at 59\" data-coord=\"top-left:(233,333); bottom-right:(865,1044)\" /></figure>",
            "id": 72,
            "page": 5,
            "text": "203 65 93 92 201 1 51 57 49 51 164 33 (156) - 54 205 35 202 34 152 50 10 47 43 45 22 220 4 223 9 155 1 57 3 (204 85 149 150 48 44 46 8 231 160 161 229 87 84 186 184 196 236 232 235 5 (228 39 31 169 271 86 83 58 82 81 (224 79 268 53 99 (230 1 48 36 37 41 168185181 ⑫⑱ 88 195 89 80 68 32 176 177 38 40 96 90 167189187 (264) 70 238251 67 63 6 192 175 1 97 (242) 269 270 145143 146255191 227237253 65 64 178 198 (194) 193 207 95 139 142(144)261265254,170252 69 66 248 78 42 267 99 98 (226) 225213209214266 71 72 77 76 60 56 (240 97 94 256174/13621212/140/260 73 114 75 12 (208) 52 53 241 91 (100) ⑩ 171 ⑫ 257141219 1⑮ 116 15 16 112)158 59 54 23 133101 102 258 173262 121263119 110 27 14 11 25 13 (210 55 (130) 131103104259 217129215/215/218117 62 120 28 18 26 11 1 211 135 1 132)128 216 137 (206) 233 118 74 13 108 2 249 07 59 29 134 22 138 30 179 (200 20 105 109 126 17 19 61 24 166 180 106 124 21 127 239 272 63 147 162 244 243 23 122 25 246 247 (234) at 1 79 245 at 122) (250 at 59"
        },
        {
            "bounding_box": [
                {
                    "x": 264,
                    "y": 1086
                },
                {
                    "x": 673,
                    "y": 1086
                },
                {
                    "x": 673,
                    "y": 1192
                },
                {
                    "x": 264,
                    "y": 1192
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>Game 4<br>AlphaGo (Black), Fan Hui (White)<br>AlphaGo wins by resignation</p>",
            "id": 73,
            "page": 5,
            "text": "Game 4 AlphaGo (Black), Fan Hui (White) AlphaGo wins by resignation"
        },
        {
            "bounding_box": [
                {
                    "x": 954,
                    "y": 218
                },
                {
                    "x": 1365,
                    "y": 218
                },
                {
                    "x": 1365,
                    "y": 326
                },
                {
                    "x": 954,
                    "y": 326
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:16px'>Game 2<br>AlphaGo (Black), Fan Hui (White)<br>AlphaGo wins by resignation</p>",
            "id": 74,
            "page": 5,
            "text": "Game 2 AlphaGo (Black), Fan Hui (White) AlphaGo wins by resignation"
        },
        {
            "bounding_box": [
                {
                    "x": 952,
                    "y": 339
                },
                {
                    "x": 1554,
                    "y": 339
                },
                {
                    "x": 1554,
                    "y": 1003
                },
                {
                    "x": 952,
                    "y": 1003
                }
            ],
            "category": "figure",
            "html": "<figure><img id='75' style='font-size:14px' alt=\"53\n82 61 52 49 50 51 30 20 14 16 19\n71 68 66 83 85 47 45 48 43 41 25 9 4 13 15\n67 3 64 58 54 55 46 44 42 7 8 24 32\n69 62 65 63 56 60 8 33 5 6 31\n70 73 72 59 145 40 176 174 11 10 22\n81 75 74 78 138) 179 180 178 12 17 21 38\n79 76 77 181 146 167175 26 18 23\n86 84 80 ⑩ 169 168171 28 27 39\n144 172 163 34 29\n143 142 141 165 161173 164 36\n139 140 166 (158) 153 96 92 90 89 35\n162) (152) 147 148 95 91 101 105 37\n121 120 157 150 151159 160 100 93 88 94 103\n127 125 118 116 149 156 98 97 99\n133 126 1 124 119117 155 154 106 110 2 104 102\n129 123 122 131 115 107 108 87 112\n136 130 128 132 134 135 109 1 13 111 114\" data-coord=\"top-left:(952,339); bottom-right:(1554,1003)\" /></figure>",
            "id": 75,
            "page": 5,
            "text": "53 82 61 52 49 50 51 30 20 14 16 19 71 68 66 83 85 47 45 48 43 41 25 9 4 13 15 67 3 64 58 54 55 46 44 42 7 8 24 32 69 62 65 63 56 60 8 33 5 6 31 70 73 72 59 145 40 176 174 11 10 22 81 75 74 78 138) 179 180 178 12 17 21 38 79 76 77 181 146 167175 26 18 23 86 84 80 ⑩ 169 168171 28 27 39 144 172 163 34 29 143 142 141 165 161173 164 36 139 140 166 (158) 153 96 92 90 89 35 162) (152) 147 148 95 91 101 105 37 121 120 157 150 151159 160 100 93 88 94 103 127 125 118 116 149 156 98 97 99 133 126 1 124 119117 155 154 106 110 2 104 102 129 123 122 131 115 107 108 87 112 136 130 128 132 134 135 109 1 13 111 114"
        },
        {
            "bounding_box": [
                {
                    "x": 908,
                    "y": 1018
                },
                {
                    "x": 1014,
                    "y": 1018
                },
                {
                    "x": 1014,
                    "y": 1059
                },
                {
                    "x": 908,
                    "y": 1059
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:14px'>182 at 169</p>",
            "id": 76,
            "page": 5,
            "text": "182 at 169"
        },
        {
            "bounding_box": [
                {
                    "x": 1649,
                    "y": 225
                },
                {
                    "x": 2061,
                    "y": 225
                },
                {
                    "x": 2061,
                    "y": 333
                },
                {
                    "x": 1649,
                    "y": 333
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:16px'>Game 3<br>Fan Hui (Black), AlphaGo (White)<br>AlphaGo wins by resignation</p>",
            "id": 77,
            "page": 5,
            "text": "Game 3 Fan Hui (Black), AlphaGo (White) AlphaGo wins by resignation"
        },
        {
            "bounding_box": [
                {
                    "x": 956,
                    "y": 1084
                },
                {
                    "x": 1367,
                    "y": 1084
                },
                {
                    "x": 1367,
                    "y": 1192
                },
                {
                    "x": 956,
                    "y": 1192
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:16px'>Game 5<br>Fan Hui (Black), AlphaGo (White)<br>AlphaGo wins by resignation</p>",
            "id": 78,
            "page": 5,
            "text": "Game 5 Fan Hui (Black), AlphaGo (White) AlphaGo wins by resignation"
        },
        {
            "bounding_box": [
                {
                    "x": 1643,
                    "y": 349
                },
                {
                    "x": 2232,
                    "y": 349
                },
                {
                    "x": 2232,
                    "y": 1000
                },
                {
                    "x": 1643,
                    "y": 1000
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='79' style='font-size:14px' alt=\"105 109 107\n62 49 48 7 5 104 103 108\n63 3 45 47 51 106\n61 57 55 43 44 46 50 166\n56 53 52 41 39 42 163 162 165 6\n54 59 37 38 40 164 147 148\n58 11 13 27 36 92 161 45 144\n9 10 15 26 67 120 69 70 160 159 1 43 102\n22 8 12 2 34 65 68 72 158 157 154 141\n21 23 64 33 35 71 153 156 55 137 (112) 142\n19 20 29 28 73 66 77 93 150 152 139 111 110\n25 91 30 74 75 79 149 146 140 138\n80 14 31 78 01 151 99 114 116\n90 87 89 98 17 94 76 27 17 15 118\n84 83 1 96 81 82 128 124 125 97 135 2 121 122\n88 85 18 32 95 16 (126) 119 131 113 1 32 1 00 136\n86 130 129 133 123 134\" data-coord=\"top-left:(1643,349); bottom-right:(2232,1000)\" /></figure>",
            "id": 79,
            "page": 5,
            "text": "105 109 107 62 49 48 7 5 104 103 108 63 3 45 47 51 106 61 57 55 43 44 46 50 166 56 53 52 41 39 42 163 162 165 6 54 59 37 38 40 164 147 148 58 11 13 27 36 92 161 45 144 9 10 15 26 67 120 69 70 160 159 1 43 102 22 8 12 2 34 65 68 72 158 157 154 141 21 23 64 33 35 71 153 156 55 137 (112) 142 19 20 29 28 73 66 77 93 150 152 139 111 110 25 91 30 74 75 79 149 146 140 138 80 14 31 78 01 151 99 114 116 90 87 89 98 17 94 76 27 17 15 118 84 83 1 96 81 82 128 124 125 97 135 2 121 122 88 85 18 32 95 16 (126) 119 131 113 1 32 1 00 136 86 130 129 133 123 134"
        },
        {
            "bounding_box": [
                {
                    "x": 238,
                    "y": 1204
                },
                {
                    "x": 872,
                    "y": 1204
                },
                {
                    "x": 872,
                    "y": 1926
                },
                {
                    "x": 238,
                    "y": 1926
                }
            ],
            "category": "figure",
            "html": "<figure><img id='80' style='font-size:14px' alt=\"79 80 68 23 120\n55 59 69 66 67 48 83 82 85 89 97 95 118 119 126\n57 54 53 21 18 70 65 64 84 81 87 10 91 4 122 121\n58 56 3 63 22 23 86 88 93 92 98\n52 60 61 24 25 90 94 5 6\n62 19 28 27 29 130 128 7 125\n50 33 39 30 26 117 (132) 127 129 9\n74 32 36 37 34 31 133\n72 73 38 41 47 35\n49 15 40 46 42 43\n51 44 45 13 77 11\n71 (140) 76 165 164\n139 (138) 131 141 12 163\n17 137 (136) 146 124 142\n113 109 135 (134) 156 55 75 157\n106 20 101 143 145 153 2 78 115\n114 16 105 103 100 99 144 147 14 154 116 158\n102 108 107 110 111 104 148 149 151 152 159 161 162\n112 150 160\n(96 at ⑩\" data-coord=\"top-left:(238,1204); bottom-right:(872,1926)\" /></figure>",
            "id": 80,
            "page": 5,
            "text": "79 80 68 23 120 55 59 69 66 67 48 83 82 85 89 97 95 118 119 126 57 54 53 21 18 70 65 64 84 81 87 10 91 4 122 121 58 56 3 63 22 23 86 88 93 92 98 52 60 61 24 25 90 94 5 6 62 19 28 27 29 130 128 7 125 50 33 39 30 26 117 (132) 127 129 9 74 32 36 37 34 31 133 72 73 38 41 47 35 49 15 40 46 42 43 51 44 45 13 77 11 71 (140) 76 165 164 139 (138) 131 141 12 163 17 137 (136) 146 124 142 113 109 135 (134) 156 55 75 157 106 20 101 143 145 153 2 78 115 114 16 105 103 100 99 144 147 14 154 116 158 102 108 107 110 111 104 148 149 151 152 159 161 162 112 150 160 (96 at ⑩"
        },
        {
            "bounding_box": [
                {
                    "x": 918,
                    "y": 1894
                },
                {
                    "x": 943,
                    "y": 1894
                },
                {
                    "x": 943,
                    "y": 1916
                },
                {
                    "x": 918,
                    "y": 1916
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:14px'>90</p>",
            "id": 81,
            "page": 5,
            "text": "90"
        },
        {
            "bounding_box": [
                {
                    "x": 946,
                    "y": 1203
                },
                {
                    "x": 1566,
                    "y": 1203
                },
                {
                    "x": 1566,
                    "y": 1945
                },
                {
                    "x": 946,
                    "y": 1945
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='82' style='font-size:14px' alt=\"49 195\n44 50 40 45 47 57 61 63 77 69 133 35 93 194\n52 42 39 38 46 99 60 7 62 71 70 66 5 1 97 36 196\n48 3 43 41 56 58 59 76 37 64 68 134\n51 164 200 166 87 97 207 65 121 72 67 125 126\n203 86 96 201 (206) 75 6\n204205 122 156 132\n55 11 13 82 (202) 92 117 116,120 155\n73 54 9 10 15 80 161 (162) 93 115 180 182\n81 8 12 78 79 84 95 114,118 119 168 179\n83 28 89 26 53 85 88 ⑮ 111108110 113 175 (174) 178) 177 192\n27 19 20 25 149 98 ⑫ 99 (106) 109 190 181\n30 24 23 91 100 102 103 107 124 188 189\n14 29 ⑳ 131129 105 (104) 159 (158) 172(170 171\n74 209 17 130 101 213 123 173 (214 167 191 (142) 1 65\n94 21 1 208 128 31 33 211 (186) 187 143 2 136 141 146\n18 22 16 32 198 34 (212) 184 185 144 138 137 135 148 47\n(176) 183 169 140 139 152 153 145\nat 15 127 at 37 151 at 141 (154) at 148 157 at 141 160 at (148)\nat 141\" data-coord=\"top-left:(946,1203); bottom-right:(1566,1945)\" /></figure>",
            "id": 82,
            "page": 5,
            "text": "49 195 44 50 40 45 47 57 61 63 77 69 133 35 93 194 52 42 39 38 46 99 60 7 62 71 70 66 5 1 97 36 196 48 3 43 41 56 58 59 76 37 64 68 134 51 164 200 166 87 97 207 65 121 72 67 125 126 203 86 96 201 (206) 75 6 204205 122 156 132 55 11 13 82 (202) 92 117 116,120 155 73 54 9 10 15 80 161 (162) 93 115 180 182 81 8 12 78 79 84 95 114,118 119 168 179 83 28 89 26 53 85 88 ⑮ 111108110 113 175 (174) 178) 177 192 27 19 20 25 149 98 ⑫ 99 (106) 109 190 181 30 24 23 91 100 102 103 107 124 188 189 14 29 ⑳ 131129 105 (104) 159 (158) 172(170 171 74 209 17 130 101 213 123 173 (214 167 191 (142) 1 65 94 21 1 208 128 31 33 211 (186) 187 143 2 136 141 146 18 22 16 32 198 34 (212) 184 185 144 138 137 135 148 47 (176) 183 169 140 139 152 153 145 at 15 127 at 37 151 at 141 (154) at 148 157 at 141 160 at (148) at 141"
        },
        {
            "bounding_box": [
                {
                    "x": 144,
                    "y": 1969
                },
                {
                    "x": 1174,
                    "y": 1969
                },
                {
                    "x": 1174,
                    "y": 2130
                },
                {
                    "x": 144,
                    "y": 2130
                }
            ],
            "category": "caption",
            "html": "<caption id='83' style='font-size:18px'>Figure 6 I Games from the match between AlphaGo and the European<br>champion, Fan Hui. Moves are shown in a numbered sequence<br>corresponding to the order in which they were played. Repeated moves<br>on the same intersection are shown in pairs below the board. The first</caption>",
            "id": 83,
            "page": 5,
            "text": "Figure 6 I Games from the match between AlphaGo and the European champion, Fan Hui. Moves are shown in a numbered sequence corresponding to the order in which they were played. Repeated moves on the same intersection are shown in pairs below the board. The first"
        },
        {
            "bounding_box": [
                {
                    "x": 924,
                    "y": 1930
                },
                {
                    "x": 947,
                    "y": 1930
                },
                {
                    "x": 947,
                    "y": 1948
                },
                {
                    "x": 924,
                    "y": 1948
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:14px'>63</p>",
            "id": 84,
            "page": 5,
            "text": "63"
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 2204
                },
                {
                    "x": 1209,
                    "y": 2204
                },
                {
                    "x": 1209,
                    "y": 2378
                },
                {
                    "x": 145,
                    "y": 2378
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:20px'>on high-performance MCTS algorithms. In addition, we included the<br>open source program GnuGo, a Go program using state-of-the-art<br>search methods that preceded MCTS. All programs were allowed 5 s<br>of computation time per move.</p>",
            "id": 85,
            "page": 5,
            "text": "on high-performance MCTS algorithms. In addition, we included the open source program GnuGo, a Go program using state-of-the-art search methods that preceded MCTS. All programs were allowed 5 s of computation time per move."
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 2381
                },
                {
                    "x": 1211,
                    "y": 2381
                },
                {
                    "x": 1211,
                    "y": 2817
                },
                {
                    "x": 146,
                    "y": 2817
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:20px'>The results of the tournament (see Fig. 4a) suggest that single-<br>machine AlphaGo is many dan ranks stronger than any previous<br>Go program, winning 494 out of 495 games (99.8%) against other<br>Go programs. To provide a greater challenge to AlphaGo, we also<br>played games with four handicap stones (that is, free moves for the<br>opponent); AlphaGo won 77%, 86%, and 99% of handicap games<br>against Crazy Stone, Zen and Pachi, respectively. The distributed ver-<br>sion of AlphaGo was significantly stronger, winning 77% of games<br>against single-machine AlphaGo and 100% ofits games against other<br>programs.</p>",
            "id": 86,
            "page": 5,
            "text": "The results of the tournament (see Fig. 4a) suggest that singlemachine AlphaGo is many dan ranks stronger than any previous Go program, winning 494 out of 495 games (99.8%) against other Go programs. To provide a greater challenge to AlphaGo, we also played games with four handicap stones (that is, free moves for the opponent); AlphaGo won 77%, 86%, and 99% of handicap games against Crazy Stone, Zen and Pachi, respectively. The distributed version of AlphaGo was significantly stronger, winning 77% of games against single-machine AlphaGo and 100% ofits games against other programs."
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 2814
                },
                {
                    "x": 1211,
                    "y": 2814
                },
                {
                    "x": 1211,
                    "y": 3124
                },
                {
                    "x": 145,
                    "y": 3124
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:20px'>We also assessed variants of AlphaGo that evaluated positions<br>using just the value network (入=0) or just rollouts (入 = 1) (see<br>Fig. 4b). Even without rollouts AlphaGo exceeded the performance<br>of all other Go programs, demonstrating that value networks provide<br>a viable alternative to Monte Carlo evaluation in Go. However, the<br>mixed evaluation (入 =0.5) performed best, winning ≥95% of games<br>against other variants. This suggests that the two position-evaluation</p>",
            "id": 87,
            "page": 5,
            "text": "We also assessed variants of AlphaGo that evaluated positions using just the value network (入=0) or just rollouts (入 = 1) (see Fig. 4b). Even without rollouts AlphaGo exceeded the performance of all other Go programs, demonstrating that value networks provide a viable alternative to Monte Carlo evaluation in Go. However, the mixed evaluation (入 =0.5) performed best, winning ≥95% of games against other variants. This suggests that the two position-evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 1972
                },
                {
                    "x": 2301,
                    "y": 1972
                },
                {
                    "x": 2301,
                    "y": 2090
                },
                {
                    "x": 1245,
                    "y": 2090
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:18px'>move number in each pair indicates when the repeat move was played, at<br>an intersection identified by the second move number (see Supplementary<br>Information).</p>",
            "id": 88,
            "page": 5,
            "text": "move number in each pair indicates when the repeat move was played, at an intersection identified by the second move number (see Supplementary Information)."
        },
        {
            "bounding_box": [
                {
                    "x": 1244,
                    "y": 2204
                },
                {
                    "x": 2308,
                    "y": 2204
                },
                {
                    "x": 2308,
                    "y": 2422
                },
                {
                    "x": 1244,
                    "y": 2422
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>mechanisms are complementary: the value network approximates the<br>outcome of games played by the strong but impractically slow pp, while<br>the rollouts can precisely score and evaluate the outcome of games<br>played by the weaker but faster rollout policy P�· Figure 5 visualizes<br>the evaluation of a real game position by AlphaGo.</p>",
            "id": 89,
            "page": 5,
            "text": "mechanisms are complementary: the value network approximates the outcome of games played by the strong but impractically slow pp, while the rollouts can precisely score and evaluate the outcome of games played by the weaker but faster rollout policy P�· Figure 5 visualizes the evaluation of a real game position by AlphaGo."
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 2423
                },
                {
                    "x": 2309,
                    "y": 2423
                },
                {
                    "x": 2309,
                    "y": 2774
                },
                {
                    "x": 1245,
                    "y": 2774
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:20px'>Finally, we evaluated the distributed version of AlphaGo against Fan<br>Hui, a professional 2 dan, and the winner of the 2013, 2014 and 2015<br>European Go championships. Over 5-9 October 2015 AlphaGo and<br>Fan Hui competed in a formal five-game match. AlphaGo won the<br>match 5 games to 0 (Fig. 6 and Extended Data Table 1). This is the<br>first time that a computer Go program has defeated a human profes-<br>sional player, without handicap, in the full game of Go-a feat that was<br>previously believed to be at least a decade away3,7,31.</p>",
            "id": 90,
            "page": 5,
            "text": "Finally, we evaluated the distributed version of AlphaGo against Fan Hui, a professional 2 dan, and the winner of the 2013, 2014 and 2015 European Go championships. Over 5-9 October 2015 AlphaGo and Fan Hui competed in a formal five-game match. AlphaGo won the match 5 games to 0 (Fig. 6 and Extended Data Table 1). This is the first time that a computer Go program has defeated a human professional player, without handicap, in the full game of Go-a feat that was previously believed to be at least a decade away3,7,31."
        },
        {
            "bounding_box": [
                {
                    "x": 1247,
                    "y": 2812
                },
                {
                    "x": 1457,
                    "y": 2812
                },
                {
                    "x": 1457,
                    "y": 2857
                },
                {
                    "x": 1247,
                    "y": 2857
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:22px'>Discussion</p>",
            "id": 91,
            "page": 5,
            "text": "Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 1246,
                    "y": 2859
                },
                {
                    "x": 2310,
                    "y": 2859
                },
                {
                    "x": 2310,
                    "y": 3124
                },
                {
                    "x": 1246,
                    "y": 3124
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:20px'>In this work we have developed a Go program, based on a combina-<br>tion of deep neural networks and tree search, that plays at the level of<br>the strongest human players, thereby achieving one of artificial intel-<br>ligence's \"grand challenges\"31-33 We have developed, for the first time,<br>effective move selection and position evaluation functions for Go,<br>based on deep neural networks that are trained by a novel combination</p>",
            "id": 92,
            "page": 5,
            "text": "In this work we have developed a Go program, based on a combination of deep neural networks and tree search, that plays at the level of the strongest human players, thereby achieving one of artificial intelligence's \"grand challenges\"31-33 We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination"
        },
        {
            "bounding_box": [
                {
                    "x": 144,
                    "y": 3166
                },
                {
                    "x": 1572,
                    "y": 3166
                },
                {
                    "x": 1572,
                    "y": 3244
                },
                {
                    "x": 144,
                    "y": 3244
                }
            ],
            "category": "footer",
            "html": "<footer id='93' style='font-size:16px'>4 8 8 I N A T U R E I V 0 L 5 2 9 I 2 8 J A N U A R Y 2 0 1 6<br>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 93,
            "page": 5,
            "text": "4 8 8 I N A T U R E I V 0 L 5 2 9 I 2 8 J A N U A R Y 2 0 1 6 Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1964,
                    "y": 99
                },
                {
                    "x": 2319,
                    "y": 99
                },
                {
                    "x": 2319,
                    "y": 153
                },
                {
                    "x": 1964,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='94' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 94,
            "page": 6,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 233
                },
                {
                    "x": 1234,
                    "y": 233
                },
                {
                    "x": 1234,
                    "y": 454
                },
                {
                    "x": 169,
                    "y": 454
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:18px'>of supervised and reinforcement learning. We have introduced a new<br>search algorithm that successfully combines neural network evalu-<br>ations with Monte Carlo rollouts. Our program AlphaGo integrates<br>these components together, at scale, in a high-performance tree search<br>engine.</p>",
            "id": 95,
            "page": 6,
            "text": "of supervised and reinforcement learning. We have introduced a new search algorithm that successfully combines neural network evaluations with Monte Carlo rollouts. Our program AlphaGo integrates these components together, at scale, in a high-performance tree search engine."
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 454
                },
                {
                    "x": 1236,
                    "y": 454
                },
                {
                    "x": 1236,
                    "y": 849
                },
                {
                    "x": 169,
                    "y": 849
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:20px'>During the match against Fan Hui, AlphaGo evaluated thousands<br>of times fewer positions than Deep Blue did in its chess match against<br>Kasparov4; compensating by selecting those positions more intelli-<br>gently, using the policy network, and evaluating them more precisely,<br>using the value network- -an approach that is perhaps closer to how<br>humans play. Furthermore, while Deep Blue relied on a handcrafted<br>evaluation function, the neural networks of AlphaGo are trained<br>directly from gameplay purely through general-purpose supervised<br>and reinforcement learning methods.</p>",
            "id": 96,
            "page": 6,
            "text": "During the match against Fan Hui, AlphaGo evaluated thousands of times fewer positions than Deep Blue did in its chess match against Kasparov4; compensating by selecting those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network- -an approach that is perhaps closer to how humans play. Furthermore, while Deep Blue relied on a handcrafted evaluation function, the neural networks of AlphaGo are trained directly from gameplay purely through general-purpose supervised and reinforcement learning methods."
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 848
                },
                {
                    "x": 1231,
                    "y": 848
                },
                {
                    "x": 1231,
                    "y": 1332
                },
                {
                    "x": 169,
                    "y": 1332
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:20px'>Go is exemplary in many ways of the difficulties faced by artificial<br>intelligence33,34: a challenging decision-making task, an intractable<br>search space, and an optimal solution so complex it appears infeasible<br>to directly approximate using a policy or value function. The previous<br>major breakthrough in computer Go, the introduction of MCTS, led to<br>corresponding advances in many other domains; for example, general<br>game-playing, classical planning, partially observed planning, sched-<br>uling, and constraint satisfaction35,36 · By combining tree search with<br>policy and value networks, AlphaGo has finally reached a professional<br>level in Go, providing hope that human-level performance can now be<br>achieved in other seemingly intractable artificial intelligence domains.</p>",
            "id": 97,
            "page": 6,
            "text": "Go is exemplary in many ways of the difficulties faced by artificial intelligence33,34: a challenging decision-making task, an intractable search space, and an optimal solution so complex it appears infeasible to directly approximate using a policy or value function. The previous major breakthrough in computer Go, the introduction of MCTS, led to corresponding advances in many other domains; for example, general game-playing, classical planning, partially observed planning, scheduling, and constraint satisfaction35,36 · By combining tree search with policy and value networks, AlphaGo has finally reached a professional level in Go, providing hope that human-level performance can now be achieved in other seemingly intractable artificial intelligence domains."
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 1354
                },
                {
                    "x": 1237,
                    "y": 1354
                },
                {
                    "x": 1237,
                    "y": 1464
                },
                {
                    "x": 170,
                    "y": 1464
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:14px'>Online Content Methods, along with any additional Extended Data display items and<br>Source Data, are available in the online version of the paper; references unique to<br>these sections appear only in the online paper.</p>",
            "id": 98,
            "page": 6,
            "text": "Online Content Methods, along with any additional Extended Data display items and Source Data, are available in the online version of the paper; references unique to these sections appear only in the online paper."
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 1488
                },
                {
                    "x": 952,
                    "y": 1488
                },
                {
                    "x": 952,
                    "y": 1529
                },
                {
                    "x": 170,
                    "y": 1529
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:16px'>Received 11 November 2015; accepted 5 January 2016.</p>",
            "id": 99,
            "page": 6,
            "text": "Received 11 November 2015; accepted 5 January 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 168,
                    "y": 1564
                },
                {
                    "x": 1228,
                    "y": 1564
                },
                {
                    "x": 1228,
                    "y": 2650
                },
                {
                    "x": 168,
                    "y": 2650
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:16px'>1. Allis, L. V. Searching for Solutions in Games and Artificial Intelligence. PhD thesis,<br>Univ. Limburg, Maastricht, The Netherlands (1994).<br>2. van den Herik, H., Uiterwijk, J. W. & van Rijswijck, J. Games solved: now and in<br>the future. Artif. Intell. 134, 277-311 (2002).<br>3. Schaeffer, J. The games computers (and people) play. Advances in Computers<br>52, 189-266 (2000).<br>4. Campbell, M., Hoane, A. & Hsu, F. Deep Blue. Artif. Intell. 134, 57-83 (2002).<br>5. Schaeffer, J. et al. A world championship caliber checkers program. Artif. Intell.<br>53, 273-289 (1992).<br>6. Buro, M. From simple features to sophisticated evaluation functions.<br>In 1st International Conference on Computers and Games, 126-145 (1999).<br>7. M�ller, M. Computer Go. Artif. Intell. 134, 145-179 (2002).<br>8. Tesauro, G. & Galperin, G. On-line policy improvement using Monte-Carlo<br>search. In Advances in Neural Information Processing, 1068-1074 (1996).<br>9. Sheppard, B. World-championship-caliber Scrabble. Artif. Intell. 134, 241-275<br>(2002).<br>10. Bouzy, B. & Helmstetter, B. Monte-Carlo Go developments. In 10th International<br>Conference on Advances in Computer Games, 159-174 (2003).<br>11. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree<br>search. In 5th International Conference on Computers and Games, 72-83<br>(2006).<br>12. Kocsis, L. & Szepesvari, C. Bandit based Monte-Carlo planning.<br>In 15th European Conference on Machine Learning, 282-293 (2006).<br>13. Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J.<br>30, 198-208 (2007).<br>14. Baudis, P. & Gailly, J.-L. Pachi: State of the art open source Go program.<br>In Advances in Computer Games, 24-38 (Springer, 2012).<br>15. Muller, M., Enzenberger, M., Arneson, B. & Segal, R. Fuego - an open-source<br>framework for board games and Go engine based on Monte-Carlo tree search.<br>IEEE Trans. Comput. Intell. AI in Games 2, 259-270 (2010).<br>16. Gelly, S. & Silver, D. Combining online and offline learning in UCT.<br>In 17th International Conference on Machine Learning, 273-280 (2007).</p>",
            "id": 100,
            "page": 6,
            "text": "1. Allis, L. V. Searching for Solutions in Games and Artificial Intelligence. PhD thesis, Univ. Limburg, Maastricht, The Netherlands (1994). 2. van den Herik, H., Uiterwijk, J. W. & van Rijswijck, J. Games solved: now and in the future. Artif. Intell. 134, 277-311 (2002). 3. Schaeffer, J. The games computers (and people) play. Advances in Computers 52, 189-266 (2000). 4. Campbell, M., Hoane, A. & Hsu, F. Deep Blue. Artif. Intell. 134, 57-83 (2002). 5. Schaeffer, J.  A world championship caliber checkers program. Artif. Intell. 53, 273-289 (1992). 6. Buro, M. From simple features to sophisticated evaluation functions. In 1st International Conference on Computers and Games, 126-145 (1999). 7. M�ller, M. Computer Go. Artif. Intell. 134, 145-179 (2002). 8. Tesauro, G. & Galperin, G. On-line policy improvement using Monte-Carlo search. In Advances in Neural Information Processing, 1068-1074 (1996). 9. Sheppard, B. World-championship-caliber Scrabble. Artif. Intell. 134, 241-275 (2002). 10. Bouzy, B. & Helmstetter, B. Monte-Carlo Go developments. In 10th International Conference on Advances in Computer Games, 159-174 (2003). 11. Coulom, R. Efficient selectivity and backup operators in Monte-Carlo tree search. In 5th International Conference on Computers and Games, 72-83 (2006). 12. Kocsis, L. & Szepesvari, C. Bandit based Monte-Carlo planning. In 15th European Conference on Machine Learning, 282-293 (2006). 13. Coulom, R. Computing Elo ratings of move patterns in the game of Go. ICGA J. 30, 198-208 (2007). 14. Baudis, P. & Gailly, J.-L. Pachi: State of the art open source Go program. In Advances in Computer Games, 24-38 (Springer, 2012). 15. Muller, M., Enzenberger, M., Arneson, B. & Segal, R. Fuego - an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Trans. Comput. Intell. AI in Games 2, 259-270 (2010). 16. Gelly, S. & Silver, D. Combining online and offline learning in UCT. In 17th International Conference on Machine Learning, 273-280 (2007)."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 226
                },
                {
                    "x": 2331,
                    "y": 226
                },
                {
                    "x": 2331,
                    "y": 1881
                },
                {
                    "x": 1269,
                    "y": 1881
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:16px'>17. Krizhevsky, A., Sutskever, 1. & Hinton, G. ImageNet classification with deep<br>convolutional neural networks. In Advances in Neural Information Processing<br>Systems, 1097-1105 (2012).<br>18. Lawrence, S., Giles, C. L., Tsoi, A. C. & Back, A. D. Face recognition:<br>a convolutional neural-network approach. IEEE Trans. Neural Netw. 8,<br>98-113 (1997).<br>19. Mnih, V. et al. Human-level control through deep reinforcement learning.<br>Nature 518, 529-533 (2015).<br>20. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015).<br>21. Stern, D., Herbrich, R. & Graepel, T. Bayesian pattern ranking for move<br>prediction in the game of Go. In International Conference of Machine Learning,<br>873-880 (2006).<br>22. Sutskever, 1. & Nair, V. Mimicking Go experts with convolutional neural<br>networks. In International Conference on Artificial Neural Networks, 101-110<br>(2008).<br>23. Maddison, C. J., Huang, A., Sutskever, I. & Silver, D. Move evaluation in Go using<br>deep convolutional neural networks. 3rd International Conference on Learning<br>Representations (2015).<br>24. Clark, C. & Storkey, A. J. Training deep convolutional neural networks to<br>play go. In 32nd International Conference on Machine Learning, 1766-1774<br>(2015).<br>25. Williams, R. J. Simple statistical gradient-following algorithms for connectionist<br>reinforcement learning. Mach. Learn. 8, 229-256 (1992).<br>26. Sutton, R., McAllester, D., Singh, S. & Mansour, Y. Policy gradient methods for<br>reinforcement learning with function approximation. In Advances in Neural<br>Information Processing Systems, 1057-1063 (2000).<br>27. Sutton, R. & Barto, A. Reinforcement Learning: an Introduction (MIT Press, 1998).<br>28. Schraudolph, N. N., Dayan, P. & Sejnowski, T. J. Temporal difference learning<br>of position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6,<br>817-824 (1994).<br>29. Enzenberger, M. Evaluation in Go by a neural network using soft segmentation.<br>In 10th Advances in Computer Games Conference, 97-108 (2003). 267.<br>30. Silver, D., Sutton, R. & M�ller, M. Temporal-difference search in computer Go.<br>Mach. Learn. 87, 183-219 (2012).<br>31. Levinovitz, A. The mystery of Go, the ancient game that computers still can't<br>win. Wired Magazine (2014).<br>32. Mechner, D. All Systems Go. The Sciences 38, 32-37 (1998).<br>33. Mandziuk, J. Computational intelligence in mind games. In Challenges for<br>Computational Intelligence, 407-442 (2007).<br>34. Berliner, H. A chronology of computer chess and its literature. Artif. Intell. 10,<br>201-214 (1978).<br>35. Browne, C. et al. A survey of Monte-Carlo tree search methods. IEEE Trans.<br>Comput. Intell. AI in Games 4, 1-43 (2012).<br>36. Gelly, S. et al. The grand challenge of computer Go: Monte Carlo tree search<br>and extensions. Commun. ACM 55, 106-113 (2012).<br>37. Coulom, R. Whole-history rating: A Bayesian rating system for players of<br>time-varying strength. In International Conference on Computers and Games,<br>113-124 (2008).<br>38. KGS. Rating system math. http://www.gokgs.com/he.p/math.html</p>",
            "id": 101,
            "page": 6,
            "text": "17. Krizhevsky, A., Sutskever, 1. & Hinton, G. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 1097-1105 (2012). 18. Lawrence, S., Giles, C. L., Tsoi, A. C. & Back, A. D. Face recognition: a convolutional neural-network approach. IEEE Trans. Neural Netw. 8, 98-113 (1997). 19. Mnih, V.  Human-level control through deep reinforcement learning. Nature 518, 529-533 (2015). 20. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015). 21. Stern, D., Herbrich, R. & Graepel, T. Bayesian pattern ranking for move prediction in the game of Go. In International Conference of Machine Learning, 873-880 (2006). 22. Sutskever, 1. & Nair, V. Mimicking Go experts with convolutional neural networks. In International Conference on Artificial Neural Networks, 101-110 (2008). 23. Maddison, C. J., Huang, A., Sutskever, I. & Silver, D. Move evaluation in Go using deep convolutional neural networks. 3rd International Conference on Learning Representations (2015). 24. Clark, C. & Storkey, A. J. Training deep convolutional neural networks to play go. In 32nd International Conference on Machine Learning, 1766-1774 (2015). 25. Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229-256 (1992). 26. Sutton, R., McAllester, D., Singh, S. & Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, 1057-1063 (2000). 27. Sutton, R. & Barto, A. Reinforcement Learning: an Introduction (MIT Press, 1998). 28. Schraudolph, N. N., Dayan, P. & Sejnowski, T. J. Temporal difference learning of position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6, 817-824 (1994). 29. Enzenberger, M. Evaluation in Go by a neural network using soft segmentation. In 10th Advances in Computer Games Conference, 97-108 (2003). 267. 30. Silver, D., Sutton, R. & M�ller, M. Temporal-difference search in computer Go. Mach. Learn. 87, 183-219 (2012). 31. Levinovitz, A. The mystery of Go, the ancient game that computers still can't win. Wired Magazine (2014). 32. Mechner, D. All Systems Go. The Sciences 38, 32-37 (1998). 33. Mandziuk, J. Computational intelligence in mind games. In Challenges for Computational Intelligence, 407-442 (2007). 34. Berliner, H. A chronology of computer chess and its literature. Artif. Intell. 10, 201-214 (1978). 35. Browne, C.  A survey of Monte-Carlo tree search methods. IEEE Trans. Comput. Intell. AI in Games 4, 1-43 (2012). 36. Gelly, S.  The grand challenge of computer Go: Monte Carlo tree search and extensions. Commun. ACM 55, 106-113 (2012). 37. Coulom, R. Whole-history rating: A Bayesian rating system for players of time-varying strength. In International Conference on Computers and Games, 113-124 (2008). 38. KGS. Rating system math. http://www.gokgs.com/he.p/math.html"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1914
                },
                {
                    "x": 2247,
                    "y": 1914
                },
                {
                    "x": 2247,
                    "y": 1954
                },
                {
                    "x": 1270,
                    "y": 1954
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:14px'>Supplementary Information is available in the online version of the paper.</p>",
            "id": 102,
            "page": 6,
            "text": "Supplementary Information is available in the online version of the paper."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1988
                },
                {
                    "x": 2321,
                    "y": 1988
                },
                {
                    "x": 2321,
                    "y": 2195
                },
                {
                    "x": 1270,
                    "y": 2195
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:16px'>Acknowledgements We thank Fan Hui for agreeing to play against AlphaGo;<br>T. Manning for refereeing the match; R. Munos and T. Schaul for helpful<br>discussions and advice; A. Cain and M. Cant for work on the visuals; P. Dayan,<br>G. Wayne, D. Kumaran, D. Purves, H. van Hasselt, A. Barreto and G. Ostrovski for<br>reviewing the paper; and the rest of the DeepMind team for their support, ideas<br>and encouragement.</p>",
            "id": 103,
            "page": 6,
            "text": "Acknowledgements We thank Fan Hui for agreeing to play against AlphaGo; T. Manning for refereeing the match; R. Munos and T. Schaul for helpful discussions and advice; A. Cain and M. Cant for work on the visuals; P. Dayan, G. Wayne, D. Kumaran, D. Purves, H. van Hasselt, A. Barreto and G. Ostrovski for reviewing the paper; and the rest of the DeepMind team for their support, ideas and encouragement."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2229
                },
                {
                    "x": 2331,
                    "y": 2229
                },
                {
                    "x": 2331,
                    "y": 2437
                },
                {
                    "x": 1270,
                    "y": 2437
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:16px'>Author Contributions A.H., G.v.d.D., J.S., I.A., M.La., A.G., T.G. and D.S. designed<br>and implemented the search in AlphaGo. C.J.M., A.G., L.S., A.H., I.A., V.P., S.D.,<br>D.G., N.K., I.S., K.K. and D.S. designed and trained the neural networks in<br>AlphaGo. J.S., J.N., A.H. and D.S. designed and implemented the evaluation<br>framework for AlphaGo. D.S., M.Le., T.L., T.G., K.K. and D.H. managed and advised<br>on the project. D.S., T.G., A.G. and D.H. wrote the paper.</p>",
            "id": 104,
            "page": 6,
            "text": "Author Contributions A.H., G.v.d.D., J.S., I.A., M.La., A.G., T.G. and D.S. designed and implemented the search in AlphaGo. C.J.M., A.G., L.S., A.H., I.A., V.P., S.D., D.G., N.K., I.S., K.K. and D.S. designed and trained the neural networks in AlphaGo. J.S., J.N., A.H. and D.S. designed and implemented the evaluation framework for AlphaGo. D.S., M.Le., T.L., T.G., K.K. and D.H. managed and advised on the project. D.S., T.G., A.G. and D.H. wrote the paper."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2471
                },
                {
                    "x": 2258,
                    "y": 2471
                },
                {
                    "x": 2258,
                    "y": 2647
                },
                {
                    "x": 1269,
                    "y": 2647
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:16px'>Author Information Reprints and permissions information is available at<br>www.nature.com/reprints. The authors declare no competing financial<br>interests. Readers are welcome to comment on the online version of the<br>paper. Correspondence and requests for materials should be addressed to<br>D.S. (davidsilver@google.com) or D.H. (demishassabis@google.com).</p>",
            "id": 105,
            "page": 6,
            "text": "Author Information Reprints and permissions information is available at www.nature.com/reprints. The authors declare no competing financial interests. Readers are welcome to comment on the online version of the paper. Correspondence and requests for materials should be addressed to D.S. (davidsilver@google.com) or D.H. (demishassabis@google.com)."
        },
        {
            "bounding_box": [
                {
                    "x": 1547,
                    "y": 3172
                },
                {
                    "x": 1589,
                    "y": 3172
                },
                {
                    "x": 1589,
                    "y": 3198
                },
                {
                    "x": 1547,
                    "y": 3198
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:14px'>2 8</p>",
            "id": 106,
            "page": 6,
            "text": "2 8"
        },
        {
            "bounding_box": [
                {
                    "x": 912,
                    "y": 3205
                },
                {
                    "x": 1596,
                    "y": 3205
                },
                {
                    "x": 1596,
                    "y": 3242
                },
                {
                    "x": 912,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<br><footer id='107' style='font-size:14px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 107,
            "page": 6,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1601,
                    "y": 3167
                },
                {
                    "x": 2338,
                    "y": 3167
                },
                {
                    "x": 2338,
                    "y": 3202
                },
                {
                    "x": 1601,
                    "y": 3202
                }
            ],
            "category": "footer",
            "html": "<br><footer id='108' style='font-size:14px'>J A N U A R Y 2 0 1 6 I V 0 L 5 2 9 I N A T U R E I 4 8 9</footer>",
            "id": 108,
            "page": 6,
            "text": "J A N U A R Y 2 0 1 6 I V 0 L 5 2 9 I N A T U R E I 4 8 9"
        },
        {
            "bounding_box": [
                {
                    "x": 159,
                    "y": 96
                },
                {
                    "x": 520,
                    "y": 96
                },
                {
                    "x": 520,
                    "y": 154
                },
                {
                    "x": 159,
                    "y": 154
                }
            ],
            "category": "header",
            "html": "<header id='109' style='font-size:22px'>RESEARCH ARTICLE</header>",
            "id": 109,
            "page": 7,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 148,
                    "y": 225
                },
                {
                    "x": 368,
                    "y": 225
                },
                {
                    "x": 368,
                    "y": 271
                },
                {
                    "x": 148,
                    "y": 271
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:22px'>METHODS</p>",
            "id": 110,
            "page": 7,
            "text": "METHODS"
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 273
                },
                {
                    "x": 1213,
                    "y": 273
                },
                {
                    "x": 1213,
                    "y": 910
                },
                {
                    "x": 147,
                    "y": 910
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:14px'>Problem setting. Many games of perfect information, such as chess, checkers,<br>othello, backgammon and Go, may be defined as alternating Markov games39. In<br>these games, there is a state space S (where state includes an indication ofthe<br>current player to play); an action space A(s) defining the legal actions in any given<br>state s E S; a state transition function f(s, a, 5) defining the successor state after<br>selecting action a in state s and random input 5 (for example, dice); and finally a<br>reward function ri(s) describing the reward received by player i in state s. We<br>restrict our attention to two-player zero-sum games, r1(s) = -12(s) =r(s), with<br>deterministic state transitions,f(s, a, E)=f(s, a), and zero rewards except at a ter-<br>minal time step T. The outcome of the game Zt = ±r(sT) is the terminal reward at<br>the end of the game from the perspective of the current player at time step t.<br>A policy p(a|s) is a probability distribution over legal actions a E A(s).<br>A value function is the expected outcome if all actions for both players are selected<br>according to policy p, that is, vP(s) = E[Zt|St = S, at... T ~ p]. Zero-sum games have<br>a unique optimal value function v*(s) that determines the outcome from state s<br>following perfect play by both players,</p>",
            "id": 111,
            "page": 7,
            "text": "Problem setting. Many games of perfect information, such as chess, checkers, othello, backgammon and Go, may be defined as alternating Markov games39. In these games, there is a state space S (where state includes an indication ofthe current player to play); an action space A(s) defining the legal actions in any given state s E S; a state transition function f(s, a, 5) defining the successor state after selecting action a in state s and random input 5 (for example, dice); and finally a reward function ri(s) describing the reward received by player i in state s. We restrict our attention to two-player zero-sum games, r1(s) = -12(s) =r(s), with deterministic state transitions,f(s, a, E)=f(s, a), and zero rewards except at a terminal time step T. The outcome of the game Zt = ±r(sT) is the terminal reward at the end of the game from the perspective of the current player at time step t. A policy p(a|s) is a probability distribution over legal actions a E A(s). A value function is the expected outcome if all actions for both players are selected according to policy p, that is, vP(s) = E[Zt|St = S, at... T ~ p]. Zero-sum games have a unique optimal value function v*(s) that determines the outcome from state s following perfect play by both players,"
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 1082
                },
                {
                    "x": 1210,
                    "y": 1082
                },
                {
                    "x": 1210,
                    "y": 1321
                },
                {
                    "x": 147,
                    "y": 1321
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:14px'>Prior work. The optimal value function can be computed recursively by minimax<br>(or equivalently negamax) search 40 Most games are too large for exhaustive min-<br>·<br>imax tree search; instead, the game is truncated by using an approximate value<br>function v(s) U v (s) in place ofterminal rewards. Depth-first minimax search with<br>alpha-beta pruning40 has achieved superhuman performance in chess4, checkers5<br>and othello6, butit has not been effective in Go7.</p>",
            "id": 112,
            "page": 7,
            "text": "Prior work. The optimal value function can be computed recursively by minimax (or equivalently negamax) search 40 Most games are too large for exhaustive min· imax tree search; instead, the game is truncated by using an approximate value function v(s) U v (s) in place ofterminal rewards. Depth-first minimax search with alpha-beta pruning40 has achieved superhuman performance in chess4, checkers5 and othello6, butit has not been effective in Go7."
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 1324
                },
                {
                    "x": 1211,
                    "y": 1324
                },
                {
                    "x": 1211,
                    "y": 1675
                },
                {
                    "x": 147,
                    "y": 1675
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='113' style='font-size:14px'>Reinforcement learning can learn to approximate the optimal value function<br>directly from games of self-play39. The majority of prior work has focused on a<br>linear combination vo(s) = �(s) · 0 of features �(s) with weights 0. Weights were<br>trained using temporal-difference learning41 in chess42,43, checkers44,45 and Go30;<br>,<br>or using linear regression in othello6 and Scrabble9. Temporal-difference learning<br>has also been used to train a neural network to approximate the optimal value<br>function, achieving superhuman performance in backgammon 46; and achiev-<br>ing weak kyu-level performance in small-board Go28,29,47<br>using convolutional<br>networks.</p>",
            "id": 113,
            "page": 7,
            "text": "Reinforcement learning can learn to approximate the optimal value function directly from games of self-play39. The majority of prior work has focused on a linear combination vo(s) = �(s) · 0 of features �(s) with weights 0. Weights were trained using temporal-difference learning41 in chess42,43, checkers44,45 and Go30; , or using linear regression in othello6 and Scrabble9. Temporal-difference learning has also been used to train a neural network to approximate the optimal value function, achieving superhuman performance in backgammon 46; and achieving weak kyu-level performance in small-board Go28,29,47 using convolutional networks."
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 1680
                },
                {
                    "x": 1212,
                    "y": 1680
                },
                {
                    "x": 1212,
                    "y": 2306
                },
                {
                    "x": 147,
                    "y": 2306
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:16px'>An alternative approach to minimax search is Monte Carlo tree search<br>(MCTS)11,12, which estimates the optimal value of interior nodes by a double<br>approximation, Vn(s) U vPn(s) 2 v*(s). The first approximation, Vn(s) 2 vP\"(s),<br>uses n Monte Carlo simulations to estimate the value function of a simulation<br>policy Pn. The second approximation, vP\"(s) U v*(s), uses a simulation policy pn<br>in place of minimax optimal actions. The simulation policy selects actions accord-<br>ing to a search control function argmaxa (Q\"(s,a) + u(s, a)), such as UCT12, that<br>selects children with higher action values, Qn(s, a) = - Vn(f(s, a)), plus a bonus<br>u(s, a) that encourages exploration; or in the absence of a search tree at state s, it<br>samples actions from a fast rollout policy P�(a|s). As more simulations are executed<br>and the search tree grows deeper, the simulation policy becomes informed by<br>increasingly accurate statistics. In the limit, both approximations become exact<br>and MCTS (for example, with UCT) converges12 to the optimal value function<br>limn→00 Vn(s) = limn→� VP (s) = v*(s). The strongest current Go programs are<br>based on MCTS13-15,36</p>",
            "id": 114,
            "page": 7,
            "text": "An alternative approach to minimax search is Monte Carlo tree search (MCTS)11,12, which estimates the optimal value of interior nodes by a double approximation, Vn(s) U vPn(s) 2 v*(s). The first approximation, Vn(s) 2 vP\"(s), uses n Monte Carlo simulations to estimate the value function of a simulation policy Pn. The second approximation, vP\"(s) U v*(s), uses a simulation policy pn in place of minimax optimal actions. The simulation policy selects actions according to a search control function argmaxa (Q\"(s,a) + u(s, a)), such as UCT12, that selects children with higher action values, Qn(s, a) = - Vn(f(s, a)), plus a bonus u(s, a) that encourages exploration; or in the absence of a search tree at state s, it samples actions from a fast rollout policy P�(a|s). As more simulations are executed and the search tree grows deeper, the simulation policy becomes informed by increasingly accurate statistics. In the limit, both approximations become exact and MCTS (for example, with UCT) converges12 to the optimal value function limn→00 Vn(s) = limn→� VP (s) = v*(s). The strongest current Go programs are based on MCTS13-15,36"
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 2308
                },
                {
                    "x": 1212,
                    "y": 2308
                },
                {
                    "x": 1212,
                    "y": 2822
                },
                {
                    "x": 147,
                    "y": 2822
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:14px'>MCTS has previously been combined with a policy that is used to narrow the<br>beam of the search tree to high-probability moves13; or to bias the bonus term<br>towards high-probability moves48. MCTS has also been combined with a value<br>function that is used to initialize action values in newly expanded nodes16, or to<br>mix Monte Carlo evaluation with minimax evaluation49. By contrast, AlphaGo's use<br>ofvalue functions is based on truncated Monte Carlo search algorithms8,9, which<br>terminate rollouts before the end ofthe game and use a value function in place of<br>the terminal reward. AlphaGo's position evaluation mixes full rollouts with trun-<br>cated rollouts, resembling in some respects the well-known temporal-difference<br>learning algorithm TD(入). AlphaGo also differs from prior work by using slower<br>but more powerful representations of the policy and value function; evaluating<br>deep neural networks is several orders of magnitude slower than linear representa-<br>tions and must therefore occur asynchronously.</p>",
            "id": 115,
            "page": 7,
            "text": "MCTS has previously been combined with a policy that is used to narrow the beam of the search tree to high-probability moves13; or to bias the bonus term towards high-probability moves48. MCTS has also been combined with a value function that is used to initialize action values in newly expanded nodes16, or to mix Monte Carlo evaluation with minimax evaluation49. By contrast, AlphaGo's use ofvalue functions is based on truncated Monte Carlo search algorithms8,9, which terminate rollouts before the end ofthe game and use a value function in place of the terminal reward. AlphaGo's position evaluation mixes full rollouts with truncated rollouts, resembling in some respects the well-known temporal-difference learning algorithm TD(入). AlphaGo also differs from prior work by using slower but more powerful representations of the policy and value function; evaluating deep neural networks is several orders of magnitude slower than linear representations and must therefore occur asynchronously."
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 2823
                },
                {
                    "x": 1211,
                    "y": 2823
                },
                {
                    "x": 1211,
                    "y": 3101
                },
                {
                    "x": 147,
                    "y": 3101
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:18px'>The performance of MCTS is to a large degree determined by the quality of the<br>rollout policy. Prior work has focused on handcrafted patterns50 or learning rollout<br>policies by supervised learning13, reinforcement learning16, simulation balanc-<br>ing51,52 or online adaptation30,53, however, it is known that rollout-based position<br>evaluation is frequently inaccurate54. AlphaGo uses relatively simple rollouts, and<br>instead addresses the challenging problem of position evaluation more directly<br>using value networks.</p>",
            "id": 116,
            "page": 7,
            "text": "The performance of MCTS is to a large degree determined by the quality of the rollout policy. Prior work has focused on handcrafted patterns50 or learning rollout policies by supervised learning13, reinforcement learning16, simulation balancing51,52 or online adaptation30,53, however, it is known that rollout-based position evaluation is frequently inaccurate54. AlphaGo uses relatively simple rollouts, and instead addresses the challenging problem of position evaluation more directly using value networks."
        },
        {
            "bounding_box": [
                {
                    "x": 1246,
                    "y": 233
                },
                {
                    "x": 2307,
                    "y": 233
                },
                {
                    "x": 2307,
                    "y": 394
                },
                {
                    "x": 1246,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:16px'>Search algorithm. To efficiently integrate large neural networks into AlphaGo, we<br>implemented an asynchronous policy and value MCTS algorithm (APV-MCTS).<br>Each node s in the search tree contains edges (s, a) for all legal actions a E A(s).<br>Each edge stores a set of statistics,</p>",
            "id": 117,
            "page": 7,
            "text": "Search algorithm. To efficiently integrate large neural networks into AlphaGo, we implemented an asynchronous policy and value MCTS algorithm (APV-MCTS). Each node s in the search tree contains edges (s, a) for all legal actions a E A(s). Each edge stores a set of statistics,"
        },
        {
            "bounding_box": [
                {
                    "x": 1390,
                    "y": 421
                },
                {
                    "x": 2163,
                    "y": 421
                },
                {
                    "x": 2163,
                    "y": 465
                },
                {
                    "x": 1390,
                    "y": 465
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:20px'>{P(s,a), Nv(s,a), Nr(s,a), Wv(s,a), Wr(s,a), Q(s,a)}</p>",
            "id": 118,
            "page": 7,
            "text": "{P(s,a), Nv(s,a), Nr(s,a), Wv(s,a), Wr(s,a), Q(s,a)}"
        },
        {
            "bounding_box": [
                {
                    "x": 1242,
                    "y": 472
                },
                {
                    "x": 2315,
                    "y": 472
                },
                {
                    "x": 2315,
                    "y": 2387
                },
                {
                    "x": 1242,
                    "y": 2387
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:14px'>where P(s, a) is the prior probability, Wv(s, a) and Wr(s, a) are Monte Carlo esti-<br>mates of total action value, accumulated over Nv(s, a) and Nr(s, a) leaf evaluations<br>and rollout rewards, respectively, and Q(s, a) is the combined mean action value for<br>that edge. Multiple simulations are executed in parallel on separate search threads.<br>The APV-MCTS algorithm proceeds in the four stages outlined in Fig. 3.<br>Selection (Fig. 3a). The first in-tree phase of each simulation begins at the root of<br>the search tree and finishes when the simulation reaches a leaf node at time step<br>L. At each of these time steps, t < L, an action is selected according to the statistics<br>in the search tree, at = argmaxa (Q(st,a) + u(st, a)) using a variant of the PUCT<br>VEbNr(s,b)<br>algorithm48, u(s, a) = cpuctP(s, a) , where Cpuct is a constant determining<br>1 + Nr(s,a)<br>the level of exploration; this search control strategy initially prefers actions with<br>high prior probability and low visit count, but asymptotically prefers actions with<br>high action value.<br>Evaluation (Fig. 3c). The leaf position SL is added to a queue for evaluation vO(SL)<br>by the value network, unless it has previously been evaluated. The second rollout<br>phase of each simulation begins at leaf node SL and continues until the end of the<br>game. At each of these time-steps, t> L, actions are selected by both players accord-<br>ing to the rollout policy, at ~ P� (·|st). When the game reaches a terminal state, the<br>outcome Zt = 土 r(ST) is computed from the final score.<br>Backup (Fig. 3d). At each in-tree step t≤ L of the simulation, the rollout statistics<br>are updated as ifit has lost nvl games, Nr(st, at) ← Nr(st, at) + nvl; Wr(st, at) ← Wr(st,<br>at) -nvl; this virtual loss55 discourages other threads from simultaneously explor-<br>ing the identical variation. At the end of the simulation, t he rollout statistics are<br>updated in a backward pass through each step t≤L, replacing the virtual losses by<br>the outcome, Nr(st, at) ← Nr(st, at) -nvl + 1; Wr(st, at) ← Wr(st, at) + nvl + Zt.<br>Asynchronously, a separate backward pass is initiated when the evaluation<br>ofthe leaf position SL completes. The output of the value network vo(sl) is used to<br>update value statistics in a second backward pass through each step t≤ L,<br>Nv(st, at) ←Nv(st, at) +1, Wv(st, at) ← Wv(st, at) + v0(SL). The overall evaluation of<br>each state action is a weighted average of the Monte Carlo estimates,<br>Wv(s,a) Wr(s,a)<br>Q(s,a) = (1 - 入) + 入 , that mixes together the value network and<br>Nv(s,a) Nr(s,a)<br>rollout evaluations with weighting parameter 入. All updates are performed<br>lock-free56.<br>Expansion (Fig. 3b). When the visit count exceeds a threshold, Nr(s, a) > nthr, the<br>successor state s' =f(s, a) is added to the search tree. The new node is initialized<br>to {N(s', a) =Nr(s', a) =0, W(s', a) = Wr(s', a) = 0, P(s',a) =po(as')}, using a tree<br>policy pr(a|s') (similar to the rollout policy but with more features, see Extended<br>Data Table 4) to provide placeholder prior probabilities for action selection. The<br>position s' is also inserted into a queue for asynchronous GPU evaluation by the<br>policy network. Prior probabilities are computed by the SL policy network pB (·|s')<br>with a softmax temperature set to B; these replace the placeholder prior probabil-<br>ities, P(s', a) ← p� (a|s'), using an atomic update. The threshold nthr is adjusted<br>dynamically to ensure that the rate at which positions are added to the policy queue<br>matches the rate at which the GPUs evaluate the policy network. Positions are<br>evaluated by both the policy network and the value network using a mini-batch<br>size of 1 to minimize end-to-end evaluation time.</p>",
            "id": 119,
            "page": 7,
            "text": "where P(s, a) is the prior probability, Wv(s, a) and Wr(s, a) are Monte Carlo estimates of total action value, accumulated over Nv(s, a) and Nr(s, a) leaf evaluations and rollout rewards, respectively, and Q(s, a) is the combined mean action value for that edge. Multiple simulations are executed in parallel on separate search threads. The APV-MCTS algorithm proceeds in the four stages outlined in Fig. 3. Selection (Fig. 3a). The first in-tree phase of each simulation begins at the root of the search tree and finishes when the simulation reaches a leaf node at time step L. At each of these time steps, t < L, an action is selected according to the statistics in the search tree, at = argmaxa (Q(st,a) + u(st, a)) using a variant of the PUCT VEbNr(s,b) algorithm48, u(s, a) = cpuctP(s, a) , where Cpuct is a constant determining 1 + Nr(s,a) the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asymptotically prefers actions with high action value. Evaluation (Fig. 3c). The leaf position SL is added to a queue for evaluation vO(SL) by the value network, unless it has previously been evaluated. The second rollout phase of each simulation begins at leaf node SL and continues until the end of the game. At each of these time-steps, t> L, actions are selected by both players according to the rollout policy, at ~ P� (·|st). When the game reaches a terminal state, the outcome Zt = 土 r(ST) is computed from the final score. Backup (Fig. 3d). At each in-tree step t≤ L of the simulation, the rollout statistics are updated as ifit has lost nvl games, Nr(st, at) ← Nr(st, at) + nvl; Wr(st, at) ← Wr(st, at) -nvl; this virtual loss55 discourages other threads from simultaneously exploring the identical variation. At the end of the simulation, t he rollout statistics are updated in a backward pass through each step t≤L, replacing the virtual losses by the outcome, Nr(st, at) ← Nr(st, at) -nvl + 1; Wr(st, at) ← Wr(st, at) + nvl + Zt. Asynchronously, a separate backward pass is initiated when the evaluation ofthe leaf position SL completes. The output of the value network vo(sl) is used to update value statistics in a second backward pass through each step t≤ L, Nv(st, at) ←Nv(st, at) +1, Wv(st, at) ← Wv(st, at) + v0(SL). The overall evaluation of each state action is a weighted average of the Monte Carlo estimates, Wv(s,a) Wr(s,a) Q(s,a) = (1 - 入) + 入 , that mixes together the value network and Nv(s,a) Nr(s,a) rollout evaluations with weighting parameter 入. All updates are performed lock-free56. Expansion (Fig. 3b). When the visit count exceeds a threshold, Nr(s, a) > nthr, the successor state s' =f(s, a) is added to the search tree. The new node is initialized to {N(s', a) =Nr(s', a) =0, W(s', a) = Wr(s', a) = 0, P(s',a) =po(as')}, using a tree policy pr(a|s') (similar to the rollout policy but with more features, see Extended Data Table 4) to provide placeholder prior probabilities for action selection. The position s' is also inserted into a queue for asynchronous GPU evaluation by the policy network. Prior probabilities are computed by the SL policy network pB (·|s') with a softmax temperature set to B; these replace the placeholder prior probabilities, P(s', a) ← p� (a|s'), using an atomic update. The threshold nthr is adjusted dynamically to ensure that the rate at which positions are added to the policy queue matches the rate at which the GPUs evaluate the policy network. Positions are evaluated by both the policy network and the value network using a mini-batch size of 1 to minimize end-to-end evaluation time."
        },
        {
            "bounding_box": [
                {
                    "x": 1247,
                    "y": 2387
                },
                {
                    "x": 2309,
                    "y": 2387
                },
                {
                    "x": 2309,
                    "y": 2861
                },
                {
                    "x": 1247,
                    "y": 2861
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:14px'>We also implemented a distributed APV-MCTS algorithm. This architecture<br>consists of a single master machine that executes the main search, many remote<br>worker CPUs that execute asynchronous rollouts, and many remote worker GPUs<br>that execute asynchronous policy and value network evaluations. The entire search<br>tree is stored on the master, which only executes the in-tree phase of each simu-<br>lation. The leaf positions are communicated to the worker CPUs, which execute<br>the rollout phase of simulation, and to the worker GPUs, which compute network<br>features and evaluate the policy and value networks. The prior probabilities ofthe<br>policy network are returned to the master, where they replace placeholder prior<br>probabilities at the newly expanded node. The rewards from rollouts and the value<br>network outputs are each returned to the master, and backed up the originating<br>search path.</p>",
            "id": 120,
            "page": 7,
            "text": "We also implemented a distributed APV-MCTS algorithm. This architecture consists of a single master machine that executes the main search, many remote worker CPUs that execute asynchronous rollouts, and many remote worker GPUs that execute asynchronous policy and value network evaluations. The entire search tree is stored on the master, which only executes the in-tree phase of each simulation. The leaf positions are communicated to the worker CPUs, which execute the rollout phase of simulation, and to the worker GPUs, which compute network features and evaluate the policy and value networks. The prior probabilities ofthe policy network are returned to the master, where they replace placeholder prior probabilities at the newly expanded node. The rewards from rollouts and the value network outputs are each returned to the master, and backed up the originating search path."
        },
        {
            "bounding_box": [
                {
                    "x": 1245,
                    "y": 2863
                },
                {
                    "x": 2308,
                    "y": 2863
                },
                {
                    "x": 2308,
                    "y": 3103
                },
                {
                    "x": 1245,
                    "y": 3103
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:14px'>At the end of search AlphaGo selects the action with maximum visit count; this<br>is less sensitive to outliers than maximizing action value15. The search tree is reused<br>at subsequent time steps: the child node corresponding to the played action<br>becomes the new root node; the subtree below this child is retained along with all<br>its statistics, while the remainder of the tree is discarded. The match version of<br>AlphaGo continues searching during the opponent's move. It extends the search</p>",
            "id": 121,
            "page": 7,
            "text": "At the end of search AlphaGo selects the action with maximum visit count; this is less sensitive to outliers than maximizing action value15. The search tree is reused at subsequent time steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded. The match version of AlphaGo continues searching during the opponent's move. It extends the search"
        },
        {
            "bounding_box": [
                {
                    "x": 878,
                    "y": 3205
                },
                {
                    "x": 1570,
                    "y": 3205
                },
                {
                    "x": 1570,
                    "y": 3242
                },
                {
                    "x": 878,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='122' style='font-size:14px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 122,
            "page": 7,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1965,
                    "y": 102
                },
                {
                    "x": 2320,
                    "y": 102
                },
                {
                    "x": 2320,
                    "y": 153
                },
                {
                    "x": 1965,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='123' style='font-size:20px'>ARTICLE RESEARCH</header>",
            "id": 123,
            "page": 8,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 168,
                    "y": 232
                },
                {
                    "x": 1231,
                    "y": 232
                },
                {
                    "x": 1231,
                    "y": 394
                },
                {
                    "x": 168,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:14px'>ifthe action maximizing visit count and the action maximizing action value disa-<br>gree. Time controls were otherwise shaped to use most time in the middle-game57.<br>AlphaGo resigns when its overall evaluation drops below an estimated 10% prob-<br>ability of winning the game, that is, maxa Q(s,a) < - 0.8.</p>",
            "id": 124,
            "page": 8,
            "text": "ifthe action maximizing visit count and the action maximizing action value disagree. Time controls were otherwise shaped to use most time in the middle-game57. AlphaGo resigns when its overall evaluation drops below an estimated 10% probability of winning the game, that is, maxa Q(s,a) < - 0.8."
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 393
                },
                {
                    "x": 1232,
                    "y": 393
                },
                {
                    "x": 1232,
                    "y": 1189
                },
                {
                    "x": 169,
                    "y": 1189
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:14px'>AlphaGo does not employ the all-moves-as-first10 or rapid action value estima-<br>tion58 heuristics used in the majority of Monte Carlo Go programs; when using<br>policy networks as prior knowledge, these biased heuristics do not appear to give<br>any additional benefit. In addition AlphaGo does not use progressive widening13,<br>dynamic komi59 or an opening book60. The parameters used by AlphaGo in the<br>·<br>Fan Hui match are listed in Extended Data Table 5.<br>Rollout policy. The rollout policy P� (a|s) is a linear softmax policy based on fast,<br>incrementally computed, local pattern-based features consisting ofboth 'response'<br>patterns around the previous move that led to state S, and 'non-response' patterns<br>around the candidate move a in state s. Each non-response pattern is a binary<br>feature matching a specific 3 x 3 pattern centred on a, defined by the colour (black,<br>white, empty) and liberty count (1, 2, ≥3) for each adjacent intersection. Each<br>response pattern is a binary feature matching the colour and liberty count in a<br>12-point diamond-shaped pattern21 centred around the previous move.<br>Additionally, a small number ofhandcrafted local features encode common-sense<br>Go rules (see Extended Data Table 4). Similar to the policy network, the weights<br>� of the rollout policy are trained from 8 million positions from human games on<br>the Tygem server to maximize log likelihood by stochastic gradient descent.<br>Rollouts execute at approximately 1,000 simulations per second per CPU thread<br>on an empty board.</p>",
            "id": 125,
            "page": 8,
            "text": "AlphaGo does not employ the all-moves-as-first10 or rapid action value estimation58 heuristics used in the majority of Monte Carlo Go programs; when using policy networks as prior knowledge, these biased heuristics do not appear to give any additional benefit. In addition AlphaGo does not use progressive widening13, dynamic komi59 or an opening book60. The parameters used by AlphaGo in the · Fan Hui match are listed in Extended Data Table 5. Rollout policy. The rollout policy P� (a|s) is a linear softmax policy based on fast, incrementally computed, local pattern-based features consisting ofboth 'response' patterns around the previous move that led to state S, and 'non-response' patterns around the candidate move a in state s. Each non-response pattern is a binary feature matching a specific 3 x 3 pattern centred on a, defined by the colour (black, white, empty) and liberty count (1, 2, ≥3) for each adjacent intersection. Each response pattern is a binary feature matching the colour and liberty count in a 12-point diamond-shaped pattern21 centred around the previous move. Additionally, a small number ofhandcrafted local features encode common-sense Go rules (see Extended Data Table 4). Similar to the policy network, the weights � of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board."
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 1179
                },
                {
                    "x": 1231,
                    "y": 1179
                },
                {
                    "x": 1231,
                    "y": 2390
                },
                {
                    "x": 169,
                    "y": 2390
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:16px'>Our rollout policy p�(a|s) contains less handcrafted knowledge than state-<br>of-the-art Go programs13. Instead, we exploit the higher-quality action selection<br>within MCTS, which is informed both by the search tree and the policy network.<br>We introduce a new technique that caches all moves from the search tree and<br>then plays similar moves during rollouts; a generalization of the 'last good reply'<br>heuristic53. At every step of the tree traversal, the most probable action is inserted<br>into a hash table, along with the 3 x 3 pattern context (colour, liberty and stone<br>counts) around both the previous move and the current move. At each step ofthe<br>rollout, the pattern context is matched against the hash table; if a match is found<br>then the stored move is played with high probability.<br>Symmetries. In previous work, the symmetries of Go have been exploited by using<br>rotationally and reflectionally invariant filters in the convolutional layers24,28,29<br>Although this may be effective in small neural networks, it actually hurts perfor-<br>mance in larger networks, as it prevents the intermediate filters from identifying<br>specific asymmetric patterns23. Instead, we exploit symmetries at run-time by<br>dynamically transforming each position s using the dihedral group of eight reflec-<br>tions and rotations, d1(s), · · ., d8(s). In an explicit symmetry ensemble, a mini-batch<br>ofall 8 positions is passed into the policy network or value network and computed<br>in parallel. For the value network, the output values are simply averaged,<br>1 �j=1 vo(dj(s)). For the policy network, the planes of output probabilities<br>vo(s) =<br>8<br>are rotated/reflected back into the original orientation, and averaged together to<br>provide an ensemble prediction, Po (·|s) = 1 �j=1 dj 1(po (·|dj(s))); this approach<br>8<br>was used in our raw network evaluation (see Extended Data Table 3). Instead,<br>APV-MCTS makes use ofan implicit symmetry ensemble that randomly selects a<br>single rotation/reflectionj E [1, 8] for each evaluation. We compute exactly one<br>evaluation for that orientation only; in each simulation we compute the value<br>ofleaf node SL by vo(dj(sL)), and allow the search procedure to average over<br>these evaluations. Similarly, we compute the policy network for a single,<br>randomly selected rotation/reflection, dj 1(po (·|dj(s))).</p>",
            "id": 126,
            "page": 8,
            "text": "Our rollout policy p�(a|s) contains less handcrafted knowledge than stateof-the-art Go programs13. Instead, we exploit the higher-quality action selection within MCTS, which is informed both by the search tree and the policy network. We introduce a new technique that caches all moves from the search tree and then plays similar moves during rollouts; a generalization of the 'last good reply' heuristic53. At every step of the tree traversal, the most probable action is inserted into a hash table, along with the 3 x 3 pattern context (colour, liberty and stone counts) around both the previous move and the current move. At each step ofthe rollout, the pattern context is matched against the hash table; if a match is found then the stored move is played with high probability. Symmetries. In previous work, the symmetries of Go have been exploited by using rotationally and reflectionally invariant filters in the convolutional layers24,28,29 Although this may be effective in small neural networks, it actually hurts performance in larger networks, as it prevents the intermediate filters from identifying specific asymmetric patterns23. Instead, we exploit symmetries at run-time by dynamically transforming each position s using the dihedral group of eight reflections and rotations, d1(s), · · ., d8(s). In an explicit symmetry ensemble, a mini-batch ofall 8 positions is passed into the policy network or value network and computed in parallel. For the value network, the output values are simply averaged, 1 �j=1 vo(dj(s)). For the policy network, the planes of output probabilities vo(s) = 8 are rotated/reflected back into the original orientation, and averaged together to provide an ensemble prediction, Po (·|s) = 1 �j=1 dj 1(po (·|dj(s))); this approach 8 was used in our raw network evaluation (see Extended Data Table 3). Instead, APV-MCTS makes use ofan implicit symmetry ensemble that randomly selects a single rotation/reflectionj E  for each evaluation. We compute exactly one evaluation for that orientation only; in each simulation we compute the value ofleaf node SL by vo(dj(sL)), and allow the search procedure to average over these evaluations. Similarly, we compute the policy network for a single, randomly selected rotation/reflection, dj 1(po (·|dj(s)))."
        },
        {
            "bounding_box": [
                {
                    "x": 168,
                    "y": 2384
                },
                {
                    "x": 1232,
                    "y": 2384
                },
                {
                    "x": 1232,
                    "y": 3087
                },
                {
                    "x": 168,
                    "y": 3087
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='127' style='font-size:14px'>Policy network: classification. We trained the policy network Po to classify posi-<br>tions according to expert moves played in the KGS data set. This data set contains<br>29.4 million positions from 160,000 games played by KGS 6 to 9 dan human play-<br>ers; 35.4% of the games are handicap games. The data set was split into a test set<br>(the first million positions) and a training set (the remaining 28.4 million posi-<br>tions). Pass moves were excluded from the data set. Each position consisted of a<br>raw board description s and the move a selected by the human. We augmented the<br>data set to include all eight reflections and rotations of each position. Symmetry<br>augmentation and input features were pre-computed for each position. For each<br>training step, we sampled a randomly selected mini-batch of m samples from<br>and applied an asynchronous stochastic<br>the augmented KGS data set, {sk, a k} k=1<br>gradient descent update to maximize the log likelihood of the action,<br>alog Po(aklsk)<br>The step size a was initialized to 0.003 and was halved<br>△o = a �k=1 do<br>m<br>every 80 million training steps, without momentum terms, and a mini-batch size<br>of m = 16. Updates were applied asynchronously on 50 GPUs using DistBelief61;<br>gradients older than 100 steps were discarded. Training took around 3 weeks for<br>340 million training steps.</p>",
            "id": 127,
            "page": 8,
            "text": "Policy network: classification. We trained the policy network Po to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games. The data set was split into a test set (the first million positions) and a training set (the remaining 28.4 million positions). Pass moves were excluded from the data set. Each position consisted of a raw board description s and the move a selected by the human. We augmented the data set to include all eight reflections and rotations of each position. Symmetry augmentation and input features were pre-computed for each position. For each training step, we sampled a randomly selected mini-batch of m samples from and applied an asynchronous stochastic the augmented KGS data set, {sk, a k} k=1 gradient descent update to maximize the log likelihood of the action, alog Po(aklsk) The step size a was initialized to 0.003 and was halved △o = a �k=1 do m every 80 million training steps, without momentum terms, and a mini-batch size of m = 16. Updates were applied asynchronously on 50 GPUs using DistBelief61; gradients older than 100 steps were discarded. Training took around 3 weeks for 340 million training steps."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 231
                },
                {
                    "x": 2334,
                    "y": 231
                },
                {
                    "x": 2334,
                    "y": 854
                },
                {
                    "x": 1268,
                    "y": 854
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='128' style='font-size:14px'>Policy network: reinforcement learning. We further trained the policy network<br>by policy gradient reinforcement learning25,26. Each iteration consisted of a mini-<br>batch of n games played in parallel, between the current policy network Pp that is<br>that uses parameters P from a previous iter-<br>being trained, and an opponent p P<br>ation, randomly sampled from a pool of opponents, so as to increase the stability<br>of training. Weights were initialized to P= P =�. Every 500 iterations, we added<br>the current parameters P to the opponent pool. Each game iin the mini-batch was<br>played out until termination at step T1, and then scored to determine the outcome<br>z't = 土 r(sTi) from each player's perspective. The games were then replayed to<br>(zit - v(s')),<br>determine the policy gradient update, △p = a �i=1 �T�=1 0 log pp(ailst)<br>n dp<br>using the REINFORCE algorithm25 with baseline v(si) for variance reduction. On<br>the first pass through the training pipeline, the baseline was set to zero; on the<br>second pass we used the value network vo(s) as a baseline; this provided a small<br>performance boost. The policy network was trained in this way for 10,000 mini-<br>batches of 128 games, using 50 GPUs, for one day.</p>",
            "id": 128,
            "page": 8,
            "text": "Policy network: reinforcement learning. We further trained the policy network by policy gradient reinforcement learning25,26. Each iteration consisted of a minibatch of n games played in parallel, between the current policy network Pp that is that uses parameters P from a previous iterbeing trained, and an opponent p P ation, randomly sampled from a pool of opponents, so as to increase the stability of training. Weights were initialized to P= P =�. Every 500 iterations, we added the current parameters P to the opponent pool. Each game iin the mini-batch was played out until termination at step T1, and then scored to determine the outcome z't = 土 r(sTi) from each player's perspective. The games were then replayed to (zit - v(s')), determine the policy gradient update, △p = a �i=1 �T�=1 0 log pp(ailst) n dp using the REINFORCE algorithm25 with baseline v(si) for variance reduction. On the first pass through the training pipeline, the baseline was set to zero; on the second pass we used the value network vo(s) as a baseline; this provided a small performance boost. The policy network was trained in this way for 10,000 minibatches of 128 games, using 50 GPUs, for one day."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 851
                },
                {
                    "x": 2334,
                    "y": 851
                },
                {
                    "x": 2334,
                    "y": 1640
                },
                {
                    "x": 1267,
                    "y": 1640
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:14px'>Value network: regression. We trained a value network vo(s) 2 vPp(s) to approx-<br>imate the value function of the RL policy network pp. To avoid overfitting to the<br>strongly correlated positions within games, we constructed a new data set of uncor-<br>related self-play positions. This data set consisted of over 30 million positions, each<br>drawn from a unique game of self-play. Each game was generated in three phases<br>by randomly sampling a time step U ~ unif{1, 450}, and sampling the first t= 1,...<br>U - 1 moves from the SL policy network, at~ po(·st); then sampling one move<br>uniformly at random from available moves, au~ unif{1, 361} (repeatedly until<br>auis legal); then sampling the remaining sequence of moves until the game termi-<br>nates, t= U + 1, · ·· T, from the RL policy network, at ~ pp(·|st). Finally, the game<br>is scored to determine the outcome Zt = ±r(sT). Only a single training example<br>(SU+1, ZU+1) is added to the data set from each game. This data provides unbiased<br>samples of the value function VPP(SU+1) = E[ZU+ 1|SU+ 1, au+ 1,.. T ~ pp] · During<br>the first two phases of generation we sample from noisier distributions so as<br>to increase the diversity of the data set. The training method was identical<br>to SL policy network training, except that the parameter update was based on<br>mean squared error between the predicted values and the observed rewards,<br>dvo(sk)<br>�� = a Ek=1(2k - vo(sk)) The value network was trained for 50 million<br>m au<br>mini-batches of 32 positions, using 50 GPUs, for one week.</p>",
            "id": 129,
            "page": 8,
            "text": "Value network: regression. We trained a value network vo(s) 2 vPp(s) to approximate the value function of the RL policy network pp. To avoid overfitting to the strongly correlated positions within games, we constructed a new data set of uncorrelated self-play positions. This data set consisted of over 30 million positions, each drawn from a unique game of self-play. Each game was generated in three phases by randomly sampling a time step U ~ unif{1, 450}, and sampling the first t= 1,... U - 1 moves from the SL policy network, at~ po(·st); then sampling one move uniformly at random from available moves, au~ unif{1, 361} (repeatedly until auis legal); then sampling the remaining sequence of moves until the game terminates, t= U + 1, · ·· T, from the RL policy network, at ~ pp(·|st). Finally, the game is scored to determine the outcome Zt = ±r(sT). Only a single training example (SU+1, ZU+1) is added to the data set from each game. This data provides unbiased samples of the value function VPP(SU+1) = E[ZU+ 1|SU+ 1, au+ 1,.. T ~ pp] · During the first two phases of generation we sample from noisier distributions so as to increase the diversity of the data set. The training method was identical to SL policy network training, except that the parameter update was based on mean squared error between the predicted values and the observed rewards, dvo(sk) �� = a Ek=1(2k - vo(sk)) The value network was trained for 50 million m au mini-batches of 32 positions, using 50 GPUs, for one week."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1642
                },
                {
                    "x": 2331,
                    "y": 1642
                },
                {
                    "x": 2331,
                    "y": 2153
                },
                {
                    "x": 1268,
                    "y": 2153
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='130' style='font-size:14px'>Features for policy/value network. Each position s was pre-processed into a set<br>of 19 x 19 feature planes. The features that we use come directly from the raw<br>representation of the game rules, indicating the status of each intersection of the<br>Go board: stone colour, liberties (adjacent empty points of stone's chain), captures,<br>legality, turns since stone was played, and (for the value network only) the current<br>colour to play. In addition, we use one simple tactical feature that computes the<br>outcome of a ladder search7. All features were computed relative to the current<br>colour to play; for example, the stone colour at each intersection was represented<br>as either player or opponent rather than black or white. Each integer feature value<br>is split into multiple 19 x 19 planes of binary values (one-hot encoding). For exam-<br>ple, separate binary feature planes are used to represent whether an intersection<br>has 1 liberty, 2 liberties,..., ≥8 liberties. The full set offeature planes are listed in<br>Extended Data Table 2.</p>",
            "id": 130,
            "page": 8,
            "text": "Features for policy/value network. Each position s was pre-processed into a set of 19 x 19 feature planes. The features that we use come directly from the raw representation of the game rules, indicating the status of each intersection of the Go board: stone colour, liberties (adjacent empty points of stone's chain), captures, legality, turns since stone was played, and (for the value network only) the current colour to play. In addition, we use one simple tactical feature that computes the outcome of a ladder search7. All features were computed relative to the current colour to play; for example, the stone colour at each intersection was represented as either player or opponent rather than black or white. Each integer feature value is split into multiple 19 x 19 planes of binary values (one-hot encoding). For example, separate binary feature planes are used to represent whether an intersection has 1 liberty, 2 liberties,..., ≥8 liberties. The full set offeature planes are listed in Extended Data Table 2."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2155
                },
                {
                    "x": 2332,
                    "y": 2155
                },
                {
                    "x": 2332,
                    "y": 2589
                },
                {
                    "x": 1269,
                    "y": 2589
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:14px'>Neural network architecture. The input to the policy network is a 19 x 19 x 48<br>image stack consisting of 48 feature planes. The first hidden layer zero pads the<br>input into a 23 x 23 image, then convolves k filters of kernel size 5 x 5 with stride<br>1 with the input image and applies a rectifier nonlinearity. Each ofthe subsequent<br>hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 x 21<br>image, then convolves k filters of kernel size 3 x 3 with stride 1, again followed<br>by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 x 1<br>with stride 1, with a different bias for each position, and applies a softmax func-<br>tion. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended<br>Data Table 3 additionally show the results of training with k = 128, 256 and<br>384 filters.</p>",
            "id": 131,
            "page": 8,
            "text": "Neural network architecture. The input to the policy network is a 19 x 19 x 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 x 23 image, then convolves k filters of kernel size 5 x 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each ofthe subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 x 21 image, then convolves k filters of kernel size 3 x 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 x 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2591
                },
                {
                    "x": 2332,
                    "y": 2591
                },
                {
                    "x": 2332,
                    "y": 2828
                },
                {
                    "x": 1269,
                    "y": 2828
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:14px'>The input to the value network is also a 19 x 19 x 48 image stack, with an addi-<br>tional binary feature plane describing the current colour to play. Hidden layers 2 to<br>11 are identical to the policy network, hidden layer 12 is an additional convolution<br>layer, hidden layer 13 convolves 1 filter ofkernel size 1 x 1 with stride 1, and hidden<br>layer 14 is a fully connected linear layer with 256 rectifier units. The output layer<br>is a fully connected linear layer with a single tanh unit.</p>",
            "id": 132,
            "page": 8,
            "text": "The input to the value network is also a 19 x 19 x 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter ofkernel size 1 x 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2829
                },
                {
                    "x": 2334,
                    "y": 2829
                },
                {
                    "x": 2334,
                    "y": 3086
                },
                {
                    "x": 1268,
                    "y": 3086
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:16px'>Evaluation. We evaluated the relative strength of computer Go programs by run-<br>ning an internal tournament and measuring the Elo rating of each program. We<br>estimate the probability that program a will beat program 6 by a logistic function<br>1 by Bayesian<br>p(a beats b) = and estimate the ratings e(·)<br>1 + exp(celo(e(b) - e(a))<br>logistic regression, computed by the BayesElo program37 using the standard<br>constant Celo = 1/400. The scale was anchored to the BayesElo rating ofprofessional</p>",
            "id": 133,
            "page": 8,
            "text": "Evaluation. We evaluated the relative strength of computer Go programs by running an internal tournament and measuring the Elo rating of each program. We estimate the probability that program a will beat program 6 by a logistic function 1 by Bayesian p(a beats b) = and estimate the ratings e(·) 1 + exp(celo(e(b) - e(a)) logistic regression, computed by the BayesElo program37 using the standard constant Celo = 1/400. The scale was anchored to the BayesElo rating ofprofessional"
        },
        {
            "bounding_box": [
                {
                    "x": 910,
                    "y": 3205
                },
                {
                    "x": 1601,
                    "y": 3205
                },
                {
                    "x": 1601,
                    "y": 3242
                },
                {
                    "x": 910,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='134' style='font-size:14px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 134,
            "page": 8,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 158,
                    "y": 97
                },
                {
                    "x": 524,
                    "y": 97
                },
                {
                    "x": 524,
                    "y": 154
                },
                {
                    "x": 158,
                    "y": 154
                }
            ],
            "category": "header",
            "html": "<header id='135' style='font-size:20px'>RESEARCH ARTICLE</header>",
            "id": 135,
            "page": 9,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 233
                },
                {
                    "x": 1211,
                    "y": 233
                },
                {
                    "x": 1211,
                    "y": 630
                },
                {
                    "x": 146,
                    "y": 630
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:16px'>Go player Fan Hui (2,908 at date of submission)62. All programs received a maxi-<br>mum of 5s computation time per move; games were scored using Chinese rules<br>with a komi of7.5 points (extra points to compensate white for playing second).<br>We also played handicap games where AlphaGo played white against existing Go<br>programs; for these games we used a non-standard handicap system in which komi<br>was retained but black was given additional stones on the usual handicap points.<br>Using these rules, a handicap of K stones is equivalent to giving K - 1 free moves<br>to black, rather than K - 1/2 free moves using standard no-komi handicap rules.<br>We used these handicap rules because AlphaGo's value network was trained spe-<br>cifically to use a komi of7.5.</p>",
            "id": 136,
            "page": 9,
            "text": "Go player Fan Hui (2,908 at date of submission)62. All programs received a maximum of 5s computation time per move; games were scored using Chinese rules with a komi of7.5 points (extra points to compensate white for playing second). We also played handicap games where AlphaGo played white against existing Go programs; for these games we used a non-standard handicap system in which komi was retained but black was given additional stones on the usual handicap points. Using these rules, a handicap of K stones is equivalent to giving K - 1 free moves to black, rather than K - 1/2 free moves using standard no-komi handicap rules. We used these handicap rules because AlphaGo's value network was trained specifically to use a komi of7.5."
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 630
                },
                {
                    "x": 1211,
                    "y": 630
                },
                {
                    "x": 1211,
                    "y": 867
                },
                {
                    "x": 146,
                    "y": 867
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:18px'>With the exception of distributed AlphaGo, each computer Go program was<br>executed on its own single machine, with identical specifications, using the latest<br>available version and the best hardware configuration supported by that program<br>(see Extended Data Table 6). In Fig. 4, approximate ranks of computer programs<br>are based on the highest KGS rank achieved by that program; however, the KGS<br>version may differ from the publicly available version.</p>",
            "id": 137,
            "page": 9,
            "text": "With the exception of distributed AlphaGo, each computer Go program was executed on its own single machine, with identical specifications, using the latest available version and the best hardware configuration supported by that program (see Extended Data Table 6). In Fig. 4, approximate ranks of computer programs are based on the highest KGS rank achieved by that program; however, the KGS version may differ from the publicly available version."
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 868
                },
                {
                    "x": 1212,
                    "y": 868
                },
                {
                    "x": 1212,
                    "y": 1305
                },
                {
                    "x": 146,
                    "y": 1305
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:16px'>The match against Fan Hui was arbitrated by an impartial referee. Five<br>formal games and five informal games were played with 7.5 komi, no handi-<br>cap, and Chinese rules. AlphaGo won these games 5-0 and 3-2 respectively<br>(Fig. 6 and Extended Data Table 1). Time controls for formal games were 1h main<br>time plus three periods of 30 s byoyomi. Time controls for informal games were<br>three periods of 30 s byoyomi. Time controls and playing conditions were chosen<br>by Fan Hui in advance of the match; it was also agreed that the overall match<br>outcome would be determined solely by the formal games. To approximately<br>assess the relative rating ofFan Hui to computer Go programs, we appended the<br>results of all ten games to our internal tournament results, ignoring differences<br>in time controls.</p>",
            "id": 138,
            "page": 9,
            "text": "The match against Fan Hui was arbitrated by an impartial referee. Five formal games and five informal games were played with 7.5 komi, no handicap, and Chinese rules. AlphaGo won these games 5-0 and 3-2 respectively (Fig. 6 and Extended Data Table 1). Time controls for formal games were 1h main time plus three periods of 30 s byoyomi. Time controls for informal games were three periods of 30 s byoyomi. Time controls and playing conditions were chosen by Fan Hui in advance of the match; it was also agreed that the overall match outcome would be determined solely by the formal games. To approximately assess the relative rating ofFan Hui to computer Go programs, we appended the results of all ten games to our internal tournament results, ignoring differences in time controls."
        },
        {
            "bounding_box": [
                {
                    "x": 148,
                    "y": 1369
                },
                {
                    "x": 1201,
                    "y": 1369
                },
                {
                    "x": 1201,
                    "y": 1747
                },
                {
                    "x": 148,
                    "y": 1747
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:18px'>39. Littman, M. L. Markov games as a framework for multi-agent reinforcement<br>learning. In 11th International Conference on Machine Learning, 157-163<br>(1994).<br>40. Knuth, D. E. & Moore, R. W. An analysis of alpha-beta pruning. Artif. Intell. 6,<br>293-326 (1975).<br>41. Sutton, R. Learning to predict by the method of temporal differences.<br>Mach. Learn. 3, 9-44 (1988).<br>42. Baxter, J., Tridgell, A. & Weaver, L. Learning to play chess using temporal<br>differences. Mach. Learn. 40, 243-263 (2000).<br>43. Veness, J., Silver, D., Blair, A. & Uther, W. Bootstrapping from game tree search.<br>In Advances in Neural Information Processing Systems (2009).</p>",
            "id": 139,
            "page": 9,
            "text": "39. Littman, M. L. Markov games as a framework for multi-agent reinforcement learning. In 11th International Conference on Machine Learning, 157-163 (1994). 40. Knuth, D. E. & Moore, R. W. An analysis of alpha-beta pruning. Artif. Intell. 6, 293-326 (1975). 41. Sutton, R. Learning to predict by the method of temporal differences. Mach. Learn. 3, 9-44 (1988). 42. Baxter, J., Tridgell, A. & Weaver, L. Learning to play chess using temporal differences. Mach. Learn. 40, 243-263 (2000). 43. Veness, J., Silver, D., Blair, A. & Uther, W. Bootstrapping from game tree search. In Advances in Neural Information Processing Systems (2009)."
        },
        {
            "bounding_box": [
                {
                    "x": 1246,
                    "y": 225
                },
                {
                    "x": 2299,
                    "y": 225
                },
                {
                    "x": 2299,
                    "y": 1745
                },
                {
                    "x": 1246,
                    "y": 1745
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:16px'>44. Samuel, A. L. Some studies in machine learning using the game of checkers<br>II - recent progress. IBM J. Res. Develop. 11, 601-617 (1967).<br>45. Schaeffer, J., Hlynka, M. & Jussila, V. Temporal difference learning applied to a<br>high-performance game-playing program. In 17th International Joint<br>Conference on Artificial Intelligence, 529-534 (2001).<br>46. Tesauro, G. TD-gammon, a self-teaching backgammon program, achieves<br>master-level play. Neural Comput. 6, 215-219 (1994).<br>47. Dahl, F. Honte, a Go-playing program using neural nets. In Machines that learn<br>to play games, 205-223 (Nova Science, 1999).<br>48. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell.<br>61, 203-230 (2011).<br>49. Lanctot, M., Winands, M. H. M., Pepels, T. & Sturtevant, N. R. Monte Carlo tree<br>search with heuristic evaluations using implicit minimax backups. In IEEE<br>Conference on Computational Intelligence and Games, 1-8 (2014).<br>50. Gelly, S., Wang, Y., Munos, R. & Teytaud, 0. Modification of UCT with patterns in<br>Monte-Carlo Go. Tech. Rep. 6062, INRIA (2006).<br>51. Silver, D. & Tesauro, G. Monte-Carlo simulation balancing. In 26th International<br>Conference on Machine Learning, 119 (2009).<br>52. Huang, S.-C., Coulom, R. & Lin, S.-S. Monte-Carlo simulation balancing in<br>practice. In 7th International Conference on Computers and Games, 81-92<br>(Springer-Verlag, 2011).<br>53. Baier, H. & Drake, P. D. The power of forgetting: improving the last-good-reply<br>policy in Monte Carlo Go. IEEE Trans. Comput. Intell. AI in Games 2, 303-309<br>(2010).<br>54. Huang, S. & M�ller, M. Investigating the limits of Monte-Carlo tree search<br>methods in computer Go. In 8th International Conference on Computers and<br>Games, 39-48 (2013).<br>55. Segal, R. B. On the scalability of parallel UCT. Computers and Games 6515,<br>36-47 (2011).<br>56. Enzenberger, M. & M�ller, M. A lock-free multithreaded Monte-Carlo tree<br>search algorithm. In 12th Advances in Computer Games Conference, 14-20<br>(2009).<br>57. Huang, S.-C., Coulom, R. & Lin, S.-S. Time management for Monte-Carlo tree<br>search applied to the game of Go. In International Conference on Technologies<br>and Applications of Artificial Intelligence, 462-466 (2010).<br>58. Gelly, S. & Silver, D. Monte-Carlo tree search and rapid action value estimation<br>in computer Go. Artif. Intell. 175, 1856-1875 (2011).<br>59. Baudi�, P. Balancing MCTS by dynamically adjusting the komi value. ICGA J.<br>34, 131 (2011).<br>60. Baier, H. & Winands, M. H. Active opening book application for Monte-Carlo<br>tree search in 19x19 Go. In Benelux Conference on Artificial Intelligence,<br>3-10 (2011).<br>61. Dean, J. et al. Large scale distributed deep networks. In Advances in Neural<br>Information Processing Systems, 1223-1231 (2012).<br>62. Go ratings. http://www.goratings.org.</p>",
            "id": 140,
            "page": 9,
            "text": "44. Samuel, A. L. Some studies in machine learning using the game of checkers II - recent progress. IBM J. Res. Develop. 11, 601-617 (1967). 45. Schaeffer, J., Hlynka, M. & Jussila, V. Temporal difference learning applied to a high-performance game-playing program. In 17th International Joint Conference on Artificial Intelligence, 529-534 (2001). 46. Tesauro, G. TD-gammon, a self-teaching backgammon program, achieves master-level play. Neural Comput. 6, 215-219 (1994). 47. Dahl, F. Honte, a Go-playing program using neural nets. In Machines that learn to play games, 205-223 (Nova Science, 1999). 48. Rosin, C. D. Multi-armed bandits with episode context. Ann. Math. Artif. Intell. 61, 203-230 (2011). 49. Lanctot, M., Winands, M. H. M., Pepels, T. & Sturtevant, N. R. Monte Carlo tree search with heuristic evaluations using implicit minimax backups. In IEEE Conference on Computational Intelligence and Games, 1-8 (2014). 50. Gelly, S., Wang, Y., Munos, R. & Teytaud, 0. Modification of UCT with patterns in Monte-Carlo Go. Tech. Rep. 6062, INRIA (2006). 51. Silver, D. & Tesauro, G. Monte-Carlo simulation balancing. In 26th International Conference on Machine Learning, 119 (2009). 52. Huang, S.-C., Coulom, R. & Lin, S.-S. Monte-Carlo simulation balancing in practice. In 7th International Conference on Computers and Games, 81-92 (Springer-Verlag, 2011). 53. Baier, H. & Drake, P. D. The power of forgetting: improving the last-good-reply policy in Monte Carlo Go. IEEE Trans. Comput. Intell. AI in Games 2, 303-309 (2010). 54. Huang, S. & M�ller, M. Investigating the limits of Monte-Carlo tree search methods in computer Go. In 8th International Conference on Computers and Games, 39-48 (2013). 55. Segal, R. B. On the scalability of parallel UCT. Computers and Games 6515, 36-47 (2011). 56. Enzenberger, M. & M�ller, M. A lock-free multithreaded Monte-Carlo tree search algorithm. In 12th Advances in Computer Games Conference, 14-20 (2009). 57. Huang, S.-C., Coulom, R. & Lin, S.-S. Time management for Monte-Carlo tree search applied to the game of Go. In International Conference on Technologies and Applications of Artificial Intelligence, 462-466 (2010). 58. Gelly, S. & Silver, D. Monte-Carlo tree search and rapid action value estimation in computer Go. Artif. Intell. 175, 1856-1875 (2011). 59. Baudi�, P. Balancing MCTS by dynamically adjusting the komi value. ICGA J. 34, 131 (2011). 60. Baier, H. & Winands, M. H. Active opening book application for Monte-Carlo tree search in 19x19 Go. In Benelux Conference on Artificial Intelligence, 3-10 (2011). 61. Dean, J.  Large scale distributed deep networks. In Advances in Neural Information Processing Systems, 1223-1231 (2012). 62. Go ratings. http://www.goratings.org."
        },
        {
            "bounding_box": [
                {
                    "x": 877,
                    "y": 3204
                },
                {
                    "x": 1569,
                    "y": 3204
                },
                {
                    "x": 1569,
                    "y": 3242
                },
                {
                    "x": 877,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='141' style='font-size:14px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 141,
            "page": 9,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1964,
                    "y": 100
                },
                {
                    "x": 2320,
                    "y": 100
                },
                {
                    "x": 2320,
                    "y": 153
                },
                {
                    "x": 1964,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='142' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 142,
            "page": 10,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 219
                },
                {
                    "x": 1286,
                    "y": 219
                },
                {
                    "x": 1286,
                    "y": 263
                },
                {
                    "x": 170,
                    "y": 263
                }
            ],
            "category": "caption",
            "html": "<caption id='143' style='font-size:18px'>Extended Data Table 1 I Details of match between AlphaGo and Fan Hui</caption>",
            "id": 143,
            "page": 10,
            "text": "Extended Data Table 1 I Details of match between AlphaGo and Fan Hui"
        },
        {
            "bounding_box": [
                {
                    "x": 172,
                    "y": 283
                },
                {
                    "x": 1488,
                    "y": 283
                },
                {
                    "x": 1488,
                    "y": 848
                },
                {
                    "x": 172,
                    "y": 848
                }
            ],
            "category": "table",
            "html": "<br><table id='144' style='font-size:20px'><tr><td>Date</td><td>Black</td><td>White</td><td>Category</td><td>Result</td></tr><tr><td>5/10/15</td><td>Fan Hui</td><td>AlphaGo</td><td>Formal</td><td>AlphaGo wins by 2.5 points</td></tr><tr><td>5/10/15</td><td>Fan Hui</td><td>AlphaGo</td><td>Informal</td><td>Fan Hui wins by resignation</td></tr><tr><td>6/10/15</td><td>AlphaGo</td><td>Fan Hui</td><td>Formal</td><td>AlphaGo wins by resignation</td></tr><tr><td>6/10/15</td><td>AlphaGo</td><td>Fan Hui</td><td>Informal</td><td>AlphaGo wins by resignation</td></tr><tr><td>7/10/15</td><td>Fan Hui</td><td>AlphaGo</td><td>Formal</td><td>AlphaGo wins by resignation</td></tr><tr><td>7/10/15</td><td>Fan Hui</td><td>AlphaGo</td><td>Informal</td><td>AlphaGo wins by resignation</td></tr><tr><td>8/10/15</td><td>AlphaGo</td><td>Fan Hui</td><td>Formal</td><td>AlphaGo wins by resignation</td></tr><tr><td>8/10/15</td><td>AlphaGo</td><td>Fan Hui</td><td>Informal</td><td>AlphaGo wins by resignation</td></tr><tr><td>9/10/15</td><td>Fan Hui</td><td>AlphaGo</td><td>Formal</td><td>AlphaGo wins by resignation</td></tr><tr><td>9/10/15</td><td>AlphaGo</td><td>Fan Hui</td><td>Informal</td><td>Fan Hui wins by resignation</td></tr></table>",
            "id": 144,
            "page": 10,
            "text": "Date Black White Category Result  5/10/15 Fan Hui AlphaGo Formal AlphaGo wins by 2.5 points  5/10/15 Fan Hui AlphaGo Informal Fan Hui wins by resignation  6/10/15 AlphaGo Fan Hui Formal AlphaGo wins by resignation  6/10/15 AlphaGo Fan Hui Informal AlphaGo wins by resignation  7/10/15 Fan Hui AlphaGo Formal AlphaGo wins by resignation  7/10/15 Fan Hui AlphaGo Informal AlphaGo wins by resignation  8/10/15 AlphaGo Fan Hui Formal AlphaGo wins by resignation  8/10/15 AlphaGo Fan Hui Informal AlphaGo wins by resignation  9/10/15 Fan Hui AlphaGo Formal AlphaGo wins by resignation  9/10/15 AlphaGo Fan Hui Informal"
        },
        {
            "bounding_box": [
                {
                    "x": 171,
                    "y": 861
                },
                {
                    "x": 1462,
                    "y": 861
                },
                {
                    "x": 1462,
                    "y": 929
                },
                {
                    "x": 171,
                    "y": 929
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='145' style='font-size:14px'>The match consisted of five formal games with longer time controls, and five informal games with shorter time controls.<br>Time controls and playing conditions were chosen by Fan Hui in advance of the match.</p>",
            "id": 145,
            "page": 10,
            "text": "The match consisted of five formal games with longer time controls, and five informal games with shorter time controls. Time controls and playing conditions were chosen by Fan Hui in advance of the match."
        },
        {
            "bounding_box": [
                {
                    "x": 910,
                    "y": 3202
                },
                {
                    "x": 1601,
                    "y": 3202
                },
                {
                    "x": 1601,
                    "y": 3242
                },
                {
                    "x": 910,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='146' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 146,
            "page": 10,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 160,
                    "y": 96
                },
                {
                    "x": 518,
                    "y": 96
                },
                {
                    "x": 518,
                    "y": 155
                },
                {
                    "x": 160,
                    "y": 155
                }
            ],
            "category": "header",
            "html": "<header id='147' style='font-size:22px'>RESEARCH ARTICLE</header>",
            "id": 147,
            "page": 11,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 215
                },
                {
                    "x": 1063,
                    "y": 215
                },
                {
                    "x": 1063,
                    "y": 260
                },
                {
                    "x": 146,
                    "y": 260
                }
            ],
            "category": "caption",
            "html": "<caption id='148' style='font-size:18px'>Extended Data Table 2 I Input features for neural networks</caption>",
            "id": 148,
            "page": 11,
            "text": "Extended Data Table 2 I Input features for neural networks"
        },
        {
            "bounding_box": [
                {
                    "x": 153,
                    "y": 285
                },
                {
                    "x": 1799,
                    "y": 285
                },
                {
                    "x": 1799,
                    "y": 995
                },
                {
                    "x": 153,
                    "y": 995
                }
            ],
            "category": "table",
            "html": "<table id='149' style='font-size:20px'><tr><td>Feature</td><td># of planes</td><td>Description</td></tr><tr><td>Stone colour</td><td>3</td><td>Player stone / opponent stone / empty</td></tr><tr><td>Ones</td><td>1</td><td>A constant plane filled with 1</td></tr><tr><td>Turns since</td><td>8</td><td>How many turns since a move was played</td></tr><tr><td>Liberties</td><td>8</td><td>Number of liberties (empty adjacent points)</td></tr><tr><td>Capture size</td><td>8</td><td>How many opponent stones would be captured</td></tr><tr><td>Self-atari size</td><td>8</td><td>How many of own stones would be captured</td></tr><tr><td>Liberties after move</td><td>8</td><td>Number of liberties after this move is played</td></tr><tr><td>Ladder capture</td><td>1</td><td>Whether a move at this point is a successful ladder capture</td></tr><tr><td>Ladder escape</td><td>1</td><td>Whether a move at this point is a successful ladder escape</td></tr><tr><td>Sensibleness</td><td>1</td><td>Whether a move is legal and does not fill its own eyes</td></tr><tr><td>Zeros</td><td>1</td><td>A constant plane filled with 0</td></tr><tr><td>Player color</td><td>1</td><td>Whether current player is black</td></tr></table>",
            "id": 149,
            "page": 11,
            "text": "Feature # of planes Description  Stone colour 3 Player stone / opponent stone / empty  Ones 1 A constant plane filled with 1  Turns since 8 How many turns since a move was played  Liberties 8 Number of liberties (empty adjacent points)  Capture size 8 How many opponent stones would be captured  Self-atari size 8 How many of own stones would be captured  Liberties after move 8 Number of liberties after this move is played  Ladder capture 1 Whether a move at this point is a successful ladder capture  Ladder escape 1 Whether a move at this point is a successful ladder escape  Sensibleness 1 Whether a move is legal and does not fill its own eyes  Zeros 1 A constant plane filled with 0  Player color 1"
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 1009
                },
                {
                    "x": 1177,
                    "y": 1009
                },
                {
                    "x": 1177,
                    "y": 1046
                },
                {
                    "x": 145,
                    "y": 1046
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:14px'>Feature planes used by the policy network (all but last feature) and value network (all features).</p>",
            "id": 150,
            "page": 11,
            "text": "Feature planes used by the policy network (all but last feature) and value network (all features)."
        },
        {
            "bounding_box": [
                {
                    "x": 878,
                    "y": 3203
                },
                {
                    "x": 1569,
                    "y": 3203
                },
                {
                    "x": 1569,
                    "y": 3242
                },
                {
                    "x": 878,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='151' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 151,
            "page": 11,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1963,
                    "y": 96
                },
                {
                    "x": 2319,
                    "y": 96
                },
                {
                    "x": 2319,
                    "y": 154
                },
                {
                    "x": 1963,
                    "y": 154
                }
            ],
            "category": "header",
            "html": "<header id='152' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 152,
            "page": 12,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 222
                },
                {
                    "x": 1325,
                    "y": 222
                },
                {
                    "x": 1325,
                    "y": 267
                },
                {
                    "x": 170,
                    "y": 267
                }
            ],
            "category": "caption",
            "html": "<caption id='153' style='font-size:18px'>Extended Data Table 3 I Supervised learning results for the policy network</caption>",
            "id": 153,
            "page": 12,
            "text": "Extended Data Table 3 I Supervised learning results for the policy network"
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 287
                },
                {
                    "x": 2093,
                    "y": 287
                },
                {
                    "x": 2093,
                    "y": 1162
                },
                {
                    "x": 169,
                    "y": 1162
                }
            ],
            "category": "table",
            "html": "<br><table id='154' style='font-size:20px'><tr><td colspan=\"4\">Architecture</td><td colspan=\"4\">Evaluation</td></tr><tr><td>Filters</td><td>Symmetries</td><td>Features</td><td>Test accu- racy %</td><td>Train accu- racy %</td><td>Raw net wins %</td><td>AlphaGo wins %</td><td>Forward time (ms)</td></tr><tr><td>128</td><td>1</td><td>48</td><td>54.6</td><td>57.0</td><td>36</td><td>53</td><td>2.8</td></tr><tr><td>192</td><td>1</td><td>48</td><td>55.4</td><td>58.0</td><td>50</td><td>50</td><td>4.8</td></tr><tr><td>256</td><td>1</td><td>48</td><td>55.9</td><td>59.1</td><td>67</td><td>55</td><td>7.1</td></tr><tr><td>256</td><td>2</td><td>48</td><td>56.5</td><td>59.8</td><td>67</td><td>38</td><td>13.9</td></tr><tr><td>256</td><td>4</td><td>48</td><td>56.9</td><td>60.2</td><td>69</td><td>14</td><td>27.6</td></tr><tr><td>256</td><td>8</td><td>48</td><td>57.0</td><td>60.4</td><td>69</td><td>5</td><td>55.3</td></tr><tr><td>192</td><td>1</td><td>4</td><td>47.6</td><td>51.4</td><td>25</td><td>15</td><td>4.8</td></tr><tr><td>192</td><td>1</td><td>12</td><td>54.7</td><td>57.1</td><td>30</td><td>34</td><td>4.8</td></tr><tr><td>192</td><td>1</td><td>20</td><td>54.7</td><td>57.2</td><td>38</td><td>40</td><td>4.8</td></tr><tr><td>192</td><td>8</td><td>4</td><td>49.2</td><td>53.2</td><td>24</td><td>2</td><td>36.8</td></tr><tr><td>192</td><td>8</td><td>12</td><td>55.7</td><td>58.3</td><td>32</td><td>3</td><td>36.8</td></tr><tr><td>192</td><td>8</td><td>20</td><td>55.8</td><td>58.4</td><td>42</td><td>3</td><td>36.8</td></tr></table>",
            "id": 154,
            "page": 12,
            "text": "Architecture Evaluation  Filters Symmetries Features Test accu- racy % Train accu- racy % Raw net wins % AlphaGo wins % Forward time (ms)  128 1 48 54.6 57.0 36 53 2.8  192 1 48 55.4 58.0 50 50 4.8  256 1 48 55.9 59.1 67 55 7.1  256 2 48 56.5 59.8 67 38 13.9  256 4 48 56.9 60.2 69 14 27.6  256 8 48 57.0 60.4 69 5 55.3  192 1 4 47.6 51.4 25 15 4.8  192 1 12 54.7 57.1 30 34 4.8  192 1 20 54.7 57.2 38 40 4.8  192 8 4 49.2 53.2 24 2 36.8  192 8 12 55.7 58.3 32 3 36.8  192 8 20 55.8 58.4 42 3"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 1175
                },
                {
                    "x": 2082,
                    "y": 1175
                },
                {
                    "x": 2082,
                    "y": 1307
                },
                {
                    "x": 170,
                    "y": 1307
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:14px'>The policy network architecture consists of 128, 192 or 256 filters in convolutional layers; an explicit symmetry ensemble over 2, 4 or 8 symmetries; using only the first 4, 12 or<br>20 input feature planes listed in Extended Data Table 1. The results consist of the test and train accuracy on the KGS data set; and the percentage of games won by given policy<br>network against AlphaGo's policy network (highlighted row 2): using the policy networks to select moves directly (raw wins); or using AlphaGo's search to select moves (AlphaGo<br>wins); and finally the computation time for a single evaluation of the policy network.</p>",
            "id": 155,
            "page": 12,
            "text": "The policy network architecture consists of 128, 192 or 256 filters in convolutional layers; an explicit symmetry ensemble over 2, 4 or 8 symmetries; using only the first 4, 12 or 20 input feature planes listed in Extended Data Table 1. The results consist of the test and train accuracy on the KGS data set; and the percentage of games won by given policy network against AlphaGo's policy network (highlighted row 2): using the policy networks to select moves directly (raw wins); or using AlphaGo's search to select moves (AlphaGo wins); and finally the computation time for a single evaluation of the policy network."
        },
        {
            "bounding_box": [
                {
                    "x": 911,
                    "y": 3203
                },
                {
                    "x": 1603,
                    "y": 3203
                },
                {
                    "x": 1603,
                    "y": 3244
                },
                {
                    "x": 911,
                    "y": 3244
                }
            ],
            "category": "footer",
            "html": "<footer id='156' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 156,
            "page": 12,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 158,
                    "y": 97
                },
                {
                    "x": 520,
                    "y": 97
                },
                {
                    "x": 520,
                    "y": 155
                },
                {
                    "x": 158,
                    "y": 155
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:22px'>RESEARCH ARTICLE</p>",
            "id": 157,
            "page": 13,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 216
                },
                {
                    "x": 1150,
                    "y": 216
                },
                {
                    "x": 1150,
                    "y": 261
                },
                {
                    "x": 145,
                    "y": 261
                }
            ],
            "category": "caption",
            "html": "<caption id='158' style='font-size:18px'>Extended Data Table 4 I Input features for rollout and tree policy</caption>",
            "id": 158,
            "page": 13,
            "text": "Extended Data Table 4 I Input features for rollout and tree policy"
        },
        {
            "bounding_box": [
                {
                    "x": 155,
                    "y": 285
                },
                {
                    "x": 1917,
                    "y": 285
                },
                {
                    "x": 1917,
                    "y": 845
                },
                {
                    "x": 155,
                    "y": 845
                }
            ],
            "category": "table",
            "html": "<table id='159' style='font-size:20px'><tr><td>Feature</td><td># of patterns</td><td>Description</td></tr><tr><td>Response</td><td>1</td><td>Whether move matches one or more response pattern features</td></tr><tr><td>Save atari</td><td>1</td><td>Move saves stone(s) from capture</td></tr><tr><td>Neighbour</td><td>8</td><td>Move is 8-connected to previous move</td></tr><tr><td>Nakade</td><td>8192</td><td>Move matches a nakade pattern at captured stone</td></tr><tr><td>Response pattern</td><td>32207</td><td>Move matches 12-point diamond pattern near previous move</td></tr><tr><td>Non-response pattern</td><td>69338</td><td>Move matches 3 x 3 pattern around move</td></tr><tr><td>Self-atari</td><td>1</td><td>Move allows stones to be captured</td></tr><tr><td>Last move distance</td><td>34</td><td>Manhattan distance to previous two moves</td></tr><tr><td>Non-response pattern</td><td>32207</td><td>Move matches 12-point diamond pattern centred around move</td></tr></table>",
            "id": 159,
            "page": 13,
            "text": "Feature # of patterns Description  Response 1 Whether move matches one or more response pattern features  Save atari 1 Move saves stone(s) from capture  Neighbour 8 Move is 8-connected to previous move  Nakade 8192 Move matches a nakade pattern at captured stone  Response pattern 32207 Move matches 12-point diamond pattern near previous move  Non-response pattern 69338 Move matches 3 x 3 pattern around move  Self-atari 1 Move allows stones to be captured  Last move distance 34 Manhattan distance to previous two moves  Non-response pattern 32207"
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 857
                },
                {
                    "x": 1894,
                    "y": 857
                },
                {
                    "x": 1894,
                    "y": 925
                },
                {
                    "x": 147,
                    "y": 925
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:14px'>Features used by the rollout policy (first set) and tree policy (first and second set). Patterns are based on stone colour (black/white/empty) and liberties (1,2,≥3)<br>at each intersection of the pattern.</p>",
            "id": 160,
            "page": 13,
            "text": "Features used by the rollout policy (first set) and tree policy (first and second set). Patterns are based on stone colour (black/white/empty) and liberties (1,2,≥3) at each intersection of the pattern."
        },
        {
            "bounding_box": [
                {
                    "x": 879,
                    "y": 3203
                },
                {
                    "x": 1569,
                    "y": 3203
                },
                {
                    "x": 1569,
                    "y": 3242
                },
                {
                    "x": 879,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='161' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 161,
            "page": 13,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1965,
                    "y": 101
                },
                {
                    "x": 2318,
                    "y": 101
                },
                {
                    "x": 2318,
                    "y": 152
                },
                {
                    "x": 1965,
                    "y": 152
                }
            ],
            "category": "header",
            "html": "<header id='162' style='font-size:20px'>ARTICLE RESEARCH</header>",
            "id": 162,
            "page": 14,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 216
                },
                {
                    "x": 1012,
                    "y": 216
                },
                {
                    "x": 1012,
                    "y": 260
                },
                {
                    "x": 170,
                    "y": 260
                }
            ],
            "category": "caption",
            "html": "<caption id='163' style='font-size:16px'>Extended Data Table 5 I Parameters used by AlphaGo</caption>",
            "id": 163,
            "page": 14,
            "text": "Extended Data Table 5 I Parameters used by AlphaGo"
        },
        {
            "bounding_box": [
                {
                    "x": 175,
                    "y": 282
                },
                {
                    "x": 897,
                    "y": 282
                },
                {
                    "x": 897,
                    "y": 629
                },
                {
                    "x": 175,
                    "y": 629
                }
            ],
            "category": "table",
            "html": "<br><table id='164' style='font-size:18px'><tr><td>Symbol</td><td>Parameter</td><td>Value</td></tr><tr><td>B</td><td>Softmax temperature</td><td>0.67</td></tr><tr><td>入</td><td>Mixing parameter</td><td>0.5</td></tr><tr><td>nvl</td><td>Virtual loss</td><td>3</td></tr><tr><td>nthr</td><td>Expansion threshold</td><td>40</td></tr><tr><td>Cpuct</td><td>Exploration constant</td><td>5</td></tr></table>",
            "id": 164,
            "page": 14,
            "text": "Symbol Parameter Value  B Softmax temperature 0.67  入 Mixing parameter 0.5  nvl Virtual loss 3  nthr Expansion threshold 40  Cpuct Exploration constant"
        },
        {
            "bounding_box": [
                {
                    "x": 909,
                    "y": 3202
                },
                {
                    "x": 1600,
                    "y": 3202
                },
                {
                    "x": 1600,
                    "y": 3242
                },
                {
                    "x": 909,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='165' style='font-size:14px'>ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 165,
            "page": 14,
            "text": "ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 159,
                    "y": 96
                },
                {
                    "x": 518,
                    "y": 96
                },
                {
                    "x": 518,
                    "y": 153
                },
                {
                    "x": 159,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='166' style='font-size:22px'>RESEARCH ARTICLE</header>",
            "id": 166,
            "page": 15,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 216
                },
                {
                    "x": 1403,
                    "y": 216
                },
                {
                    "x": 1403,
                    "y": 262
                },
                {
                    "x": 145,
                    "y": 262
                }
            ],
            "category": "caption",
            "html": "<caption id='167' style='font-size:18px'>Extended Data Table 6 I Results of a tournament between different Go programs</caption>",
            "id": 167,
            "page": 15,
            "text": "Extended Data Table 6 I Results of a tournament between different Go programs"
        },
        {
            "bounding_box": [
                {
                    "x": 150,
                    "y": 279
                },
                {
                    "x": 2078,
                    "y": 279
                },
                {
                    "x": 2078,
                    "y": 916
                },
                {
                    "x": 150,
                    "y": 916
                }
            ],
            "category": "table",
            "html": "<table id='168' style='font-size:20px'><tr><td>Short name</td><td>Computer Player</td><td>Version</td><td>Time settings</td><td>CPUs</td><td>GPUs</td><td>KGS Rank</td><td>Elo</td></tr><tr><td>arvp</td><td>Distributed AlphaGo</td><td>See Methods</td><td>5 seconds</td><td>1202</td><td>176</td><td>-</td><td>3140</td></tr><tr><td>arup</td><td>AlphaGo</td><td>See Methods</td><td>5 seconds</td><td>48</td><td>8</td><td>-</td><td>2890</td></tr><tr><td>CS</td><td>CrazyStone</td><td>2015</td><td>5 seconds</td><td>32</td><td></td><td>6d</td><td>1929</td></tr><tr><td>ZN</td><td>Zen</td><td>5</td><td>5 seconds</td><td>8</td><td></td><td>6d</td><td>1888</td></tr><tr><td>PC</td><td>Pachi</td><td>10.99</td><td>400,000 sims</td><td>16</td><td></td><td>2d</td><td>1298</td></tr><tr><td>FG</td><td>Fuego</td><td>svn1989</td><td>100,000 sims</td><td>16</td><td></td><td>-</td><td>1148</td></tr><tr><td>GG</td><td>GnuGo</td><td>3.8</td><td>level 10</td><td>1</td><td></td><td>5k</td><td>431</td></tr><tr><td>CS4</td><td>CrazyStone</td><td>4 handicap stones</td><td>5 seconds</td><td>32</td><td></td><td></td><td>2526</td></tr><tr><td>ZN4</td><td>Zen</td><td>4 handicap stones</td><td>5 seconds</td><td>8</td><td></td><td>-</td><td>2413</td></tr><tr><td>PC4</td><td>Pachi</td><td>4 handicap stones</td><td>400,000 sims</td><td>16</td><td></td><td>-</td><td>1756</td></tr></table>",
            "id": 168,
            "page": 15,
            "text": "Short name Computer Player Version Time settings CPUs GPUs KGS Rank Elo  arvp Distributed AlphaGo See Methods 5 seconds 1202 176 - 3140  arup AlphaGo See Methods 5 seconds 48 8 - 2890  CS CrazyStone 2015 5 seconds 32  6d 1929  ZN Zen 5 5 seconds 8  6d 1888  PC Pachi 10.99 400,000 sims 16  2d 1298  FG Fuego svn1989 100,000 sims 16  - 1148  GG GnuGo 3.8 level 10 1  5k 431  CS4 CrazyStone 4 handicap stones 5 seconds 32   2526  ZN4 Zen 4 handicap stones 5 seconds 8  - 2413  PC4 Pachi 4 handicap stones 400,000 sims 16  -"
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 931
                },
                {
                    "x": 2049,
                    "y": 931
                },
                {
                    "x": 2049,
                    "y": 1000
                },
                {
                    "x": 145,
                    "y": 1000
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='169' style='font-size:14px'>Each program played with a maximum of 5 s thinking time per move; the games against Fan Hui were conducted using longer time controls, as described in Methods. CN4, ZN4<br>and PC4 were given 4 handicap stones; komi was 7.5 in all games. Elo ratings were computed by BayesElo.</p>",
            "id": 169,
            "page": 15,
            "text": "Each program played with a maximum of 5 s thinking time per move; the games against Fan Hui were conducted using longer time controls, as described in Methods. CN4, ZN4 and PC4 were given 4 handicap stones; komi was 7.5 in all games. Elo ratings were computed by BayesElo."
        },
        {
            "bounding_box": [
                {
                    "x": 877,
                    "y": 3203
                },
                {
                    "x": 1569,
                    "y": 3203
                },
                {
                    "x": 1569,
                    "y": 3242
                },
                {
                    "x": 877,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='170' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 170,
            "page": 15,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1964,
                    "y": 98
                },
                {
                    "x": 2319,
                    "y": 98
                },
                {
                    "x": 2319,
                    "y": 153
                },
                {
                    "x": 1964,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='171' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 171,
            "page": 16,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 217
                },
                {
                    "x": 1522,
                    "y": 217
                },
                {
                    "x": 1522,
                    "y": 260
                },
                {
                    "x": 170,
                    "y": 260
                }
            ],
            "category": "caption",
            "html": "<caption id='172' style='font-size:18px'>Extended Data Table 7 I Results of a tournament between different variants of AlphaGo</caption>",
            "id": 172,
            "page": 16,
            "text": "Extended Data Table 7 I Results of a tournament between different variants of AlphaGo"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 281
                },
                {
                    "x": 1917,
                    "y": 281
                },
                {
                    "x": 1917,
                    "y": 782
                },
                {
                    "x": 170,
                    "y": 782
                }
            ],
            "category": "table",
            "html": "<table id='173' style='font-size:20px'><tr><td>Short name</td><td>Policy network</td><td>Value network</td><td>Rollouts</td><td>Mixing constant</td><td>Policy GPUs</td><td>Value GPUs</td><td>Elo rating</td></tr><tr><td>arup</td><td>Po</td><td>vo</td><td>P�</td><td>入 = 0.5</td><td>2</td><td>6</td><td>2890</td></tr><tr><td>�vp</td><td>Po</td><td>vo</td><td>-</td><td>入 = 0</td><td>2</td><td>6</td><td>2177</td></tr><tr><td>�rp</td><td>Po</td><td>-</td><td>P�</td><td>入 = 1</td><td>8</td><td>0</td><td>2416</td></tr><tr><td>arv</td><td>[pr</td><td>vo</td><td>��</td><td>入 = 0.5</td><td>0</td><td>8</td><td>2077</td></tr><tr><td>av</td><td>PT</td><td>vo</td><td>-</td><td>入 = 0</td><td>0</td><td>8</td><td>1655</td></tr><tr><td>ar</td><td>PT</td><td></td><td>��</td><td>入 = 1</td><td>0</td><td>0</td><td>1457</td></tr><tr><td>ap</td><td>Po</td><td></td><td></td><td></td><td>0</td><td>0</td><td>1517</td></tr></table>",
            "id": 173,
            "page": 16,
            "text": "Short name Policy network Value network Rollouts Mixing constant Policy GPUs Value GPUs Elo rating  arup Po vo P� 入 = 0.5 2 6 2890  �vp Po vo - 入 = 0 2 6 2177  �rp Po - P� 入 = 1 8 0 2416  arv [pr vo �� 入 = 0.5 0 8 2077  av PT vo - 入 = 0 0 8 1655  ar PT  �� 入 = 1 0 0 1457  ap Po    0 0"
        },
        {
            "bounding_box": [
                {
                    "x": 171,
                    "y": 796
                },
                {
                    "x": 1921,
                    "y": 796
                },
                {
                    "x": 1921,
                    "y": 892
                },
                {
                    "x": 171,
                    "y": 892
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='174' style='font-size:14px'>Evaluating positions using rollouts only (�rp, ar), value nets only (�vp, �v), or mixing both (�rvp, �rv); either using the policy network Po(�rvp, �vp, �rp), or no policy<br>network (�rvp, �vp, �rp), that is, instead using the placeholder probabilities from the tree policy PT throughout. Each program used 5 s per move on a single machine<br>with 48 CPUs and 8 GPUs. Elo ratings were computed by BayesElo.</p>",
            "id": 174,
            "page": 16,
            "text": "Evaluating positions using rollouts only (�rp, ar), value nets only (�vp, �v), or mixing both (�rvp, �rv); either using the policy network Po(�rvp, �vp, �rp), or no policy network (�rvp, �vp, �rp), that is, instead using the placeholder probabilities from the tree policy PT throughout. Each program used 5 s per move on a single machine with 48 CPUs and 8 GPUs. Elo ratings were computed by BayesElo."
        },
        {
            "bounding_box": [
                {
                    "x": 910,
                    "y": 3202
                },
                {
                    "x": 1603,
                    "y": 3202
                },
                {
                    "x": 1603,
                    "y": 3244
                },
                {
                    "x": 910,
                    "y": 3244
                }
            ],
            "category": "footer",
            "html": "<footer id='175' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 175,
            "page": 16,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 158,
                    "y": 97
                },
                {
                    "x": 517,
                    "y": 97
                },
                {
                    "x": 517,
                    "y": 154
                },
                {
                    "x": 158,
                    "y": 154
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:22px'>RESEARCH ARTICLE</p>",
            "id": 176,
            "page": 17,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 147,
                    "y": 218
                },
                {
                    "x": 1865,
                    "y": 218
                },
                {
                    "x": 1865,
                    "y": 302
                },
                {
                    "x": 147,
                    "y": 302
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:18px'>Extended Data Table 8 I Results of a tournament between AlphaGo and distributed AlphaGo, testing scalability<br>with hardware</p>",
            "id": 177,
            "page": 17,
            "text": "Extended Data Table 8 I Results of a tournament between AlphaGo and distributed AlphaGo, testing scalability with hardware"
        },
        {
            "bounding_box": [
                {
                    "x": 153,
                    "y": 320
                },
                {
                    "x": 1897,
                    "y": 320
                },
                {
                    "x": 1897,
                    "y": 1133
                },
                {
                    "x": 153,
                    "y": 1133
                }
            ],
            "category": "table",
            "html": "<br><table id='178' style='font-size:20px'><tr><td>AlphaGo</td><td>Search threads</td><td>CPUs</td><td>GPUs</td><td>Elo</td></tr><tr><td>Asynchronous</td><td>1</td><td>48</td><td>8</td><td>2203</td></tr><tr><td>Asynchronous</td><td>2</td><td>48</td><td>8</td><td>2393</td></tr><tr><td>Asynchronous</td><td>4</td><td>48</td><td>8</td><td>2564</td></tr><tr><td>Asynchronous</td><td>8</td><td>48</td><td>8</td><td>2665</td></tr><tr><td>Asynchronous</td><td>16</td><td>48</td><td>8</td><td>2778</td></tr><tr><td>Asynchronous</td><td>32</td><td>48</td><td>8</td><td>2867</td></tr><tr><td>Asynchronous</td><td>40</td><td>48</td><td>8</td><td>2890</td></tr><tr><td>Asynchronous</td><td>40</td><td>48</td><td>1</td><td>2181</td></tr><tr><td>Asynchronous</td><td>40</td><td>48</td><td>2</td><td>2738</td></tr><tr><td>Asynchronous</td><td>40</td><td>48</td><td>4</td><td>2850</td></tr><tr><td>Distributed</td><td>12</td><td>428</td><td>64</td><td>2937</td></tr><tr><td>Distributed</td><td>24</td><td>764</td><td>112</td><td>3079</td></tr><tr><td>Distributed</td><td>40</td><td>1202</td><td>176</td><td>3140</td></tr><tr><td>Distributed</td><td>64</td><td>1920</td><td>280</td><td>3168</td></tr></table>",
            "id": 178,
            "page": 17,
            "text": "AlphaGo Search threads CPUs GPUs Elo  Asynchronous 1 48 8 2203  Asynchronous 2 48 8 2393  Asynchronous 4 48 8 2564  Asynchronous 8 48 8 2665  Asynchronous 16 48 8 2778  Asynchronous 32 48 8 2867  Asynchronous 40 48 8 2890  Asynchronous 40 48 1 2181  Asynchronous 40 48 2 2738  Asynchronous 40 48 4 2850  Distributed 12 428 64 2937  Distributed 24 764 112 3079  Distributed 40 1202 176 3140  Distributed 64 1920 280"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 1149
                },
                {
                    "x": 1333,
                    "y": 1149
                },
                {
                    "x": 1333,
                    "y": 1185
                },
                {
                    "x": 146,
                    "y": 1185
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:14px'>Each program played with a maximum of 2s thinking time per move. Elo ratings were computed by BayesElo.</p>",
            "id": 179,
            "page": 17,
            "text": "Each program played with a maximum of 2s thinking time per move. Elo ratings were computed by BayesElo."
        },
        {
            "bounding_box": [
                {
                    "x": 877,
                    "y": 3204
                },
                {
                    "x": 1571,
                    "y": 3204
                },
                {
                    "x": 1571,
                    "y": 3242
                },
                {
                    "x": 877,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='180' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 180,
            "page": 17,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1961,
                    "y": 96
                },
                {
                    "x": 2323,
                    "y": 96
                },
                {
                    "x": 2323,
                    "y": 153
                },
                {
                    "x": 1961,
                    "y": 153
                }
            ],
            "category": "header",
            "html": "<header id='181' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 181,
            "page": 18,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 219
                },
                {
                    "x": 1392,
                    "y": 219
                },
                {
                    "x": 1392,
                    "y": 264
                },
                {
                    "x": 169,
                    "y": 264
                }
            ],
            "category": "caption",
            "html": "<caption id='182' style='font-size:18px'>Extended Data Table 9 I Cross-table of win rates in per cent between programs</caption>",
            "id": 182,
            "page": 18,
            "text": "Extended Data Table 9 I Cross-table of win rates in per cent between programs"
        },
        {
            "bounding_box": [
                {
                    "x": 160,
                    "y": 274
                },
                {
                    "x": 2012,
                    "y": 274
                },
                {
                    "x": 2012,
                    "y": 1935
                },
                {
                    "x": 160,
                    "y": 1935
                }
            ],
            "category": "table",
            "html": "<table id='183' style='font-size:20px'><tr><td></td><td>arvp</td><td>�vp</td><td>arp</td><td>arv</td><td>ar</td><td>av</td><td>ap</td></tr><tr><td>�rvp</td><td></td><td>1 [0;5]</td><td>5 [4;7]</td><td>0 [0;4]</td><td>0 [0;8]</td><td>0 [0;19]</td><td>0 [0; 19]</td></tr><tr><td>avp</td><td>99 [95; 100]</td><td></td><td>61 [52;69]</td><td>35 [25;48]</td><td>6 [1;27]</td><td>0 [0;22]</td><td>1 [0;6]</td></tr><tr><td>arp</td><td>95 [93;96]</td><td>39 [31;48]</td><td>-</td><td>13 [7;23]</td><td>0 [0;9]</td><td>0 [0;22]</td><td>4 [1; 21]</td></tr><tr><td>arv</td><td>100 [96; 100]</td><td>65 [52;75]</td><td>87 [77;93]</td><td></td><td>0 [0;18]</td><td>29 [8;64]</td><td>48 [33; 65]</td></tr><tr><td>ar</td><td>100 [92; 100]</td><td>94 [73;99]</td><td>100 [91; 100]</td><td>100 [82; 100]</td><td>-</td><td>78 [45;94]</td><td>78 [71; 84]</td></tr><tr><td>av</td><td>100 [81; 100]</td><td>100 [78; 100]</td><td>100 [78; 100]</td><td>71 [36;92]</td><td>22 [6;55]</td><td></td><td>30 [16;48]</td></tr><tr><td>ap</td><td>100 [81; 100]</td><td>99 [94; 100]</td><td>96 [79;99]</td><td>52 [35;67]</td><td>22 [16;29]</td><td>70 [52;84]</td><td>、</td></tr><tr><td>CS</td><td>100 [97; 100]</td><td>74 [66;81]</td><td>98 [94;99]</td><td>80 [70;87]</td><td>5 [3;7]</td><td>36 [16;61]</td><td>8 [5; 14]</td></tr><tr><td>ZN</td><td>99 [93; 100]</td><td>84 [67;93]</td><td>98 [93;99]</td><td>92 [67;99]</td><td>6 [2;19]</td><td>40 [12;77]</td><td>100 [65; 100]</td></tr><tr><td>PC</td><td>100 [98; 100]</td><td>99 [95; 100]</td><td>100 [98; 100]</td><td>98 [89; 100]</td><td>78 [73;81]</td><td>87 [68;95]</td><td>55 [47; 62]</td></tr><tr><td>FG</td><td>100 [97; 100]</td><td>99 [93; 100]</td><td>100 [96; 100]</td><td>100 [91; 100]</td><td>78 [73;83]</td><td>100 [65; 100]</td><td>65 [55; 73]</td></tr><tr><td>GG</td><td>100 [44; 100]</td><td>100 [34; 100]</td><td>100 [68; 100]</td><td>100 [57; 100]</td><td>99 [97; 100]</td><td>67 [21;94]</td><td>99 [95; 100]</td></tr><tr><td>CS4</td><td>77 [69;84]</td><td>12 [8; 18]</td><td>53 [44; 61]</td><td>15 [8;24]</td><td>0 [0;3]</td><td>0 [0;30]</td><td>0 [0;8]</td></tr><tr><td>ZN4</td><td>86 [77; 92]</td><td>25 [16;38]</td><td>67 [56; 76]</td><td>14 [7;27]</td><td>0 [0;12]</td><td>0 [0;43]</td><td>-</td></tr><tr><td>PC4</td><td>99 [97; 100]</td><td>82 [75;88]</td><td>98 [95; 99]</td><td>89 [79; 95]</td><td>32 [26; 39]</td><td>13 [3;36]</td><td>35 [25;46]</td></tr></table>",
            "id": 183,
            "page": 18,
            "text": "arvp �vp arp arv ar av ap  �rvp  1 [0;5] 5 [4;7] 0 [0;4] 0 [0;8] 0 [0;19] 0 [0; 19]  avp 99 [95; 100]  61 [52;69] 35 [25;48] 6 [1;27] 0 [0;22] 1 [0;6]  arp 95 [93;96] 39 [31;48] - 13 [7;23] 0 [0;9] 0 [0;22] 4 [1; 21]  arv 100 [96; 100] 65 [52;75] 87 [77;93]  0 [0;18] 29 [8;64] 48 [33; 65]  ar 100 [92; 100] 94 [73;99] 100 [91; 100] 100 [82; 100] - 78 [45;94] 78 [71; 84]  av 100 [81; 100] 100 [78; 100] 100 [78; 100] 71 [36;92] 22 [6;55]  30 [16;48]  ap 100 [81; 100] 99 [94; 100] 96 [79;99] 52 [35;67] 22 [16;29] 70 [52;84] 、  CS 100 [97; 100] 74 [66;81] 98 [94;99] 80 [70;87] 5 [3;7] 36 [16;61] 8 [5; 14]  ZN 99 [93; 100] 84 [67;93] 98 [93;99] 92 [67;99] 6 [2;19] 40 [12;77] 100 [65; 100]  PC 100 [98; 100] 99 [95; 100] 100 [98; 100] 98 [89; 100] 78 [73;81] 87 [68;95] 55 [47; 62]  FG 100 [97; 100] 99 [93; 100] 100 [96; 100] 100 [91; 100] 78 [73;83] 100 [65; 100] 65 [55; 73]  GG 100 [44; 100] 100 [34; 100] 100 [68; 100] 100 [57; 100] 99 [97; 100] 67 [21;94] 99 [95; 100]  CS4 77 [69;84] 12 [8; 18] 53 [44; 61] 15 [8;24] 0 [0;3] 0 [0;30] 0 [0;8]  ZN4 86 [77; 92] 25 [16;38] 67 [56; 76] 14 [7;27] 0 [0;12] 0 [0;43]  PC4 99 [97; 100] 82 [75;88] 98 [95; 99] 89 [79; 95] 32 [26; 39] 13 [3;36]"
        },
        {
            "bounding_box": [
                {
                    "x": 166,
                    "y": 1939
                },
                {
                    "x": 1964,
                    "y": 1939
                },
                {
                    "x": 1964,
                    "y": 2009
                },
                {
                    "x": 166,
                    "y": 2009
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='184' style='font-size:14px'>95% Agresti-Coull confidence intervals in grey. Each program played with a maximum of 5s thinking time per move. CN4, ZN4 and PC4 were given 4 handicap stones;<br>komi was 7.5 in all games. Distributed AlphaGo scored 77% [70;82] against �rvp and 100% against all other programs (no handicap games were played).</p>",
            "id": 184,
            "page": 18,
            "text": "95% Agresti-Coull confidence intervals in grey. Each program played with a maximum of 5s thinking time per move. CN4, ZN4 and PC4 were given 4 handicap stones; komi was 7.5 in all games. Distributed AlphaGo scored 77% [70;82] against �rvp and 100% against all other programs (no handicap games were played)."
        },
        {
            "bounding_box": [
                {
                    "x": 908,
                    "y": 3202
                },
                {
                    "x": 1604,
                    "y": 3202
                },
                {
                    "x": 1604,
                    "y": 3243
                },
                {
                    "x": 908,
                    "y": 3243
                }
            ],
            "category": "footer",
            "html": "<footer id='185' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 185,
            "page": 18,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 155,
                    "y": 98
                },
                {
                    "x": 519,
                    "y": 98
                },
                {
                    "x": 519,
                    "y": 153
                },
                {
                    "x": 155,
                    "y": 153
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:22px'>RESEARCH ARTICLE</p>",
            "id": 186,
            "page": 19,
            "text": "RESEARCH ARTICLE"
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 216
                },
                {
                    "x": 1987,
                    "y": 216
                },
                {
                    "x": 1987,
                    "y": 261
                },
                {
                    "x": 145,
                    "y": 261
                }
            ],
            "category": "caption",
            "html": "<caption id='187' style='font-size:18px'>Extended Data Table 10 I Cross-table of win rates in per cent between programs in the single-machine scalability study</caption>",
            "id": 187,
            "page": 19,
            "text": "Extended Data Table 10 I Cross-table of win rates in per cent between programs in the single-machine scalability study"
        },
        {
            "bounding_box": [
                {
                    "x": 142,
                    "y": 278
                },
                {
                    "x": 2145,
                    "y": 278
                },
                {
                    "x": 2145,
                    "y": 1526
                },
                {
                    "x": 142,
                    "y": 1526
                }
            ],
            "category": "table",
            "html": "<table id='188' style='font-size:20px'><tr><td rowspan=\"2\" colspan=\"2\">Threads GPU</td><td>1</td><td>2</td><td>4</td><td>8</td><td>16</td><td>32</td><td>40</td><td>40</td><td>40</td><td>40</td></tr><tr><td>8</td><td>8</td><td>8</td><td>8</td><td>8</td><td>8</td><td>8</td><td>4</td><td>2</td><td>1</td></tr><tr><td>1</td><td>8</td><td>-</td><td>70 [61;78]</td><td>90 [84;94]</td><td>94 [83;98]</td><td>86 [72;94]</td><td>98 [91;100]</td><td>98 [92;99]</td><td>100 [76;100]</td><td>96 [91;98]</td><td>38 [25;52]</td></tr><tr><td>2</td><td>8</td><td>30 [22;39]</td><td>-</td><td>72 [61;81]</td><td>81 [71;88]</td><td>86 [76;93]</td><td>92 [83;97]</td><td>93 [86;96]</td><td>83 [69;91]</td><td>84 [75;90]</td><td>26 [17;38]</td></tr><tr><td>4</td><td>8</td><td>10 [6;16]</td><td>28 [19;39]</td><td>-</td><td>62 [53;70]</td><td>71 [61;80]</td><td>82 [71;89]</td><td>84 [74;90]</td><td>81 [69;89]</td><td>78 [63;88]</td><td>18 [10;28]</td></tr><tr><td>8</td><td>8</td><td>6 [2;17]</td><td>19 [12;29]</td><td>38 [30;47]</td><td>-</td><td>61 [51;71]</td><td>65 [51;76]</td><td>73 [62;82]</td><td>74 [59;85]</td><td>64 [55;73]</td><td>12 [3;34]</td></tr><tr><td>16</td><td>8</td><td>14 [6;28]</td><td>14 [7;24]</td><td>29 [20;39]</td><td>39 [29;49]</td><td>-</td><td>52 [41;63]</td><td>61 [50;71]</td><td>52 [41;64]</td><td>41 [32;51]</td><td>5 [1;25]</td></tr><tr><td>32</td><td>8</td><td>2 [0;9]</td><td>8 [3;17]</td><td>18 [11;29]</td><td>35 [24;49]</td><td>48 [37;59]</td><td>-</td><td>52 [42;63]</td><td>44 [32;57]</td><td>26 [17;36]</td><td>0 [0;30]</td></tr><tr><td>40</td><td>8</td><td>2 [1;8]</td><td>8 [4;14]</td><td>16 [10;26]</td><td>27 [18;38]</td><td>39 [29;50]</td><td>48 [37;58]</td><td>-</td><td>43 [30;56]</td><td>41 [26;58]</td><td>4 [1;18]</td></tr><tr><td>40</td><td>4</td><td>0 [0;24]</td><td>17 [9;31]</td><td>19 [11;31]</td><td>26 [15;41]</td><td>48 [36;59]</td><td>56 [43;68]</td><td>57 [44;70]</td><td>-</td><td>29 [18;41]</td><td>2 [0;11]</td></tr><tr><td>40</td><td>2</td><td>4 [2;9]</td><td>16 [10;25]</td><td>22 [12;37]</td><td>36 [27;45]</td><td>59 [49;68]</td><td>74 [64;83]</td><td>59 [42;74]</td><td>71 [59;82]</td><td>-</td><td>5 [1;17]</td></tr><tr><td>40</td><td>1</td><td>62 [48;75]</td><td>74 [62;83]</td><td>82 [72;90]</td><td>88 [66;97]</td><td>95 [75;99]</td><td>100 [70;100]</td><td>96 [82;99]</td><td>98 [89;100]</td><td>95 [83;99]</td><td>-</td></tr></table>",
            "id": 188,
            "page": 19,
            "text": "Threads GPU 1 2 4 8 16 32 40 40 40 40  8 8 8 8 8 8 8 4 2 1  1 8 - 70 [61;78] 90 [84;94] 94 [83;98] 86 [72;94] 98 [91;100] 98 [92;99] 100 [76;100] 96 [91;98] 38 [25;52]  2 8 30 [22;39] - 72 [61;81] 81 [71;88] 86 [76;93] 92 [83;97] 93 [86;96] 83 [69;91] 84 [75;90] 26 [17;38]  4 8 10 [6;16] 28 [19;39] - 62 [53;70] 71 [61;80] 82 [71;89] 84 [74;90] 81 [69;89] 78 [63;88] 18 [10;28]  8 8 6 [2;17] 19 [12;29] 38 [30;47] - 61 [51;71] 65 [51;76] 73 [62;82] 74 [59;85] 64 [55;73] 12 [3;34]  16 8 14 [6;28] 14 [7;24] 29 [20;39] 39 [29;49] - 52 [41;63] 61 [50;71] 52 [41;64] 41 [32;51] 5 [1;25]  32 8 2 [0;9] 8 [3;17] 18 [11;29] 35 [24;49] 48 [37;59] - 52 [42;63] 44 [32;57] 26 [17;36] 0 [0;30]  40 8 2 [1;8] 8 [4;14] 16 [10;26] 27 [18;38] 39 [29;50] 48 [37;58] - 43 [30;56] 41 [26;58] 4 [1;18]  40 4 0 [0;24] 17 [9;31] 19 [11;31] 26 [15;41] 48 [36;59] 56 [43;68] 57 [44;70] - 29 [18;41] 2 [0;11]  40 2 4 [2;9] 16 [10;25] 22 [12;37] 36 [27;45] 59 [49;68] 74 [64;83] 59 [42;74] 71 [59;82] - 5 [1;17]  40 1 62 [48;75] 74 [62;83] 82 [72;90] 88 [66;97] 95 [75;99] 100 [70;100] 96 [82;99] 98 [89;100] 95 [83;99]"
        },
        {
            "bounding_box": [
                {
                    "x": 144,
                    "y": 1535
                },
                {
                    "x": 1385,
                    "y": 1535
                },
                {
                    "x": 1385,
                    "y": 1572
                },
                {
                    "x": 144,
                    "y": 1572
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='189' style='font-size:14px'>95% Agresti-Coull confidence intervals in grey. Each program played with 2s per move; komi was 7.5 in all games.</p>",
            "id": 189,
            "page": 19,
            "text": "95% Agresti-Coull confidence intervals in grey. Each program played with 2s per move; komi was 7.5 in all games."
        },
        {
            "bounding_box": [
                {
                    "x": 877,
                    "y": 3202
                },
                {
                    "x": 1570,
                    "y": 3202
                },
                {
                    "x": 1570,
                    "y": 3242
                },
                {
                    "x": 877,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='190' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 190,
            "page": 19,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        },
        {
            "bounding_box": [
                {
                    "x": 1964,
                    "y": 98
                },
                {
                    "x": 2320,
                    "y": 98
                },
                {
                    "x": 2320,
                    "y": 152
                },
                {
                    "x": 1964,
                    "y": 152
                }
            ],
            "category": "header",
            "html": "<header id='191' style='font-size:22px'>ARTICLE RESEARCH</header>",
            "id": 191,
            "page": 20,
            "text": "ARTICLE RESEARCH"
        },
        {
            "bounding_box": [
                {
                    "x": 169,
                    "y": 216
                },
                {
                    "x": 1413,
                    "y": 216
                },
                {
                    "x": 1413,
                    "y": 300
                },
                {
                    "x": 169,
                    "y": 300
                }
            ],
            "category": "caption",
            "html": "<caption id='192' style='font-size:18px'>Extended Data Table 11 T Cross-table of win rates in per cent between programs<br>in the distributed scalability study</caption>",
            "id": 192,
            "page": 20,
            "text": "Extended Data Table 11 T Cross-table of win rates in per cent between programs in the distributed scalability study"
        },
        {
            "bounding_box": [
                {
                    "x": 173,
                    "y": 316
                },
                {
                    "x": 1409,
                    "y": 316
                },
                {
                    "x": 1409,
                    "y": 1162
                },
                {
                    "x": 173,
                    "y": 1162
                }
            ],
            "category": "table",
            "html": "<table id='193' style='font-size:20px'><tr><td rowspan=\"3\" colspan=\"3\">Threads GPU</td><td>40</td><td>12</td><td>24</td><td>40</td><td>64</td></tr><tr><td>8</td><td>64</td><td>112</td><td>176</td><td>280</td></tr><tr><td>CPU 48</td><td>428</td><td>764</td><td>1202</td><td>1920</td></tr><tr><td>40</td><td>8</td><td>48</td><td>-</td><td>52 [43; 61]</td><td>68 [59; 76]</td><td>77 [70; 82]</td><td>81 [65; 91]</td></tr><tr><td>12</td><td>64</td><td>428</td><td>48 [39; 57]</td><td>-</td><td>64 [54; 73]</td><td>62 [41; 79]</td><td>83 [55; 95]</td></tr><tr><td>24</td><td>112</td><td>764</td><td>32 [24; 41]</td><td>36 [27; 46]</td><td>-</td><td>36 [20; 57]</td><td>60 [51; 69]</td></tr><tr><td>40</td><td>176</td><td>1202</td><td>23 [18; 30]</td><td>38 [21; 59]</td><td>64 [43; 80]</td><td>-</td><td>53 [39; 67]</td></tr><tr><td>64</td><td>280</td><td>1920</td><td>19 [9; 35]</td><td>17 [5;45]</td><td>40 [31; 49]</td><td>47 [33; 61]</td><td>-</td></tr></table>",
            "id": 193,
            "page": 20,
            "text": "Threads GPU 40 12 24 40 64  8 64 112 176 280  CPU 48 428 764 1202 1920  40 8 48 - 52 [43; 61] 68 [59; 76] 77 [70; 82] 81 [65; 91]  12 64 428 48 [39; 57] - 64 [54; 73] 62 [41; 79] 83 [55; 95]  24 112 764 32 [24; 41] 36 [27; 46] - 36 [20; 57] 60 [51; 69]  40 176 1202 23 [18; 30] 38 [21; 59] 64 [43; 80] - 53 [39; 67]  64 280 1920 19 [9; 35] 17 [5;45] 40 [31; 49] 47 [33; 61]"
        },
        {
            "bounding_box": [
                {
                    "x": 170,
                    "y": 1177
                },
                {
                    "x": 1407,
                    "y": 1177
                },
                {
                    "x": 1407,
                    "y": 1215
                },
                {
                    "x": 170,
                    "y": 1215
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='194' style='font-size:14px'>95% Agresti-Coull confidence intervals in grey. Each program played with 2s per move; komi was 7.5 in all games.</p>",
            "id": 194,
            "page": 20,
            "text": "95% Agresti-Coull confidence intervals in grey. Each program played with 2s per move; komi was 7.5 in all games."
        },
        {
            "bounding_box": [
                {
                    "x": 910,
                    "y": 3203
                },
                {
                    "x": 1603,
                    "y": 3203
                },
                {
                    "x": 1603,
                    "y": 3242
                },
                {
                    "x": 910,
                    "y": 3242
                }
            ],
            "category": "footer",
            "html": "<footer id='195' style='font-size:16px'>Ⓒ 2016 Macmillan Publishers Limited. All rights reserved</footer>",
            "id": 195,
            "page": 20,
            "text": "Ⓒ 2016 Macmillan Publishers Limited. All rights reserved"
        }
    ]
}