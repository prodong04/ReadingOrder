{
    "id": "62ae21cc-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/1904.09925v5.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 636,
                    "y": 435
                },
                {
                    "x": 1841,
                    "y": 435
                },
                {
                    "x": 1841,
                    "y": 504
                },
                {
                    "x": 636,
                    "y": 504
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Attention Augmented Convolutional Networks</p>",
            "id": 0,
            "page": 1,
            "text": "Attention Augmented Convolutional Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 464,
                    "y": 601
                },
                {
                    "x": 2023,
                    "y": 601
                },
                {
                    "x": 2023,
                    "y": 780
                },
                {
                    "x": 464,
                    "y": 780
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Irwan Bello Barret Zoph Ashish Vaswani Jonathon Shlens<br>Quoc V. Le<br>Google Brain</p>",
            "id": 1,
            "page": 1,
            "text": "Irwan Bello Barret Zoph Ashish Vaswani Jonathon Shlens\nQuoc V. Le\nGoogle Brain"
        },
        {
            "bounding_box": [
                {
                    "x": 679,
                    "y": 784
                },
                {
                    "x": 1805,
                    "y": 784
                },
                {
                    "x": 1805,
                    "y": 834
                },
                {
                    "x": 679,
                    "y": 834
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:16px'>{ibello, barretzoph, avaswani , shlens , qv1}@google · com</p>",
            "id": 2,
            "page": 1,
            "text": "{ibello, barretzoph, avaswani , shlens , qv1}@google · com"
        },
        {
            "bounding_box": [
                {
                    "x": 601,
                    "y": 947
                },
                {
                    "x": 799,
                    "y": 947
                },
                {
                    "x": 799,
                    "y": 1003
                },
                {
                    "x": 601,
                    "y": 1003
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 1047
                },
                {
                    "x": 1200,
                    "y": 1047
                },
                {
                    "x": 1200,
                    "y": 2551
                },
                {
                    "x": 197,
                    "y": 2551
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>Convolutional networks have been the paradigm of<br>choice in many computer vision applications. The convolu-<br>tion operation however has a significant weakness in that it<br>only operates on a local neighborhood, thus missing global<br>information. Self-attention, on the other hand, has emerged<br>as a recent advance to capture long range interactions, but<br>has mostly been applied to sequence modeling and gener-<br>ative modeling tasks. In this paper, we consider the use of<br>self-attentionfor discriminative visual tasks as an alterna-<br>tive to convolutions. We introduce a novel two-dimensional<br>relative self-attention mechanism that proves competitive<br>in replacing convolutions as a stand-alone computational<br>primitive for image classification. We find in control exper-<br>iments that the best results are obtained when combining<br>both convolutions and self-attention. We therefore propose<br>to augment convolutional operators with this self-attention<br>mechanism by concatenating convolutional feature maps<br>with a set offeature maps produced via self-attention. Ex-<br>tensive experiments show that Attention Augmentation leads<br>to consistent improvements in image classification on Im-<br>ageNet and object detection on COCO across many dif-<br>ferent models and scales, including ResNets and a state-<br>of-the art mobile constrained network, while keeping the<br>number of parameters similar. In particular, our method<br>achieves a 1.3% top-1 accuracy improvement on ImageNet<br>classification over a ResNet50 baseline and outperforms<br>other attention mechanisms for images such as Squeeze-<br>and-Excitation [17]. It also achieves an improvement of<br>1.4 mAP in COCO Object Detection on top of a RetinaNet<br>baseline.</p>",
            "id": 4,
            "page": 1,
            "text": "Convolutional networks have been the paradigm of\nchoice in many computer vision applications. The convolu-\ntion operation however has a significant weakness in that it\nonly operates on a local neighborhood, thus missing global\ninformation. Self-attention, on the other hand, has emerged\nas a recent advance to capture long range interactions, but\nhas mostly been applied to sequence modeling and gener-\native modeling tasks. In this paper, we consider the use of\nself-attentionfor discriminative visual tasks as an alterna-\ntive to convolutions. We introduce a novel two-dimensional\nrelative self-attention mechanism that proves competitive\nin replacing convolutions as a stand-alone computational\nprimitive for image classification. We find in control exper-\niments that the best results are obtained when combining\nboth convolutions and self-attention. We therefore propose\nto augment convolutional operators with this self-attention\nmechanism by concatenating convolutional feature maps\nwith a set offeature maps produced via self-attention. Ex-\ntensive experiments show that Attention Augmentation leads\nto consistent improvements in image classification on Im-\nageNet and object detection on COCO across many dif-\nferent models and scales, including ResNets and a state-\nof-the art mobile constrained network, while keeping the\nnumber of parameters similar. In particular, our method\nachieves a 1.3% top-1 accuracy improvement on ImageNet\nclassification over a ResNet50 baseline and outperforms\nother attention mechanisms for images such as Squeeze-\nand-Excitation [17]. It also achieves an improvement of\n1.4 mAP in COCO Object Detection on top of a RetinaNet\nbaseline."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2641
                },
                {
                    "x": 532,
                    "y": 2641
                },
                {
                    "x": 532,
                    "y": 2695
                },
                {
                    "x": 203,
                    "y": 2695
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2726
                },
                {
                    "x": 1201,
                    "y": 2726
                },
                {
                    "x": 1201,
                    "y": 2978
                },
                {
                    "x": 201,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Convolutional Neural Networks have enjoyed tremen-<br>dous success in many computer vision applications, espe-<br>cially in image classification [24, 23]. The design of the<br>convolutional layer imposes 1) locality via a limited recep-<br>tive field and 2) translation equivariance via weight sharing.</p>",
            "id": 6,
            "page": 1,
            "text": "Convolutional Neural Networks have enjoyed tremen-\ndous success in many computer vision applications, espe-\ncially in image classification [24, 23]. The design of the\nconvolutional layer imposes 1) locality via a limited recep-\ntive field and 2) translation equivariance via weight sharing."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 968
                },
                {
                    "x": 2233,
                    "y": 968
                },
                {
                    "x": 2233,
                    "y": 1710
                },
                {
                    "x": 1324,
                    "y": 1710
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='7' style='font-size:14px' alt=\"81\n80\n79.1%\n79 78.7% 78.9%\naccuracy 78.4%\n78 77.7% 78.4%\n77.9%\n77.5%\n77\n76 76.4%\ntop-1 75 74.7%\n74 74.3% ResNet\nSE-ResNet\n73.6%\n73\nAA-ResNet (ours)\n72\n20 30 40 50 60 70\nnumber of parameters (millions)\" data-coord=\"top-left:(1324,968); bottom-right:(2233,1710)\" /></figure>",
            "id": 7,
            "page": 1,
            "text": "81\n80\n79.1%\n79 78.7% 78.9%\naccuracy 78.4%\n78 77.7% 78.4%\n77.9%\n77.5%\n77\n76 76.4%\ntop-1 75 74.7%\n74 74.3% ResNet\nSE-ResNet\n73.6%\n73\nAA-ResNet (ours)\n72\n20 30 40 50 60 70\nnumber of parameters (millions)"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1745
                },
                {
                    "x": 2280,
                    "y": 1745
                },
                {
                    "x": 2280,
                    "y": 2024
                },
                {
                    "x": 1278,
                    "y": 2024
                }
            ],
            "category": "caption",
            "html": "<caption id='8' style='font-size:16px'>Figure 1. Attention Augmentation systematically improves im-<br>age classification across a large variety of networks of different<br>scales. ImageNet classification accuracy [9] versus the number of<br>parameters for baseline models (ResNet) [14], models augmented<br>with channel-wise attention (SE-ResNet) [17] and our proposed<br>architecture (AA-ResNet).</caption>",
            "id": 8,
            "page": 1,
            "text": "Figure 1. Attention Augmentation systematically improves im-\nage classification across a large variety of networks of different\nscales. ImageNet classification accuracy [9] versus the number of\nparameters for baseline models (ResNet) [14], models augmented\nwith channel-wise attention (SE-ResNet) [17] and our proposed\narchitecture (AA-ResNet)."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2118
                },
                {
                    "x": 2279,
                    "y": 2118
                },
                {
                    "x": 2279,
                    "y": 2371
                },
                {
                    "x": 1278,
                    "y": 2371
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:16px'>Both these properties prove to be crucial inductive biases<br>when designing models that operate over images. However,<br>the local nature of the convolutional kernel prevents it from<br>capturing global contexts in an image, often necessary for<br>better recognition of objects in images [33].</p>",
            "id": 9,
            "page": 1,
            "text": "Both these properties prove to be crucial inductive biases\nwhen designing models that operate over images. However,\nthe local nature of the convolutional kernel prevents it from\ncapturing global contexts in an image, often necessary for\nbetter recognition of objects in images [33]."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2378
                },
                {
                    "x": 2278,
                    "y": 2378
                },
                {
                    "x": 2278,
                    "y": 2979
                },
                {
                    "x": 1276,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>Self-attention [43], on the other hand, has emerged as a<br>recent advance to capture long range interactions, but has<br>mostly been applied to sequence modeling and generative<br>modeling tasks. The key idea behind self-attention is to<br>produce a weighted average of values computed from hid-<br>den units. Unlike the pooling or the convolutional operator,<br>the weights used in the weighted average operation are pro-<br>duced dynamically via a similarity function between hid-<br>den units. As a result, the interaction between input signals<br>depends on the signals themselves rather than being prede-<br>termined by their relative location like in convolutions. In<br>particular, this allows self-attention to capture long range</p>",
            "id": 10,
            "page": 1,
            "text": "Self-attention [43], on the other hand, has emerged as a\nrecent advance to capture long range interactions, but has\nmostly been applied to sequence modeling and generative\nmodeling tasks. The key idea behind self-attention is to\nproduce a weighted average of values computed from hid-\nden units. Unlike the pooling or the convolutional operator,\nthe weights used in the weighted average operation are pro-\nduced dynamically via a similarity function between hid-\nden units. As a result, the interaction between input signals\ndepends on the signals themselves rather than being prede-\ntermined by their relative location like in convolutions. In\nparticular, this allows self-attention to capture long range"
        },
        {
            "bounding_box": [
                {
                    "x": 61,
                    "y": 899
                },
                {
                    "x": 149,
                    "y": 899
                },
                {
                    "x": 149,
                    "y": 2310
                },
                {
                    "x": 61,
                    "y": 2310
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2020<br>Sep<br>9<br>[cs.CV]<br>arXiv:1904.09925v5</footer>",
            "id": 11,
            "page": 1,
            "text": "2020\nSep\n9\n[cs.CV]\narXiv:1904.09925v5"
        },
        {
            "bounding_box": [
                {
                    "x": 301,
                    "y": 320
                },
                {
                    "x": 2189,
                    "y": 320
                },
                {
                    "x": 2189,
                    "y": 940
                },
                {
                    "x": 301,
                    "y": 940
                }
            ],
            "category": "figure",
            "html": "<figure><img id='12' style='font-size:14px' alt=\"Input Attention maps Weighted average of the values Output\nW\nH Head, Head2\nNh= 2\nValues\nStandard convolution\" data-coord=\"top-left:(301,320); bottom-right:(2189,940)\" /></figure>",
            "id": 12,
            "page": 2,
            "text": "Input Attention maps Weighted average of the values Output\nW\nH Head, Head2\nNh= 2\nValues\nStandard convolution"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1016
                },
                {
                    "x": 2279,
                    "y": 1016
                },
                {
                    "x": 2279,
                    "y": 1205
                },
                {
                    "x": 199,
                    "y": 1205
                }
            ],
            "category": "caption",
            "html": "<caption id='13' style='font-size:14px'>Figure 2. Attention-augmented convolution: For each spatial location (h, w), Nh attention maps over the image are computed from<br>queries and keys. These attention maps are used to compute Nh weighted averages of the values V. The results are then concatenated,<br>reshaped to match the original volume's spatial dimensions and mixed with a pointwise convolution. Multi-head attention is applied in<br>parallel to a standard convolution operation and the outputs are concatenated.</caption>",
            "id": 13,
            "page": 2,
            "text": "Figure 2. Attention-augmented convolution: For each spatial location (h, w), Nh attention maps over the image are computed from\nqueries and keys. These attention maps are used to compute Nh weighted averages of the values V. The results are then concatenated,\nreshaped to match the original volume's spatial dimensions and mixed with a pointwise convolution. Multi-head attention is applied in\nparallel to a standard convolution operation and the outputs are concatenated."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1289
                },
                {
                    "x": 1169,
                    "y": 1289
                },
                {
                    "x": 1169,
                    "y": 1333
                },
                {
                    "x": 203,
                    "y": 1333
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:18px'>interactions without increasing the number of parameters.</p>",
            "id": 14,
            "page": 2,
            "text": "interactions without increasing the number of parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1359
                },
                {
                    "x": 1199,
                    "y": 1359
                },
                {
                    "x": 1199,
                    "y": 2106
                },
                {
                    "x": 200,
                    "y": 2106
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:16px'>In this paper, we consider the use of self-attention for<br>discriminative visual tasks as an alternative to convolu-<br>tions. We develop a novel two-dimensional relative self-<br>attention mechanism [37] that maintains translation equiv-<br>ariance while being infused with relative position informa-<br>tion, making it well suited for images. Our self-attention<br>formulation proves competitive for replacing convolutions<br>entirely, however we find in control experiments that the<br>best results are obtained when combining both. We there-<br>fore do not completely abandon the idea of convolutions,<br>but instead propose to augment convolutions with this self-<br>attention mechanism. This is achieved by concatenating<br>convolutional feature maps, which enforce locality, to self-<br>attentional feature maps capable of modeling longer range<br>dependencies (see Figure 2).</p>",
            "id": 15,
            "page": 2,
            "text": "In this paper, we consider the use of self-attention for\ndiscriminative visual tasks as an alternative to convolu-\ntions. We develop a novel two-dimensional relative self-\nattention mechanism [37] that maintains translation equiv-\nariance while being infused with relative position informa-\ntion, making it well suited for images. Our self-attention\nformulation proves competitive for replacing convolutions\nentirely, however we find in control experiments that the\nbest results are obtained when combining both. We there-\nfore do not completely abandon the idea of convolutions,\nbut instead propose to augment convolutions with this self-\nattention mechanism. This is achieved by concatenating\nconvolutional feature maps, which enforce locality, to self-\nattentional feature maps capable of modeling longer range\ndependencies (see Figure 2)."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2127
                },
                {
                    "x": 1199,
                    "y": 2127
                },
                {
                    "x": 1199,
                    "y": 2978
                },
                {
                    "x": 199,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>We test our method on the CIFAR-100 and ImageNet<br>classification [22, 9] and the COCO object detection [27]<br>tasks, across a wide range of architectures at different com-<br>putational budgets, including a state-of-the art resource<br>constrained architecture [42]. Attention Augmentation<br>yields systematic improvements with minimal additional<br>computational burden and notably outperforms the popu-<br>lar Squeeze-and-Excitation [17] channelwise attention ap-<br>proach in all experiments. In particular, Attention Augmen-<br>tation achieves a 1.3% top-1 accuracy ImageNet on top of<br>a ResNet50 baseline and 1.4 mAP increase in COCO ob-<br>ject detection on top of a RetinaNet baseline. Suprisingly,<br>experiments also reveal that fully self-attentional models,<br>a special case of Attention Augmentation, only perform<br>slightly worse than their fully convolutional counterparts on<br>ImageNet, indicating that self-attention is a powerful stand-<br>alone computational primitive for image classification.</p>",
            "id": 16,
            "page": 2,
            "text": "We test our method on the CIFAR-100 and ImageNet\nclassification [22, 9] and the COCO object detection [27]\ntasks, across a wide range of architectures at different com-\nputational budgets, including a state-of-the art resource\nconstrained architecture [42]. Attention Augmentation\nyields systematic improvements with minimal additional\ncomputational burden and notably outperforms the popu-\nlar Squeeze-and-Excitation [17] channelwise attention ap-\nproach in all experiments. In particular, Attention Augmen-\ntation achieves a 1.3% top-1 accuracy ImageNet on top of\na ResNet50 baseline and 1.4 mAP increase in COCO ob-\nject detection on top of a RetinaNet baseline. Suprisingly,\nexperiments also reveal that fully self-attentional models,\na special case of Attention Augmentation, only perform\nslightly worse than their fully convolutional counterparts on\nImageNet, indicating that self-attention is a powerful stand-\nalone computational primitive for image classification."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1281
                },
                {
                    "x": 1636,
                    "y": 1281
                },
                {
                    "x": 1636,
                    "y": 1330
                },
                {
                    "x": 1280,
                    "y": 1330
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:22px'>2. Related Work</p>",
            "id": 17,
            "page": 2,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1368
                },
                {
                    "x": 1837,
                    "y": 1368
                },
                {
                    "x": 1837,
                    "y": 1418
                },
                {
                    "x": 1280,
                    "y": 1418
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>2.1. Convolutional networks</p>",
            "id": 18,
            "page": 2,
            "text": "2.1. Convolutional networks"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1450
                },
                {
                    "x": 2278,
                    "y": 1450
                },
                {
                    "x": 2278,
                    "y": 2247
                },
                {
                    "x": 1277,
                    "y": 2247
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:18px'>Modern computer vision has been built on powerful im-<br>age featurizers learned on image classification tasks such<br>as CIFAR-10 [22] and ImageNet [9]. These datasets have<br>been used as benchmarks for delineating better image fea-<br>turizations and network architectures across a broad range<br>of tasks [21]. For example, improving the \"backbone\" net-<br>work typically leads to improvements in object detection<br>[19] and image segmentation [6]. These observations have<br>inspired the research and design of new architectures, which<br>are typically derived from the composition of convolution<br>operations across an array of spatial scales and skip con-<br>nections [23, 41, 39, 40, 14, 47, 13]. Indeed, automated<br>search strategies for designing architectures based on con-<br>volutional primitives result in state-of-the-art accuracy on<br>large-scale image classification tasks that translate across a<br>range of tasks [55, 21].</p>",
            "id": 19,
            "page": 2,
            "text": "Modern computer vision has been built on powerful im-\nage featurizers learned on image classification tasks such\nas CIFAR-10 [22] and ImageNet [9]. These datasets have\nbeen used as benchmarks for delineating better image fea-\nturizations and network architectures across a broad range\nof tasks [21]. For example, improving the \"backbone\" net-\nwork typically leads to improvements in object detection\n[19] and image segmentation [6]. These observations have\ninspired the research and design of new architectures, which\nare typically derived from the composition of convolution\noperations across an array of spatial scales and skip con-\nnections [23, 41, 39, 40, 14, 47, 13]. Indeed, automated\nsearch strategies for designing architectures based on con-\nvolutional primitives result in state-of-the-art accuracy on\nlarge-scale image classification tasks that translate across a\nrange of tasks [55, 21]."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2297
                },
                {
                    "x": 2045,
                    "y": 2297
                },
                {
                    "x": 2045,
                    "y": 2345
                },
                {
                    "x": 1282,
                    "y": 2345
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>2.2. Attention mechanisms in networks</p>",
            "id": 20,
            "page": 2,
            "text": "2.2. Attention mechanisms in networks"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2379
                },
                {
                    "x": 2277,
                    "y": 2379
                },
                {
                    "x": 2277,
                    "y": 2978
                },
                {
                    "x": 1277,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:18px'>Attention has enjoyed widespread adoption as a com-<br>putational module for modeling sequences because of its<br>ability to capture long distance interactions [2, 44, 4, 3].<br>Most notably, Bahdanau et al. [2] first proposed to com-<br>bine attention with a Recurrent Neural Network [15] for<br>alignment in Machine Translation. Attention was further<br>extended by Vaswani et al. [43], where the self-attentional<br>Transformer architecture achieved state-of-the-art results in<br>Machine Translation. Using self-attention in cooperation<br>with convolutions is a theme shared by recent work in Nat-<br>ural Language Processing [49] and Reinforcement Learn-<br>ing [52]. For example, the QANet [50] and Evolved Trans-</p>",
            "id": 21,
            "page": 2,
            "text": "Attention has enjoyed widespread adoption as a com-\nputational module for modeling sequences because of its\nability to capture long distance interactions [2, 44, 4, 3].\nMost notably, Bahdanau et al. [2] first proposed to com-\nbine attention with a Recurrent Neural Network [15] for\nalignment in Machine Translation. Attention was further\nextended by Vaswani et al. [43], where the self-attentional\nTransformer architecture achieved state-of-the-art results in\nMachine Translation. Using self-attention in cooperation\nwith convolutions is a theme shared by recent work in Nat-\nural Language Processing [49] and Reinforcement Learn-\ning [52]. For example, the QANet [50] and Evolved Trans-"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 1154
                },
                {
                    "x": 199,
                    "y": 1154
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:16px'>former [38] architectures alternate between self-attention<br>layers and convolution layers for Question Answering ap-<br>plications and Machine Translation respectively. Addi-<br>tionally, multiple attention mechanisms have been pro-<br>posed for visual tasks to address the weaknesses of con-<br>volutions [17, 16, 7, 46, 45, 53]. For instance, Squeeze-<br>and-Excitation [17] and Gather-Excite [16] reweigh feature<br>channels using signals aggregated from entire feature maps,<br>while BAM [31] and CBAM [46] refine convolutional fea-<br>tures independently in the channel and spatial dimensions.<br>In non-local neural networks [45], improvements are shown<br>in video classification and object detection via the addi-<br>tive use of a few non-local residual blocks that employ<br>self-attention in convolutional architectures. However, non-<br>local blocks are only added to the architecture after Ima-<br>geNet pretraining and are initialized in such a way that they<br>do not break pretraining.</p>",
            "id": 22,
            "page": 3,
            "text": "former [38] architectures alternate between self-attention\nlayers and convolution layers for Question Answering ap-\nplications and Machine Translation respectively. Addi-\ntionally, multiple attention mechanisms have been pro-\nposed for visual tasks to address the weaknesses of con-\nvolutions [17, 16, 7, 46, 45, 53]. For instance, Squeeze-\nand-Excitation [17] and Gather-Excite [16] reweigh feature\nchannels using signals aggregated from entire feature maps,\nwhile BAM [31] and CBAM [46] refine convolutional fea-\ntures independently in the channel and spatial dimensions.\nIn non-local neural networks [45], improvements are shown\nin video classification and object detection via the addi-\ntive use of a few non-local residual blocks that employ\nself-attention in convolutional architectures. However, non-\nlocal blocks are only added to the architecture after Ima-\ngeNet pretraining and are initialized in such a way that they\ndo not break pretraining."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1160
                },
                {
                    "x": 1200,
                    "y": 1160
                },
                {
                    "x": 1200,
                    "y": 1907
                },
                {
                    "x": 199,
                    "y": 1907
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:16px'>In contrast, our attention augmented networks do not rely<br>on pretraining of their fully convolutional counterparts and<br>employ self-attention along the entire architecture. The use<br>of multi-head attention allows the model to attend jointly<br>to both spatial and feature subspaces. Additionally, we en-<br>hance the representational power of self-attention over im-<br>ages by extending relative self-attention [37, 18] to two di-<br>mensional inputs allowing us to model translation equivari-<br>ance in a principled way. Finally our method produces addi-<br>tional feature maps, rather than recalibrating convolutional<br>features via addition [45, 53] or gating [17, 16, 31, 46]. This<br>property allows us to flexibly adjust the fraction of atten-<br>tional channels and consider a spectrum of architectures,<br>ranging from fully convolutional to fully attentional mod-<br>els.</p>",
            "id": 23,
            "page": 3,
            "text": "In contrast, our attention augmented networks do not rely\non pretraining of their fully convolutional counterparts and\nemploy self-attention along the entire architecture. The use\nof multi-head attention allows the model to attend jointly\nto both spatial and feature subspaces. Additionally, we en-\nhance the representational power of self-attention over im-\nages by extending relative self-attention [37, 18] to two di-\nmensional inputs allowing us to model translation equivari-\nance in a principled way. Finally our method produces addi-\ntional feature maps, rather than recalibrating convolutional\nfeatures via addition [45, 53] or gating [17, 16, 31, 46]. This\nproperty allows us to flexibly adjust the fraction of atten-\ntional channels and consider a spectrum of architectures,\nranging from fully convolutional to fully attentional mod-\nels."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1979
                },
                {
                    "x": 449,
                    "y": 1979
                },
                {
                    "x": 449,
                    "y": 2032
                },
                {
                    "x": 201,
                    "y": 2032
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:22px'>3. Methods</p>",
            "id": 24,
            "page": 3,
            "text": "3. Methods"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2071
                },
                {
                    "x": 1199,
                    "y": 2071
                },
                {
                    "x": 1199,
                    "y": 2520
                },
                {
                    "x": 201,
                    "y": 2520
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>We now formally describe our proposed Attention Aug-<br>mentation method. We use the following naming conven-<br>tions: H, W and Fin refer to the height, width and number<br>of input filters of an activation map. Nh, dv and dk respec-<br>tively refer the number of heads, the depth of values and the<br>depth of queries and keys in multihead-attention (MHA).<br>We further assume that Nh divides dv and dk evenly and<br>denote dh and dk the depth of values and queries/keys per<br>attention head.</p>",
            "id": 25,
            "page": 3,
            "text": "We now formally describe our proposed Attention Aug-\nmentation method. We use the following naming conven-\ntions: H, W and Fin refer to the height, width and number\nof input filters of an activation map. Nh, dv and dk respec-\ntively refer the number of heads, the depth of values and the\ndepth of queries and keys in multihead-attention (MHA).\nWe further assume that Nh divides dv and dk evenly and\ndenote dh and dk the depth of values and queries/keys per\nattention head."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2578
                },
                {
                    "x": 800,
                    "y": 2578
                },
                {
                    "x": 800,
                    "y": 2633
                },
                {
                    "x": 201,
                    "y": 2633
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:18px'>3.1. Self-attention over images</p>",
            "id": 26,
            "page": 3,
            "text": "3.1. Self-attention over images"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2664
                },
                {
                    "x": 1200,
                    "y": 2664
                },
                {
                    "x": 1200,
                    "y": 2868
                },
                {
                    "x": 200,
                    "y": 2868
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:14px'>Given an input tensor of shape (H, W, Fin), 1 flatten<br>we<br>it to a matrix X E RHW xFin and perform multihead atten-<br>tion as proposed in the Transformer architecture [43]. The<br>output of the self-attention mechanism for a single head h</p>",
            "id": 27,
            "page": 3,
            "text": "Given an input tensor of shape (H, W, Fin), 1 flatten\nwe\nit to a matrix X E RHW xFin and perform multihead atten-\ntion as proposed in the Transformer architecture [43]. The\noutput of the self-attention mechanism for a single head h"
        },
        {
            "bounding_box": [
                {
                    "x": 252,
                    "y": 2929
                },
                {
                    "x": 851,
                    "y": 2929
                },
                {
                    "x": 851,
                    "y": 2975
                },
                {
                    "x": 252,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>1We omit the batch dimension for simplicity.</p>",
            "id": 28,
            "page": 3,
            "text": "1We omit the batch dimension for simplicity."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 307
                },
                {
                    "x": 1649,
                    "y": 307
                },
                {
                    "x": 1649,
                    "y": 352
                },
                {
                    "x": 1280,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:14px'>can be formulated as:</p>",
            "id": 29,
            "page": 3,
            "text": "can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 586
                },
                {
                    "x": 2278,
                    "y": 586
                },
                {
                    "x": 2278,
                    "y": 844
                },
                {
                    "x": 1279,
                    "y": 844
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>where Wq, Wk E RFin xdh and Wv E RFin xdh learned<br>are<br>linear transformations that map the input X to queries Q =<br>XWq, keys K = XWk and values V = XWv. The outputs<br>of all heads are then concatenated and projected again as<br>follows:</p>",
            "id": 30,
            "page": 3,
            "text": "where Wq, Wk E RFin xdh and Wv E RFin xdh learned\nare\nlinear transformations that map the input X to queries Q =\nXWq, keys K = XWk and values V = XWv. The outputs\nof all heads are then concatenated and projected again as\nfollows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 991
                },
                {
                    "x": 2279,
                    "y": 991
                },
                {
                    "x": 2279,
                    "y": 1296
                },
                {
                    "x": 1278,
                    "y": 1296
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:16px'>where WO E Rdv xdv is a learned linear transformation.<br>MHA (X) is then reshaped into a tensor of shape (H, W, dv)<br>to match the original spatial dimensions. We note that<br>multi-head attention incurs a complexity of O((HW)2dk)<br>and a memory cost of O((HW): 2 Nh) as it requires to store<br>attention maps for each head.</p>",
            "id": 31,
            "page": 3,
            "text": "where WO E Rdv xdv is a learned linear transformation.\nMHA (X) is then reshaped into a tensor of shape (H, W, dv)\nto match the original spatial dimensions. We note that\nmulti-head attention incurs a complexity of O((HW)2dk)\nand a memory cost of O((HW): 2 Nh) as it requires to store\nattention maps for each head."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1356
                },
                {
                    "x": 2136,
                    "y": 1356
                },
                {
                    "x": 2136,
                    "y": 1405
                },
                {
                    "x": 1279,
                    "y": 1405
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:18px'>3.1.1 Two-dimensional Positional Embeddings</p>",
            "id": 32,
            "page": 3,
            "text": "3.1.1 Two-dimensional Positional Embeddings"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1434
                },
                {
                    "x": 2279,
                    "y": 1434
                },
                {
                    "x": 2279,
                    "y": 1534
                },
                {
                    "x": 1281,
                    "y": 1534
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>Without explicit information about positions, self-attention<br>is permutation equivariant:</p>",
            "id": 33,
            "page": 3,
            "text": "Without explicit information about positions, self-attention\nis permutation equivariant:"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1668
                },
                {
                    "x": 2278,
                    "y": 1668
                },
                {
                    "x": 2278,
                    "y": 2114
                },
                {
                    "x": 1279,
                    "y": 2114
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>for any permutation � of the pixel locations, making it in-<br>effective for modeling highly structured data such as im-<br>ages. Multiple positional encodings that augment activation<br>maps with explicit spatial information have been proposed<br>to alleviate related issues. In particular, the Image Trans-<br>former [32] extends the sinusoidal waves first introduced in<br>the original Transformer [43] to 2 dimensional inputs and<br>CoordConv [29] concatenates positional channels to an ac-<br>tivation map.</p>",
            "id": 34,
            "page": 3,
            "text": "for any permutation � of the pixel locations, making it in-\neffective for modeling highly structured data such as im-\nages. Multiple positional encodings that augment activation\nmaps with explicit spatial information have been proposed\nto alleviate related issues. In particular, the Image Trans-\nformer [32] extends the sinusoidal waves first introduced in\nthe original Transformer [43] to 2 dimensional inputs and\nCoordConv [29] concatenates positional channels to an ac-\ntivation map."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2118
                },
                {
                    "x": 2279,
                    "y": 2118
                },
                {
                    "x": 2279,
                    "y": 2567
                },
                {
                    "x": 1278,
                    "y": 2567
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:14px'>However these encodings did not help in our experi-<br>ments on image classification and object detection (see Sec-<br>tion 4.5). We hypothesize that this is because such posi-<br>tional encodings, while not permutation equivariant, do not<br>satisfy translation equivariance, which is a desirable prop-<br>erty when dealing with images. As a solution, we propose<br>to extend the use of relative position encodings [37] to two<br>dimensions and present a memory efficient implementation<br>based on the Music Transformer [18].</p>",
            "id": 35,
            "page": 3,
            "text": "However these encodings did not help in our experi-\nments on image classification and object detection (see Sec-\ntion 4.5). We hypothesize that this is because such posi-\ntional encodings, while not permutation equivariant, do not\nsatisfy translation equivariance, which is a desirable prop-\nerty when dealing with images. As a solution, we propose\nto extend the use of relative position encodings [37] to two\ndimensions and present a memory efficient implementation\nbased on the Music Transformer [18]."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2624
                },
                {
                    "x": 2279,
                    "y": 2624
                },
                {
                    "x": 2279,
                    "y": 2976
                },
                {
                    "x": 1279,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:16px'>Relative positional embeddings: Introduced in [37] for<br>the purpose of language modeling, relative self-attention<br>augments self-attention with relative position embeddings<br>and enables translation equivariance while preventing per-<br>mutation equivariance. We implement two-dimensional rel-<br>ative self-attention by independently adding relative height<br>information and relative width information. The attention</p>",
            "id": 36,
            "page": 3,
            "text": "Relative positional embeddings: Introduced in [37] for\nthe purpose of language modeling, relative self-attention\naugments self-attention with relative position embeddings\nand enables translation equivariance while preventing per-\nmutation equivariance. We implement two-dimensional rel-\native self-attention by independently adding relative height\ninformation and relative width information. The attention"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 406
                },
                {
                    "x": 201,
                    "y": 406
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>logit for how much pixel i = (ix, iy) attends to pixel<br>j = (jx,jy) is computed as:</p>",
            "id": 37,
            "page": 4,
            "text": "logit for how much pixel i = (ix, iy) attends to pixel\nj = (jx,jy) is computed as:"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 636
                },
                {
                    "x": 1199,
                    "y": 636
                },
                {
                    "x": 1199,
                    "y": 888
                },
                {
                    "x": 200,
                    "y": 888
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:14px'>where qi is the query vector for pixel i (the i-th row of Q),<br>kj is the key vector for pixel j (the j-th row of K) and Wro -ix<br>and Hro -iy embeddings for relative width jx -ix<br>are learned<br>and relative height jy - iy, respectively. The output of head<br>h now becomes:</p>",
            "id": 38,
            "page": 4,
            "text": "where qi is the query vector for pixel i (the i-th row of Q),\nkj is the key vector for pixel j (the j-th row of K) and Wro -ix\nand Hro -iy embeddings for relative width jx -ix\nare learned\nand relative height jy - iy, respectively. The output of head\nh now becomes:"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1139
                },
                {
                    "x": 1197,
                    "y": 1139
                },
                {
                    "x": 1197,
                    "y": 1306
                },
                {
                    "x": 201,
                    "y": 1306
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>where Srel , Swel E RHW x HW relative po-<br>are matrices of<br>sition logits along height and width dimensions that satisfy<br>Srel [i, j] = qT jy-iy Swel[i,j] = qT r Jx-ix.<br>H<br>and</p>",
            "id": 39,
            "page": 4,
            "text": "where Srel , Swel E RHW x HW relative po-\nare matrices of\nsition logits along height and width dimensions that satisfy\nSrel [i, j] = qT jy-iy Swel[i,j] = qT r Jx-ix.\nH\nand"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1302
                },
                {
                    "x": 1199,
                    "y": 1302
                },
                {
                    "x": 1199,
                    "y": 1946
                },
                {
                    "x": 200,
                    "y": 1946
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:14px'>The relative attention algorithm in [37] explicitly<br>stores all relative embeddings rij in a tensor of shape<br>(HW, HW, dh), thus incurring an additional memory cost<br>of O((HW)2dh). This compares to O((HW)2 Nh) for the<br>position-unaware version self-attention that does not use<br>position encodings. As we typically have Nh < dk, such an<br>implementation can prove extremely prohibitive and restrict<br>the number ofimages that can fitin a minibatch. Instead, we<br>extend the memory efficient relative masked attention algo-<br>rithm presented in [18] to unmasked relative self-attention<br>over 2 dimensional inputs. Our implementation has a mem-<br>ory cost of O(HW dr). We leave the Tensorflow code of<br>the algorithm in the Appendix.</p>",
            "id": 40,
            "page": 4,
            "text": "The relative attention algorithm in [37] explicitly\nstores all relative embeddings rij in a tensor of shape\n(HW, HW, dh), thus incurring an additional memory cost\nof O((HW)2dh). This compares to O((HW)2 Nh) for the\nposition-unaware version self-attention that does not use\nposition encodings. As we typically have Nh < dk, such an\nimplementation can prove extremely prohibitive and restrict\nthe number ofimages that can fitin a minibatch. Instead, we\nextend the memory efficient relative masked attention algo-\nrithm presented in [18] to unmasked relative self-attention\nover 2 dimensional inputs. Our implementation has a mem-\nory cost of O(HW dr). We leave the Tensorflow code of\nthe algorithm in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1949
                },
                {
                    "x": 1199,
                    "y": 1949
                },
                {
                    "x": 1199,
                    "y": 2152
                },
                {
                    "x": 201,
                    "y": 2152
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:14px'>The relative positional embeeddings rH and rW are<br>learned and shared across heads but not layers. For each<br>layer, we add (2(H + W) - 2)dh parameters to model rel-<br>ative distances along height and width.</p>",
            "id": 41,
            "page": 4,
            "text": "The relative positional embeeddings rH and rW are\nlearned and shared across heads but not layers. For each\nlayer, we add (2(H + W) - 2)dh parameters to model rel-\native distances along height and width."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2196
                },
                {
                    "x": 962,
                    "y": 2196
                },
                {
                    "x": 962,
                    "y": 2247
                },
                {
                    "x": 201,
                    "y": 2247
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>3.2. Attention Augmented Convolution</p>",
            "id": 42,
            "page": 4,
            "text": "3.2. Attention Augmented Convolution"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2278
                },
                {
                    "x": 1199,
                    "y": 2278
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 200,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:16px'>Multiple previously proposed attention mechanisms over<br>images [17, 16, 31, 46] suggest that the convolution op-<br>erator is limited by its locality and lack of understanding<br>of global contexts. These methods capture long-range de-<br>pendencies by recalibrating convolutional feature maps. In<br>particular, Squeeze-and-Excitation (SE) [17] and Gather-<br>Excite (GE) [16] perform channelwise reweighing while<br>BAM [31] and CBAM [46] reweigh both channels and<br>spatial positions independently. In contrast to these ap-<br>proaches, we 1) use an attention mechanism that can attend<br>jointly to spatial and feature subspaces (each head corre-<br>sponding to a feature subspace) and 2) introduce additional<br>feature maps rather than refining them. Figure 2 summa-<br>rizes our proposed augmented convolution.</p>",
            "id": 43,
            "page": 4,
            "text": "Multiple previously proposed attention mechanisms over\nimages [17, 16, 31, 46] suggest that the convolution op-\nerator is limited by its locality and lack of understanding\nof global contexts. These methods capture long-range de-\npendencies by recalibrating convolutional feature maps. In\nparticular, Squeeze-and-Excitation (SE) [17] and Gather-\nExcite (GE) [16] perform channelwise reweighing while\nBAM [31] and CBAM [46] reweigh both channels and\nspatial positions independently. In contrast to these ap-\nproaches, we 1) use an attention mechanism that can attend\njointly to spatial and feature subspaces (each head corre-\nsponding to a feature subspace) and 2) introduce additional\nfeature maps rather than refining them. Figure 2 summa-\nrizes our proposed augmented convolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 305
                },
                {
                    "x": 2279,
                    "y": 305
                },
                {
                    "x": 2279,
                    "y": 553
                },
                {
                    "x": 1278,
                    "y": 553
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:14px'>Concatenating convolutional and attentional feature<br>maps: Formally, consider an original convolution oper-<br>ator with kernel size k, Fin input filters and Fout output<br>filters. The corresponding attention augmented convolution<br>can be written as</p>",
            "id": 44,
            "page": 4,
            "text": "Concatenating convolutional and attentional feature\nmaps: Formally, consider an original convolution oper-\nator with kernel size k, Fin input filters and Fout output\nfilters. The corresponding attention augmented convolution\ncan be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 688
                },
                {
                    "x": 2279,
                    "y": 688
                },
                {
                    "x": 2279,
                    "y": 1105
                },
                {
                    "x": 1279,
                    "y": 1105
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:14px'>dv the ratio of attentional channels to<br>We denote v =<br>Fout<br>dk the ratio of<br>number of original output filters and K =<br>Fout<br>key depth to number of original output filters. Similarly to<br>the convolution, the proposed attention augmented convo-<br>lution 1) is equivariant to translation and 2) can readily op-<br>erate on inputs of different spatial dimensions. We include<br>Tensorflow code for the proposed attention augmented con-<br>volution in the Appendix A.3.</p>",
            "id": 45,
            "page": 4,
            "text": "dv the ratio of attentional channels to\nWe denote v =\nFout\ndk the ratio of\nnumber of original output filters and K =\nFout\nkey depth to number of original output filters. Similarly to\nthe convolution, the proposed attention augmented convo-\nlution 1) is equivariant to translation and 2) can readily op-\nerate on inputs of different spatial dimensions. We include\nTensorflow code for the proposed attention augmented con-\nvolution in the Appendix A.3."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1162
                },
                {
                    "x": 2279,
                    "y": 1162
                },
                {
                    "x": 2279,
                    "y": 1562
                },
                {
                    "x": 1279,
                    "y": 1562
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:14px'>Effect on number of parameters: Multihead attention<br>introduces a 1x1 convolution with Fin input filters and<br>(2dk +dv) = Fout (2k+v) output filters to compute queries,<br>keys and values and an additional 1x1 convolution with<br>dv = FoutU input and output filters to mix the contribu-<br>tion of different heads. Considering the decrease in filters<br>in the convolutional part, this leads to the following change<br>in parameters:</p>",
            "id": 46,
            "page": 4,
            "text": "Effect on number of parameters: Multihead attention\nintroduces a 1x1 convolution with Fin input filters and\n(2dk +dv) = Fout (2k+v) output filters to compute queries,\nkeys and values and an additional 1x1 convolution with\ndv = FoutU input and output filters to mix the contribu-\ntion of different heads. Considering the decrease in filters\nin the convolutional part, this leads to the following change\nin parameters:"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1722
                },
                {
                    "x": 2278,
                    "y": 1722
                },
                {
                    "x": 2278,
                    "y": 2119
                },
                {
                    "x": 1280,
                    "y": 2119
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:14px'>where we ignore the parameters introduced by relative po-<br>sition embeddings for simplicity as these are negligible. In<br>practice, this causes a slight decrease in parameters when<br>replacing 3x3 convolutions and a slight increase in parame-<br>ters when replacing 1x1 convolutions. Interestingly, we find<br>in experiments that attention augmented networks still sig-<br>nificantly outperform their fully convolutional counterparts<br>while using less parameters.</p>",
            "id": 47,
            "page": 4,
            "text": "where we ignore the parameters introduced by relative po-\nsition embeddings for simplicity as these are negligible. In\npractice, this causes a slight decrease in parameters when\nreplacing 3x3 convolutions and a slight increase in parame-\nters when replacing 1x1 convolutions. Interestingly, we find\nin experiments that attention augmented networks still sig-\nnificantly outperform their fully convolutional counterparts\nwhile using less parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2178
                },
                {
                    "x": 2277,
                    "y": 2178
                },
                {
                    "x": 2277,
                    "y": 2576
                },
                {
                    "x": 1279,
                    "y": 2576
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:14px'>Attention Augmented Convolutional Architectures: In<br>all our experiments, the augmented convolution is followed<br>by a batch normalization [20] layer which can learn to scale<br>the contribution of the convolution feature maps and the at-<br>tention feature maps. We apply our augmented convolution<br>once per residual block similarly to other visual attention<br>mechanisms [17, 16, 31, 46] and along the entire architec-<br>ture as memory permits (see Section 4 for more details).</p>",
            "id": 48,
            "page": 4,
            "text": "Attention Augmented Convolutional Architectures: In\nall our experiments, the augmented convolution is followed\nby a batch normalization [20] layer which can learn to scale\nthe contribution of the convolution feature maps and the at-\ntention feature maps. We apply our augmented convolution\nonce per residual block similarly to other visual attention\nmechanisms [17, 16, 31, 46] and along the entire architec-\nture as memory permits (see Section 4 for more details)."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2579
                },
                {
                    "x": 2278,
                    "y": 2579
                },
                {
                    "x": 2278,
                    "y": 2977
                },
                {
                    "x": 1278,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:14px'>Since the memory cost O((Nh (HW)2) can be pro-<br>hibitive for large spatial dimensions, we augment convolu-<br>tions with attention starting from the last layer (with small-<br>est spatial dimension) until we hit memory constraints. To<br>reduce the memory footprint of augmented networks, we<br>typically resort to a smaller batch size and sometimes addi-<br>tionally downsample the inputs to self-attention in the lay-<br>ers with the largest spatial dimensions where it is applied.</p>",
            "id": 49,
            "page": 4,
            "text": "Since the memory cost O((Nh (HW)2) can be pro-\nhibitive for large spatial dimensions, we augment convolu-\ntions with attention starting from the last layer (with small-\nest spatial dimension) until we hit memory constraints. To\nreduce the memory footprint of augmented networks, we\ntypically resort to a smaller batch size and sometimes addi-\ntionally downsample the inputs to self-attention in the lay-\ners with the largest spatial dimensions where it is applied."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 305
                },
                {
                    "x": 1200,
                    "y": 456
                },
                {
                    "x": 200,
                    "y": 456
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:20px'>Downsampling is performed by applying 3x3 average pool-<br>ing with stride 2 while the following upsampling (required<br>for the concatenation) is obtained via bilinear interpolation.</p>",
            "id": 50,
            "page": 5,
            "text": "Downsampling is performed by applying 3x3 average pool-\ning with stride 2 while the following upsampling (required\nfor the concatenation) is obtained via bilinear interpolation."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 504
                },
                {
                    "x": 534,
                    "y": 504
                },
                {
                    "x": 534,
                    "y": 557
                },
                {
                    "x": 201,
                    "y": 557
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:22px'>4. Experiments</p>",
            "id": 51,
            "page": 5,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 588
                },
                {
                    "x": 1200,
                    "y": 588
                },
                {
                    "x": 1200,
                    "y": 1339
                },
                {
                    "x": 200,
                    "y": 1339
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:18px'>In the subsequent experiments, we test Attention Aug-<br>mentation on standard computer vision architectures such<br>as ResNets [14, 47, 13], and MnasNet [42] on the CIFAR-<br>100 [22], ImageNet [9] and COCO [25] datasets. Our ex-<br>periments show that Attention Augmentation leads to sys-<br>tematic improvements on both image classification and ob-<br>ject detection tasks across a broad array of architectures and<br>computational demands. We validate the utility of the pro-<br>posed two-dimensional relative attention mechanism in ab-<br>lation experiments. In all experiments, we substitute con-<br>volutional feature maps with self-attention feature maps as<br>it makes for an easier comparison against the baseline mod-<br>els. Unless specified otherwise, all results correspond to our<br>two-dimensional relative self-attention mechanism. Exper-<br>imental details can be found in the Appendix.</p>",
            "id": 52,
            "page": 5,
            "text": "In the subsequent experiments, we test Attention Aug-\nmentation on standard computer vision architectures such\nas ResNets [14, 47, 13], and MnasNet [42] on the CIFAR-\n100 [22], ImageNet [9] and COCO [25] datasets. Our ex-\nperiments show that Attention Augmentation leads to sys-\ntematic improvements on both image classification and ob-\nject detection tasks across a broad array of architectures and\ncomputational demands. We validate the utility of the pro-\nposed two-dimensional relative attention mechanism in ab-\nlation experiments. In all experiments, we substitute con-\nvolutional feature maps with self-attention feature maps as\nit makes for an easier comparison against the baseline mod-\nels. Unless specified otherwise, all results correspond to our\ntwo-dimensional relative self-attention mechanism. Exper-\nimental details can be found in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1370
                },
                {
                    "x": 903,
                    "y": 1370
                },
                {
                    "x": 903,
                    "y": 1423
                },
                {
                    "x": 202,
                    "y": 1423
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:20px'>4.1. CIFAR-100 image classification</p>",
            "id": 53,
            "page": 5,
            "text": "4.1. CIFAR-100 image classification"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1449
                },
                {
                    "x": 1201,
                    "y": 1449
                },
                {
                    "x": 1201,
                    "y": 2200
                },
                {
                    "x": 200,
                    "y": 2200
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>We first investigate how Attention Augmentation per-<br>forms on CIFAR-100 [22], a standard benchmark for low-<br>resolution imagery, using a Wide ResNet architecture [51].<br>The Wide-ResNet-28-10 architecture is comprised of 3<br>stages of 4 residual blocks each using two 3 x 3 convolu-<br>tions. We augment the Wide-ResNet-28-10 by augmenting<br>the first convolution of all residual blocks with relative at-<br>tention using Nh=8 heads and k=2v=0.2 and a minimum of<br>20 dimensions per head for the keys. We compare Attention<br>Augmentation (AA) against other forms of attention includ-<br>ing Squeeze-and-Excitation (SE) [17] and the parameter-<br>free formulation of Gather-Excite (GE) [16]. Table 1 shows<br>that Attention Augmentation improves performance both<br>over the baseline network and Squeeze-and-Excitation at a<br>similar parameter and complexity cost.</p>",
            "id": 54,
            "page": 5,
            "text": "We first investigate how Attention Augmentation per-\nforms on CIFAR-100 [22], a standard benchmark for low-\nresolution imagery, using a Wide ResNet architecture [51].\nThe Wide-ResNet-28-10 architecture is comprised of 3\nstages of 4 residual blocks each using two 3 x 3 convolu-\ntions. We augment the Wide-ResNet-28-10 by augmenting\nthe first convolution of all residual blocks with relative at-\ntention using Nh=8 heads and k=2v=0.2 and a minimum of\n20 dimensions per head for the keys. We compare Attention\nAugmentation (AA) against other forms of attention includ-\ning Squeeze-and-Excitation (SE) [17] and the parameter-\nfree formulation of Gather-Excite (GE) [16]. Table 1 shows\nthat Attention Augmentation improves performance both\nover the baseline network and Squeeze-and-Excitation at a\nsimilar parameter and complexity cost."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2244
                },
                {
                    "x": 1205,
                    "y": 2244
                },
                {
                    "x": 1205,
                    "y": 2488
                },
                {
                    "x": 203,
                    "y": 2488
                }
            ],
            "category": "table",
            "html": "<table id='55' style='font-size:16px'><tr><td>Architecture</td><td>Params</td><td>GFlops</td><td>top-1</td><td>top-5</td></tr><tr><td>Wide-ResNet [51]</td><td>36.3M</td><td>10.4</td><td>80.3</td><td>95.0</td></tr><tr><td>GE-Wide-ResNet [16]</td><td>36.3M</td><td>10.4</td><td>79.8</td><td>95.0</td></tr><tr><td>SE-Wide-ResNet [17]</td><td>36.5M</td><td>10.4</td><td>81.0</td><td>95.3</td></tr><tr><td>AA-Wide-ResNet (ours)</td><td>36.2M</td><td>10.9</td><td>81.6</td><td>95.2</td></tr></table>",
            "id": 55,
            "page": 5,
            "text": "Architecture Params GFlops top-1 top-5\n Wide-ResNet [51] 36.3M 10.4 80.3 95.0\n GE-Wide-ResNet [16] 36.3M 10.4 79.8 95.0\n SE-Wide-ResNet [17] 36.5M 10.4 81.0 95.3\n AA-Wide-ResNet (ours) 36.2M 10.9 81.6"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2522
                },
                {
                    "x": 1198,
                    "y": 2522
                },
                {
                    "x": 1198,
                    "y": 2613
                },
                {
                    "x": 202,
                    "y": 2613
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:16px'>Table 1. Image classification on the CIFAR-100 dataset [22] using<br>the Wide-ResNet 28-10 architecture [51].</p>",
            "id": 56,
            "page": 5,
            "text": "Table 1. Image classification on the CIFAR-100 dataset [22] using\nthe Wide-ResNet 28-10 architecture [51]."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2698
                },
                {
                    "x": 1120,
                    "y": 2698
                },
                {
                    "x": 1120,
                    "y": 2750
                },
                {
                    "x": 200,
                    "y": 2750
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:22px'>4.2. ImageNet image classification with ResNet</p>",
            "id": 57,
            "page": 5,
            "text": "4.2. ImageNet image classification with ResNet"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2778
                },
                {
                    "x": 1201,
                    "y": 2778
                },
                {
                    "x": 1201,
                    "y": 2976
                },
                {
                    "x": 201,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>We next examine how Attention Augmentation performs<br>on ImageNet [9, 21], a standard large-scale dataset for high<br>resolution imagery, across an array of architectures. We<br>start with the ResNet architecture [14, 47, 13] because ofits</p>",
            "id": 58,
            "page": 5,
            "text": "We next examine how Attention Augmentation performs\non ImageNet [9, 21], a standard large-scale dataset for high\nresolution imagery, across an array of architectures. We\nstart with the ResNet architecture [14, 47, 13] because ofits"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 305
                },
                {
                    "x": 2279,
                    "y": 305
                },
                {
                    "x": 2279,
                    "y": 1054
                },
                {
                    "x": 1276,
                    "y": 1054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:18px'>widespread use and its ability to easily scale across several<br>computational budgets. The building block in ResNet-34<br>comprises two 3x3 convolutions with the same number of<br>output filters. ResNet-50 and its larger counterparts use a<br>bottleneck block comprising of 1x1, 3x3, 1x1 convolutions<br>where the last pointwise convolution expands the number<br>of filters and the first one contracts the number of filters.<br>We modify all ResNets by augmenting the 3x3 convolu-<br>tions as this decreases number of parameters.2 We apply<br>Attention Augmentation in each residual block of the last 3<br>stages of the architecture - when the spatial dimensions of<br>the activation maps are 28x28, 14x14 and 7x7 - and down-<br>sample only during the first stage. All attention augmented<br>networks use k=2v=0.2, except for ResNet-34 which uses<br>k=v=0.25. The number of attention heads is fixed to Nh=8.</p>",
            "id": 59,
            "page": 5,
            "text": "widespread use and its ability to easily scale across several\ncomputational budgets. The building block in ResNet-34\ncomprises two 3x3 convolutions with the same number of\noutput filters. ResNet-50 and its larger counterparts use a\nbottleneck block comprising of 1x1, 3x3, 1x1 convolutions\nwhere the last pointwise convolution expands the number\nof filters and the first one contracts the number of filters.\nWe modify all ResNets by augmenting the 3x3 convolu-\ntions as this decreases number of parameters.2 We apply\nAttention Augmentation in each residual block of the last 3\nstages of the architecture - when the spatial dimensions of\nthe activation maps are 28x28, 14x14 and 7x7 - and down-\nsample only during the first stage. All attention augmented\nnetworks use k=2v=0.2, except for ResNet-34 which uses\nk=v=0.25. The number of attention heads is fixed to Nh=8."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1094
                },
                {
                    "x": 2277,
                    "y": 1094
                },
                {
                    "x": 2277,
                    "y": 1387
                },
                {
                    "x": 1280,
                    "y": 1387
                }
            ],
            "category": "table",
            "html": "<table id='60' style='font-size:14px'><tr><td>Architecture</td><td>Params (M)</td><td>△Infer</td><td>△Train</td><td>top-1</td></tr><tr><td>ResNet-50</td><td>25.6</td><td>-</td><td>-</td><td>76.4</td></tr><tr><td>SE [17]</td><td>28.1</td><td>+12%</td><td>+92%</td><td>77.5 (77.0)</td></tr><tr><td>BAM [31]</td><td>25.9</td><td>+19%</td><td>+43%</td><td>77.3</td></tr><tr><td>CBAM [46]</td><td>28.1</td><td>+56%</td><td>+132%</td><td>77.4 (77.4)</td></tr><tr><td>GALA [28]</td><td>29.4</td><td>+86%</td><td>+133%</td><td>77.5 (77.3)</td></tr><tr><td>AA (v = 0.25)</td><td>24.3</td><td>+29%</td><td>+25%</td><td>77.7</td></tr></table>",
            "id": 60,
            "page": 5,
            "text": "Architecture Params (M) △Infer △Train top-1\n ResNet-50 25.6 - - 76.4\n SE [17] 28.1 +12% +92% 77.5 (77.0)\n BAM [31] 25.9 +19% +43% 77.3\n CBAM [46] 28.1 +56% +132% 77.4 (77.4)\n GALA [28] 29.4 +86% +133% 77.5 (77.3)\n AA (v = 0.25) 24.3 +29% +25%"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1418
                },
                {
                    "x": 2279,
                    "y": 1418
                },
                {
                    "x": 2279,
                    "y": 1738
                },
                {
                    "x": 1279,
                    "y": 1738
                }
            ],
            "category": "caption",
            "html": "<caption id='61' style='font-size:14px'>Table 2. Image classification performance of different attention<br>mechanisms on the ImageNet dataset. △ refers to the increase<br>in latency times compared to the ResNet50 on a single Tesla V100<br>GPU with Tensorflow using a batch size of 128. For fair compar-<br>ison, we also include top-1 results (in parentheses) when scaling<br>networks in width to match ~ 25.6M parameters as the ResNet50<br>baseline.</caption>",
            "id": 61,
            "page": 5,
            "text": "Table 2. Image classification performance of different attention\nmechanisms on the ImageNet dataset. △ refers to the increase\nin latency times compared to the ResNet50 on a single Tesla V100\nGPU with Tensorflow using a batch size of 128. For fair compar-\nison, we also include top-1 results (in parentheses) when scaling\nnetworks in width to match ~ 25.6M parameters as the ResNet50\nbaseline."
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 1819
                },
                {
                    "x": 2269,
                    "y": 1819
                },
                {
                    "x": 2269,
                    "y": 2440
                },
                {
                    "x": 1284,
                    "y": 2440
                }
            ],
            "category": "table",
            "html": "<table id='62' style='font-size:16px'><tr><td>Architecture</td><td>GFlops</td><td>Params</td><td>top-1</td><td>top-5</td></tr><tr><td>ResNet-34 [14]</td><td>7.4</td><td>21.8M</td><td>73.6</td><td>91.5</td></tr><tr><td>SE-ResNet-34 [17]</td><td>7.4</td><td>22.0M</td><td>74.3</td><td>91.8</td></tr><tr><td>AA-ResNet-34 (ours)</td><td>7.1</td><td>20.7M</td><td>74.7</td><td>92.0</td></tr><tr><td>ResNet-50 [14]</td><td>8.2</td><td>25.6M</td><td>76.4</td><td>93.1</td></tr><tr><td>SE-ResNet-50 [17]</td><td>8.2</td><td>28.1M</td><td>77.5</td><td>93.7</td></tr><tr><td>AA-ResNet-50 (ours)</td><td>8.3</td><td>25.8M</td><td>77.7</td><td>93.8</td></tr><tr><td>ResNet-101 [14]</td><td>15.6</td><td>44.5M</td><td>77.9</td><td>94.0</td></tr><tr><td>SE-ResNet-101 [17]</td><td>15.6</td><td>49.3M</td><td>78.4</td><td>94.2</td></tr><tr><td>AA-ResNet-101 (ours)</td><td>16.1</td><td>45.4M</td><td>78.7</td><td>94.4</td></tr><tr><td>ResNet-152 [14]</td><td>23.0</td><td>60.2M</td><td>78.4</td><td>94.2</td></tr><tr><td>SE-ResNet-152 [17]</td><td>23.1</td><td>66.8M</td><td>78.9</td><td>94.5</td></tr><tr><td>AA-ResNet-152 (ours)</td><td>23.8</td><td>61.6M</td><td>79.1</td><td>94.6</td></tr></table>",
            "id": 62,
            "page": 5,
            "text": "Architecture GFlops Params top-1 top-5\n ResNet-34 [14] 7.4 21.8M 73.6 91.5\n SE-ResNet-34 [17] 7.4 22.0M 74.3 91.8\n AA-ResNet-34 (ours) 7.1 20.7M 74.7 92.0\n ResNet-50 [14] 8.2 25.6M 76.4 93.1\n SE-ResNet-50 [17] 8.2 28.1M 77.5 93.7\n AA-ResNet-50 (ours) 8.3 25.8M 77.7 93.8\n ResNet-101 [14] 15.6 44.5M 77.9 94.0\n SE-ResNet-101 [17] 15.6 49.3M 78.4 94.2\n AA-ResNet-101 (ours) 16.1 45.4M 78.7 94.4\n ResNet-152 [14] 23.0 60.2M 78.4 94.2\n SE-ResNet-152 [17] 23.1 66.8M 78.9 94.5\n AA-ResNet-152 (ours) 23.8 61.6M 79.1"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2471
                },
                {
                    "x": 2277,
                    "y": 2471
                },
                {
                    "x": 2277,
                    "y": 2607
                },
                {
                    "x": 1282,
                    "y": 2607
                }
            ],
            "category": "caption",
            "html": "<caption id='63' style='font-size:16px'>Table 3. Image classification on the ImageNet dataset [9] across<br>a range of ResNet architectures: ResNet-34, ResNet-50, Resnet-<br>101, and ResNet-152 [14, 47, 13].</caption>",
            "id": 63,
            "page": 5,
            "text": "Table 3. Image classification on the ImageNet dataset [9] across\na range of ResNet architectures: ResNet-34, ResNet-50, Resnet-\n101, and ResNet-152 [14, 47, 13]."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2661
                },
                {
                    "x": 2279,
                    "y": 2661
                },
                {
                    "x": 2279,
                    "y": 2859
                },
                {
                    "x": 1278,
                    "y": 2859
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:18px'>Table 2 benchmarks Attention Augmentation against<br>channel and spatial attention mechanisms BAM [31],<br>CBAM [46] and GALA [28] with channel reduction ra-<br>tio 0 = 16 on the ResNet50 architecture. Despite the</p>",
            "id": 64,
            "page": 5,
            "text": "Table 2 benchmarks Attention Augmentation against\nchannel and spatial attention mechanisms BAM [31],\nCBAM [46] and GALA [28] with channel reduction ra-\ntio 0 = 16 on the ResNet50 architecture. Despite the"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2892
                },
                {
                    "x": 2276,
                    "y": 2892
                },
                {
                    "x": 2276,
                    "y": 2972
                },
                {
                    "x": 1283,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:14px'>2We found that augmenting the pointwise expansions works just as well<br>but does not save parameters or computations.</p>",
            "id": 65,
            "page": 5,
            "text": "2We found that augmenting the pointwise expansions works just as well\nbut does not save parameters or computations."
        },
        {
            "bounding_box": [
                {
                    "x": 245,
                    "y": 292
                },
                {
                    "x": 1162,
                    "y": 292
                },
                {
                    "x": 1162,
                    "y": 728
                },
                {
                    "x": 245,
                    "y": 728
                }
            ],
            "category": "table",
            "html": "<table id='66' style='font-size:14px'><tr><td>Architecture</td><td>GFlops</td><td>Params</td><td>top-1</td><td>top-5</td></tr><tr><td>MnasNet-0.75</td><td>0.45</td><td>2.91M</td><td>73.3</td><td>91.3</td></tr><tr><td>AA-MnasNet-0.75</td><td>0.51</td><td>3.02M</td><td>73.9</td><td>91.6</td></tr><tr><td>MnasNet-1.0</td><td>0.63</td><td>3.89M</td><td>75.2</td><td>92.4</td></tr><tr><td>AA-MnasNet-1.0</td><td>0.70</td><td>4.06M</td><td>75.7</td><td>92.6</td></tr><tr><td>MnasNet-1.25</td><td>1.01</td><td>5.26M</td><td>76.7</td><td>93.2</td></tr><tr><td>AA-MnasNet-1.25</td><td>1.11</td><td>5.53M</td><td>77.2</td><td>93.6</td></tr><tr><td>MnasNet-1.4</td><td>1.17</td><td>6.10M</td><td>77.2</td><td>93.5</td></tr><tr><td>AA-MnasNet-1.4</td><td>1.29</td><td>6.44M</td><td>77.7</td><td>93.8</td></tr></table>",
            "id": 66,
            "page": 6,
            "text": "Architecture GFlops Params top-1 top-5\n MnasNet-0.75 0.45 2.91M 73.3 91.3\n AA-MnasNet-0.75 0.51 3.02M 73.9 91.6\n MnasNet-1.0 0.63 3.89M 75.2 92.4\n AA-MnasNet-1.0 0.70 4.06M 75.7 92.6\n MnasNet-1.25 1.01 5.26M 76.7 93.2\n AA-MnasNet-1.25 1.11 5.53M 77.2 93.6\n MnasNet-1.4 1.17 6.10M 77.2 93.5\n AA-MnasNet-1.4 1.29 6.44M 77.7"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 758
                },
                {
                    "x": 1197,
                    "y": 758
                },
                {
                    "x": 1197,
                    "y": 850
                },
                {
                    "x": 204,
                    "y": 850
                }
            ],
            "category": "caption",
            "html": "<caption id='67' style='font-size:14px'>Table 4. Baseline and attention augmented MnasNet [42] accura-<br>cies with width multipliers 0.75, 1.0, 1.25 and 1.4.</caption>",
            "id": 67,
            "page": 6,
            "text": "Table 4. Baseline and attention augmented MnasNet [42] accura-\ncies with width multipliers 0.75, 1.0, 1.25 and 1.4."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 944
                },
                {
                    "x": 1200,
                    "y": 944
                },
                {
                    "x": 1200,
                    "y": 1697
                },
                {
                    "x": 201,
                    "y": 1697
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>lack of specialized kernels (See Appendix A.3), Attention<br>Augmentation offers a competitive accuracy/computational<br>trade-off compared to previously proposed attention mech-<br>anisms. Table 3 compares the non-augmented networks and<br>Squeeze-and-Excitation (SE) [17] across different network<br>scales. In all experiments, Attention Augmentation sig-<br>nificantly increases performance over the non-augmented<br>baseline and notably outperforms Squeeze-and-Excitation<br>(SE) [17] while being more parameter efficient (Figure 1).<br>Remarkably, our AA-ResNet-50 performs comparably to<br>the baseline ResNet-101 and our AA-ResNet-101 outper-<br>forms the baseline ResNet-152. These results suggest that<br>attention augmentation is preferable to simply making net-<br>works deeper. We include and discuss attention maps visu-<br>alizations from different pixel positions in the appendix.</p>",
            "id": 68,
            "page": 6,
            "text": "lack of specialized kernels (See Appendix A.3), Attention\nAugmentation offers a competitive accuracy/computational\ntrade-off compared to previously proposed attention mech-\nanisms. Table 3 compares the non-augmented networks and\nSqueeze-and-Excitation (SE) [17] across different network\nscales. In all experiments, Attention Augmentation sig-\nnificantly increases performance over the non-augmented\nbaseline and notably outperforms Squeeze-and-Excitation\n(SE) [17] while being more parameter efficient (Figure 1).\nRemarkably, our AA-ResNet-50 performs comparably to\nthe baseline ResNet-101 and our AA-ResNet-101 outper-\nforms the baseline ResNet-152. These results suggest that\nattention augmentation is preferable to simply making net-\nworks deeper. We include and discuss attention maps visu-\nalizations from different pixel positions in the appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1740
                },
                {
                    "x": 1030,
                    "y": 1740
                },
                {
                    "x": 1030,
                    "y": 1792
                },
                {
                    "x": 202,
                    "y": 1792
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:22px'>4.3. ImageNet classification with MnasNet</p>",
            "id": 69,
            "page": 6,
            "text": "4.3. ImageNet classification with MnasNet"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1824
                },
                {
                    "x": 1200,
                    "y": 1824
                },
                {
                    "x": 1200,
                    "y": 2677
                },
                {
                    "x": 201,
                    "y": 2677
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:18px'>In this section, we inspect the use of Attention Aug-<br>mentation in a resource constrained setting by conducting<br>ImageNet experiments with the MnasNet architecture [42],<br>which is an extremely parameter-efficient architecture. In<br>particular, the MnasNet was found by neural architec-<br>ture search [54], using only the highly optimized mo-<br>bile inverted bottleneck block [36] and the Squeeze-and-<br>Excitation operation [17] as the primitives in its search<br>space. We apply Attention Augmentation to the mobile<br>inverted bottleneck by replacing convolutional channels in<br>the expansion pointwise convolution using k=2v=0.1 and<br>Nh: =4 heads. Our augmented MnasNets use augmented in-<br>verted bottlenecks in the last 13 blocks out of 18 in the<br>MnasNet architecture, starting when the spatial dimension<br>is 28x28. We downsample only in the first stage where At-<br>tention Augmentation is applied. We leave the final point-<br>wise convolution, also referred to as the \"head\", unchanged.</p>",
            "id": 70,
            "page": 6,
            "text": "In this section, we inspect the use of Attention Aug-\nmentation in a resource constrained setting by conducting\nImageNet experiments with the MnasNet architecture [42],\nwhich is an extremely parameter-efficient architecture. In\nparticular, the MnasNet was found by neural architec-\nture search [54], using only the highly optimized mo-\nbile inverted bottleneck block [36] and the Squeeze-and-\nExcitation operation [17] as the primitives in its search\nspace. We apply Attention Augmentation to the mobile\ninverted bottleneck by replacing convolutional channels in\nthe expansion pointwise convolution using k=2v=0.1 and\nNh: =4 heads. Our augmented MnasNets use augmented in-\nverted bottlenecks in the last 13 blocks out of 18 in the\nMnasNet architecture, starting when the spatial dimension\nis 28x28. We downsample only in the first stage where At-\ntention Augmentation is applied. We leave the final point-\nwise convolution, also referred to as the \"head\", unchanged."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2679
                },
                {
                    "x": 1201,
                    "y": 2679
                },
                {
                    "x": 1201,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:18px'>In Table 4, we report ImageNet accuracies for the base-<br>line MnasNet and its attention augmented variants at dif-<br>ferent width multipliers. Our experiments show that At-<br>tention Augmentation yields accuracy improvements across<br>all width multipliers. Augmenting MnasNets with relative<br>self-attention incurs a slight parameter increase, however</p>",
            "id": 71,
            "page": 6,
            "text": "In Table 4, we report ImageNet accuracies for the base-\nline MnasNet and its attention augmented variants at dif-\nferent width multipliers. Our experiments show that At-\ntention Augmentation yields accuracy improvements across\nall width multipliers. Augmenting MnasNets with relative\nself-attention incurs a slight parameter increase, however"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 323
                },
                {
                    "x": 2249,
                    "y": 323
                },
                {
                    "x": 2249,
                    "y": 1059
                },
                {
                    "x": 1314,
                    "y": 1059
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='72' style='font-size:20px' alt=\"78 77.7%\n77.2%\n77\n77.2%\naccuracy 76 75.7%\n76.7%\ntop-1 75\n75.2%\n73.9%\n74\nMnasNet\n73 73.3% AA-MnasNet (ours)\n2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0\nnumber of parameters (millions)\" data-coord=\"top-left:(1314,323); bottom-right:(2249,1059)\" /></figure>",
            "id": 72,
            "page": 6,
            "text": "78 77.7%\n77.2%\n77\n77.2%\naccuracy 76 75.7%\n76.7%\ntop-1 75\n75.2%\n73.9%\n74\nMnasNet\n73 73.3% AA-MnasNet (ours)\n2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0\nnumber of parameters (millions)"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1099
                },
                {
                    "x": 2278,
                    "y": 1099
                },
                {
                    "x": 2278,
                    "y": 1238
                },
                {
                    "x": 1278,
                    "y": 1238
                }
            ],
            "category": "caption",
            "html": "<caption id='73' style='font-size:14px'>Figure 3. ImageNet top-1 accuracy as a function of number of pa-<br>rameters for MnasNet (black) and Attention-Augmented-MnasNet<br>(red) with width multipliers 0.75, 1.0, 1.25 and 1.4.</caption>",
            "id": 73,
            "page": 6,
            "text": "Figure 3. ImageNet top-1 accuracy as a function of number of pa-\nrameters for MnasNet (black) and Attention-Augmented-MnasNet\n(red) with width multipliers 0.75, 1.0, 1.25 and 1.4."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1330
                },
                {
                    "x": 2278,
                    "y": 1330
                },
                {
                    "x": 2278,
                    "y": 1626
                },
                {
                    "x": 1278,
                    "y": 1626
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:16px'>we verify in Figure 3 that the accuracy improvements are<br>not just explained by the parameter increase. Additionally,<br>we note that the MnasNet architecture employs Squeeze-<br>and-Excitation at multiple locations that were optimally se-<br>lected via architecture search, further suggesting the bene-<br>fits of our method.</p>",
            "id": 74,
            "page": 6,
            "text": "we verify in Figure 3 that the accuracy improvements are\nnot just explained by the parameter increase. Additionally,\nwe note that the MnasNet architecture employs Squeeze-\nand-Excitation at multiple locations that were optimally se-\nlected via architecture search, further suggesting the bene-\nfits of our method."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1666
                },
                {
                    "x": 2096,
                    "y": 1666
                },
                {
                    "x": 2096,
                    "y": 1717
                },
                {
                    "x": 1280,
                    "y": 1717
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:22px'>4.4. Object Detection with COCO dataset</p>",
            "id": 75,
            "page": 6,
            "text": "4.4. Object Detection with COCO dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1749
                },
                {
                    "x": 2278,
                    "y": 1749
                },
                {
                    "x": 2278,
                    "y": 2144
                },
                {
                    "x": 1278,
                    "y": 2144
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>We next investigate the use of Attention Augmentation<br>on the task of object detection on the COCO dataset [27].<br>We employ the RetinaNet architecture with a ResNet-50<br>and ResNet-101 backbone as done in [26], using the open-<br>sourced RetinaNet codebase.3 We apply Attention Aug-<br>mentation uniquely on the ResNet backbone, modifying<br>them similarly as in our ImageNet classification experi-<br>ments.</p>",
            "id": 76,
            "page": 6,
            "text": "We next investigate the use of Attention Augmentation\non the task of object detection on the COCO dataset [27].\nWe employ the RetinaNet architecture with a ResNet-50\nand ResNet-101 backbone as done in [26], using the open-\nsourced RetinaNet codebase.3 We apply Attention Aug-\nmentation uniquely on the ResNet backbone, modifying\nthem similarly as in our ImageNet classification experi-\nments."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2152
                },
                {
                    "x": 2278,
                    "y": 2152
                },
                {
                    "x": 2278,
                    "y": 2850
                },
                {
                    "x": 1276,
                    "y": 2850
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:18px'>Our relative self-attention mechanism improves the per-<br>formance of the RetinaNet on both ResNet-50 and ResNet-<br>101 as shown in Table 5. Most notably, Attention Aug-<br>mentation yields a 1.4% mAP improvement over a strong<br>RetinaNet baseline from [26]. In contrast to the success<br>of Squeeze-and-Excitation in image classification with Im-<br>ageNet, our experiments show that adding Squeeze-and-<br>Excitation operators in the backbone network of the Reti-<br>naNet significantly hurts performance, in spite of grid<br>searching over the squeeze ratio 0 E {4, 8, 16}. We hy-<br>pothesize that localization requires precise spatial informa-<br>tion which SE discards during the spatial pooling operation,<br>thereby negatively affecting performance. Self-attention on<br>the other hand maintains spatial information and is likely to</p>",
            "id": 77,
            "page": 6,
            "text": "Our relative self-attention mechanism improves the per-\nformance of the RetinaNet on both ResNet-50 and ResNet-\n101 as shown in Table 5. Most notably, Attention Aug-\nmentation yields a 1.4% mAP improvement over a strong\nRetinaNet baseline from [26]. In contrast to the success\nof Squeeze-and-Excitation in image classification with Im-\nageNet, our experiments show that adding Squeeze-and-\nExcitation operators in the backbone network of the Reti-\nnaNet significantly hurts performance, in spite of grid\nsearching over the squeeze ratio 0 E {4, 8, 16}. We hy-\npothesize that localization requires precise spatial informa-\ntion which SE discards during the spatial pooling operation,\nthereby negatively affecting performance. Self-attention on\nthe other hand maintains spatial information and is likely to"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2892
                },
                {
                    "x": 2266,
                    "y": 2892
                },
                {
                    "x": 2266,
                    "y": 2973
                },
                {
                    "x": 1277,
                    "y": 2973
                }
            ],
            "category": "footer",
            "html": "<footer id='78' style='font-size:16px'>3https: //github . com/tensorflow/tpu/tree/master/<br>models/official/retinanet</footer>",
            "id": 78,
            "page": 6,
            "text": "3https: //github . com/tensorflow/tpu/tree/master/\nmodels/official/retinanet"
        },
        {
            "bounding_box": [
                {
                    "x": 561,
                    "y": 291
                },
                {
                    "x": 1919,
                    "y": 291
                },
                {
                    "x": 1919,
                    "y": 661
                },
                {
                    "x": 561,
                    "y": 661
                }
            ],
            "category": "table",
            "html": "<table id='79' style='font-size:20px'><tr><td>Backbone architecture</td><td>GFlops</td><td>Params</td><td>mAPcoco</td><td>mAP50</td><td>mAP75</td></tr><tr><td>ResNet-50 [26]</td><td>182</td><td>33.4M</td><td>36.8</td><td>54.5</td><td>39.5</td></tr><tr><td>SE-ResNet-50 [17]</td><td>183</td><td>35.9M</td><td>36.5</td><td>54.0</td><td>39.1</td></tr><tr><td>AA-ResNet-50 (ours)</td><td>182</td><td>33.1M</td><td>38.2</td><td>56.5</td><td>40.7</td></tr><tr><td>ResNet-101 [26]</td><td>243</td><td>52.4M</td><td>38.5</td><td>56.4</td><td>41.2</td></tr><tr><td>SE-ResNet-101 [17]</td><td>243</td><td>57.2M</td><td>37.4</td><td>55.0</td><td>39.9</td></tr><tr><td>AA-ResNet-101 (ours)</td><td>245</td><td>51.7M</td><td>39.2</td><td>57.8</td><td>41.9</td></tr></table>",
            "id": 79,
            "page": 7,
            "text": "Backbone architecture GFlops Params mAPcoco mAP50 mAP75\n ResNet-50 [26] 182 33.4M 36.8 54.5 39.5\n SE-ResNet-50 [17] 183 35.9M 36.5 54.0 39.1\n AA-ResNet-50 (ours) 182 33.1M 38.2 56.5 40.7\n ResNet-101 [26] 243 52.4M 38.5 56.4 41.2\n SE-ResNet-101 [17] 243 57.2M 37.4 55.0 39.9\n AA-ResNet-101 (ours) 245 51.7M 39.2 57.8"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 690
                },
                {
                    "x": 2276,
                    "y": 690
                },
                {
                    "x": 2276,
                    "y": 785
                },
                {
                    "x": 201,
                    "y": 785
                }
            ],
            "category": "caption",
            "html": "<caption id='80' style='font-size:16px'>Table 5. Object detection on the COCO dataset [27] using the RetinaNet architecture [26] with different backbone architectures. We report<br>mean Average Precision at three different IoU values.</caption>",
            "id": 80,
            "page": 7,
            "text": "Table 5. Object detection on the COCO dataset [27] using the RetinaNet architecture [26] with different backbone architectures. We report\nmean Average Precision at three different IoU values."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 867
                },
                {
                    "x": 1200,
                    "y": 867
                },
                {
                    "x": 1200,
                    "y": 1116
                },
                {
                    "x": 200,
                    "y": 1116
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:20px'>be able to identify object boundaries successfully. Visual-<br>izations of attention maps (See Figures 9 and 10 in the Ap-<br>pendix) reveal that some heads are indeed delineating ob-<br>jects from their background which might be important for<br>localization.</p>",
            "id": 81,
            "page": 7,
            "text": "be able to identify object boundaries successfully. Visual-\nizations of attention maps (See Figures 9 and 10 in the Ap-\npendix) reveal that some heads are indeed delineating ob-\njects from their background which might be important for\nlocalization."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1184
                },
                {
                    "x": 586,
                    "y": 1184
                },
                {
                    "x": 586,
                    "y": 1238
                },
                {
                    "x": 201,
                    "y": 1238
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:22px'>4.5. Ablation Study</p>",
            "id": 82,
            "page": 7,
            "text": "4.5. Ablation Study"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1272
                },
                {
                    "x": 1200,
                    "y": 1272
                },
                {
                    "x": 1200,
                    "y": 2175
                },
                {
                    "x": 199,
                    "y": 2175
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:18px'>Fully-attentional vision models: In this section, we in-<br>vestigate the performance of Attention Augmentation as a<br>function of the fraction of attentional channels. As we in-<br>crease this fraction to 100%, we begin to replace a Con-<br>vNet with a fully attentional model, only leaving pointwise<br>convolutions and the stem unchanged. Table 6 presents the<br>performance of Attention Augmentation on the ResNet-50<br>architecture for varying ratios K=U E {0.25, 0.5, 0.75, 1.0}.<br>Performance slightly degrades as the ratio of attentional<br>channels increases, which we hypothesize is partly ex-<br>plained by the average pooling operation for downsampling<br>at the first stage where Attention Augmentation is applied.<br>Attention Augmentation proves however quite robust to the<br>fraction of attentional channels. For instance, AA-ResNet-<br>50 with k=v=0.75 outperforms its ResNet-50 counterpart,<br>while being more parameter and flops efficient, indicating<br>that mostly employing attentional channels is readily com-<br>petitive.</p>",
            "id": 83,
            "page": 7,
            "text": "Fully-attentional vision models: In this section, we in-\nvestigate the performance of Attention Augmentation as a\nfunction of the fraction of attentional channels. As we in-\ncrease this fraction to 100%, we begin to replace a Con-\nvNet with a fully attentional model, only leaving pointwise\nconvolutions and the stem unchanged. Table 6 presents the\nperformance of Attention Augmentation on the ResNet-50\narchitecture for varying ratios K=U E {0.25, 0.5, 0.75, 1.0}.\nPerformance slightly degrades as the ratio of attentional\nchannels increases, which we hypothesize is partly ex-\nplained by the average pooling operation for downsampling\nat the first stage where Attention Augmentation is applied.\nAttention Augmentation proves however quite robust to the\nfraction of attentional channels. For instance, AA-ResNet-\n50 with k=v=0.75 outperforms its ResNet-50 counterpart,\nwhile being more parameter and flops efficient, indicating\nthat mostly employing attentional channels is readily com-\npetitive."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2183
                },
                {
                    "x": 1200,
                    "y": 2183
                },
                {
                    "x": 1200,
                    "y": 2733
                },
                {
                    "x": 199,
                    "y": 2733
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:18px'>Perhaps surprisingly, these experiments also reveal that<br>our proposed self-attention mechanism is a powerful stand-<br>alone computational primitive for image classification and<br>that fully attentional models are viable for discriminative vi-<br>sual tasks. In particular, AA-ResNet-50 with K=v=1, which<br>uses exclusively attentional channels, is only 2.5% worse<br>in accuracy than its fully convolutional counterpart, in spite<br>of downsampling with average pooling and having 25% less<br>parameters. Notably, this fully attentional architecture also<br>outperforms ResNet-34 while being more parameter and<br>flops efficient (see Table 6).</p>",
            "id": 84,
            "page": 7,
            "text": "Perhaps surprisingly, these experiments also reveal that\nour proposed self-attention mechanism is a powerful stand-\nalone computational primitive for image classification and\nthat fully attentional models are viable for discriminative vi-\nsual tasks. In particular, AA-ResNet-50 with K=v=1, which\nuses exclusively attentional channels, is only 2.5% worse\nin accuracy than its fully convolutional counterpart, in spite\nof downsampling with average pooling and having 25% less\nparameters. Notably, this fully attentional architecture also\noutperforms ResNet-34 while being more parameter and\nflops efficient (see Table 6)."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2812
                },
                {
                    "x": 1200,
                    "y": 2812
                },
                {
                    "x": 1200,
                    "y": 2972
                },
                {
                    "x": 199,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:14px'>4We consider pointwise convolutions as dense layers. This architecture<br>employs 4 non-pointwise convolutions in the stem and the first stage of the<br>architecture, but we believe such operations can be replaced by attention<br>too.</p>",
            "id": 85,
            "page": 7,
            "text": "4We consider pointwise convolutions as dense layers. This architecture\nemploys 4 non-pointwise convolutions in the stem and the first stage of the\narchitecture, but we believe such operations can be replaced by attention\ntoo."
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 854
                },
                {
                    "x": 2244,
                    "y": 854
                },
                {
                    "x": 2244,
                    "y": 1226
                },
                {
                    "x": 1311,
                    "y": 1226
                }
            ],
            "category": "table",
            "html": "<br><table id='86' style='font-size:16px'><tr><td>Architecture</td><td>GFlops</td><td>Params</td><td>top-1</td><td>top-5</td></tr><tr><td>ResNet-34 [14]</td><td>7.4</td><td>21.8M</td><td>73.6</td><td>91.5</td></tr><tr><td>ResNet-50 [14]</td><td>8.2</td><td>25.6M</td><td>76.4</td><td>93.1</td></tr><tr><td>K = v = 0.25</td><td>7.9</td><td>24.3M</td><td>77.7</td><td>93.8</td></tr><tr><td>K = v = 0.5</td><td>7.3</td><td>22.3M</td><td>77.3</td><td>93.6</td></tr><tr><td>K = v = 0.75</td><td>6.8</td><td>20.7M</td><td>76.7</td><td>93.2</td></tr><tr><td>K = v = 1.0</td><td>6.3</td><td>19.4M</td><td>73.9</td><td>91.5</td></tr></table>",
            "id": 86,
            "page": 7,
            "text": "Architecture GFlops Params top-1 top-5\n ResNet-34 [14] 7.4 21.8M 73.6 91.5\n ResNet-50 [14] 8.2 25.6M 76.4 93.1\n K = v = 0.25 7.9 24.3M 77.7 93.8\n K = v = 0.5 7.3 22.3M 77.3 93.6\n K = v = 0.75 6.8 20.7M 76.7 93.2\n K = v = 1.0 6.3 19.4M 73.9"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1257
                },
                {
                    "x": 2276,
                    "y": 1257
                },
                {
                    "x": 2276,
                    "y": 1345
                },
                {
                    "x": 1282,
                    "y": 1345
                }
            ],
            "category": "caption",
            "html": "<caption id='87' style='font-size:18px'>Table 6. Attention Augmented ResNet-50 with varying ratios of<br>attentional channels.</caption>",
            "id": 87,
            "page": 7,
            "text": "Table 6. Attention Augmented ResNet-50 with varying ratios of\nattentional channels."
        },
        {
            "bounding_box": [
                {
                    "x": 1320,
                    "y": 1421
                },
                {
                    "x": 2259,
                    "y": 1421
                },
                {
                    "x": 2259,
                    "y": 2133
                },
                {
                    "x": 1320,
                    "y": 2133
                }
            ],
            "category": "figure",
            "html": "<figure><img id='88' style='font-size:14px' alt=\"80\n78 77.7%\n77.3%\n76.7%\n77.5%\naccuracy\n76 76.4%\n76.6%\n75.4%\n73.9%\n74\ntop-1\n72\nno position 71.1%\n70 with position\n0 25 50 75 100\nfraction of attention versus convolutional channels\" data-coord=\"top-left:(1320,1421); bottom-right:(2259,2133)\" /></figure>",
            "id": 88,
            "page": 7,
            "text": "80\n78 77.7%\n77.3%\n76.7%\n77.5%\naccuracy\n76 76.4%\n76.6%\n75.4%\n73.9%\n74\ntop-1\n72\nno position 71.1%\n70 with position\n0 25 50 75 100\nfraction of attention versus convolutional channels"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2195
                },
                {
                    "x": 2278,
                    "y": 2195
                },
                {
                    "x": 2278,
                    "y": 2335
                },
                {
                    "x": 1279,
                    "y": 2335
                }
            ],
            "category": "caption",
            "html": "<caption id='89' style='font-size:16px'>Figure 4. Effect of relative position embeddings as the ratio<br>of attentional channels increases on our Attention-Augmented<br>ResNet50.</caption>",
            "id": 89,
            "page": 7,
            "text": "Figure 4. Effect of relative position embeddings as the ratio\nof attentional channels increases on our Attention-Augmented\nResNet50."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2419
                },
                {
                    "x": 2280,
                    "y": 2419
                },
                {
                    "x": 2280,
                    "y": 2922
                },
                {
                    "x": 1278,
                    "y": 2922
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:20px'>Importance of position encodings: In Figure 4, we show<br>the effect of our proposed two-dimensional relative posi-<br>tion encodings as a function of the fraction of attentional<br>channels. As expected, experiments demonstrate that our<br>relative position encodings become increasingly more im-<br>portant as the architecture employs more attentional chan-<br>nels. In particular, the fully self-attentional ResNet-50 gains<br>2.8% top-1 ImageNet accuracy when using relative position<br>encodings, which indicates the necessity of maintaining po-<br>sition information for fully self-attentional vision models.</p>",
            "id": 90,
            "page": 7,
            "text": "Importance of position encodings: In Figure 4, we show\nthe effect of our proposed two-dimensional relative posi-\ntion encodings as a function of the fraction of attentional\nchannels. As expected, experiments demonstrate that our\nrelative position encodings become increasingly more im-\nportant as the architecture employs more attentional chan-\nnels. In particular, the fully self-attentional ResNet-50 gains\n2.8% top-1 ImageNet accuracy when using relative position\nencodings, which indicates the necessity of maintaining po-\nsition information for fully self-attentional vision models."
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 2929
                },
                {
                    "x": 2275,
                    "y": 2929
                },
                {
                    "x": 2275,
                    "y": 2976
                },
                {
                    "x": 1332,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>We additionally compare our proposed two-dimensional</p>",
            "id": 91,
            "page": 7,
            "text": "We additionally compare our proposed two-dimensional"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 291
                },
                {
                    "x": 1180,
                    "y": 291
                },
                {
                    "x": 1180,
                    "y": 761
                },
                {
                    "x": 221,
                    "y": 761
                }
            ],
            "category": "table",
            "html": "<table id='92' style='font-size:16px'><tr><td>Architecture</td><td>Position Encodings</td><td>top-1</td><td>top-5</td></tr><tr><td>AA-ResNet-34</td><td>None</td><td>74.4</td><td>91.9</td></tr><tr><td>AA-ResNet-34</td><td>2d Sine</td><td>74.4</td><td>92.0</td></tr><tr><td>AA-ResNet-34</td><td>CoordConv</td><td>74.4</td><td>92.0</td></tr><tr><td>AA-ResNet-34</td><td>Relative (ours)</td><td>74.7</td><td>92.0</td></tr><tr><td>AA-ResNet-50</td><td>None</td><td>77.5</td><td>93.7</td></tr><tr><td>AA-ResNet-50</td><td>2d Sine</td><td>77.5</td><td>93.7</td></tr><tr><td>AA-ResNet-50</td><td>CoordConv</td><td>77.5</td><td>93.8</td></tr><tr><td>AA-ResNet-50</td><td>Relative (ours)</td><td>77.7</td><td>93.8</td></tr></table>",
            "id": 92,
            "page": 8,
            "text": "Architecture Position Encodings top-1 top-5\n AA-ResNet-34 None 74.4 91.9\n AA-ResNet-34 2d Sine 74.4 92.0\n AA-ResNet-34 CoordConv 74.4 92.0\n AA-ResNet-34 Relative (ours) 74.7 92.0\n AA-ResNet-50 None 77.5 93.7\n AA-ResNet-50 2d Sine 77.5 93.7\n AA-ResNet-50 CoordConv 77.5 93.8\n AA-ResNet-50 Relative (ours) 77.7"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 793
                },
                {
                    "x": 1194,
                    "y": 793
                },
                {
                    "x": 1194,
                    "y": 883
                },
                {
                    "x": 204,
                    "y": 883
                }
            ],
            "category": "caption",
            "html": "<caption id='93' style='font-size:14px'>Table 7. Effects of different position encodings in Attention Aug-<br>mentation on ImageNet classification.</caption>",
            "id": 93,
            "page": 8,
            "text": "Table 7. Effects of different position encodings in Attention Aug-\nmentation on ImageNet classification."
        },
        {
            "bounding_box": [
                {
                    "x": 228,
                    "y": 928
                },
                {
                    "x": 1176,
                    "y": 928
                },
                {
                    "x": 1176,
                    "y": 1141
                },
                {
                    "x": 228,
                    "y": 1141
                }
            ],
            "category": "table",
            "html": "<table id='94' style='font-size:20px'><tr><td>Position Encodings</td><td>mAPcoco</td><td>mAP50</td><td>mAP75</td></tr><tr><td>None</td><td>37.7</td><td>56.0</td><td>40.2</td></tr><tr><td>CoordConv [29]</td><td>37.4</td><td>55.5</td><td>40.1</td></tr><tr><td>Relative (ours)</td><td>38.2</td><td>56.5</td><td>40.7</td></tr></table>",
            "id": 94,
            "page": 8,
            "text": "Position Encodings mAPcoco mAP50 mAP75\n None 37.7 56.0 40.2\n CoordConv [29] 37.4 55.5 40.1\n Relative (ours) 38.2 56.5"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1176
                },
                {
                    "x": 1198,
                    "y": 1176
                },
                {
                    "x": 1198,
                    "y": 1312
                },
                {
                    "x": 203,
                    "y": 1312
                }
            ],
            "category": "caption",
            "html": "<caption id='95' style='font-size:14px'>Table 8. Effects of different position encodings in Attention Aug-<br>mentation on the COCO object detection task using a RetinaNet<br>AA-ResNet-50 backbone.</caption>",
            "id": 95,
            "page": 8,
            "text": "Table 8. Effects of different position encodings in Attention Aug-\nmentation on the COCO object detection task using a RetinaNet\nAA-ResNet-50 backbone."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1395
                },
                {
                    "x": 1200,
                    "y": 1395
                },
                {
                    "x": 1200,
                    "y": 1891
                },
                {
                    "x": 201,
                    "y": 1891
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:16px'>relative position encodings to other position encoding<br>schemes. We apply Attention Augmentation using the same<br>hyperparameters as 4.2 with the following different posi-<br>tion encoding schemes: 1) The position-unaware version of<br>self-attention (referred to as None), 2) a two-dimensional<br>implementation of the sinusoidal positional waves (referred<br>to as 2d Sine) as used in [32], 3) CoordConv [29] for which<br>we concatenate (x,y,r) coordinate channels to the inputs of<br>the attention function, and 4) our proposed two-dimensional<br>relative position encodings (referred to as Relative).</p>",
            "id": 96,
            "page": 8,
            "text": "relative position encodings to other position encoding\nschemes. We apply Attention Augmentation using the same\nhyperparameters as 4.2 with the following different posi-\ntion encoding schemes: 1) The position-unaware version of\nself-attention (referred to as None), 2) a two-dimensional\nimplementation of the sinusoidal positional waves (referred\nto as 2d Sine) as used in [32], 3) CoordConv [29] for which\nwe concatenate (x,y,r) coordinate channels to the inputs of\nthe attention function, and 4) our proposed two-dimensional\nrelative position encodings (referred to as Relative)."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1895
                },
                {
                    "x": 1199,
                    "y": 1895
                },
                {
                    "x": 1199,
                    "y": 2492
                },
                {
                    "x": 199,
                    "y": 2492
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:16px'>In Table 7 and 8, we present the results on ImageNet<br>classification and the COCO object detection task respec-<br>tively. On both tasks, Attention Augmentation without po-<br>sition encodings already yields improvements over the fully<br>convolutional non-augmented variants. Our experiments<br>also reveal that the sinusoidal encodings and the coordinate<br>convolution do not provide improvements over the position-<br>unaware version of Attention Augmentation. We obtain ad-<br>ditional improvements when using our two-dimensional rel-<br>ative attention, demonstrating the utility of preserving trans-<br>lation equivariance while preventing permutation equivari-<br>ance.</p>",
            "id": 97,
            "page": 8,
            "text": "In Table 7 and 8, we present the results on ImageNet\nclassification and the COCO object detection task respec-\ntively. On both tasks, Attention Augmentation without po-\nsition encodings already yields improvements over the fully\nconvolutional non-augmented variants. Our experiments\nalso reveal that the sinusoidal encodings and the coordinate\nconvolution do not provide improvements over the position-\nunaware version of Attention Augmentation. We obtain ad-\nditional improvements when using our two-dimensional rel-\native attention, demonstrating the utility of preserving trans-\nlation equivariance while preventing permutation equivari-\nance."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2540
                },
                {
                    "x": 846,
                    "y": 2540
                },
                {
                    "x": 846,
                    "y": 2596
                },
                {
                    "x": 200,
                    "y": 2596
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:20px'>5. Discussion and future work</p>",
            "id": 98,
            "page": 8,
            "text": "5. Discussion and future work"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2628
                },
                {
                    "x": 1200,
                    "y": 2628
                },
                {
                    "x": 1200,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:16px'>In this work, we consider the use of self-attention for vi-<br>sion models as an alternative to convolutions. We introduce<br>a novel two-dimensional relative self-attention mechanism<br>for images that enables training of competitive fully self-<br>attentional vision models on image classification for the first<br>time. We propose to augment convolutional operators with<br>this self-attention mechanism and validate the superiority of</p>",
            "id": 99,
            "page": 8,
            "text": "In this work, we consider the use of self-attention for vi-\nsion models as an alternative to convolutions. We introduce\na novel two-dimensional relative self-attention mechanism\nfor images that enables training of competitive fully self-\nattentional vision models on image classification for the first\ntime. We propose to augment convolutional operators with\nthis self-attention mechanism and validate the superiority of"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 307
                },
                {
                    "x": 2278,
                    "y": 307
                },
                {
                    "x": 2278,
                    "y": 552
                },
                {
                    "x": 1278,
                    "y": 552
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:16px'>this approach over other attention schemes. Extensive ex-<br>periments show that Attention Augmentation leads to sys-<br>tematic improvements on both image classification and ob-<br>ject detection tasks across a wide range of architectures and<br>computational settings.</p>",
            "id": 100,
            "page": 8,
            "text": "this approach over other attention schemes. Extensive ex-\nperiments show that Attention Augmentation leads to sys-\ntematic improvements on both image classification and ob-\nject detection tasks across a wide range of architectures and\ncomputational settings."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 558
                },
                {
                    "x": 2280,
                    "y": 558
                },
                {
                    "x": 2280,
                    "y": 1450
                },
                {
                    "x": 1276,
                    "y": 1450
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:16px'>Several open questions from this work remain. In fu-<br>ture work, we will focus on the fully attentional regime<br>and explore how different attention mechanisms trade off<br>computational efficiency versus representational power. For<br>instance, identifying a local attention mechanism may re-<br>sult in an efficient and scalable computational mechanism<br>that could prevent the need for downsampling with average<br>pooling [34]. Additionally, it is plausible that architectural<br>design choices that are well suited when exclusively relying<br>on convolutions are suboptimal when using self-attention<br>mechanisms. As such, it would be interesting to see if us-<br>ing Attention Augmentation as a primitive in automated ar-<br>chitecture search procedures proves useful to find even bet-<br>ter models than those previously found in image classifica-<br>tion [55], object detection [12], image segmentation [6] and<br>other domains [5, 1 , 35, 8]. Finally, one can ask to which<br>degree fully attentional models can replace convolutional<br>networks for visual tasks.</p>",
            "id": 101,
            "page": 8,
            "text": "Several open questions from this work remain. In fu-\nture work, we will focus on the fully attentional regime\nand explore how different attention mechanisms trade off\ncomputational efficiency versus representational power. For\ninstance, identifying a local attention mechanism may re-\nsult in an efficient and scalable computational mechanism\nthat could prevent the need for downsampling with average\npooling [34]. Additionally, it is plausible that architectural\ndesign choices that are well suited when exclusively relying\non convolutions are suboptimal when using self-attention\nmechanisms. As such, it would be interesting to see if us-\ning Attention Augmentation as a primitive in automated ar-\nchitecture search procedures proves useful to find even bet-\nter models than those previously found in image classifica-\ntion [55], object detection [12], image segmentation [6] and\nother domains [5, 1 , 35, 8]. Finally, one can ask to which\ndegree fully attentional models can replace convolutional\nnetworks for visual tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1497
                },
                {
                    "x": 1703,
                    "y": 1497
                },
                {
                    "x": 1703,
                    "y": 1548
                },
                {
                    "x": 1281,
                    "y": 1548
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='102' style='font-size:22px'>Acknowledgements</p>",
            "id": 102,
            "page": 8,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1578
                },
                {
                    "x": 2278,
                    "y": 1578
                },
                {
                    "x": 2278,
                    "y": 1776
                },
                {
                    "x": 1279,
                    "y": 1776
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:18px'>The authors would like to thank Tsung-Yi Lin, Pra-<br>jit Ramachandran, Mingxing Tan, Yanping Huang and the<br>Google Brain team for insightful comments and discus-<br>sions.</p>",
            "id": 103,
            "page": 8,
            "text": "The authors would like to thank Tsung-Yi Lin, Pra-\njit Ramachandran, Mingxing Tan, Yanping Huang and the\nGoogle Brain team for insightful comments and discus-\nsions."
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 1822
                },
                {
                    "x": 1523,
                    "y": 1822
                },
                {
                    "x": 1523,
                    "y": 1874
                },
                {
                    "x": 1283,
                    "y": 1874
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:20px'>References</p>",
            "id": 104,
            "page": 8,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1298,
                    "y": 1903
                },
                {
                    "x": 2279,
                    "y": 1903
                },
                {
                    "x": 2279,
                    "y": 2978
                },
                {
                    "x": 1298,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:14px'>[1] Maximilian Alber, Irwan Bello, Barret Zoph, Pieter-Jan Kin-<br>dermans, Prajit Ramachandran, and Quoc V. Le. Backprop<br>evolution. CoRR, abs/1808.02822, 2018. 8<br>[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.<br>Neural machine translation by jointly learning to align and<br>translate. In International Conference on Learning Repre-<br>sentations, 2015. 2<br>[3] Irwan Bello, Sayali Kulkarni, Sagar Jain, Craig Boutilier,<br>Ed Huai-hsin Chi, Elad Eban, Xiyang Luo, Alan Mackey,<br>and Ofer Meshi. Seq2slate: Re-ranking and slate optimiza-<br>tion with rnns. CoRR, abs/1810.02019, 2018. 2<br>[4] Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi,<br>and Samy Bengio. Neural combinatorial optimization with<br>reinforcement learning. 2016. 2<br>[5] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le.<br>Neural optimizer search with reinforcement learning. In Pro-<br>ceedings of the 34th International Conference on Machine<br>Learning - Volume 70, ICML'17, pages 459-468. JMLR.org,<br>2017. 8<br>[6] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George<br>Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam,<br>and Jon Shlens. Searching for efficient multi-scale archi-<br>tectures for dense image prediction. In Advances in Neural</p>",
            "id": 105,
            "page": 8,
            "text": "[1] Maximilian Alber, Irwan Bello, Barret Zoph, Pieter-Jan Kin-\ndermans, Prajit Ramachandran, and Quoc V. Le. Backprop\nevolution. CoRR, abs/1808.02822, 2018. 8\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In International Conference on Learning Repre-\nsentations, 2015. 2\n[3] Irwan Bello, Sayali Kulkarni, Sagar Jain, Craig Boutilier,\nEd Huai-hsin Chi, Elad Eban, Xiyang Luo, Alan Mackey,\nand Ofer Meshi. Seq2slate: Re-ranking and slate optimiza-\ntion with rnns. CoRR, abs/1810.02019, 2018. 2\n[4] Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi,\nand Samy Bengio. Neural combinatorial optimization with\nreinforcement learning. 2016. 2\n[5] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le.\nNeural optimizer search with reinforcement learning. In Pro-\nceedings of the 34th International Conference on Machine\nLearning - Volume 70, ICML'17, pages 459-468. JMLR.org,\n2017. 8\n[6] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George\nPapandreou, Barret Zoph, Florian Schroff, Hartwig Adam,\nand Jon Shlens. Searching for efficient multi-scale archi-\ntectures for dense image prediction. In Advances in Neural"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 310
                },
                {
                    "x": 1200,
                    "y": 310
                },
                {
                    "x": 1200,
                    "y": 394
                },
                {
                    "x": 286,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:18px'>Information Processing Systems, pages 8713-8724, 2018. 2,<br>8</p>",
            "id": 106,
            "page": 9,
            "text": "Information Processing Systems, pages 8713-8724, 2018. 2,\n8"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 400
                },
                {
                    "x": 1199,
                    "y": 400
                },
                {
                    "x": 1199,
                    "y": 2975
                },
                {
                    "x": 204,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:14px'>[7] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng<br>Yan, and Jiashi Feng. A2 -nets: Double attention networks.<br>CoRR, abs/1810.11579, 2018. 3<br>[8] Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Va-<br>sudevan, and Quoc V. Le. Autoaugment: Learning augmen-<br>tation policies from data. CoRR, abs/1805.09501, 2018. 8<br>[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,<br>and Li Fei-Fei. Imagenet: A large-scale hierarchical image<br>database. In IEEE Conference on Computer Vision and Pat-<br>tern Recognition. IEEE, 2009. 1, 2, 5<br>[10] Xavier Gastaldi. Shake-shake regularization. arXiv preprint<br>arXiv:1705.07485, 2017. 11<br>[11] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:<br>A regularization method for convolutional networks. In<br>Advances in Neural Information Processing Systems, pages<br>10750-10760, 2018. 11<br>[12] Golnaz Ghiasi, Tsung- Yi Lin, Ruoming Pang, and Quoc V<br>Le. NAS-FPN: Learning scalable feature pyramid architec-<br>ture for object detection. In The IEEE Conference on Com-<br>puter Vision and Pattern Recognition (CVPR), June 2019. 8<br>[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In IEEE Con-<br>ference on Computer Vision and Pattern Recognition, 2016.<br>2, 5<br>[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Identity mappings in deep residual networks. In European<br>Conference on Computer Vision, 2016. 1, 2, 5, 7, 11<br>[15] Sepp Hochreiter and Juergen Schmidhuber. Long short-term<br>memory. Neural Computation, 1997. 2<br>[16] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea<br>Vedaldi. Gather-excite: Exploiting feature context in convo-<br>lutional neural networks. In Advances in Neural Information<br>Processing Systems, pages 9423-9433, 2018. 3, 4, 5<br>[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-<br>works. In Proceedings of the IEEE Conference on Computer<br>Vision and Pattern Recognition, 2018. 1, 2, 3, 4, 5, 6, 7<br>[18] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-<br>reit, Noam Shazeer, Curtis Hawthorne, Andrew M Dai,<br>Matthew D Hoffman, and Douglas Eck. Music transformer.<br>In Advances in Neural Processing Systems, 2018. 3, 4<br>[19] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,<br>Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-<br>jna, Yang Song, Sergio Guadarrama, et al. Speed/accu-<br>racy trade-offs for modern convolutional object detectors. In<br>IEEE Conference on Computer Vision and Pattern Recogni-<br>tion, 2017. 2<br>[20] Sergey Ioffe and Christian Szegedy. Batch normalization:<br>Accelerating deep network training by reducing internal CO-<br>variate shift. In International Conference on Learning Rep-<br>resentations, 2015. 4<br>[21] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do bet-<br>ter imagenet models transfer better? In Proceedings of the<br>IEEE Conference on Computer Vision and Pattern Recogni-<br>tion, 2019. 2, 5</p>",
            "id": 107,
            "page": 9,
            "text": "[7] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\nYan, and Jiashi Feng. A2 -nets: Double attention networks.\nCoRR, abs/1810.11579, 2018. 3\n[8] Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Va-\nsudevan, and Quoc V. Le. Autoaugment: Learning augmen-\ntation policies from data. CoRR, abs/1805.09501, 2018. 8\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In IEEE Conference on Computer Vision and Pat-\ntern Recognition. IEEE, 2009. 1, 2, 5\n[10] Xavier Gastaldi. Shake-shake regularization. arXiv preprint\narXiv:1705.07485, 2017. 11\n[11] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:\nA regularization method for convolutional networks. In\nAdvances in Neural Information Processing Systems, pages\n10750-10760, 2018. 11\n[12] Golnaz Ghiasi, Tsung- Yi Lin, Ruoming Pang, and Quoc V\nLe. NAS-FPN: Learning scalable feature pyramid architec-\nture for object detection. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2019. 8\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In IEEE Con-\nference on Computer Vision and Pattern Recognition, 2016.\n2, 5\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In European\nConference on Computer Vision, 2016. 1, 2, 5, 7, 11\n[15] Sepp Hochreiter and Juergen Schmidhuber. Long short-term\nmemory. Neural Computation, 1997. 2\n[16] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\nVedaldi. Gather-excite: Exploiting feature context in convo-\nlutional neural networks. In Advances in Neural Information\nProcessing Systems, pages 9423-9433, 2018. 3, 4, 5\n[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2018. 1, 2, 3, 4, 5, 6, 7\n[18] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-\nreit, Noam Shazeer, Curtis Hawthorne, Andrew M Dai,\nMatthew D Hoffman, and Douglas Eck. Music transformer.\nIn Advances in Neural Processing Systems, 2018. 3, 4\n[19] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,\nAnoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-\njna, Yang Song, Sergio Guadarrama, et al. Speed/accu-\nracy trade-offs for modern convolutional object detectors. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2017. 2\n[20] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal CO-\nvariate shift. In International Conference on Learning Rep-\nresentations, 2015. 4\n[21] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do bet-\nter imagenet models transfer better? In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2019. 2, 5"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 305
                },
                {
                    "x": 2288,
                    "y": 305
                },
                {
                    "x": 2288,
                    "y": 2980
                },
                {
                    "x": 1275,
                    "y": 2980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:14px'>[22] Alex Krizhevsky. Learning multiple layers of features from<br>tiny images. Technical report, University of Toronto, 2009.<br>2, 5<br>[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.<br>Imagenet classification with deep convolutional neural net-<br>works. In Advances in Neural Information Processing Sys-<br>tem, 2012. 1, 2<br>[24] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick<br>Haffner. Gradient-based learning applied to document recog-<br>nition. Proceedings of the IEEE, 1998. 1<br>[25] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,<br>Bharath Hariharan, and Serge Belongie. Feature pyramid<br>networks for object detection. In Proceedings of the IEEE<br>Conference on Computer Vision and Pattern Recognition,<br>2017. 5<br>[26] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and<br>Piotr Dollar. Focal loss for dense object detection. In Pro-<br>ceedings of the IEEE international conference on computer<br>vision, pages 2980-2988, 2017. 6, 7, 11<br>[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,<br>Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence<br>Zitnick. Microsoft COCO: Common objects in context. In<br>European Conference on Computer Vision, pages 740-755.<br>Springer, 2014. 2, 6, 7<br>[28] Drew Linsley, Dan Scheibler, Sven Eberhardt, and Thomas<br>Serre. Global-and-local attention networks for visual recog-<br>nition. CoRR, abs/1805.08819, 2018. 5<br>[29] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski<br>Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An<br>intriguing failing of convolutional neural networks and the<br>coordconv solution. In Advances in Neural Information Pro-<br>cessing Systems, pages 9628-9639, 2018. 3, 8<br>[30] Ilya Loshchilov and Frank Hutter. SGDR: Stochas-<br>tic gradient descent with warm restarts. arXiv preprint<br>arXiv:1608.03983, 2016. 11<br>[31] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So<br>Kweon. Bam: bottleneck attention module. In British Ma-<br>chine Vision Conference, 2018. 3, 4, 5<br>[32] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz<br>Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-<br>age transformer. In International Conference on Machine<br>Learning, 2018. 3, 8<br>[33] Andrew Rabinovich, Andrea Vedaldi, Carolina Galleguillos,<br>Eric Wiewiora, and Serge Belongie. Objects in context.<br>2007. 1<br>[34] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan<br>Bello, Anselm Levskaya, and Jonathon Shlens. Stand-alone<br>self-attention in vision models. CoRR, abs/1906.05909,<br>2019. 8<br>[35] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Search-<br>ing for activation functions. CoRR, abs/1710.05941, 2017.<br>8<br>[36] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-<br>moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted<br>residuals and linear bottlenecks. In Proceedings of the IEEE<br>Conference on Computer Vision and Pattern Recognition,<br>pages 4510-4520, 2018. 6</p>",
            "id": 108,
            "page": 9,
            "text": "[22] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. Technical report, University of Toronto, 2009.\n2, 5\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In Advances in Neural Information Processing Sys-\ntem, 2012. 1, 2\n[24] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 1998. 1\n[25] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\n2017. 5\n[26] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Dollar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980-2988, 2017. 6, 7, 11\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEuropean Conference on Computer Vision, pages 740-755.\nSpringer, 2014. 2, 6, 7\n[28] Drew Linsley, Dan Scheibler, Sven Eberhardt, and Thomas\nSerre. Global-and-local attention networks for visual recog-\nnition. CoRR, abs/1805.08819, 2018. 5\n[29] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski\nSuch, Eric Frank, Alex Sergeev, and Jason Yosinski. An\nintriguing failing of convolutional neural networks and the\ncoordconv solution. In Advances in Neural Information Pro-\ncessing Systems, pages 9628-9639, 2018. 3, 8\n[30] Ilya Loshchilov and Frank Hutter. SGDR: Stochas-\ntic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016. 11\n[31] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So\nKweon. Bam: bottleneck attention module. In British Ma-\nchine Vision Conference, 2018. 3, 4, 5\n[32] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International Conference on Machine\nLearning, 2018. 3, 8\n[33] Andrew Rabinovich, Andrea Vedaldi, Carolina Galleguillos,\nEric Wiewiora, and Serge Belongie. Objects in context.\n2007. 1\n[34] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. CoRR, abs/1906.05909,\n2019. 8\n[35] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Search-\ning for activation functions. CoRR, abs/1710.05941, 2017.\n8\n[36] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 4510-4520, 2018. 6"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 290
                },
                {
                    "x": 1201,
                    "y": 290
                },
                {
                    "x": 1201,
                    "y": 2973
                },
                {
                    "x": 200,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:14px'>[37] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-<br>attention with relative position representations. arXiv<br>preprint arXiv:1803.02155, 2018. 2, 3, 4<br>[38] David R. So, Chen Liang, and Quoc V. Le. The evolved<br>transformer. CoRR, abs/1901.11117, 2019. 3<br>[39] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and<br>Alex Alemi. Inception-v4, Inception-Resnet and the impact<br>of residual connections on learning. In International Con-<br>ference on Learning Representations Workshop Track, 2016.<br>2<br>[40] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,<br>Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent<br>Vanhoucke, and Andrew Rabinovich. Going deeper with<br>convolutions. In IEEE Conference on Computer Vision and<br>Pattern Recognition, 2015. 2<br>[41] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon<br>Shlens, and Zbigniew Wojna. Rethinking the Inception ar-<br>chitecture for computer vision. In IEEE Conference on Com-<br>puter Vision and Pattern Recognition, 2016. 2, 11<br>[42] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,<br>and Quoc V Le. Mnasnet: Platform-aware neural architec-<br>ture search for mobile. In Proceedings of the IEEE Confer-<br>ence on Computer Vision and Pattern Recognition, 2018. 2,<br>5, 6, 11<br>[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia<br>Polosukhin. Attention is all you need. In Advances in Neural<br>Information Processing Systems, pages 5998-6008, 2017. 1,<br>2, 3<br>[44] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer<br>networks. In NIPS, pages 2692-2700, 2015. 2<br>[45] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-<br>ing He. Non-local neural networks. In Proceedings of the<br>IEEE Conference on Computer Vision and Pattern Recogni-<br>tion, pages 7794-7803, 2018. 3<br>[46] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In<br>So Kweon. Cbam: Convolutional block attention module.<br>In Proceedings ofthe European Conference on Computer Vi-<br>sion (ECCV), pages 3-19, 2018. 3, 4, 5<br>[47] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and<br>Kaiming He. Aggregated residual transformations for deep<br>neural networks. In Proceedings of the IEEE Conference on<br>Computer Vision and Pattern Recognition, 2017. 2, 5<br>[48] Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba, and<br>Koichi Kise. Shakedrop regularization for deep residual<br>learning. arXiv preprint arXiv:1802.02375, 2018. 11<br>[49] Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S.<br>Chao, and Zhaopeng Tu. Convolutional self-attention net-<br>work. In CoRR, volume abs/1810.13320, 2018. 2<br>[50] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui<br>Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le.<br>QAnet: Combining local convolution with global self-<br>attention for reading comprehension. In International Con-<br>ference on Learning Representations, 2018. 2<br>[51] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-<br>works. In British Machine Vision Conference, 2016. 5</p>",
            "id": 109,
            "page": 10,
            "text": "[37] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. arXiv\npreprint arXiv:1803.02155, 2018. 2, 3, 4\n[38] David R. So, Chen Liang, and Quoc V. Le. The evolved\ntransformer. CoRR, abs/1901.11117, 2019. 3\n[39] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\nAlex Alemi. Inception-v4, Inception-Resnet and the impact\nof residual connections on learning. In International Con-\nference on Learning Representations Workshop Track, 2016.\n2\n[40] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In IEEE Conference on Computer Vision and\nPattern Recognition, 2015. 2\n[41] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the Inception ar-\nchitecture for computer vision. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016. 2, 11\n[42] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\nand Quoc V Le. Mnasnet: Platform-aware neural architec-\nture search for mobile. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 2018. 2,\n5, 6, 11\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998-6008, 2017. 1,\n2, 3\n[44] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer\nnetworks. In NIPS, pages 2692-2700, 2015. 2\n[45] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 7794-7803, 2018. 3\n[46] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In\nSo Kweon. Cbam: Convolutional block attention module.\nIn Proceedings ofthe European Conference on Computer Vi-\nsion (ECCV), pages 3-19, 2018. 3, 4, 5\n[47] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017. 2, 5\n[48] Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba, and\nKoichi Kise. Shakedrop regularization for deep residual\nlearning. arXiv preprint arXiv:1802.02375, 2018. 11\n[49] Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S.\nChao, and Zhaopeng Tu. Convolutional self-attention net-\nwork. In CoRR, volume abs/1810.13320, 2018. 2\n[50] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V Le.\nQAnet: Combining local convolution with global self-\nattention for reading comprehension. In International Con-\nference on Learning Representations, 2018. 2\n[51] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In British Machine Vision Conference, 2016. 5"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 305
                },
                {
                    "x": 2279,
                    "y": 305
                },
                {
                    "x": 2279,
                    "y": 1150
                },
                {
                    "x": 1280,
                    "y": 1150
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:14px'>[52] Vinicius Zambaldi, David Raposo, Adam Santoro, Vic-<br>tor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David<br>Reichert, Timothy Lillicrap, Edward Lockhart, Murray<br>Shanahan, Victoria Langston, Razvan Pascanu, Matthew<br>Botvinick, Oriol Vinyals, and Peter Battaglia. Deep rein-<br>forcement learning with relational inductive biases. In ICLR,<br>2019. 2<br>[53] Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and<br>Augustus Odena. Self-attention generative adversarial net-<br>works. arXiv:1805.08318, 2018. 3<br>[54] Barret Zoph and Quoc V. Le. Neural architecture search<br>with reinforcement learning. In International Conference on<br>Learning Representations, 2017. 6<br>[55] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V<br>Le. Learning transferable architectures for scalable image<br>recognition. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pages 8697-8710,<br>2018. 2, 8, 1 1</p>",
            "id": 110,
            "page": 10,
            "text": "[52] Vinicius Zambaldi, David Raposo, Adam Santoro, Vic-\ntor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David\nReichert, Timothy Lillicrap, Edward Lockhart, Murray\nShanahan, Victoria Langston, Razvan Pascanu, Matthew\nBotvinick, Oriol Vinyals, and Peter Battaglia. Deep rein-\nforcement learning with relational inductive biases. In ICLR,\n2019. 2\n[53] Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and\nAugustus Odena. Self-attention generative adversarial net-\nworks. arXiv:1805.08318, 2018. 3\n[54] Barret Zoph and Quoc V. Le. Neural architecture search\nwith reinforcement learning. In International Conference on\nLearning Representations, 2017. 6\n[55] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8697-8710,\n2018. 2, 8, 1 1"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 301
                },
                {
                    "x": 481,
                    "y": 301
                },
                {
                    "x": 481,
                    "y": 354
                },
                {
                    "x": 204,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:22px'>A. Appendix</p>",
            "id": 111,
            "page": 11,
            "text": "A. Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 381
                },
                {
                    "x": 708,
                    "y": 381
                },
                {
                    "x": 708,
                    "y": 434
                },
                {
                    "x": 203,
                    "y": 434
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='112' style='font-size:20px'>A.1. Experimental details</p>",
            "id": 112,
            "page": 11,
            "text": "A.1. Experimental details"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 461
                },
                {
                    "x": 1200,
                    "y": 461
                },
                {
                    "x": 1200,
                    "y": 813
                },
                {
                    "x": 200,
                    "y": 813
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>Tuning Unless specified otherwise, we use the default hy-<br>perparameters found in reference baseline codebases with-<br>out tuning. K was searched in {0.1, 0.2, 0.5}, v in {0.0, 0.1,<br>0.25, 0.5, 0.75, 1.0} and the number of heads was chosen<br>based on memory constraints (starting from 8 and decreas-<br>ing when necessary). We report the final accuracy for each<br>run without performing early stopping.</p>",
            "id": 113,
            "page": 11,
            "text": "Tuning Unless specified otherwise, we use the default hy-\nperparameters found in reference baseline codebases with-\nout tuning. K was searched in {0.1, 0.2, 0.5}, v in {0.0, 0.1,\n0.25, 0.5, 0.75, 1.0} and the number of heads was chosen\nbased on memory constraints (starting from 8 and decreas-\ning when necessary). We report the final accuracy for each\nrun without performing early stopping."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 859
                },
                {
                    "x": 1199,
                    "y": 859
                },
                {
                    "x": 1199,
                    "y": 1511
                },
                {
                    "x": 200,
                    "y": 1511
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:18px'>CIFAR-100 Given the low resolution of CIFAR-100 im-<br>ages, we do not downsample feature maps before the at-<br>tention operation and instead resort to a smaller batch size.<br>We train all networks for 500 epochs using synchronous<br>SGD with momentum 0.9 distributed across 8 TESLA V100<br>GPUs. The learning rate is linearly scaled from 0 to<br>0.2B /256, where B is the total batch size, for the first 5%<br>training epochs and then annealed with cosine decay [30].<br>We use standard CIFAR preprocessing: mean normalizing,<br>random flipping and cropping [55, 10, 48]. Non-augmented<br>architectures are trained with a batch size of 1024 and a<br>weight decay of 2e-4. Augmented architectures are trained<br>with batch size of 256 and a weight decay of 5e-4.</p>",
            "id": 114,
            "page": 11,
            "text": "CIFAR-100 Given the low resolution of CIFAR-100 im-\nages, we do not downsample feature maps before the at-\ntention operation and instead resort to a smaller batch size.\nWe train all networks for 500 epochs using synchronous\nSGD with momentum 0.9 distributed across 8 TESLA V100\nGPUs. The learning rate is linearly scaled from 0 to\n0.2B /256, where B is the total batch size, for the first 5%\ntraining epochs and then annealed with cosine decay [30].\nWe use standard CIFAR preprocessing: mean normalizing,\nrandom flipping and cropping [55, 10, 48]. Non-augmented\narchitectures are trained with a batch size of 1024 and a\nweight decay of 2e-4. Augmented architectures are trained\nwith batch size of 256 and a weight decay of 5e-4."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1557
                },
                {
                    "x": 1199,
                    "y": 1557
                },
                {
                    "x": 1199,
                    "y": 2111
                },
                {
                    "x": 201,
                    "y": 2111
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:18px'>ImageNet classification with ResNet We train all<br>ResNet architectures for 100 epochs using synchronous<br>SGD with momentum 0.9 across 8 TESLA V100 GPUs and<br>weight decay of 1e-4. We use the largest batch size per<br>worker B E {32, 64, 128, 256} that fits in a minibatch. The<br>initial learning rate is scaled linearly according to the total<br>batch size using a base learning rate of 0.128 for total batch<br>size of 256. During training, we linearly scale the learning<br>rate from 0 to this value for the first 5% of training epochs<br>and divide it by 10 at epochs 30, 60, 80 and 90. We use<br>standard Inception data augmentation as described in [41].</p>",
            "id": 115,
            "page": 11,
            "text": "ImageNet classification with ResNet We train all\nResNet architectures for 100 epochs using synchronous\nSGD with momentum 0.9 across 8 TESLA V100 GPUs and\nweight decay of 1e-4. We use the largest batch size per\nworker B E {32, 64, 128, 256} that fits in a minibatch. The\ninitial learning rate is scaled linearly according to the total\nbatch size using a base learning rate of 0.128 for total batch\nsize of 256. During training, we linearly scale the learning\nrate from 0 to this value for the first 5% of training epochs\nand divide it by 10 at epochs 30, 60, 80 and 90. We use\nstandard Inception data augmentation as described in [41]."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2159
                },
                {
                    "x": 1199,
                    "y": 2159
                },
                {
                    "x": 1199,
                    "y": 2462
                },
                {
                    "x": 201,
                    "y": 2462
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:18px'>ImageNet classification with MnasNet We follow the<br>training setup described in [42] and train all networks for<br>350 epochs with the RMSProp optimizer using exponential<br>learning rate decay. When training our augmented Mnas-<br>Nets, we divide the learning rate by 2 and adjusted the learn-<br>ing rate decay SO that the final learning rate stays the same.</p>",
            "id": 116,
            "page": 11,
            "text": "ImageNet classification with MnasNet We follow the\ntraining setup described in [42] and train all networks for\n350 epochs with the RMSProp optimizer using exponential\nlearning rate decay. When training our augmented Mnas-\nNets, we divide the learning rate by 2 and adjusted the learn-\ning rate decay SO that the final learning rate stays the same."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2509
                },
                {
                    "x": 1200,
                    "y": 2509
                },
                {
                    "x": 1200,
                    "y": 2709
                },
                {
                    "x": 200,
                    "y": 2709
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>Object Detection with COCO dataset We follow the<br>setup described in [26, 11] and train the RetinaNet from<br>scratch for 150 epochs without using ImageNet pretraining<br>for the ResNet backbone.</p>",
            "id": 117,
            "page": 11,
            "text": "Object Detection with COCO dataset We follow the\nsetup described in [26, 11] and train the RetinaNet from\nscratch for 150 epochs without using ImageNet pretraining\nfor the ResNet backbone."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2746
                },
                {
                    "x": 930,
                    "y": 2746
                },
                {
                    "x": 930,
                    "y": 2800
                },
                {
                    "x": 203,
                    "y": 2800
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:20px'>A.2. Computational & Memory costs</p>",
            "id": 118,
            "page": 11,
            "text": "A.2. Computational & Memory costs"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2827
                },
                {
                    "x": 1199,
                    "y": 2827
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>Table 9 provides the breakdown of self-attention re-<br>lated computational costs per image. Storing attention<br>maps in each layer induces a memory cost of Nh (HW)2</p>",
            "id": 119,
            "page": 11,
            "text": "Table 9 provides the breakdown of self-attention re-\nlated computational costs per image. Storing attention\nmaps in each layer induces a memory cost of Nh (HW)2"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 307
                },
                {
                    "x": 2279,
                    "y": 307
                },
                {
                    "x": 2279,
                    "y": 454
                },
                {
                    "x": 1278,
                    "y": 454
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:18px'>bfloat16. At inference, the memory cost for storing at-<br>tention maps is only 1.2% of the memory required to store<br>model parameters (49MB).</p>",
            "id": 120,
            "page": 11,
            "text": "bfloat16. At inference, the memory cost for storing at-\ntention maps is only 1.2% of the memory required to store\nmodel parameters (49MB)."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 520
                },
                {
                    "x": 2294,
                    "y": 520
                },
                {
                    "x": 2294,
                    "y": 814
                },
                {
                    "x": 1277,
                    "y": 814
                }
            ],
            "category": "table",
            "html": "<table id='121' style='font-size:14px'><tr><td>Layer</td><td>Memory</td><td>Params</td><td>FLOPS</td></tr><tr><td>{Stage 2 - H=W=14} * 4</td><td>600KB</td><td>43k</td><td>22M</td></tr><tr><td>{Stage 3 - H=W=14} * 6</td><td>600KB</td><td>90k</td><td>40M</td></tr><tr><td>{Stage 4 - H=W=7} * 3</td><td>37.5KB</td><td>190k</td><td>19M</td></tr><tr><td>Training</td><td>6MB (total)</td><td>1.3M</td><td>390M</td></tr><tr><td>Inference</td><td>600KB (max)</td><td>1.3M</td><td>390M</td></tr></table>",
            "id": 121,
            "page": 11,
            "text": "Layer Memory Params FLOPS\n {Stage 2 - H=W=14} * 4 600KB 43k 22M\n {Stage 3 - H=W=14} * 6 600KB 90k 40M\n {Stage 4 - H=W=7} * 3 37.5KB 190k 19M\n Training 6MB (total) 1.3M 390M\n Inference 600KB (max) 1.3M"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 846
                },
                {
                    "x": 2279,
                    "y": 846
                },
                {
                    "x": 2279,
                    "y": 987
                },
                {
                    "x": 1280,
                    "y": 987
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:14px'>Table 9. Computational costs associated with self-attention in the<br>forward pass of the ResNet50. During inference, we only consider<br>the largest memory cost since activations are not stored.</p>",
            "id": 122,
            "page": 11,
            "text": "Table 9. Computational costs associated with self-attention in the\nforward pass of the ResNet50. During inference, we only consider\nthe largest memory cost since activations are not stored."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1059
                },
                {
                    "x": 2279,
                    "y": 1059
                },
                {
                    "x": 2279,
                    "y": 1208
                },
                {
                    "x": 1278,
                    "y": 1208
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:16px'>Figures 5 and 6 show the accuracies of our attention<br>augmented networks across FLOPS counts, which correlate<br>with running times across hardware platforms.</p>",
            "id": 123,
            "page": 11,
            "text": "Figures 5 and 6 show the accuracies of our attention\naugmented networks across FLOPS counts, which correlate\nwith running times across hardware platforms."
        },
        {
            "bounding_box": [
                {
                    "x": 1404,
                    "y": 1292
                },
                {
                    "x": 2146,
                    "y": 1292
                },
                {
                    "x": 2146,
                    "y": 1886
                },
                {
                    "x": 1404,
                    "y": 1886
                }
            ],
            "category": "figure",
            "html": "<figure><img id='124' style='font-size:14px' alt=\"81\n80\n79.1%\n79 78.7%\n78.9%\naccuracy.\n78.4%\n78 77.7%\n78.4%\n77.9%\n77 77.5%\n76 76.4%\ntop-1 75 74.7%\nResNet\n74 74.3%\nSE-ResNet\n73 73.6%\nAA-ResNet (ours)\n72\n5 10 15 20 25\ncomputational demand (GFlops).\" data-coord=\"top-left:(1404,1292); bottom-right:(2146,1886)\" /></figure>",
            "id": 124,
            "page": 11,
            "text": "81\n80\n79.1%\n79 78.7%\n78.9%\naccuracy.\n78.4%\n78 77.7%\n78.4%\n77.9%\n77 77.5%\n76 76.4%\ntop-1 75 74.7%\nResNet\n74 74.3%\nSE-ResNet\n73 73.6%\nAA-ResNet (ours)\n72\n5 10 15 20 25\ncomputational demand (GFlops)."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1918
                },
                {
                    "x": 2279,
                    "y": 1918
                },
                {
                    "x": 2279,
                    "y": 2057
                },
                {
                    "x": 1279,
                    "y": 2057
                }
            ],
            "category": "caption",
            "html": "<caption id='125' style='font-size:16px'>Figure 5. ImageNet top-1 accuracy as a function of computational<br>demand for variety of ResNet architectures [14]. From left to right:<br>ResNet-34, ResNet-50, ResNet-101 and ResNet-152.</caption>",
            "id": 125,
            "page": 11,
            "text": "Figure 5. ImageNet top-1 accuracy as a function of computational\ndemand for variety of ResNet architectures [14]. From left to right:\nResNet-34, ResNet-50, ResNet-101 and ResNet-152."
        },
        {
            "bounding_box": [
                {
                    "x": 1409,
                    "y": 2149
                },
                {
                    "x": 2134,
                    "y": 2149
                },
                {
                    "x": 2134,
                    "y": 2734
                },
                {
                    "x": 1409,
                    "y": 2734
                }
            ],
            "category": "figure",
            "html": "<figure><img id='126' style='font-size:14px' alt=\"78 77.7%\n77.2%\n77\n77.2%\naccuracy\n76.7%\n76 75.7%\ntop-1 75\n75.2%\n73.9%\n74\nMnasNet\n73 73.3% AA-MnasNet (ours)\n0.4 0.6 0.8 1.0 1.2 1.4\ncomputational demand (GFlops).\" data-coord=\"top-left:(1409,2149); bottom-right:(2134,2734)\" /></figure>",
            "id": 126,
            "page": 11,
            "text": "78 77.7%\n77.2%\n77\n77.2%\naccuracy\n76.7%\n76 75.7%\ntop-1 75\n75.2%\n73.9%\n74\nMnasNet\n73 73.3% AA-MnasNet (ours)\n0.4 0.6 0.8 1.0 1.2 1.4\ncomputational demand (GFlops)."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2767
                },
                {
                    "x": 2279,
                    "y": 2767
                },
                {
                    "x": 2279,
                    "y": 2906
                },
                {
                    "x": 1279,
                    "y": 2906
                }
            ],
            "category": "caption",
            "html": "<caption id='127' style='font-size:16px'>Figure 6. ImageNet top-1 accuracy as a function of computational<br>demand for MnasNet (black) and Attention-Augmented-MnasNet<br>(red) with width multipliers 0.75, 1.0, 1.25 and 1.4.</caption>",
            "id": 127,
            "page": 11,
            "text": "Figure 6. ImageNet top-1 accuracy as a function of computational\ndemand for MnasNet (black) and Attention-Augmented-MnasNet\n(red) with width multipliers 0.75, 1.0, 1.25 and 1.4."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 302
                },
                {
                    "x": 1134,
                    "y": 302
                },
                {
                    "x": 1134,
                    "y": 354
                },
                {
                    "x": 203,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:22px'>A.3. 2D Relative Self-Attention implementation</p>",
            "id": 128,
            "page": 12,
            "text": "A.3. 2D Relative Self-Attention implementation"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 378
                },
                {
                    "x": 1200,
                    "y": 378
                },
                {
                    "x": 1200,
                    "y": 831
                },
                {
                    "x": 200,
                    "y": 831
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:22px'>While our method is simple and only requires matrix<br>multiplication, addition and the softmax operation (Equa-<br>tions 3 and 4), our implementation relies on non-trivial op-<br>erations (e.g. tiling, transposing and reshaping) because<br>no low-level kernels currently exist for hardware platforms.<br>Future work may develop specialized kernels as previously<br>done for convolutions. Therefore, we believe that current<br>latency times (Table 2) reflect the lack of dedicated engi-<br>neering as opposed to inefficiency in the proposed method.</p>",
            "id": 129,
            "page": 12,
            "text": "While our method is simple and only requires matrix\nmultiplication, addition and the softmax operation (Equa-\ntions 3 and 4), our implementation relies on non-trivial op-\nerations (e.g. tiling, transposing and reshaping) because\nno low-level kernels currently exist for hardware platforms.\nFuture work may develop specialized kernels as previously\ndone for convolutions. Therefore, we believe that current\nlatency times (Table 2) reflect the lack of dedicated engi-\nneering as opposed to inefficiency in the proposed method."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 893
                },
                {
                    "x": 499,
                    "y": 893
                },
                {
                    "x": 499,
                    "y": 929
                },
                {
                    "x": 205,
                    "y": 929
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:18px'>def shape_list(x) :</p>",
            "id": 130,
            "page": 12,
            "text": "def shape_list(x) :"
        },
        {
            "bounding_box": [
                {
                    "x": 233,
                    "y": 928
                },
                {
                    "x": 1117,
                    "y": 928
                },
                {
                    "x": 1117,
                    "y": 1101
                },
                {
                    "x": 233,
                    "y": 1101
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:16px'>\"\" \"Return list of dims, statically where possible. \"\"\"<br>static = x · get_shape() · as_list ()<br>shape = tf · shape(x)<br>ret = []<br>for i , static_dim in enumerate (static) :</p>",
            "id": 131,
            "page": 12,
            "text": "\"\" \"Return list of dims, statically where possible. \"\"\"\nstatic = x · get_shape() · as_list ()\nshape = tf · shape(x)\nret = []\nfor i , static_dim in enumerate (static) :"
        },
        {
            "bounding_box": [
                {
                    "x": 263,
                    "y": 1106
                },
                {
                    "x": 749,
                    "y": 1106
                },
                {
                    "x": 749,
                    "y": 1176
                },
                {
                    "x": 263,
                    "y": 1176
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:16px'>dim = static_dim or shape [i]<br>ret · append (dim)</p>",
            "id": 132,
            "page": 12,
            "text": "dim = static_dim or shape [i]\nret · append (dim)"
        },
        {
            "bounding_box": [
                {
                    "x": 231,
                    "y": 1180
                },
                {
                    "x": 404,
                    "y": 1180
                },
                {
                    "x": 404,
                    "y": 1211
                },
                {
                    "x": 231,
                    "y": 1211
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:16px'>return ret</p>",
            "id": 133,
            "page": 12,
            "text": "return ret"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1247
                },
                {
                    "x": 713,
                    "y": 1247
                },
                {
                    "x": 713,
                    "y": 1281
                },
                {
                    "x": 203,
                    "y": 1281
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:18px'>def split_heads_2d(inputs, Nh) :</p>",
            "id": 134,
            "page": 12,
            "text": "def split_heads_2d(inputs, Nh) :"
        },
        {
            "bounding_box": [
                {
                    "x": 234,
                    "y": 1280
                },
                {
                    "x": 920,
                    "y": 1280
                },
                {
                    "x": 920,
                    "y": 1321
                },
                {
                    "x": 234,
                    "y": 1321
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='135' style='font-size:18px'>\"\"\"Split channels into multiple heads. \"\"\"</p>",
            "id": 135,
            "page": 12,
            "text": "\"\"\"Split channels into multiple heads. \"\"\""
        },
        {
            "bounding_box": [
                {
                    "x": 231,
                    "y": 1316
                },
                {
                    "x": 951,
                    "y": 1316
                },
                {
                    "x": 951,
                    "y": 1460
                },
                {
                    "x": 231,
                    "y": 1460
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:18px'>B, H, W, d = shape_list (inputs)<br>ret_shape = [B, H, W, Nh, d // Nh]<br>split = tf · reshape(inputs, ret_shape)<br>return tf · transpose(split, [0, 3, 1, 2, 4])</p>",
            "id": 136,
            "page": 12,
            "text": "B, H, W, d = shape_list (inputs)\nret_shape = [B, H, W, Nh, d // Nh]\nsplit = tf · reshape(inputs, ret_shape)\nreturn tf · transpose(split, [0, 3, 1, 2, 4])"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1496
                },
                {
                    "x": 678,
                    "y": 1496
                },
                {
                    "x": 678,
                    "y": 1528
                },
                {
                    "x": 203,
                    "y": 1528
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>def conbine.heads_2d(inputs) :</p>",
            "id": 137,
            "page": 12,
            "text": "def conbine.heads_2d(inputs) :"
        },
        {
            "bounding_box": [
                {
                    "x": 238,
                    "y": 1530
                },
                {
                    "x": 1019,
                    "y": 1530
                },
                {
                    "x": 1019,
                    "y": 1562
                },
                {
                    "x": 238,
                    "y": 1562
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:14px'>\" \" \"Combine heads (inverse of split_heads_2d) ·<br>\" \"\"</p>",
            "id": 138,
            "page": 12,
            "text": "\" \" \"Combine heads (inverse of split_heads_2d) ·\n\" \"\""
        },
        {
            "bounding_box": [
                {
                    "x": 232,
                    "y": 1566
                },
                {
                    "x": 1182,
                    "y": 1566
                },
                {
                    "x": 1182,
                    "y": 1706
                },
                {
                    "x": 232,
                    "y": 1706
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:18px'>transposed = tf. transpose(inputs, [0, 2, 3, 1, 4] )<br>Nh, channels = shape_list (transposed) [-2:]<br>ret_shape = shape_list (transposed) [:-2] + [Nh * channels]<br>return tf .reshape (transposed, ret_shape)</p>",
            "id": 139,
            "page": 12,
            "text": "transposed = tf. transpose(inputs, [0, 2, 3, 1, 4] )\nNh, channels = shape_list (transposed) [-2:]\nret_shape = shape_list (transposed) [:-2] + [Nh * channels]\nreturn tf .reshape (transposed, ret_shape)"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1741
                },
                {
                    "x": 495,
                    "y": 1741
                },
                {
                    "x": 495,
                    "y": 1773
                },
                {
                    "x": 205,
                    "y": 1773
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>def rel_to_abs(x) :</p>",
            "id": 140,
            "page": 12,
            "text": "def rel_to_abs(x) :"
        },
        {
            "bounding_box": [
                {
                    "x": 1131,
                    "y": 1778
                },
                {
                    "x": 1182,
                    "y": 1778
                },
                {
                    "x": 1182,
                    "y": 1797
                },
                {
                    "x": 1131,
                    "y": 1797
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:14px'>\"\"\"</p>",
            "id": 141,
            "page": 12,
            "text": "\"\"\""
        },
        {
            "bounding_box": [
                {
                    "x": 233,
                    "y": 1788
                },
                {
                    "x": 1090,
                    "y": 1788
                },
                {
                    "x": 1090,
                    "y": 1920
                },
                {
                    "x": 233,
                    "y": 1920
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:18px'>\"\"\"Converts tensor from relative to aboslute indexing.<br># [B, Nh, L, 2L-1]<br>B, Nh, L, - shape_list(x)<br>=<br># Pad to shift from relative to absolute indexing.</p>",
            "id": 142,
            "page": 12,
            "text": "\"\"\"Converts tensor from relative to aboslute indexing.\n# [B, Nh, L, 2L-1]\nB, Nh, L, - shape_list(x)\n=\n# Pad to shift from relative to absolute indexing."
        },
        {
            "bounding_box": [
                {
                    "x": 232,
                    "y": 1917
                },
                {
                    "x": 1166,
                    "y": 1917
                },
                {
                    "x": 1166,
                    "y": 2233
                },
                {
                    "x": 232,
                    "y": 2233
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:16px'>col_pad = tf . zeros((B, Nh, L, 1))<br>x = tf · concat ([x, col_pad] , axis=3)<br>flat_x = tf · reshape(x, [B, Nh, L * 2 * L])<br>flat_pad = tf · zeros ((B, Nh, L-1))<br>flat_x_padded = tf . concat ([flat_x, flat_pad] , axis=2)<br># Reshape and slice out the padded elements.<br>final_x = tf · reshape (flat_x_padded, [B, Nh, L+1, 2*L-1])<br>final _x = final_x[: , : :L, L-1:]<br>,<br>return final_x</p>",
            "id": 143,
            "page": 12,
            "text": "col_pad = tf . zeros((B, Nh, L, 1))\nx = tf · concat ([x, col_pad] , axis=3)\nflat_x = tf · reshape(x, [B, Nh, L * 2 * L])\nflat_pad = tf · zeros ((B, Nh, L-1))\nflat_x_padded = tf . concat ([flat_x, flat_pad] , axis=2)\n# Reshape and slice out the padded elements.\nfinal_x = tf · reshape (flat_x_padded, [B, Nh, L+1, 2*L-1])\nfinal _x = final_x[: , : :L, L-1:]\n,\nreturn final_x"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2271
                },
                {
                    "x": 1162,
                    "y": 2271
                },
                {
                    "x": 1162,
                    "y": 2305
                },
                {
                    "x": 203,
                    "y": 2305
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>def relative_logits_1d(q, rel_k, H, W, Nh, transpose_mask) :</p>",
            "id": 144,
            "page": 12,
            "text": "def relative_logits_1d(q, rel_k, H, W, Nh, transpose_mask) :"
        },
        {
            "bounding_box": [
                {
                    "x": 235,
                    "y": 2305
                },
                {
                    "x": 1090,
                    "y": 2305
                },
                {
                    "x": 1090,
                    "y": 2408
                },
                {
                    "x": 235,
                    "y": 2408
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='145' style='font-size:18px'>\"\"\"Compute relative logits along one dimenion. \"\"\"<br>rel_logits = tf . einsum( 'bhxyd,md->bhxym , q, rel_k)<br>,<br># Collapse height and heads</p>",
            "id": 145,
            "page": 12,
            "text": "\"\"\"Compute relative logits along one dimenion. \"\"\"\nrel_logits = tf . einsum( 'bhxyd,md->bhxym , q, rel_k)\n,\n# Collapse height and heads"
        },
        {
            "bounding_box": [
                {
                    "x": 232,
                    "y": 2412
                },
                {
                    "x": 632,
                    "y": 2412
                },
                {
                    "x": 632,
                    "y": 2447
                },
                {
                    "x": 232,
                    "y": 2447
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='146' style='font-size:16px'>rel_logits = tf · reshape(</p>",
            "id": 146,
            "page": 12,
            "text": "rel_logits = tf · reshape("
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2447
                },
                {
                    "x": 926,
                    "y": 2447
                },
                {
                    "x": 926,
                    "y": 2482
                },
                {
                    "x": 293,
                    "y": 2482
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='147' style='font-size:18px'>rel_logits, [-1, Nh * H, W, 2 * W-1])</p>",
            "id": 147,
            "page": 12,
            "text": "rel_logits, [-1, Nh * H, W, 2 * W-1])"
        },
        {
            "bounding_box": [
                {
                    "x": 232,
                    "y": 2484
                },
                {
                    "x": 802,
                    "y": 2484
                },
                {
                    "x": 802,
                    "y": 2516
                },
                {
                    "x": 232,
                    "y": 2516
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:18px'>rel_logits = rel_to_abs (rel_logits)</p>",
            "id": 148,
            "page": 12,
            "text": "rel_logits = rel_to_abs (rel_logits)"
        },
        {
            "bounding_box": [
                {
                    "x": 232,
                    "y": 2503
                },
                {
                    "x": 1152,
                    "y": 2503
                },
                {
                    "x": 1152,
                    "y": 2801
                },
                {
                    "x": 232,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='149' style='font-size:18px'># Shape it and tile height times<br>rel_logits = tf · reshape (rel_logits, [-1, Nh, H, W, W])<br>rel_logits = tf · expand_dims (rel_logits, axis=3)<br>rel_logits = tf · tile(rel_logits, [1, 1 , 1 , H, 1, 1])<br># Reshape for adding to the logits.<br>rel_logits = tf · transpose (rel_logits, transpose_mask)<br>rel_logits = tf · reshape (rel_logits, [-1, Nh, H*W, H*W] )<br>return rel_logits</p>",
            "id": 149,
            "page": 12,
            "text": "# Shape it and tile height times\nrel_logits = tf · reshape (rel_logits, [-1, Nh, H, W, W])\nrel_logits = tf · expand_dims (rel_logits, axis=3)\nrel_logits = tf · tile(rel_logits, [1, 1 , 1 , H, 1, 1])\n# Reshape for adding to the logits.\nrel_logits = tf · transpose (rel_logits, transpose_mask)\nrel_logits = tf · reshape (rel_logits, [-1, Nh, H*W, H*W] )\nreturn rel_logits"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 446
                },
                {
                    "x": 1914,
                    "y": 446
                },
                {
                    "x": 1914,
                    "y": 480
                },
                {
                    "x": 1281,
                    "y": 480
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:18px'>def relative_logits(q, H, W, Nh, dkh) :</p>",
            "id": 150,
            "page": 12,
            "text": "def relative_logits(q, H, W, Nh, dkh) :"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2837
                },
                {
                    "x": 1194,
                    "y": 2837
                },
                {
                    "x": 1194,
                    "y": 2929
                },
                {
                    "x": 200,
                    "y": 2929
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:20px'>Figure 7. Helper functions in Tensorflow for 2D relative self-<br>attention.</p>",
            "id": 151,
            "page": 12,
            "text": "Figure 7. Helper functions in Tensorflow for 2D relative self-\nattention."
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 483
                },
                {
                    "x": 2019,
                    "y": 483
                },
                {
                    "x": 2019,
                    "y": 550
                },
                {
                    "x": 1311,
                    "y": 550
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='152' style='font-size:18px'>\"\"\"Compute relative logits ·<br>\"\"\"<br># Relative logits in width dimension first.</p>",
            "id": 152,
            "page": 12,
            "text": "\"\"\"Compute relative logits ·\n\"\"\"\n# Relative logits in width dimension first."
        },
        {
            "bounding_box": [
                {
                    "x": 1309,
                    "y": 548
                },
                {
                    "x": 1887,
                    "y": 548
                },
                {
                    "x": 1887,
                    "y": 590
                },
                {
                    "x": 1309,
                    "y": 590
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='153' style='font-size:14px'>rel_embeddings_w = tf · get_variable(</p>",
            "id": 153,
            "page": 12,
            "text": "rel_embeddings_w = tf · get_variable("
        },
        {
            "bounding_box": [
                {
                    "x": 1376,
                    "y": 586
                },
                {
                    "x": 2238,
                    "y": 586
                },
                {
                    "x": 2238,
                    "y": 657
                },
                {
                    "x": 1376,
                    "y": 657
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:14px'>'r_width' , shape=(2*W - 1, dkh) ,<br>initializer=tf · randon_normal_initializer (dkh**-0.5))</p>",
            "id": 154,
            "page": 12,
            "text": "'r_width' , shape=(2*W - 1, dkh) ,\ninitializer=tf · randon_normal_initializer (dkh**-0.5))"
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 658
                },
                {
                    "x": 1596,
                    "y": 658
                },
                {
                    "x": 1596,
                    "y": 689
                },
                {
                    "x": 1311,
                    "y": 689
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:16px'>Nh, HW , HW]</p>",
            "id": 155,
            "page": 12,
            "text": "Nh, HW , HW]"
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 694
                },
                {
                    "x": 2193,
                    "y": 694
                },
                {
                    "x": 2193,
                    "y": 766
                },
                {
                    "x": 1311,
                    "y": 766
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:18px'>rel_logits_w = relative_logits_id(<br>q, rel_embeddings_w, H, W, Nh, [0, 1, 2, 4, 3, 5])</p>",
            "id": 156,
            "page": 12,
            "text": "rel_logits_w = relative_logits_id(\nq, rel_embeddings_w, H, W, Nh, [0, 1, 2, 4, 3, 5])"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 798
                },
                {
                    "x": 2054,
                    "y": 798
                },
                {
                    "x": 2054,
                    "y": 980
                },
                {
                    "x": 1310,
                    "y": 980
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:18px'># Relative logits in height dimension next.<br># For ease, we 1) transpose height and width,<br># 2) repeat the above steps and<br># 3) transpose to eventually put the logits<br># in their right positions.</p>",
            "id": 157,
            "page": 12,
            "text": "# Relative logits in height dimension next.\n# For ease, we 1) transpose height and width,\n# 2) repeat the above steps and\n# 3) transpose to eventually put the logits\n# in their right positions."
        },
        {
            "bounding_box": [
                {
                    "x": 1309,
                    "y": 975
                },
                {
                    "x": 1883,
                    "y": 975
                },
                {
                    "x": 1883,
                    "y": 1009
                },
                {
                    "x": 1309,
                    "y": 1009
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:14px'>rel_embeddings_h = tf · get_variable(</p>",
            "id": 158,
            "page": 12,
            "text": "rel_embeddings_h = tf · get_variable("
        },
        {
            "bounding_box": [
                {
                    "x": 1379,
                    "y": 1012
                },
                {
                    "x": 2238,
                    "y": 1012
                },
                {
                    "x": 2238,
                    "y": 1078
                },
                {
                    "x": 1379,
                    "y": 1078
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='159' style='font-size:16px'>'r_height , shape=(2 * H - 1, dkh) ,<br>,<br>initializer=tf · randon_normal_initializer (dkh** -0.5))</p>",
            "id": 159,
            "page": 12,
            "text": "'r_height , shape=(2 * H - 1, dkh) ,\n,\ninitializer=tf · randon_normal_initializer (dkh** -0.5))"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1082
                },
                {
                    "x": 1872,
                    "y": 1082
                },
                {
                    "x": 1872,
                    "y": 1150
                },
                {
                    "x": 1312,
                    "y": 1150
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:16px'>[B , Nh, HW, HW]<br>rel_logits_h = relative_logits_1d(</p>",
            "id": 160,
            "page": 12,
            "text": "[B , Nh, HW, HW]\nrel_logits_h = relative_logits_1d("
        },
        {
            "bounding_box": [
                {
                    "x": 1371,
                    "y": 1152
                },
                {
                    "x": 2144,
                    "y": 1152
                },
                {
                    "x": 2144,
                    "y": 1226
                },
                {
                    "x": 1371,
                    "y": 1226
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:18px'>tf · transpose(q, [0, 1 , 3, 2, 4]) ,<br>rel_embeddings_h, W, H, Nh, [0, 1, 4, 2, 5, 3])</p>",
            "id": 161,
            "page": 12,
            "text": "tf · transpose(q, [0, 1 , 3, 2, 4]) ,\nrel_embeddings_h, W, H, Nh, [0, 1, 4, 2, 5, 3])"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 1257
                },
                {
                    "x": 1850,
                    "y": 1257
                },
                {
                    "x": 1850,
                    "y": 1294
                },
                {
                    "x": 1310,
                    "y": 1294
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:20px'>return rel_logits_h, rel_logits_w</p>",
            "id": 162,
            "page": 12,
            "text": "return rel_logits_h, rel_logits_w"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1327
                },
                {
                    "x": 2220,
                    "y": 1327
                },
                {
                    "x": 2220,
                    "y": 1363
                },
                {
                    "x": 1279,
                    "y": 1363
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:18px'>def self _attention_2d(inputs, dk, dv, Nh, relative=True) :</p>",
            "id": 163,
            "page": 12,
            "text": "def self _attention_2d(inputs, dk, dv, Nh, relative=True) :"
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 1364
                },
                {
                    "x": 1875,
                    "y": 1364
                },
                {
                    "x": 1875,
                    "y": 1385
                },
                {
                    "x": 1315,
                    "y": 1385
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:14px'>\"\" \"\"\"</p>",
            "id": 164,
            "page": 12,
            "text": "\"\" \"\"\""
        },
        {
            "bounding_box": [
                {
                    "x": 1356,
                    "y": 1364
                },
                {
                    "x": 1815,
                    "y": 1364
                },
                {
                    "x": 1815,
                    "y": 1397
                },
                {
                    "x": 1356,
                    "y": 1397
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:18px'>\"2d relative self-attention.</p>",
            "id": 165,
            "page": 12,
            "text": "\"2d relative self-attention."
        },
        {
            "bounding_box": [
                {
                    "x": 1309,
                    "y": 1399
                },
                {
                    "x": 1817,
                    "y": 1399
                },
                {
                    "x": 1817,
                    "y": 1434
                },
                {
                    "x": 1309,
                    "y": 1434
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='166' style='font-size:14px'>= shape_list (inputs)<br>-, H, W, -</p>",
            "id": 166,
            "page": 12,
            "text": "= shape_list (inputs)\n-, H, W, -"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 1427
                },
                {
                    "x": 2265,
                    "y": 1427
                },
                {
                    "x": 2265,
                    "y": 1541
                },
                {
                    "x": 1310,
                    "y": 1541
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='167' style='font-size:16px'>dkh = dk // Nh<br>dvh = dv // Nh<br>flatten_hw = lambda x, d: tf.reshape(x, [-1, Nh, H*W, d])</p>",
            "id": 167,
            "page": 12,
            "text": "dkh = dk // Nh\ndvh = dv // Nh\nflatten_hw = lambda x, d: tf.reshape(x, [-1, Nh, H*W, d])"
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 1576
                },
                {
                    "x": 1610,
                    "y": 1576
                },
                {
                    "x": 1610,
                    "y": 1608
                },
                {
                    "x": 1311,
                    "y": 1608
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:18px'># Compute q, k, V</p>",
            "id": 168,
            "page": 12,
            "text": "# Compute q, k, V"
        },
        {
            "bounding_box": [
                {
                    "x": 1309,
                    "y": 1610
                },
                {
                    "x": 2075,
                    "y": 1610
                },
                {
                    "x": 2075,
                    "y": 1717
                },
                {
                    "x": 1309,
                    "y": 1717
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='169' style='font-size:16px'>kqv = tf · layers · conv2d(inputs, 2 * dk + dv, 1)<br>k, q, v = tf · split(kqv, [dk, dk, dv] , axis=3)<br>q *= dkh ** -0. 5 # scaled dot-product</p>",
            "id": 169,
            "page": 12,
            "text": "kqv = tf · layers · conv2d(inputs, 2 * dk + dv, 1)\nk, q, v = tf · split(kqv, [dk, dk, dv] , axis=3)\nq *= dkh ** -0. 5 # scaled dot-product"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 1750
                },
                {
                    "x": 2193,
                    "y": 1750
                },
                {
                    "x": 2193,
                    "y": 1892
                },
                {
                    "x": 1310,
                    "y": 1892
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:16px'># After splitting, shape is [B, Nh, H, W, dkh or dvh]<br>q = split_heads_2d(q, Nh)<br>k = split_heads_2d(k, Nh)<br>v = split_heads_2d(v, Nh)</p>",
            "id": 170,
            "page": 12,
            "text": "# After splitting, shape is [B, Nh, H, W, dkh or dvh]\nq = split_heads_2d(q, Nh)\nk = split_heads_2d(k, Nh)\nv = split_heads_2d(v, Nh)"
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 1926
                },
                {
                    "x": 1598,
                    "y": 1926
                },
                {
                    "x": 1598,
                    "y": 1960
                },
                {
                    "x": 1311,
                    "y": 1960
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:18px'># [B, Nh, HW, HW]</p>",
            "id": 171,
            "page": 12,
            "text": "# [B, Nh, HW, HW]"
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 1964
                },
                {
                    "x": 2266,
                    "y": 1964
                },
                {
                    "x": 2266,
                    "y": 2034
                },
                {
                    "x": 1315,
                    "y": 2034
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:18px'>logits = tf .matmul (flatten_hw (q, dkh) , flatten_hw (k, dkh) ,<br>transpose_b=True)</p>",
            "id": 172,
            "page": 12,
            "text": "logits = tf .matmul (flatten_hw (q, dkh) , flatten_hw (k, dkh) ,\ntranspose_b=True)"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 2070
                },
                {
                    "x": 1353,
                    "y": 2070
                },
                {
                    "x": 1353,
                    "y": 2101
                },
                {
                    "x": 1312,
                    "y": 2101
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:18px'>if</p>",
            "id": 173,
            "page": 12,
            "text": "if"
        },
        {
            "bounding_box": [
                {
                    "x": 1336,
                    "y": 2071
                },
                {
                    "x": 2265,
                    "y": 2071
                },
                {
                    "x": 2265,
                    "y": 2189
                },
                {
                    "x": 1336,
                    "y": 2189
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='174' style='font-size:18px'>relative:<br>rel_logits_h, rel_logits_w = relative_logits(q, H, W, Nh,<br>dkh)</p>",
            "id": 174,
            "page": 12,
            "text": "relative:\nrel_logits_h, rel_logits_w = relative_logits(q, H, W, Nh,\ndkh)"
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 2280
                },
                {
                    "x": 2177,
                    "y": 2280
                },
                {
                    "x": 2177,
                    "y": 2415
                },
                {
                    "x": 1311,
                    "y": 2415
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:16px'>weights = tf · nn. softmax (logits)<br>attn_out = tf · matmul (weights, flatten_hw(v, dvh))<br>attn_out = tf · reshape (attn_out, [-1, Nh, H, W, dvh])<br>attn_out = combine_heads_2d(attn_out)</p>",
            "id": 175,
            "page": 12,
            "text": "weights = tf · nn. softmax (logits)\nattn_out = tf · matmul (weights, flatten_hw(v, dvh))\nattn_out = tf · reshape (attn_out, [-1, Nh, H, W, dvh])\nattn_out = combine_heads_2d(attn_out)"
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 2418
                },
                {
                    "x": 2038,
                    "y": 2418
                },
                {
                    "x": 2038,
                    "y": 2528
                },
                {
                    "x": 1311,
                    "y": 2528
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:16px'># Project heads<br>attn_out = tf . layers.conv2d(attn_out, dv, 1)<br>return attn_out</p>",
            "id": 176,
            "page": 12,
            "text": "# Project heads\nattn_out = tf . layers.conv2d(attn_out, dv, 1)\nreturn attn_out"
        },
        {
            "bounding_box": [
                {
                    "x": 1339,
                    "y": 2173
                },
                {
                    "x": 1706,
                    "y": 2173
                },
                {
                    "x": 1706,
                    "y": 2245
                },
                {
                    "x": 1339,
                    "y": 2245
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='177' style='font-size:18px'>logits += rel_logits_h<br>logits += rel_logits_w</p>",
            "id": 177,
            "page": 12,
            "text": "logits += rel_logits_h\nlogits += rel_logits_w"
        },
        {
            "bounding_box": [
                {
                    "x": 1397,
                    "y": 2636
                },
                {
                    "x": 1889,
                    "y": 2636
                },
                {
                    "x": 1889,
                    "y": 2667
                },
                {
                    "x": 1397,
                    "y": 2667
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:20px'>kernel_size=k, padding='same')</p>",
            "id": 178,
            "page": 12,
            "text": "kernel_size=k, padding='same')"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 2741
                },
                {
                    "x": 2070,
                    "y": 2741
                },
                {
                    "x": 2070,
                    "y": 2775
                },
                {
                    "x": 1310,
                    "y": 2775
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:18px'>return tf.concat ( [conv_out, attn_out] , axis=3)</p>",
            "id": 179,
            "page": 12,
            "text": "return tf.concat ( [conv_out, attn_out] , axis=3)"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 2671
                },
                {
                    "x": 2183,
                    "y": 2671
                },
                {
                    "x": 2183,
                    "y": 2739
                },
                {
                    "x": 1312,
                    "y": 2739
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='180' style='font-size:18px'>attn_out = self _attention_2d(X, dk, dv, Nh, relative=<br>relative)</p>",
            "id": 180,
            "page": 12,
            "text": "attn_out = self _attention_2d(X, dk, dv, Nh, relative=\nrelative)"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2563
                },
                {
                    "x": 2246,
                    "y": 2563
                },
                {
                    "x": 2246,
                    "y": 2633
                },
                {
                    "x": 1281,
                    "y": 2633
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='181' style='font-size:18px'>def augmented_conv2d(X, Fout, k, dk, dv, Nh, relative) :<br>conv_out = tf .layers.conv2d(inputs=X, filters=Fout - dv,</p>",
            "id": 181,
            "page": 12,
            "text": "def augmented_conv2d(X, Fout, k, dk, dv, Nh, relative) :\nconv_out = tf .layers.conv2d(inputs=X, filters=Fout - dv,"
        },
        {
            "bounding_box": [
                {
                    "x": 1355,
                    "y": 2817
                },
                {
                    "x": 2200,
                    "y": 2817
                },
                {
                    "x": 2200,
                    "y": 2861
                },
                {
                    "x": 1355,
                    "y": 2861
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:20px'>Figure 8. Tensorflow code for 2D relative self-attention.</p>",
            "id": 182,
            "page": 12,
            "text": "Figure 8. Tensorflow code for 2D relative self-attention."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 304
                },
                {
                    "x": 774,
                    "y": 304
                },
                {
                    "x": 774,
                    "y": 353
                },
                {
                    "x": 204,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:20px'>A.4. Attention visualizations.</p>",
            "id": 183,
            "page": 13,
            "text": "A.4. Attention visualizations."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 379
                },
                {
                    "x": 1202,
                    "y": 379
                },
                {
                    "x": 1202,
                    "y": 578
                },
                {
                    "x": 202,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:16px'>In Figure 10, we present attention maps visualizations<br>for the input image shown in Figure 9. We see that attention<br>heads learn to specialize to different content and notably can<br>delineate object boundaries.</p>",
            "id": 184,
            "page": 13,
            "text": "In Figure 10, we present attention maps visualizations\nfor the input image shown in Figure 9. We see that attention\nheads learn to specialize to different content and notably can\ndelineate object boundaries."
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 677
                },
                {
                    "x": 1141,
                    "y": 677
                },
                {
                    "x": 1141,
                    "y": 1439
                },
                {
                    "x": 379,
                    "y": 1439
                }
            ],
            "category": "figure",
            "html": "<figure><img id='185' alt=\"\" data-coord=\"top-left:(379,677); bottom-right:(1141,1439)\" /></figure>",
            "id": 185,
            "page": 13,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1579
                },
                {
                    "x": 1201,
                    "y": 1579
                },
                {
                    "x": 1201,
                    "y": 1718
                },
                {
                    "x": 202,
                    "y": 1718
                }
            ],
            "category": "caption",
            "html": "<caption id='186' style='font-size:14px'>Figure 9. An input image. The red crosses indexed 1 to 4 represent<br>the pixel locations for which we show the attention maps in Figure<br>10.</caption>",
            "id": 186,
            "page": 13,
            "text": "Figure 9. An input image. The red crosses indexed 1 to 4 represent\nthe pixel locations for which we show the attention maps in Figure\n10."
        },
        {
            "bounding_box": [
                {
                    "x": 1330,
                    "y": 587
                },
                {
                    "x": 2270,
                    "y": 587
                },
                {
                    "x": 2270,
                    "y": 2458
                },
                {
                    "x": 1330,
                    "y": 2458
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='187' alt=\"\" data-coord=\"top-left:(1330,587); bottom-right:(2270,2458)\" /></figure>",
            "id": 187,
            "page": 13,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2493
                },
                {
                    "x": 2277,
                    "y": 2493
                },
                {
                    "x": 2277,
                    "y": 2682
                },
                {
                    "x": 1278,
                    "y": 2682
                }
            ],
            "category": "caption",
            "html": "<caption id='188' style='font-size:14px'>Figure 10. Visualization of attention maps for an augmented con-<br>volution in the Attention-Augmented-ResNet50. Rows corre-<br>spond to the 8 different heads and columns correspond to the 4<br>pixel locations depicted in the input image (See Figure 9).</caption>",
            "id": 188,
            "page": 13,
            "text": "Figure 10. Visualization of attention maps for an augmented con-\nvolution in the Attention-Augmented-ResNet50. Rows corre-\nspond to the 8 different heads and columns correspond to the 4\npixel locations depicted in the input image (See Figure 9)."
        }
    ]
}