{
    "id": "32bb5a6e-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1312.6229v4.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 451
                },
                {
                    "x": 2066,
                    "y": 451
                },
                {
                    "x": 2066,
                    "y": 696
                },
                {
                    "x": 486,
                    "y": 696
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>OverFeat:<br>Integrated Recognition, Localization and Detection<br>using Convolutional Networks</p>",
            "id": 0,
            "page": 1,
            "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 706,
                    "y": 874
                },
                {
                    "x": 1842,
                    "y": 874
                },
                {
                    "x": 1842,
                    "y": 967
                },
                {
                    "x": 706,
                    "y": 967
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:18px'>Pierre Sermanet David Eigen<br>Xiang Zhang Michael Mathieu Rob Fergus Yann LeCun</p>",
            "id": 1,
            "page": 1,
            "text": "Pierre Sermanet David Eigen Xiang Zhang Michael Mathieu Rob Fergus Yann LeCun"
        },
        {
            "bounding_box": [
                {
                    "x": 617,
                    "y": 965
                },
                {
                    "x": 1933,
                    "y": 965
                },
                {
                    "x": 1933,
                    "y": 1107
                },
                {
                    "x": 617,
                    "y": 1107
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:16px'>Courant Institute of Mathematical Sciences, New York University<br>719 Broadway, 12th Floor, New York, NY 10003<br>sermanet , deigen, xi ang, mathieu, fergus, yann@cs · nyu · edu</p>",
            "id": 2,
            "page": 1,
            "text": "Courant Institute of Mathematical Sciences, New York University 719 Broadway, 12th Floor, New York, NY 10003 sermanet , deigen, xi ang, mathieu, fergus, yann@cs · nyu · edu"
        },
        {
            "bounding_box": [
                {
                    "x": 1176,
                    "y": 1223
                },
                {
                    "x": 1372,
                    "y": 1223
                },
                {
                    "x": 1372,
                    "y": 1275
                },
                {
                    "x": 1176,
                    "y": 1275
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 591,
                    "y": 1340
                },
                {
                    "x": 1961,
                    "y": 1340
                },
                {
                    "x": 1961,
                    "y": 1895
                },
                {
                    "x": 591,
                    "y": 1895
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>We present an integrated framework for using Convolutional Networks for classi-<br>fication, localization and detection. We show how a multiscale and sliding window<br>approach can be efficiently implemented within a ConvNet. We also introduce a<br>novel deep learning approach to localization by learning to predict object bound-<br>aries. Bounding boxes are then accumulated rather than suppressed in order to<br>increase detection confidence. We show that different tasks can be learned simul-<br>taneously using a single shared network. This integrated framework is the winner<br>of the localization task of the ImageNet Large Scale Visual Recognition Challenge<br>2013 (ILSVRC2013) and obtained very competitive results for the detection and<br>classifications tasks. In post-competition work, we establish a new state of the art<br>for the detection task. Finally, we release a feature extractor from our best model<br>called OverFeat.</p>",
            "id": 4,
            "page": 1,
            "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2014
                },
                {
                    "x": 799,
                    "y": 2014
                },
                {
                    "x": 799,
                    "y": 2072
                },
                {
                    "x": 444,
                    "y": 2072
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1 Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2130
                },
                {
                    "x": 2109,
                    "y": 2130
                },
                {
                    "x": 2109,
                    "y": 2456
                },
                {
                    "x": 442,
                    "y": 2456
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Recognizing the category of the dominant object in an image is a tasks to which Convolutional<br>Networks (ConvNets) [17] have been applied for many years, whether the objects were handwritten<br>characters [16], house numbers [24], textureless toys [18], traffic signs [3, 26], objects from the<br>Caltech-101 dataset [14], or objects from the 1000-category ImageNet dataset [15]. The accuracy<br>of ConvNets on small datasets such as Caltech-101, while decent, has not been record-breaking.<br>However, the advent of larger datasets has enabled ConvNets to significantly advance the state of<br>the art on datasets such as the 1000-category ImageNet [5].</p>",
            "id": 6,
            "page": 1,
            "text": "Recognizing the category of the dominant object in an image is a tasks to which Convolutional Networks (ConvNets)  have been applied for many years, whether the objects were handwritten characters , house numbers , textureless toys , traffic signs , objects from the Caltech-101 dataset , or objects from the 1000-category ImageNet dataset . The accuracy of ConvNets on small datasets such as Caltech-101, while decent, has not been record-breaking. However, the advent of larger datasets has enabled ConvNets to significantly advance the state of the art on datasets such as the 1000-category ImageNet ."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2477
                },
                {
                    "x": 2110,
                    "y": 2477
                },
                {
                    "x": 2110,
                    "y": 2663
                },
                {
                    "x": 442,
                    "y": 2663
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>The main advantage of ConvNets for many such tasks is that the entire system is trained end to<br>end, from raw pixels to ultimate categories, thereby alleviating the requirement to manually design<br>a suitable feature extractor. The main disadvantage is their ravenous appetite for labeled training<br>samples.</p>",
            "id": 7,
            "page": 1,
            "text": "The main advantage of ConvNets for many such tasks is that the entire system is trained end to end, from raw pixels to ultimate categories, thereby alleviating the requirement to manually design a suitable feature extractor. The main disadvantage is their ravenous appetite for labeled training samples."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2683
                },
                {
                    "x": 2110,
                    "y": 2683
                },
                {
                    "x": 2110,
                    "y": 3057
                },
                {
                    "x": 442,
                    "y": 3057
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>The main point of this paper is to show that training a convolutional network to simultaneously<br>classify, locate and detect objects in images can boost the classification accuracy and the detection<br>and localization accuracy of all tasks. The paper proposes a new integrated approach to object<br>detection, recognition, and localization with a single ConvNet. We also introduce a novel method for<br>localization and detection by accumulating predicted bounding boxes. We suggest that by combining<br>many localization predictions, detection can be performed without training on background samples<br>and that it is possible to avoid the time-consuming and complicated bootstrapping training passes.<br>Not training on background also lets the network focus solely on positive classes for higher accuracy.</p>",
            "id": 8,
            "page": 1,
            "text": "The main point of this paper is to show that training a convolutional network to simultaneously classify, locate and detect objects in images can boost the classification accuracy and the detection and localization accuracy of all tasks. The paper proposes a new integrated approach to object detection, recognition, and localization with a single ConvNet. We also introduce a novel method for localization and detection by accumulating predicted bounding boxes. We suggest that by combining many localization predictions, detection can be performed without training on background samples and that it is possible to avoid the time-consuming and complicated bootstrapping training passes. Not training on background also lets the network focus solely on positive classes for higher accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 63,
                    "y": 816
                },
                {
                    "x": 149,
                    "y": 816
                },
                {
                    "x": 149,
                    "y": 2239
                },
                {
                    "x": 63,
                    "y": 2239
                }
            ],
            "category": "footer",
            "html": "<br><footer id='9' style='font-size:14px'>2014<br>Feb<br>24<br>[cs.CV]<br>arXiv:1312.6229v4</footer>",
            "id": 9,
            "page": 1,
            "text": "2014 Feb 24 [cs.CV] arXiv:1312.6229v4"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3131
                },
                {
                    "x": 1290,
                    "y": 3131
                },
                {
                    "x": 1290,
                    "y": 3171
                },
                {
                    "x": 1261,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='10' style='font-size:14px'>1</footer>",
            "id": 10,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 346
                },
                {
                    "x": 2105,
                    "y": 346
                },
                {
                    "x": 2105,
                    "y": 438
                },
                {
                    "x": 441,
                    "y": 438
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:14px'>Experiments are conducted on the ImageNet ILSVRC 2012 and 2013 datasets and establish state of<br>the art results on the ILSVRC 2013 localization and detection tasks.</p>",
            "id": 11,
            "page": 2,
            "text": "Experiments are conducted on the ImageNet ILSVRC 2012 and 2013 datasets and establish state of the art results on the ILSVRC 2013 localization and detection tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 463
                },
                {
                    "x": 2107,
                    "y": 463
                },
                {
                    "x": 2107,
                    "y": 923
                },
                {
                    "x": 442,
                    "y": 923
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:16px'>While images from the ImageNet classification dataset are largely chosen to contain a roughly-<br>centered object that fills much of the image, objects of interest sometimes vary significantly in size<br>and position within the image. The first idea in addressing this is to apply a ConvNet at multiple<br>locations in the image, in a sliding window fashion, and over multiple scales. Even with this,<br>however, many viewing windows may contain a perfectly identifiable portion of the object (say,<br>the head of a dog), but not the entire object, nor even the center of the object. This leads to decent<br>classification but poor localization and detection. Thus, the second idea is to train the system to not<br>only produce a distribution over categories for each window, but also to produce a prediction of the<br>location and size of the bounding box containing the object relative to the window. The third idea is<br>to accumulate the evidence for each category at each location and size.</p>",
            "id": 12,
            "page": 2,
            "text": "While images from the ImageNet classification dataset are largely chosen to contain a roughlycentered object that fills much of the image, objects of interest sometimes vary significantly in size and position within the image. The first idea in addressing this is to apply a ConvNet at multiple locations in the image, in a sliding window fashion, and over multiple scales. Even with this, however, many viewing windows may contain a perfectly identifiable portion of the object (say, the head of a dog), but not the entire object, nor even the center of the object. This leads to decent classification but poor localization and detection. Thus, the second idea is to train the system to not only produce a distribution over categories for each window, but also to produce a prediction of the location and size of the bounding box containing the object relative to the window. The third idea is to accumulate the evidence for each category at each location and size."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 945
                },
                {
                    "x": 2108,
                    "y": 945
                },
                {
                    "x": 2108,
                    "y": 1130
                },
                {
                    "x": 441,
                    "y": 1130
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:20px'>Many authors have proposed to use ConvNets for detection and localization with a sliding window<br>over multiple scales, going back to the early 1990's for multi-character strings [20], faces [30], and<br>hands [22]. More recently, ConvNets have been shown to yield state of the art performance on text<br>detection in natural images [4], face detection [8, 23] and pedestrian detection [25].</p>",
            "id": 13,
            "page": 2,
            "text": "Many authors have proposed to use ConvNets for detection and localization with a sliding window over multiple scales, going back to the early 1990's for multi-character strings , faces , and hands . More recently, ConvNets have been shown to yield state of the art performance on text detection in natural images , face detection  and pedestrian detection ."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1153
                },
                {
                    "x": 2107,
                    "y": 1153
                },
                {
                    "x": 2107,
                    "y": 1748
                },
                {
                    "x": 441,
                    "y": 1748
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:14px'>Several authors have also proposed to train ConvNets to directly predict the instantiation parameters<br>of the objects to be located, such as the position relative to the viewing window, or the pose of<br>the object. For example Osadchy et al. [23] describe a ConvNet for simultaneous face detection<br>and pose estimation. Faces are represented by a 3D manifold in the nine-dimensional output space.<br>Positions on the manifold indicate the pose (pitch, yaw, and roll). When the training image is a<br>face, the network is trained to produce a point on the manifold at the location of the known pose.<br>If the image is not a face, the output is pushed away from the manifold. At test time, the distance<br>to the manifold indicate whether the image contains a face, and the position of the closest point on<br>the manifold indicates pose. Taylor et al. [27, 28] use a ConvNet to estimate the location of body<br>parts (hands, head, etc) SO as to derive the human body pose. They use a metric learning criterion<br>to train the network to produce points on a body pose manifold. Hinton et al. have also proposed<br>to train networks to compute explicit instantiation parameters of features as part of a recognition<br>process [12].</p>",
            "id": 14,
            "page": 2,
            "text": "Several authors have also proposed to train ConvNets to directly predict the instantiation parameters of the objects to be located, such as the position relative to the viewing window, or the pose of the object. For example Osadchy   describe a ConvNet for simultaneous face detection and pose estimation. Faces are represented by a 3D manifold in the nine-dimensional output space. Positions on the manifold indicate the pose (pitch, yaw, and roll). When the training image is a face, the network is trained to produce a point on the manifold at the location of the known pose. If the image is not a face, the output is pushed away from the manifold. At test time, the distance to the manifold indicate whether the image contains a face, and the position of the closest point on the manifold indicates pose. Taylor   use a ConvNet to estimate the location of body parts (hands, head, etc) SO as to derive the human body pose. They use a metric learning criterion to train the network to produce points on a body pose manifold. Hinton  have also proposed to train networks to compute explicit instantiation parameters of features as part of a recognition process ."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1770
                },
                {
                    "x": 2109,
                    "y": 1770
                },
                {
                    "x": 2109,
                    "y": 2504
                },
                {
                    "x": 441,
                    "y": 2504
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:20px'>Other authors have proposed to perform object localization via ConvNet-based segmentation. The<br>simplest approach consists in training the ConvNet to classify the central pixel (or voxel for vol-<br>umetric images) of its viewing window as a boundary between regions or not [13]. But when the<br>regions must be categorized, it is preferable to perform semantic segmentation. The main idea is to<br>train the ConvNet to classify the central pixel of the viewing window with the category of the ob-<br>ject it belongs to, using the window as context for the decision. Applications range from biological<br>image analysis [21], to obstacle tagging for mobile robots [10] to tagging of photos [7]. The ad-<br>vantage of this approach is that the bounding contours need not be rectangles, and the regions need<br>not be well-circumscribed objects. The disadvantage is that it requires dense pixel-level labels for<br>training. This segmentation pre-processing or object proposal step has recently gained popularity in<br>traditional computer vision to reduce the search space of position, scale and aspect ratio for detec-<br>tion [19, 2, 6, 29]. Hence an expensive classification method can be applied at the optimal location<br>in the search space, thus increasing recognition accuracy. Additionally, [29, 1] suggest that these<br>methods improve accuracy by drastically reducing unlikely object regions, hence reducing potential<br>false positives. Our dense sliding window method, however, is able to outperform object proposal<br>methods on the ILSVRC13 detection dataset.</p>",
            "id": 15,
            "page": 2,
            "text": "Other authors have proposed to perform object localization via ConvNet-based segmentation. The simplest approach consists in training the ConvNet to classify the central pixel (or voxel for volumetric images) of its viewing window as a boundary between regions or not . But when the regions must be categorized, it is preferable to perform semantic segmentation. The main idea is to train the ConvNet to classify the central pixel of the viewing window with the category of the object it belongs to, using the window as context for the decision. Applications range from biological image analysis , to obstacle tagging for mobile robots  to tagging of photos . The advantage of this approach is that the bounding contours need not be rectangles, and the regions need not be well-circumscribed objects. The disadvantage is that it requires dense pixel-level labels for training. This segmentation pre-processing or object proposal step has recently gained popularity in traditional computer vision to reduce the search space of position, scale and aspect ratio for detection . Hence an expensive classification method can be applied at the optimal location in the search space, thus increasing recognition accuracy. Additionally,  suggest that these methods improve accuracy by drastically reducing unlikely object regions, hence reducing potential false positives. Our dense sliding window method, however, is able to outperform object proposal methods on the ILSVRC13 detection dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2526
                },
                {
                    "x": 2107,
                    "y": 2526
                },
                {
                    "x": 2107,
                    "y": 2801
                },
                {
                    "x": 441,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:20px'>Krizhevsky et al. [15] recently demonstrated impressive classification performance using a large<br>ConvNet. The authors also entered the ImageNet 2012 competition, winning both the classification<br>and localization challenges. Although they demonstrated an impressive localization performance,<br>there has been no published work describing how their approach. Our paper is thus the first to<br>provide a clear explanation how ConvNets can be used for localization and detection for ImageNet<br>data.</p>",
            "id": 16,
            "page": 2,
            "text": "Krizhevsky   recently demonstrated impressive classification performance using a large ConvNet. The authors also entered the ImageNet 2012 competition, winning both the classification and localization challenges. Although they demonstrated an impressive localization performance, there has been no published work describing how their approach. Our paper is thus the first to provide a clear explanation how ConvNets can be used for localization and detection for ImageNet data."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2826
                },
                {
                    "x": 2107,
                    "y": 2826
                },
                {
                    "x": 2107,
                    "y": 2966
                },
                {
                    "x": 441,
                    "y": 2966
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:16px'>In this paper we use the terms localization and detection in a way that is consistent with their use in<br>the ImageNet 2013 competition, namely that the only difference is the evaluation criterion used and<br>both involve predicting the bounding box for each object in the image.</p>",
            "id": 17,
            "page": 2,
            "text": "In this paper we use the terms localization and detection in a way that is consistent with their use in the ImageNet 2013 competition, namely that the only difference is the evaluation criterion used and both involve predicting the bounding box for each object in the image."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1260,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='18' style='font-size:14px'>2</footer>",
            "id": 18,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 604,
                    "y": 291
                },
                {
                    "x": 1945,
                    "y": 291
                },
                {
                    "x": 1945,
                    "y": 1878
                },
                {
                    "x": 604,
                    "y": 1878
                }
            ],
            "category": "figure",
            "html": "<figure><img id='19' style='font-size:14px' alt=\"Top 5: Groundtruth:\nwhite wolf white wolf\nwhite wolf white wolf (2)\ntim ber wolf white wolf (3)\ntim ber wolf white wolf (4)\nArctic fox white wolf (5)\nTop predictions: Groundtruth:\nperson (confidence 6.0) drum\n하나 - 201111111111 lamp\nlamp (2)\nguitar\nperson\nperson (2)\nperson (3)\nmicrophone\nmicrophone (2)\" data-coord=\"top-left:(604,291); bottom-right:(1945,1878)\" /></figure>",
            "id": 19,
            "page": 3,
            "text": "Top 5: Groundtruth: white wolf white wolf white wolf white wolf (2) tim ber wolf white wolf (3) tim ber wolf white wolf (4) Arctic fox white wolf (5) Top predictions: Groundtruth: person (confidence 6.0) drum 하나 - 201111111111 lamp lamp (2) guitar person person (2) person (3) microphone microphone (2)"
        },
        {
            "bounding_box": [
                {
                    "x": 1296,
                    "y": 1864
                },
                {
                    "x": 1528,
                    "y": 1864
                },
                {
                    "x": 1528,
                    "y": 1901
                },
                {
                    "x": 1296,
                    "y": 1901
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:14px'>microphone (3)</p>",
            "id": 20,
            "page": 3,
            "text": "microphone (3)"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1938
                },
                {
                    "x": 2107,
                    "y": 1938
                },
                {
                    "x": 2107,
                    "y": 2172
                },
                {
                    "x": 442,
                    "y": 2172
                }
            ],
            "category": "caption",
            "html": "<caption id='21' style='font-size:20px'>Figure 1: Localization (top) and detection tasks (bottom). The left images contains our predic-<br>tions (ordered by decreasing confidence) while the right images show the groundtruth labels. The<br>detection image (bottom) illustrates the higher difficulty of the detection dataset, which can contain<br>many small objects while the classification and localization images typically contain a single large<br>object.</caption>",
            "id": 21,
            "page": 3,
            "text": "Figure 1: Localization (top) and detection tasks (bottom). The left images contains our predictions (ordered by decreasing confidence) while the right images show the groundtruth labels. The detection image (bottom) illustrates the higher difficulty of the detection dataset, which can contain many small objects while the classification and localization images typically contain a single large object."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2269
                },
                {
                    "x": 796,
                    "y": 2269
                },
                {
                    "x": 796,
                    "y": 2323
                },
                {
                    "x": 444,
                    "y": 2323
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:22px'>2 Vision Tasks</p>",
            "id": 22,
            "page": 3,
            "text": "2 Vision Tasks"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2387
                },
                {
                    "x": 2108,
                    "y": 2387
                },
                {
                    "x": 2108,
                    "y": 2571
                },
                {
                    "x": 443,
                    "y": 2571
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:18px'>In this paper, we explore three computer vision tasks in increasing order of difficulty: (i) classi-<br>fication, (ii) localization, and (iii) detection. Each task is a sub-task of the next. While all tasks<br>are adressed using a single framework and a shared feature learning base, we will describe them<br>separately in the following sections.</p>",
            "id": 23,
            "page": 3,
            "text": "In this paper, we explore three computer vision tasks in increasing order of difficulty: (i) classification, (ii) localization, and (iii) detection. Each task is a sub-task of the next. While all tasks are adressed using a single framework and a shared feature learning base, we will describe them separately in the following sections."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2593
                },
                {
                    "x": 2109,
                    "y": 2593
                },
                {
                    "x": 2109,
                    "y": 3058
                },
                {
                    "x": 442,
                    "y": 3058
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:18px'>Throughout the paper, we report results on the 2013 ImageNet Large Scale Visual Recognition Chal-<br>lenge (ILSVRC2013). In the classification task of this challenge, each image is assigned a single<br>label corresponding to the main object in the image. Five guesses are allowed to find the correct<br>answer (this is because images can also contain multiple unlabeled objects). The localization task<br>is similar in that 5 guesses are allowed per image, but in addition, a bounding box for the predicted<br>object must be returned with each guess. To be considered correct, the predicted box must match<br>the groundtruth by at least 50% (using the PASCAL criterion of union over intersection), as well as<br>be labeled with the correct class (i.e. each prediction is a label and bounding box that are associated<br>together). The detection task differs from localization in that there can be any number of objects<br>in each image (including zero), and false positives are penalized by the mean average precision</p>",
            "id": 24,
            "page": 3,
            "text": "Throughout the paper, we report results on the 2013 ImageNet Large Scale Visual Recognition Challenge (ILSVRC2013). In the classification task of this challenge, each image is assigned a single label corresponding to the main object in the image. Five guesses are allowed to find the correct answer (this is because images can also contain multiple unlabeled objects). The localization task is similar in that 5 guesses are allowed per image, but in addition, a bounding box for the predicted object must be returned with each guess. To be considered correct, the predicted box must match the groundtruth by at least 50% (using the PASCAL criterion of union over intersection), as well as be labeled with the correct class (i.e. each prediction is a label and bounding box that are associated together). The detection task differs from localization in that there can be any number of objects in each image (including zero), and false positives are penalized by the mean average precision"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3132
                },
                {
                    "x": 1287,
                    "y": 3132
                },
                {
                    "x": 1287,
                    "y": 3169
                },
                {
                    "x": 1261,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='25' style='font-size:16px'>3</footer>",
            "id": 25,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 344
                },
                {
                    "x": 2109,
                    "y": 344
                },
                {
                    "x": 2109,
                    "y": 671
                },
                {
                    "x": 442,
                    "y": 671
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:16px'>(mAP) measure. The localization task is a convenient intermediate step between classification and<br>detection, and allows us to evaluate our localization method independently of challenges specific to<br>detection (such as learning a background class). In Fig. 1, we show examples of images with our<br>localization/detection predictions as well as corresponding groundtruth. Note that classification and<br>localization share the same dataset, while detection also has additional data where objects can be<br>smaller. The detection data also contain a set of images where certain objects are absent. This can<br>be used for bootstrapping, but we have not made use of it in this work.</p>",
            "id": 26,
            "page": 4,
            "text": "(mAP) measure. The localization task is a convenient intermediate step between classification and detection, and allows us to evaluate our localization method independently of challenges specific to detection (such as learning a background class). In Fig. 1, we show examples of images with our localization/detection predictions as well as corresponding groundtruth. Note that classification and localization share the same dataset, while detection also has additional data where objects can be smaller. The detection data also contain a set of images where certain objects are absent. This can be used for bootstrapping, but we have not made use of it in this work."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 745
                },
                {
                    "x": 816,
                    "y": 745
                },
                {
                    "x": 816,
                    "y": 798
                },
                {
                    "x": 443,
                    "y": 798
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:22px'>3 Classification</p>",
            "id": 27,
            "page": 4,
            "text": "3 Classification"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 855
                },
                {
                    "x": 2108,
                    "y": 855
                },
                {
                    "x": 2108,
                    "y": 1041
                },
                {
                    "x": 442,
                    "y": 1041
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>Our classification architecture is similar to the best ILSVRC12 architecture by Krizhevsky et al. [15].<br>However, we improve on the network design and the inference step. Because of time constraints,<br>some of the training features in Krizhevsky's model were not explored, and so we expect our results<br>can be improved even further. These are discussed in the future work section 6</p>",
            "id": 28,
            "page": 4,
            "text": "Our classification architecture is similar to the best ILSVRC12 architecture by Krizhevsky  . However, we improve on the network design and the inference step. Because of time constraints, some of the training features in Krizhevsky's model were not explored, and so we expect our results can be improved even further. These are discussed in the future work section 6"
        },
        {
            "bounding_box": [
                {
                    "x": 774,
                    "y": 1060
                },
                {
                    "x": 1770,
                    "y": 1060
                },
                {
                    "x": 1770,
                    "y": 2027
                },
                {
                    "x": 774,
                    "y": 2027
                }
            ],
            "category": "figure",
            "html": "<figure><img id='29' alt=\"\" data-coord=\"top-left:(774,1060); bottom-right:(1770,2027)\" /></figure>",
            "id": 29,
            "page": 4,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 822,
                    "y": 2057
                },
                {
                    "x": 1725,
                    "y": 2057
                },
                {
                    "x": 1725,
                    "y": 2106
                },
                {
                    "x": 822,
                    "y": 2106
                }
            ],
            "category": "caption",
            "html": "<caption id='30' style='font-size:20px'>Figure 2: Layer 1 (top) and layer 2 filters (bottom).</caption>",
            "id": 30,
            "page": 4,
            "text": "Figure 2: Layer 1 (top) and layer 2 filters (bottom)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2203
                },
                {
                    "x": 1035,
                    "y": 2203
                },
                {
                    "x": 1035,
                    "y": 2256
                },
                {
                    "x": 442,
                    "y": 2256
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:20px'>3.1 Model Design and Training</p>",
            "id": 31,
            "page": 4,
            "text": "3.1 Model Design and Training"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2293
                },
                {
                    "x": 2107,
                    "y": 2293
                },
                {
                    "x": 2107,
                    "y": 2755
                },
                {
                    "x": 443,
                    "y": 2755
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>We train the network on the ImageNet 2012 training set (1.2 million images and C = 1000 classes)<br>[5]. Our model uses the same fixed input size approach proposed by Krizhevsky et al. [15] during<br>training but turns to multi-scale for classification as described in the next section. Each image is<br>downsampled so that the smallest dimension is 256 pixels. We then extract 5 random crops (and<br>their horizontal flips) of size 221x221 pixels and present these to the network in mini-batches of<br>size 128. The weights in the network are initialized randomly with (�, o) = (0, 1 x 10-2). They<br>are then updated by stochastic gradient descent, accompanied by momentum term of 0.6 and an l2<br>weight decay of 1 x 10-5 The learning rate is initially 5 x 10-2 and is successively decreased by<br>·<br>a factor of 0.5 after (30, 50, 60, 70, 80) epochs. DropOut [11] with a rate of 0.5 is employed on the<br>fully connected layers (6th and 7th) in the classifier.</p>",
            "id": 32,
            "page": 4,
            "text": "We train the network on the ImageNet 2012 training set (1.2 million images and C = 1000 classes) . Our model uses the same fixed input size approach proposed by Krizhevsky   during training but turns to multi-scale for classification as described in the next section. Each image is downsampled so that the smallest dimension is 256 pixels. We then extract 5 random crops (and their horizontal flips) of size 221x221 pixels and present these to the network in mini-batches of size 128. The weights in the network are initialized randomly with (�, o) = (0, 1 x 10-2). They are then updated by stochastic gradient descent, accompanied by momentum term of 0.6 and an l2 weight decay of 1 x 10-5 The learning rate is initially 5 x 10-2 and is successively decreased by · a factor of 0.5 after (30, 50, 60, 70, 80) epochs. DropOut  with a rate of 0.5 is employed on the fully connected layers (6th and 7th) in the classifier."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2777
                },
                {
                    "x": 2109,
                    "y": 2777
                },
                {
                    "x": 2109,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:18px'>We detail the architecture sizes in tables 1 and 3. Note that during training, we treat this architecture<br>as non-spatial (output maps of size 1x1), as opposed to the inference step, which produces spatial<br>outputs. Layers 1-5 are similar to Krizhevsky et al. [15], using rectification (\"relu\") non-linearities<br>and max pooling, but with the following differences: (i) no contrast normalization is used; (ii)<br>pooling regions are non-overlapping and (iii) our model has larger 1st and 2nd layer feature maps,<br>thanks to a smaller stride (2 instead of 4). A larger stride is beneficial for speed but will hurt accuracy.</p>",
            "id": 33,
            "page": 4,
            "text": "We detail the architecture sizes in tables 1 and 3. Note that during training, we treat this architecture as non-spatial (output maps of size 1x1), as opposed to the inference step, which produces spatial outputs. Layers 1-5 are similar to Krizhevsky  , using rectification (\"relu\") non-linearities and max pooling, but with the following differences: (i) no contrast normalization is used; (ii) pooling regions are non-overlapping and (iii) our model has larger 1st and 2nd layer feature maps, thanks to a smaller stride (2 instead of 4). A larger stride is beneficial for speed but will hurt accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1287,
                    "y": 3134
                },
                {
                    "x": 1287,
                    "y": 3167
                },
                {
                    "x": 1260,
                    "y": 3167
                }
            ],
            "category": "footer",
            "html": "<footer id='34' style='font-size:14px'>4</footer>",
            "id": 34,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 333
                },
                {
                    "x": 2025,
                    "y": 333
                },
                {
                    "x": 2025,
                    "y": 707
                },
                {
                    "x": 517,
                    "y": 707
                }
            ],
            "category": "table",
            "html": "<table id='35' style='font-size:14px'><tr><td>Layer</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>Output 8</td></tr><tr><td>Stage</td><td>conv + max</td><td>conv + max</td><td>conv</td><td>conv</td><td>conv + max</td><td>full</td><td>full</td><td>full</td></tr><tr><td># channels</td><td>96</td><td>256</td><td>512</td><td>1024</td><td>1024</td><td>3072</td><td>4096</td><td>1000</td></tr><tr><td>Filter size</td><td>11x11</td><td>5x5</td><td>3x3</td><td>3x3</td><td>3x3</td><td></td><td></td><td>-</td></tr><tr><td>Conv. stride</td><td>4x4</td><td>1x1</td><td>1x1</td><td>1x1</td><td>1x1</td><td></td><td></td><td>-</td></tr><tr><td>Pooling size</td><td>2x2</td><td>2x2</td><td>-</td><td>-</td><td>2x2</td><td></td><td></td><td>-</td></tr><tr><td>Pooling stride</td><td>2x2</td><td>2x2</td><td>-</td><td>-</td><td>2x2</td><td></td><td></td><td>-</td></tr><tr><td>Zero-Padding size</td><td>-</td><td>-</td><td>lxlxlxl</td><td>lxlxlxl</td><td>lxlxlxl</td><td></td><td></td><td>-</td></tr><tr><td>Spatial input size</td><td>231x231</td><td>24x24</td><td>12x12</td><td>12x12</td><td>12x12</td><td>6x6</td><td>1x1</td><td>1x1</td></tr></table>",
            "id": 35,
            "page": 5,
            "text": "Layer 1 2 3 4 5 6 7 Output 8  Stage conv + max conv + max conv conv conv + max full full full  # channels 96 256 512 1024 1024 3072 4096 1000  Filter size 11x11 5x5 3x3 3x3 3x3    Conv. stride 4x4 1x1 1x1 1x1 1x1    Pooling size 2x2 2x2 - - 2x2    Pooling stride 2x2 2x2 - - 2x2    Zero-Padding size - - lxlxlxl lxlxlxl lxlxlxl    Spatial input size 231x231 24x24 12x12 12x12 12x12 6x6 1x1"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 760
                },
                {
                    "x": 2108,
                    "y": 760
                },
                {
                    "x": 2108,
                    "y": 1040
                },
                {
                    "x": 441,
                    "y": 1040
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:18px'>Table 1: Architecture specifics for fast model. The spatial size of the feature maps depends on<br>the input image size, which varies during our inference step (see Table 5 in the Appendix). Here<br>we show training spatial sizes. Layer 5 is the top convolutional layer. Subsequent layers are fully<br>connected, and applied in sliding window fashion at test time. The fully-connected layers can also<br>be seen as 1x1 convolutions in a spatial setting. Similar sizes for accurate model can be found in<br>the Appendix.</p>",
            "id": 36,
            "page": 5,
            "text": "Table 1: Architecture specifics for fast model. The spatial size of the feature maps depends on the input image size, which varies during our inference step (see Table 5 in the Appendix). Here we show training spatial sizes. Layer 5 is the top convolutional layer. Subsequent layers are fully connected, and applied in sliding window fashion at test time. The fully-connected layers can also be seen as 1x1 convolutions in a spatial setting. Similar sizes for accurate model can be found in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1148
                },
                {
                    "x": 2108,
                    "y": 1148
                },
                {
                    "x": 2108,
                    "y": 1289
                },
                {
                    "x": 441,
                    "y": 1289
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:18px'>In Fig. 2, we show the filter coefficients from the first two convolutional layers. The first layer filters<br>capture orientated edges, patterns and blobs. In the second layer, the filters have a variety of forms,<br>some diffuse, others with strong line structures or oriented edges.</p>",
            "id": 37,
            "page": 5,
            "text": "In Fig. 2, we show the filter coefficients from the first two convolutional layers. The first layer filters capture orientated edges, patterns and blobs. In the second layer, the filters have a variety of forms, some diffuse, others with strong line structures or oriented edges."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1348
                },
                {
                    "x": 868,
                    "y": 1348
                },
                {
                    "x": 868,
                    "y": 1397
                },
                {
                    "x": 442,
                    "y": 1397
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:20px'>3.2 Feature Extractor</p>",
            "id": 38,
            "page": 5,
            "text": "3.2 Feature Extractor"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1436
                },
                {
                    "x": 2108,
                    "y": 1436
                },
                {
                    "x": 2108,
                    "y": 1716
                },
                {
                    "x": 441,
                    "y": 1716
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>Along with this paper, we release a feature extractor named \"OverFeat\" 1 in order to provide power-<br>ful features for computer vision research. Two models are provided, a fast and accurate one. Each<br>architecture is described in tables 1 and 3. We also compare their sizes in Table 4 in terms of param-<br>eters and connections. The accurate model is more accurate than the fast one (14.18% classification<br>error as opposed to 16.39% in Table 2), however it requires nearly twice as many connections. Using<br>a committee of 7 accurate models reaches 13.6% classification error as shown in Fig. 4.</p>",
            "id": 39,
            "page": 5,
            "text": "Along with this paper, we release a feature extractor named \"OverFeat\" 1 in order to provide powerful features for computer vision research. Two models are provided, a fast and accurate one. Each architecture is described in tables 1 and 3. We also compare their sizes in Table 4 in terms of parameters and connections. The accurate model is more accurate than the fast one (14.18% classification error as opposed to 16.39% in Table 2), however it requires nearly twice as many connections. Using a committee of 7 accurate models reaches 13.6% classification error as shown in Fig. 4."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1773
                },
                {
                    "x": 1000,
                    "y": 1773
                },
                {
                    "x": 1000,
                    "y": 1822
                },
                {
                    "x": 442,
                    "y": 1822
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:18px'>3.3 Multi-Scale Classification</p>",
            "id": 40,
            "page": 5,
            "text": "3.3 Multi-Scale Classification"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1863
                },
                {
                    "x": 2108,
                    "y": 1863
                },
                {
                    "x": 2108,
                    "y": 2048
                },
                {
                    "x": 442,
                    "y": 2048
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>In [15], multi-view voting is used to boost performance: a fixed set of 10 views (4 corners and center,<br>with horizontal flip) is averaged. However, this approach can ignore many regions of the image, and<br>is computationally redundant when views overlap. Additionally, it is only applied at a single scale,<br>which may not be the scale at which the ConvNet will respond with optimal confidence.</p>",
            "id": 41,
            "page": 5,
            "text": "In , multi-view voting is used to boost performance: a fixed set of 10 views (4 corners and center, with horizontal flip) is averaged. However, this approach can ignore many regions of the image, and is computationally redundant when views overlap. Additionally, it is only applied at a single scale, which may not be the scale at which the ConvNet will respond with optimal confidence."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2071
                },
                {
                    "x": 2108,
                    "y": 2071
                },
                {
                    "x": 2108,
                    "y": 2345
                },
                {
                    "x": 440,
                    "y": 2345
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:18px'>Instead, we explore the entire image by densely running the network at each location and at multiple<br>scales. While the sliding window approach may be computationally prohibitive for certain types<br>of model, it is inherently efficient in the case of ConvNets (see section 3.5). This approach yields<br>significantly more views for voting, which increases robustness while remaining efficient. The result<br>of convolving a ConvNet on an image of arbitrary size is a spatial map of C-dimensional vectors at<br>each scale.</p>",
            "id": 42,
            "page": 5,
            "text": "Instead, we explore the entire image by densely running the network at each location and at multiple scales. While the sliding window approach may be computationally prohibitive for certain types of model, it is inherently efficient in the case of ConvNets (see section 3.5). This approach yields significantly more views for voting, which increases robustness while remaining efficient. The result of convolving a ConvNet on an image of arbitrary size is a spatial map of C-dimensional vectors at each scale."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2368
                },
                {
                    "x": 2107,
                    "y": 2368
                },
                {
                    "x": 2107,
                    "y": 2737
                },
                {
                    "x": 442,
                    "y": 2737
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:18px'>However, the total subsampling ratio in the network described above is 2x3x2x3, or 36. Hence<br>when applied densely, this architecture can only produce a classification vector every 36 pixels in<br>the input dimension along each axis. This coarse distribution of outputs decreases performance<br>compared to the 10-view scheme because the network windows are not well aligned with the objects<br>in the images. The better aligned the network window and the object, the strongest the confidence of<br>the network response. To circumvent this problem, we take an approach similar to that introduced<br>by Giusti et al. [9], and apply the last subsampling operation at every offset. This removes the loss<br>of resolution from this layer, yielding a total subsampling ratio of x12 instead of x36.</p>",
            "id": 43,
            "page": 5,
            "text": "However, the total subsampling ratio in the network described above is 2x3x2x3, or 36. Hence when applied densely, this architecture can only produce a classification vector every 36 pixels in the input dimension along each axis. This coarse distribution of outputs decreases performance compared to the 10-view scheme because the network windows are not well aligned with the objects in the images. The better aligned the network window and the object, the strongest the confidence of the network response. To circumvent this problem, we take an approach similar to that introduced by Giusti  , and apply the last subsampling operation at every offset. This removes the loss of resolution from this layer, yielding a total subsampling ratio of x12 instead of x36."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2759
                },
                {
                    "x": 2108,
                    "y": 2759
                },
                {
                    "x": 2108,
                    "y": 2900
                },
                {
                    "x": 442,
                    "y": 2900
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>We now explain in detail how the resolution augmentation is performed. We use 6 scales of input<br>which result in unpooled layer 5 maps of varying resolution (see Table 5 for details). These are then<br>pooled and presented to the classifier using the following procedure, illustrated in Fig. 3:</p>",
            "id": 44,
            "page": 5,
            "text": "We now explain in detail how the resolution augmentation is performed. We use 6 scales of input which result in unpooled layer 5 maps of varying resolution (see Table 5 for details). These are then pooled and presented to the classifier using the following procedure, illustrated in Fig. 3:"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2941
                },
                {
                    "x": 1922,
                    "y": 2941
                },
                {
                    "x": 1922,
                    "y": 2988
                },
                {
                    "x": 446,
                    "y": 2988
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:18px'>(a) For a single image, at a given scale, we start with the unpooled layer 5 feature maps.</p>",
            "id": 45,
            "page": 5,
            "text": "(a) For a single image, at a given scale, we start with the unpooled layer 5 feature maps."
        },
        {
            "bounding_box": [
                {
                    "x": 500,
                    "y": 3008
                },
                {
                    "x": 1356,
                    "y": 3008
                },
                {
                    "x": 1356,
                    "y": 3052
                },
                {
                    "x": 500,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:22px'>http://chonyu.aciadcupty/photosoftwarectranteriatt/</p>",
            "id": 46,
            "page": 5,
            "text": "http://chonyu.aciadcupty/photosoftwarectranteriatt/"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3169
                },
                {
                    "x": 1260,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='47' style='font-size:14px'>5</footer>",
            "id": 47,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 344
                },
                {
                    "x": 2110,
                    "y": 344
                },
                {
                    "x": 2110,
                    "y": 783
                },
                {
                    "x": 442,
                    "y": 783
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:20px'>(b) Each of unpooled maps undergoes a 3x3 max pooling operation (non-overlapping regions),<br>repeated 3x3 times for (△ x, △y) pixel offsets of {0, 1, 2}.<br>(c) This produces a set of pooled feature maps, replicated (3x3) times for different (△x, △y) com-<br>binations.<br>(d) The classifier (layers 6,7,8) has a fixed input size of 5x5 and produces a C-dimensional output<br>vector for each location within the pooled maps. The classifier is applied in sliding-window<br>fashion to the pooled maps, yielding C-dimensional output maps (for a given (△x, △y) combi-<br>nation).<br>(e) The output maps for different (△x, △y) combinations are reshaped into a single 3D output map<br>(two spatial dimensions x C classes).</p>",
            "id": 48,
            "page": 6,
            "text": "(b) Each of unpooled maps undergoes a 3x3 max pooling operation (non-overlapping regions), repeated 3x3 times for (△ x, △y) pixel offsets of {0, 1, 2}. (c) This produces a set of pooled feature maps, replicated (3x3) times for different (△x, △y) combinations. (d) The classifier (layers 6,7,8) has a fixed input size of 5x5 and produces a C-dimensional output vector for each location within the pooled maps. The classifier is applied in sliding-window fashion to the pooled maps, yielding C-dimensional output maps (for a given (△x, △y) combination). (e) The output maps for different (△x, △y) combinations are reshaped into a single 3D output map (two spatial dimensions x C classes)."
        },
        {
            "bounding_box": [
                {
                    "x": 792,
                    "y": 835
                },
                {
                    "x": 1773,
                    "y": 835
                },
                {
                    "x": 1773,
                    "y": 1318
                },
                {
                    "x": 792,
                    "y": 1318
                }
            ],
            "category": "figure",
            "html": "<figure><img id='49' style='font-size:14px' alt=\"(b) △=2 1 2 3 4 5 6\n△=1 1 2 3 4 5 6\n△=0 1 2 3 4 5 6\nLayer 5\n(a) unpooled\n1 2 3 4 5 6 7 8 9 1011 121314151617181920 map\n(c) A#2 , Classifier (6,7,8) (e) Output map\n!A=1! Classifier (6,7,8)\nX C classes\n' |△=이 : Classifier (6,7,8)\nX 256 maps (d) X C classes\" data-coord=\"top-left:(792,835); bottom-right:(1773,1318)\" /></figure>",
            "id": 49,
            "page": 6,
            "text": "(b) △=2 1 2 3 4 5 6 △=1 1 2 3 4 5 6 △=0 1 2 3 4 5 6 Layer 5 (a) unpooled 1 2 3 4 5 6 7 8 9 1011 121314151617181920 map (c) A#2 , Classifier (6,7,8) (e) Output map !A=1! Classifier (6,7,8) X C classes ' |△=이 : Classifier (6,7,8) X 256 maps (d) X C classes"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1355
                },
                {
                    "x": 2108,
                    "y": 1355
                },
                {
                    "x": 2108,
                    "y": 1634
                },
                {
                    "x": 441,
                    "y": 1634
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:22px'>Figure 3: 1D illustration (to scale) of output map computation for classification, using y-dimension<br>from scale 2 as an example (see Table 5). (a): 20 pixel unpooled layer 5 feature map. (b): max<br>pooling over non-overlapping 3 pixel groups, using offsets of △ = {0, 1, 2} pixels (red, green, blue<br>respectively). (c): The resulting 6 pixel pooled maps, for different △. (d): 5 pixel classifier (layers<br>6,7) is applied in sliding window fashion to pooled maps, yielding 2 pixel by C maps for each △.<br>(e): reshaped into 6 pixel by C output maps.</p>",
            "id": 50,
            "page": 6,
            "text": "Figure 3: 1D illustration (to scale) of output map computation for classification, using y-dimension from scale 2 as an example (see Table 5). (a): 20 pixel unpooled layer 5 feature map. (b): max pooling over non-overlapping 3 pixel groups, using offsets of △ = {0, 1, 2} pixels (red, green, blue respectively). (c): The resulting 6 pixel pooled maps, for different △. (d): 5 pixel classifier (layers 6,7) is applied in sliding window fashion to pooled maps, yielding 2 pixel by C maps for each △. (e): reshaped into 6 pixel by C output maps."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1679
                },
                {
                    "x": 2107,
                    "y": 1679
                },
                {
                    "x": 2107,
                    "y": 1866
                },
                {
                    "x": 442,
                    "y": 1866
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:20px'>These operations can be viewed as shifting the classifier's viewing window by 1 pixel through pool-<br>ing layers without subsampling and using skip-kernels in the following layer (where values in the<br>neighborhood are non-adjacent). Or equivalently, as applying the final pooling layer and fully-<br>connected stack at every possible offset, and assembling the results by interleaving the outputs.</p>",
            "id": 51,
            "page": 6,
            "text": "These operations can be viewed as shifting the classifier's viewing window by 1 pixel through pooling layers without subsampling and using skip-kernels in the following layer (where values in the neighborhood are non-adjacent). Or equivalently, as applying the final pooling layer and fullyconnected stack at every possible offset, and assembling the results by interleaving the outputs."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1888
                },
                {
                    "x": 2108,
                    "y": 1888
                },
                {
                    "x": 2108,
                    "y": 2071
                },
                {
                    "x": 441,
                    "y": 2071
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:18px'>The procedure above is repeated for the horizontally flipped version of each image. We then produce<br>the final classification by (i) taking the spatial max for each class, at each scale and flip; (ii) averaging<br>the resulting C-dimensional vectors from different scales and flips and (iii) taking the top-1 or top-5<br>elements (depending on the evaluation criterion) from the mean class vector.</p>",
            "id": 52,
            "page": 6,
            "text": "The procedure above is repeated for the horizontally flipped version of each image. We then produce the final classification by (i) taking the spatial max for each class, at each scale and flip; (ii) averaging the resulting C-dimensional vectors from different scales and flips and (iii) taking the top-1 or top-5 elements (depending on the evaluation criterion) from the mean class vector."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2096
                },
                {
                    "x": 2108,
                    "y": 2096
                },
                {
                    "x": 2108,
                    "y": 2555
                },
                {
                    "x": 441,
                    "y": 2555
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:16px'>At an intuitive level, the two halves of the network - i.e. feature extraction layers (1-5) and classifier<br>layers (6-output) are used in opposite ways. In the feature extraction portion, the filters are<br>convolved across the entire image in one pass. From a computational perspective, this is far more<br>efficient than sliding a fixed-size feature extractor over the image and then aggregating the results<br>from different locations2. However, these principles are reversed for the classifier portion of the<br>network. Here, we want to hunt for a fixed-size representation in the layer 5 feature maps across<br>different positions and scales. Thus the classifier has a fixed-size 5x5 input and is exhaustively<br>applied to the layer 5 maps. The exhaustive pooling scheme (with single pixel shifts (△x, △y))<br>ensures that we can obtain fine alignment between the classifier and the representation of the object<br>in the feature map.</p>",
            "id": 53,
            "page": 6,
            "text": "At an intuitive level, the two halves of the network - i.e. feature extraction layers (1-5) and classifier layers (6-output) are used in opposite ways. In the feature extraction portion, the filters are convolved across the entire image in one pass. From a computational perspective, this is far more efficient than sliding a fixed-size feature extractor over the image and then aggregating the results from different locations2. However, these principles are reversed for the classifier portion of the network. Here, we want to hunt for a fixed-size representation in the layer 5 feature maps across different positions and scales. Thus the classifier has a fixed-size 5x5 input and is exhaustively applied to the layer 5 maps. The exhaustive pooling scheme (with single pixel shifts (△x, △y)) ensures that we can obtain fine alignment between the classifier and the representation of the object in the feature map."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2610
                },
                {
                    "x": 676,
                    "y": 2610
                },
                {
                    "x": 676,
                    "y": 2657
                },
                {
                    "x": 444,
                    "y": 2657
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>3.4 Results</p>",
            "id": 54,
            "page": 6,
            "text": "3.4 Results"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2695
                },
                {
                    "x": 2109,
                    "y": 2695
                },
                {
                    "x": 2109,
                    "y": 2973
                },
                {
                    "x": 442,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:18px'>In Table 2, we experiment with different approaches, and compare them to the single network model<br>of Krizhevsky et al. [15] for reference. The approach described above, with 6 scales, achieves a<br>top-5 error rate of 13.6%. As might be expected, using fewer scales hurts performance: the single-<br>scale model is worse with 16.97% top-5 error. The fine stride technique illustrated in Fig. 3 brings a<br>relatively small improvementin the single scale regime, butis also of importance for the multi-scale<br>gains shown here.</p>",
            "id": 55,
            "page": 6,
            "text": "In Table 2, we experiment with different approaches, and compare them to the single network model of Krizhevsky   for reference. The approach described above, with 6 scales, achieves a top-5 error rate of 13.6%. As might be expected, using fewer scales hurts performance: the singlescale model is worse with 16.97% top-5 error. The fine stride technique illustrated in Fig. 3 brings a relatively small improvementin the single scale regime, butis also of importance for the multi-scale gains shown here."
        },
        {
            "bounding_box": [
                {
                    "x": 498,
                    "y": 3007
                },
                {
                    "x": 1786,
                    "y": 3007
                },
                {
                    "x": 1786,
                    "y": 3052
                },
                {
                    "x": 498,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:14px'>2Our network with 6 scales takes around 2 secs on a K20x GPU to process one image</p>",
            "id": 56,
            "page": 6,
            "text": "2Our network with 6 scales takes around 2 secs on a K20x GPU to process one image"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='57' style='font-size:14px'>6</footer>",
            "id": 57,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 668,
                    "y": 332
                },
                {
                    "x": 1876,
                    "y": 332
                },
                {
                    "x": 1876,
                    "y": 840
                },
                {
                    "x": 668,
                    "y": 840
                }
            ],
            "category": "table",
            "html": "<table id='58' style='font-size:14px'><tr><td>Approach</td><td>Top-1 error %</td><td>Top-5 error %</td></tr><tr><td>Krizhevsky et al. [15]</td><td>40.7</td><td>18.2</td></tr><tr><td>OverFeat - 1 fast model, scale 1, coarse stride</td><td>39.28</td><td>17.12</td></tr><tr><td>OverFeat - 1 fast model, scale 1, fine stride</td><td>39.01</td><td>16.97</td></tr><tr><td>OverFeat - 1 fast model, 4 scales (1,2,4,6), fine stride</td><td>38.57</td><td>16.39</td></tr><tr><td>OverFeat - 1 fast model, 6 scales (1-6), fine stride</td><td>38.12</td><td>16.27</td></tr><tr><td>OverFeat - 1 accurate model, 4 corners + center + flip</td><td>35.60</td><td>14.71</td></tr><tr><td>OverFeat - 1 accurate model, 4 scales, fine stride</td><td>35.74</td><td>14.18</td></tr><tr><td>OverFeat - 7 fast models, 4 scales, fine stride</td><td>35.10</td><td>13.86</td></tr><tr><td>OverFeat - 7 accurate models, 4 scales, fine stride</td><td>33.96</td><td>13.24</td></tr></table>",
            "id": 58,
            "page": 7,
            "text": "Approach Top-1 error % Top-5 error %  Krizhevsky   40.7 18.2  OverFeat - 1 fast model, scale 1, coarse stride 39.28 17.12  OverFeat - 1 fast model, scale 1, fine stride 39.01 16.97  OverFeat - 1 fast model, 4 scales (1,2,4,6), fine stride 38.57 16.39  OverFeat - 1 fast model, 6 scales (1-6), fine stride 38.12 16.27  OverFeat - 1 accurate model, 4 corners + center + flip 35.60 14.71  OverFeat - 1 accurate model, 4 scales, fine stride 35.74 14.18  OverFeat - 7 fast models, 4 scales, fine stride 35.10 13.86  OverFeat - 7 accurate models, 4 scales, fine stride 33.96"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 891
                },
                {
                    "x": 2108,
                    "y": 891
                },
                {
                    "x": 2108,
                    "y": 990
                },
                {
                    "x": 442,
                    "y": 990
                }
            ],
            "category": "caption",
            "html": "<caption id='59' style='font-size:18px'>Table 2: Classification experiments on validation set. Fine/coarse stride refers to the number of<br>△ values used when applying the classifier. Fine: △ = 0, 1, 2; coarse: △ = 0.</caption>",
            "id": 59,
            "page": 7,
            "text": "Table 2: Classification experiments on validation set. Fine/coarse stride refers to the number of △ values used when applying the classifier. Fine: △ = 0, 1, 2; coarse: △ = 0."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 1125
                },
                {
                    "x": 2054,
                    "y": 1125
                },
                {
                    "x": 2054,
                    "y": 1987
                },
                {
                    "x": 516,
                    "y": 1987
                }
            ],
            "category": "figure",
            "html": "<figure><img id='60' style='font-size:16px' alt=\"ImageNet11 pre-training 11.2%\nClarifai\n11.7%\nNUS validation fine-tuning 13.0%\nZF 13.5%\nAndrew Howard 13.6%\n13.6%\n7 big models\nOverFeat\n7 fast models 14.2%\nUvA - Euvision 14.3%\nAdobe 15.2%\nVGG 15.2%\nSuperVision 7 models + ImageNet11\n15.3%\nCognitive Vision 16.1%\nSuperVision 5 models\n16.4%\n0% 2% 4% 6% 8% 10% 12% 14% 16% 18%\nTop 5 error rate\nILSVRC12 ILSVRC13 Post competition\" data-coord=\"top-left:(516,1125); bottom-right:(2054,1987)\" /></figure>",
            "id": 60,
            "page": 7,
            "text": "ImageNet11 pre-training 11.2% Clarifai 11.7% NUS validation fine-tuning 13.0% ZF 13.5% Andrew Howard 13.6% 13.6% 7 big models OverFeat 7 fast models 14.2% UvA - Euvision 14.3% Adobe 15.2% VGG 15.2% SuperVision 7 models + ImageNet11 15.3% Cognitive Vision 16.1% SuperVision 5 models 16.4% 0% 2% 4% 6% 8% 10% 12% 14% 16% 18% Top 5 error rate ILSVRC12 ILSVRC13 Post competition"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2050
                },
                {
                    "x": 2108,
                    "y": 2050
                },
                {
                    "x": 2108,
                    "y": 2195
                },
                {
                    "x": 441,
                    "y": 2195
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:18px'>Figure 4: Test set classification results. During the competition, OverFeat yielded 14.2% top 5<br>error rate using an average of 7 fast models. In post-competition work, OverFeat ranks fifth with<br>13.6% error using bigger models (more features and more layers).</p>",
            "id": 61,
            "page": 7,
            "text": "Figure 4: Test set classification results. During the competition, OverFeat yielded 14.2% top 5 error rate using an average of 7 fast models. In post-competition work, OverFeat ranks fifth with 13.6% error using bigger models (more features and more layers)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2290
                },
                {
                    "x": 2110,
                    "y": 2290
                },
                {
                    "x": 2110,
                    "y": 2615
                },
                {
                    "x": 442,
                    "y": 2615
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:20px'>We report the test set results of the 2013 competition in Fig. 4 where our model (OverFeat) obtained<br>14.2% accuracy by voting of 7 ConvNets (each trained with different initializations) and ranked 5th<br>out of 18 teams. The best accuracy using only ILSVRC13 data was 11.7%. Pre-training with extra<br>data from the ImageNet Fall11 dataset improved this number to 11.2%. In post-competition work,<br>we improve the OverFeat results down to 13.6% error by using bigger models (more features and<br>more layers). Due to time constraints, these bigger models are not fully trained, more improvements<br>are expected to appear in time.</p>",
            "id": 62,
            "page": 7,
            "text": "We report the test set results of the 2013 competition in Fig. 4 where our model (OverFeat) obtained 14.2% accuracy by voting of 7 ConvNets (each trained with different initializations) and ranked 5th out of 18 teams. The best accuracy using only ILSVRC13 data was 11.7%. Pre-training with extra data from the ImageNet Fall11 dataset improved this number to 11.2%. In post-competition work, we improve the OverFeat results down to 13.6% error by using bigger models (more features and more layers). Due to time constraints, these bigger models are not fully trained, more improvements are expected to appear in time."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2683
                },
                {
                    "x": 1272,
                    "y": 2683
                },
                {
                    "x": 1272,
                    "y": 2733
                },
                {
                    "x": 443,
                    "y": 2733
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:22px'>3.5 ConvNets and Sliding Window Efficiency</p>",
            "id": 63,
            "page": 7,
            "text": "3.5 ConvNets and Sliding Window Efficiency"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2777
                },
                {
                    "x": 2110,
                    "y": 2777
                },
                {
                    "x": 2110,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:20px'>In contrast to many sliding-window approaches that compute an entire pipeline for each window of<br>the input one at a time, ConvNets are inherently efficient when applied in a sliding fashion because<br>they naturally share computations common to overlapping regions. When applying our network<br>to larger images at test time, we simply apply each convolution over the extent of the full image.<br>This extends the output of each layer to cover the new image size, eventually producing a map of<br>output class predictions, with one spatial location for each \"window\" (field of view) of input. This</p>",
            "id": 64,
            "page": 7,
            "text": "In contrast to many sliding-window approaches that compute an entire pipeline for each window of the input one at a time, ConvNets are inherently efficient when applied in a sliding fashion because they naturally share computations common to overlapping regions. When applying our network to larger images at test time, we simply apply each convolution over the extent of the full image. This extends the output of each layer to cover the new image size, eventually producing a map of output class predictions, with one spatial location for each \"window\" (field of view) of input. This"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1261,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='65' style='font-size:16px'>7</footer>",
            "id": 65,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 391
                },
                {
                    "x": 2035,
                    "y": 391
                },
                {
                    "x": 2035,
                    "y": 1263
                },
                {
                    "x": 509,
                    "y": 1263
                }
            ],
            "category": "figure",
            "html": "<figure><img id='66' style='font-size:14px' alt=\"■\n1x1 1x1 1x1\n5x5\n10x10\n5x5 2x2 5x5 1x1 1x1\n14x14\nconvolution pooling conv conv conv\ninput 1st stage classifier output\n■\n2x2 2x2 2x2\n6x6\n12x12\n5x5 2x2 5x5 1x1 1x1\n16x16 convolution pooling conv conv conv\ninput 1st stage classifier output\" data-coord=\"top-left:(509,391); bottom-right:(2035,1263)\" /></figure>",
            "id": 66,
            "page": 8,
            "text": "■ 1x1 1x1 1x1 5x5 10x10 5x5 2x2 5x5 1x1 1x1 14x14 convolution pooling conv conv conv input 1st stage classifier output ■ 2x2 2x2 2x2 6x6 12x12 5x5 2x2 5x5 1x1 1x1 16x16 convolution pooling conv conv conv input 1st stage classifier output"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1294
                },
                {
                    "x": 2108,
                    "y": 1294
                },
                {
                    "x": 2108,
                    "y": 1528
                },
                {
                    "x": 440,
                    "y": 1528
                }
            ],
            "category": "caption",
            "html": "<caption id='67' style='font-size:18px'>Figure 5: The efficiency of ConvNets for detection. During training, a ConvNet produces only a<br>single spatial output (top). But when applied at test time over a larger image, it produces a spatial<br>output map, e.g. 2x2 (bottom). Since all layers are applied convolutionally, the extra computa-<br>tion required for the larger image is limited to the yellow regions. This diagram omits the feature<br>dimension for simplicity.</caption>",
            "id": 67,
            "page": 8,
            "text": "Figure 5: The efficiency of ConvNets for detection. During training, a ConvNet produces only a single spatial output (top). But when applied at test time over a larger image, it produces a spatial output map, e.g. 2x2 (bottom). Since all layers are applied convolutionally, the extra computation required for the larger image is limited to the yellow regions. This diagram omits the feature dimension for simplicity."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1619
                },
                {
                    "x": 2106,
                    "y": 1619
                },
                {
                    "x": 2106,
                    "y": 1711
                },
                {
                    "x": 441,
                    "y": 1711
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>is diagrammed in Fig. 5. Convolutions are applied bottom-up, SO that the computations common to<br>neighboring windows need only be done once.</p>",
            "id": 68,
            "page": 8,
            "text": "is diagrammed in Fig. 5. Convolutions are applied bottom-up, SO that the computations common to neighboring windows need only be done once."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1737
                },
                {
                    "x": 2108,
                    "y": 1737
                },
                {
                    "x": 2108,
                    "y": 1921
                },
                {
                    "x": 441,
                    "y": 1921
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>Note that the last layers of our architecture are fully connected linear layers. At test time, these<br>layers are effectively replaced by convolution operations with kernels of 1x1 spatial extent. The<br>entire ConvNet is then simply a sequence of convolutions, max-pooling and thresholding operations<br>exclusively.</p>",
            "id": 69,
            "page": 8,
            "text": "Note that the last layers of our architecture are fully connected linear layers. At test time, these layers are effectively replaced by convolution operations with kernels of 1x1 spatial extent. The entire ConvNet is then simply a sequence of convolutions, max-pooling and thresholding operations exclusively."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1994
                },
                {
                    "x": 793,
                    "y": 1994
                },
                {
                    "x": 793,
                    "y": 2048
                },
                {
                    "x": 443,
                    "y": 2048
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:22px'>4 Localization</p>",
            "id": 70,
            "page": 8,
            "text": "4 Localization"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2103
                },
                {
                    "x": 2108,
                    "y": 2103
                },
                {
                    "x": 2108,
                    "y": 2287
                },
                {
                    "x": 442,
                    "y": 2287
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>Starting from our classification-trained network, we replace the classifier layers by a regression<br>network and train it to predict object bounding boxes at each spatial location and scale. We then<br>combine the regression predictions together, along with the classification results at each location, as<br>we now describe.</p>",
            "id": 71,
            "page": 8,
            "text": "Starting from our classification-trained network, we replace the classifier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale. We then combine the regression predictions together, along with the classification results at each location, as we now describe."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2350
                },
                {
                    "x": 959,
                    "y": 2350
                },
                {
                    "x": 959,
                    "y": 2400
                },
                {
                    "x": 443,
                    "y": 2400
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:20px'>4.1 Generating Predictions</p>",
            "id": 72,
            "page": 8,
            "text": "4.1 Generating Predictions"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2439
                },
                {
                    "x": 2108,
                    "y": 2439
                },
                {
                    "x": 2108,
                    "y": 2717
                },
                {
                    "x": 442,
                    "y": 2717
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>To generate object bounding box predictions, we simultaneously run the classifier and regressor<br>networks across all locations and scales. Since these share the same feature extraction layers, only<br>the final regression layers need to be recomputed after computing the classification network. The<br>output of the final softmax layer for a class c at each location provides a score of confidence that<br>an object of class c is present (though not necessarily fully contained) in the corresponding field of<br>view. Thus we can assign a confidence to each bounding box.</p>",
            "id": 73,
            "page": 8,
            "text": "To generate object bounding box predictions, we simultaneously run the classifier and regressor networks across all locations and scales. Since these share the same feature extraction layers, only the final regression layers need to be recomputed after computing the classification network. The output of the final softmax layer for a class c at each location provides a score of confidence that an object of class c is present (though not necessarily fully contained) in the corresponding field of view. Thus we can assign a confidence to each bounding box."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2778
                },
                {
                    "x": 890,
                    "y": 2778
                },
                {
                    "x": 890,
                    "y": 2828
                },
                {
                    "x": 443,
                    "y": 2828
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:22px'>4.2 Regressor Training</p>",
            "id": 74,
            "page": 8,
            "text": "4.2 Regressor Training"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2867
                },
                {
                    "x": 2110,
                    "y": 2867
                },
                {
                    "x": 2110,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:18px'>The regression network takes as input the pooled feature maps from layer 5. It has 2 fully-connected<br>hidden layers of size 4096 and 1024 channels, respectively. The final output layer has 4 units which<br>specify the coordinates for the bounding box edges. As with classification, there are (3x3) copies<br>throughout, resulting from the △x, △y shifts. The architecture is shown in Fig. 8.</p>",
            "id": 75,
            "page": 8,
            "text": "The regression network takes as input the pooled feature maps from layer 5. It has 2 fully-connected hidden layers of size 4096 and 1024 channels, respectively. The final output layer has 4 units which specify the coordinates for the bounding box edges. As with classification, there are (3x3) copies throughout, resulting from the △x, △y shifts. The architecture is shown in Fig. 8."
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1261,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='76' style='font-size:14px'>8</footer>",
            "id": 76,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 535,
                    "y": 292
                },
                {
                    "x": 2017,
                    "y": 292
                },
                {
                    "x": 2017,
                    "y": 2689
                },
                {
                    "x": 535,
                    "y": 2689
                }
            ],
            "category": "figure",
            "html": "<figure><img id='77' style='font-size:14px' alt=\"Noxtrics.mm.0.163050.taob\nhearPimes 035-1,010\n15.7X3.3minio.296552 10,0000:10mm\ncombao.utu.8iangs.com\n0:08861AIR ntt.0:426846 JINX\nJair115-misc.U.gb.avg 0.64\nhop.2mtx.0.tanv.o.co.co\nanril.com 0.15.0元\n0.1L QIY 0:08\nHeorklle mix 0.00mvq-0.00\ncommunical 160\n신전쟁 Whild 1820\n0101\n⌀21mm QUIS DOT 100\" data-coord=\"top-left:(535,292); bottom-right:(2017,2689)\" /></figure>",
            "id": 77,
            "page": 9,
            "text": "Noxtrics.mm.0.163050.taob hearPimes 035-1,010 15.7X3.3minio.296552 10,0000:10mm combao.utu.8iangs.com 0:08861AIR ntt.0:426846 JINX Jair115-misc.U.gb.avg 0.64 hop.2mtx.0.tanv.o.co.co anril.com 0.15.0元 0.1L QIY 0:08 Heorklle mix 0.00mvq-0.00 communical 160 신전쟁 Whild 1820 0101 ⌀21mm QUIS DOT 100"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2714
                },
                {
                    "x": 2107,
                    "y": 2714
                },
                {
                    "x": 2107,
                    "y": 2949
                },
                {
                    "x": 441,
                    "y": 2949
                }
            ],
            "category": "caption",
            "html": "<caption id='78' style='font-size:20px'>Figure 6: Localization/Detection pipeline. The raw classifier/detector outputs a class and a con-<br>fidence for each location (1st diagram). The resolution of these predictions can be increased using<br>the method described in section 3.3 (2nd diagram). The regression then predicts the location scale<br>of the object with respect to each window (3rd diagram). These bounding boxes are then merge and<br>accumulated to a small number of objects (4th diagram).</caption>",
            "id": 78,
            "page": 9,
            "text": "Figure 6: Localization/Detection pipeline. The raw classifier/detector outputs a class and a confidence for each location (1st diagram). The resolution of these predictions can be increased using the method described in section 3.3 (2nd diagram). The regression then predicts the location scale of the object with respect to each window (3rd diagram). These bounding boxes are then merge and accumulated to a small number of objects (4th diagram)."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3168
                },
                {
                    "x": 1260,
                    "y": 3168
                }
            ],
            "category": "footer",
            "html": "<footer id='79' style='font-size:16px'>9</footer>",
            "id": 79,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 488,
                    "y": 310
                },
                {
                    "x": 2054,
                    "y": 310
                },
                {
                    "x": 2054,
                    "y": 1237
                },
                {
                    "x": 488,
                    "y": 1237
                }
            ],
            "category": "figure",
            "html": "<figure><img id='80' style='font-size:14px' alt=\"- brainbling Delli⌀nch\nIL\n⌀ims 3x256x335 dinegx2 - 257\nyand ⌀oodor\n16\" data-coord=\"top-left:(488,310); bottom-right:(2054,1237)\" /></figure>",
            "id": 80,
            "page": 10,
            "text": "- brainbling Delli⌀nch IL ⌀ims 3x256x335 dinegx2 - 257 yand ⌀oodor 16"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1282
                },
                {
                    "x": 2109,
                    "y": 1282
                },
                {
                    "x": 2109,
                    "y": 1659
                },
                {
                    "x": 441,
                    "y": 1659
                }
            ],
            "category": "caption",
            "html": "<caption id='81' style='font-size:18px'>Figure 7: Examples of bounding boxes produced by the regression network, before being com-<br>bined into final predictions. The examples shown here are at a single scale. Predictions may be<br>more optimal at other scales depending on the objects. Here, most of the bounding boxes which are<br>initially organized as a grid, converge to a single location and scale. This indicates that the network<br>is very confident in the location of the object, as opposed to being spread out randomly. The top left<br>image shows that it can also correctly identify multiple location if several objects are present. The<br>various aspect ratios of the predicted bounding boxes shows that the network is able to cope with<br>various object poses.</caption>",
            "id": 81,
            "page": 10,
            "text": "Figure 7: Examples of bounding boxes produced by the regression network, before being combined into final predictions. The examples shown here are at a single scale. Predictions may be more optimal at other scales depending on the objects. Here, most of the bounding boxes which are initially organized as a grid, converge to a single location and scale. This indicates that the network is very confident in the location of the object, as opposed to being spread out randomly. The top left image shows that it can also correctly identify multiple location if several objects are present. The various aspect ratios of the predicted bounding boxes shows that the network is able to cope with various object poses."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1746
                },
                {
                    "x": 2108,
                    "y": 1746
                },
                {
                    "x": 2108,
                    "y": 2161
                },
                {
                    "x": 441,
                    "y": 2161
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:18px'>We fix the feature extraction layers (1-5) from the classification network and train the regression<br>network using an l2 loss between the predicted and true bounding box for each example. The final<br>regressor layer is class-specific, having 1000 different versions, one for each class. We train this<br>network using the same set of scales as described in Section 3. We compare the prediction of the<br>regressor net at each spatial location with the ground-truth bounding box, shifted into the frame of<br>reference of the regressor's translation offset within the convolution (see Fig. 8). However, we do<br>not train the regressor on bounding boxes with less than 50% overlap with the input field of view:<br>since the object is mostly outside of these locations, it will be better handled by regression windows<br>that do contain the object.</p>",
            "id": 82,
            "page": 10,
            "text": "We fix the feature extraction layers (1-5) from the classification network and train the regression network using an l2 loss between the predicted and true bounding box for each example. The final regressor layer is class-specific, having 1000 different versions, one for each class. We train this network using the same set of scales as described in Section 3. We compare the prediction of the regressor net at each spatial location with the ground-truth bounding box, shifted into the frame of reference of the regressor's translation offset within the convolution (see Fig. 8). However, we do not train the regressor on bounding boxes with less than 50% overlap with the input field of view: since the object is mostly outside of these locations, it will be better handled by regression windows that do contain the object."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2183
                },
                {
                    "x": 2108,
                    "y": 2183
                },
                {
                    "x": 2108,
                    "y": 2554
                },
                {
                    "x": 441,
                    "y": 2554
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:16px'>Training the regressors in a multi-scale manner is important for the across-scale prediction combi-<br>nation. Training on a single scale will perform well on that scale and still perform reasonably on<br>other scales. However training multi-scale will make predictions match correctly across scales and<br>exponentially increase the confidence of the merged predictions. In turn, this allows us to perform<br>well with a few scales only, rather than many scales as is typically the case in detection. The typical<br>ratio from one scale to another in pedestrian detection [25] is about 1.05 to 1.1, here however we use<br>a large ratio of approximately 1.4 (this number differs for each scale since dimensions are adjusted<br>to fit exactly the stride of our network) which allows us to run our system faster.</p>",
            "id": 83,
            "page": 10,
            "text": "Training the regressors in a multi-scale manner is important for the across-scale prediction combination. Training on a single scale will perform well on that scale and still perform reasonably on other scales. However training multi-scale will make predictions match correctly across scales and exponentially increase the confidence of the merged predictions. In turn, this allows us to perform well with a few scales only, rather than many scales as is typically the case in detection. The typical ratio from one scale to another in pedestrian detection  is about 1.05 to 1.1, here however we use a large ratio of approximately 1.4 (this number differs for each scale since dimensions are adjusted to fit exactly the stride of our network) which allows us to run our system faster."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2614
                },
                {
                    "x": 958,
                    "y": 2614
                },
                {
                    "x": 958,
                    "y": 2662
                },
                {
                    "x": 442,
                    "y": 2662
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:22px'>4.3 Combining Predictions</p>",
            "id": 84,
            "page": 10,
            "text": "4.3 Combining Predictions"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2702
                },
                {
                    "x": 2104,
                    "y": 2702
                },
                {
                    "x": 2104,
                    "y": 2797
                },
                {
                    "x": 443,
                    "y": 2797
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:20px'>We combine the individual predictions (see Fig. 7) via a greedy merge strategy applied to the regres-<br>sor bounding boxes, using the following algorithm.</p>",
            "id": 85,
            "page": 10,
            "text": "We combine the individual predictions (see Fig. 7) via a greedy merge strategy applied to the regressor bounding boxes, using the following algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2843
                },
                {
                    "x": 2110,
                    "y": 2843
                },
                {
                    "x": 2110,
                    "y": 3052
                },
                {
                    "x": 443,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:16px'>(a) Assign to Cs the set of classes in the top k for each scale s E 1 · · 6, found by taking the<br>maximum detection class outputs across spatial locations for that scale.<br>(b) Assign to Bs the set of bounding boxes predicted by the regressor network for each class in Cs,<br>across all spatial locations at scale s.</p>",
            "id": 86,
            "page": 10,
            "text": "(a) Assign to Cs the set of classes in the top k for each scale s E 1 · · 6, found by taking the maximum detection class outputs across spatial locations for that scale. (b) Assign to Bs the set of bounding boxes predicted by the regressor network for each class in Cs, across all spatial locations at scale s."
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1254,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='87' style='font-size:16px'>10</footer>",
            "id": 87,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 795,
                    "y": 300
                },
                {
                    "x": 1745,
                    "y": 300
                },
                {
                    "x": 1745,
                    "y": 1290
                },
                {
                    "x": 795,
                    "y": 1290
                }
            ],
            "category": "figure",
            "html": "<figure><img id='88' style='font-size:20px' alt=\"(a) Layer 5 pooled maps (b) Regression\nLayer 1 maps\nX 256 channels X 4096 channels\nX (3x3) (△x,△y) shifts X (3x3) (△x,△y) shifts\n(c) Regression (d) Regression\nLayer 2 maps Layer 3\n(per-class)\nX 4 channels\n(top, left, bottom,\nX 1024 channels right box edges)\nX (3x3) (△x,△y) shifts X (3x3) (△x,△y) shifts\" data-coord=\"top-left:(795,300); bottom-right:(1745,1290)\" /></figure>",
            "id": 88,
            "page": 11,
            "text": "(a) Layer 5 pooled maps (b) Regression Layer 1 maps X 256 channels X 4096 channels X (3x3) (△x,△y) shifts X (3x3) (△x,△y) shifts (c) Regression (d) Regression Layer 2 maps Layer 3 (per-class) X 4 channels (top, left, bottom, X 1024 channels right box edges) X (3x3) (△x,△y) shifts X (3x3) (△x,△y) shifts"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1328
                },
                {
                    "x": 2110,
                    "y": 1328
                },
                {
                    "x": 2110,
                    "y": 1750
                },
                {
                    "x": 440,
                    "y": 1750
                }
            ],
            "category": "caption",
            "html": "<caption id='89' style='font-size:16px'>Figure 8: Application of the regression network to layer 5 features, at scale 2, for example. (a)<br>The input to the regressor at this scale are 6x7 pixels spatially by 256 channels for each of the<br>(3x3) △x, △y shifts. (b) Each unit in the 1st layer of the regression net is connected to a 5x5 spatial<br>neighborhoodin the layer 5 maps, as well as all 256 channels. Shifting the 5x5 neighborhood around<br>results in a map of 2x3 spatial extent, for each of the 4096 channels in the layer, and for each of<br>the (3x3) △x, △y shifts. (c) The 2nd regression layer has 1024 units and is fully connected (i.e. the<br>purple element only connects to the purple element in (b), across all 4096 channels). (d) The output<br>of the regression network is a 4-vector (specifying the edges of the bounding box) for each location<br>in the 2x3 map, and for each of the (3x3) △x, △y shifts.</caption>",
            "id": 89,
            "page": 11,
            "text": "Figure 8: Application of the regression network to layer 5 features, at scale 2, for example. (a) The input to the regressor at this scale are 6x7 pixels spatially by 256 channels for each of the (3x3) △x, △y shifts. (b) Each unit in the 1st layer of the regression net is connected to a 5x5 spatial neighborhoodin the layer 5 maps, as well as all 256 channels. Shifting the 5x5 neighborhood around results in a map of 2x3 spatial extent, for each of the 4096 channels in the layer, and for each of the (3x3) △x, △y shifts. (c) The 2nd regression layer has 1024 units and is fully connected (i.e. the purple element only connects to the purple element in (b), across all 4096 channels). (d) The output of the regression network is a 4-vector (specifying the edges of the bounding box) for each location in the 2x3 map, and for each of the (3x3) △x, △y shifts."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1935
                },
                {
                    "x": 976,
                    "y": 1935
                },
                {
                    "x": 976,
                    "y": 1985
                },
                {
                    "x": 444,
                    "y": 1985
                }
            ],
            "category": "caption",
            "html": "<caption id='90' style='font-size:18px'>(d) Repeat merging until done:</caption>",
            "id": 90,
            "page": 11,
            "text": "(d) Repeat merging until done:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2014
                },
                {
                    "x": 1466,
                    "y": 2014
                },
                {
                    "x": 1466,
                    "y": 2245
                },
                {
                    "x": 444,
                    "y": 2245
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:20px'>(e) (b*, 62) = argminb1 #b2EBmatch_score(b1, b2)<br>(f) If match_score(b1,b2) > t, stop.<br>(g) Otherwise, set B ← B\\{6*,62} U box_merge(bi, 62)</p>",
            "id": 91,
            "page": 11,
            "text": "(e) (b*, 62) = argminb1 #b2EBmatch_score(b1, b2) (f) If match_score(b1,b2) > t, stop. (g) Otherwise, set B ← B\\{6*,62} U box_merge(bi, 62)"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2296
                },
                {
                    "x": 2108,
                    "y": 2296
                },
                {
                    "x": 2108,
                    "y": 2433
                },
                {
                    "x": 442,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:14px'>In the above, we compute match_score using the sum of the distance between centers of the two<br>bounding boxes and the intersection area of the boxes. box_merge compute the average of the<br>bounding boxes' coordinates.</p>",
            "id": 92,
            "page": 11,
            "text": "In the above, we compute match_score using the sum of the distance between centers of the two bounding boxes and the intersection area of the boxes. box_merge compute the average of the bounding boxes' coordinates."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2455
                },
                {
                    "x": 2108,
                    "y": 2455
                },
                {
                    "x": 2108,
                    "y": 3054
                },
                {
                    "x": 440,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:16px'>The final prediction is given by taking the merged bounding boxes with maximum class scores. This<br>is computed by cumulatively adding the detection class outputs associated with the input windows<br>from which each bounding box was predicted. See Fig. 6 for an example of bounding boxes merged<br>into a single high-confidence bounding box. In that example, some turtle and whale bounding boxes<br>appear in the intermediate multi-scale steps, but disappear in the final detection image. Not only do<br>these bounding boxes have low classification confidence (at most 0.11 and 0.12 respectively), their<br>collection is not as coherent as the bear bounding boxes to get a significant confidence boost. The<br>bear boxes have a strong confidence (approximately 0.5 on average per scale) and high matching<br>scores. Hence after merging, many bear bounding boxes are fused into a single very high confidence<br>box, while false positives disappear below the detection threshold due their lack of bounding box<br>coherence and confidence. This analysis suggest that our approach is naturally more robust to false<br>positives coming from the pure-classification model than traditional non-maximum suppression, by<br>rewarding bounding box coherence.</p>",
            "id": 93,
            "page": 11,
            "text": "The final prediction is given by taking the merged bounding boxes with maximum class scores. This is computed by cumulatively adding the detection class outputs associated with the input windows from which each bounding box was predicted. See Fig. 6 for an example of bounding boxes merged into a single high-confidence bounding box. In that example, some turtle and whale bounding boxes appear in the intermediate multi-scale steps, but disappear in the final detection image. Not only do these bounding boxes have low classification confidence (at most 0.11 and 0.12 respectively), their collection is not as coherent as the bear bounding boxes to get a significant confidence boost. The bear boxes have a strong confidence (approximately 0.5 on average per scale) and high matching scores. Hence after merging, many bear bounding boxes are fused into a single very high confidence box, while false positives disappear below the detection threshold due their lack of bounding box coherence and confidence. This analysis suggest that our approach is naturally more robust to false positives coming from the pure-classification model than traditional non-maximum suppression, by rewarding bounding box coherence."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='94' style='font-size:14px'>11</footer>",
            "id": 94,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 397
                },
                {
                    "x": 2049,
                    "y": 397
                },
                {
                    "x": 2049,
                    "y": 1097
                },
                {
                    "x": 511,
                    "y": 1097
                }
            ],
            "category": "figure",
            "html": "<figure><img id='95' style='font-size:14px' alt=\"SCR, 4 scales 30.0%\nSCR, 3 scales 31.3%\nSCR, 2 scales 31.5%\nSCR, 1 scale 36.0%\nSCR, centered crop 40.0%\nPCR, 3 scales 44.1%\n0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50%\nTop 5 error rate\" data-coord=\"top-left:(511,397); bottom-right:(2049,1097)\" /></figure>",
            "id": 95,
            "page": 12,
            "text": "SCR, 4 scales 30.0% SCR, 3 scales 31.3% SCR, 2 scales 31.5% SCR, 1 scale 36.0% SCR, centered crop 40.0% PCR, 3 scales 44.1% 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% Top 5 error rate"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1174
                },
                {
                    "x": 2108,
                    "y": 1174
                },
                {
                    "x": 2108,
                    "y": 1269
                },
                {
                    "x": 442,
                    "y": 1269
                }
            ],
            "category": "caption",
            "html": "<caption id='96' style='font-size:18px'>Figure 9: Localization experiments on ILSVRC12 validation set. We experiment with different<br>number of scales and with the use of single-class regression (SCR) or per-class regression (PCR).</caption>",
            "id": 96,
            "page": 12,
            "text": "Figure 9: Localization experiments on ILSVRC12 validation set. We experiment with different number of scales and with the use of single-class regression (SCR) or per-class regression (PCR)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1349
                },
                {
                    "x": 775,
                    "y": 1349
                },
                {
                    "x": 775,
                    "y": 1398
                },
                {
                    "x": 442,
                    "y": 1398
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>4.4 Experiments</p>",
            "id": 97,
            "page": 12,
            "text": "4.4 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1435
                },
                {
                    "x": 2107,
                    "y": 1435
                },
                {
                    "x": 2107,
                    "y": 1618
                },
                {
                    "x": 442,
                    "y": 1618
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:18px'>We apply our network to the Imagenet 2012 validation set using the localization criterion specified<br>for the competition. The results for this are shown in Fig. 9. Fig. 10 shows the results of the 2012<br>and 2013 localization competitions (the train and test data are the same for both of these years). Our<br>method is the winner of the 2013 competition with 29.9% error.</p>",
            "id": 98,
            "page": 12,
            "text": "We apply our network to the Imagenet 2012 validation set using the localization criterion specified for the competition. The results for this are shown in Fig. 9. Fig. 10 shows the results of the 2012 and 2013 localization competitions (the train and test data are the same for both of these years). Our method is the winner of the 2013 competition with 29.9% error."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1645
                },
                {
                    "x": 2107,
                    "y": 1645
                },
                {
                    "x": 2107,
                    "y": 1826
                },
                {
                    "x": 442,
                    "y": 1826
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:18px'>Our multiscale and multi-view approach was critical to obtaining good performance, as can be seen<br>in Fig. 9: Using only a single centered crop, our regressor network achieves an error rate of 40%. By<br>combining regressor predictions from all spatial locations at two scales, we achieve a vastly better<br>error rate of 31.5%. Adding a third and fourth scale further improves performance to 30.0% error.</p>",
            "id": 99,
            "page": 12,
            "text": "Our multiscale and multi-view approach was critical to obtaining good performance, as can be seen in Fig. 9: Using only a single centered crop, our regressor network achieves an error rate of 40%. By combining regressor predictions from all spatial locations at two scales, we achieve a vastly better error rate of 31.5%. Adding a third and fourth scale further improves performance to 30.0% error."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1850
                },
                {
                    "x": 2106,
                    "y": 1850
                },
                {
                    "x": 2106,
                    "y": 2172
                },
                {
                    "x": 440,
                    "y": 2172
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:18px'>Using a different top layer for each class in the regressor network for each class (Per-Class Regres-<br>sor (PCR) in Fig. 9) surprisingly did not outperform using only a single network shared among all<br>classes (44.1 % VS. 31.3%). This may be because there are relatively few examples per class an-<br>notated with bounding boxes in the training set, while the network has 1000 times more top-layer<br>parameters, resulting in insufficient training. It is possible this approach may be improved by shar-<br>ing parameters only among similar classes (e.g. training one network for all classes of dogs, another<br>for vehicles, etc.).</p>",
            "id": 100,
            "page": 12,
            "text": "Using a different top layer for each class in the regressor network for each class (Per-Class Regressor (PCR) in Fig. 9) surprisingly did not outperform using only a single network shared among all classes (44.1 % VS. 31.3%). This may be because there are relatively few examples per class annotated with bounding boxes in the training set, while the network has 1000 times more top-layer parameters, resulting in insufficient training. It is possible this approach may be improved by sharing parameters only among similar classes (e.g. training one network for all classes of dogs, another for vehicles, etc.)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2238
                },
                {
                    "x": 732,
                    "y": 2238
                },
                {
                    "x": 732,
                    "y": 2291
                },
                {
                    "x": 442,
                    "y": 2291
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:22px'>5 Detection</p>",
            "id": 101,
            "page": 12,
            "text": "5 Detection"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2341
                },
                {
                    "x": 2107,
                    "y": 2341
                },
                {
                    "x": 2107,
                    "y": 2892
                },
                {
                    "x": 441,
                    "y": 2892
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:18px'>Detection training is similar to classification training but in a spatial manner. Multiple location of<br>an image may be trained simultaneously. Since the model is convolutional, all weights are shared<br>among all locations. The main difference with the localization task, is the necessity to predict a<br>background class when no object is present. Traditionally, negative examples are initially taken at<br>random for training. Then the most offending negative errors are added to the training set in boot-<br>strapping passes. Independent bootstrapping passes render training complicated and risk potential<br>mismatches between the negative examples collection and training times. Additionally, the size of<br>bootstrapping passes needs to be tuned to make sure training does not overfit on a small set. To cir-<br>cumvent all these problems, we perform negative training on the fly, by selecting a few interesting<br>negative examples per image such as random ones or most offending ones. This approach is more<br>computationally expensive, butrenders the procedure much simpler. And since the feature extraction<br>is initially trained with the classification task, the detection fine-tuning is not as long anyway.</p>",
            "id": 102,
            "page": 12,
            "text": "Detection training is similar to classification training but in a spatial manner. Multiple location of an image may be trained simultaneously. Since the model is convolutional, all weights are shared among all locations. The main difference with the localization task, is the necessity to predict a background class when no object is present. Traditionally, negative examples are initially taken at random for training. Then the most offending negative errors are added to the training set in bootstrapping passes. Independent bootstrapping passes render training complicated and risk potential mismatches between the negative examples collection and training times. Additionally, the size of bootstrapping passes needs to be tuned to make sure training does not overfit on a small set. To circumvent all these problems, we perform negative training on the fly, by selecting a few interesting negative examples per image such as random ones or most offending ones. This approach is more computationally expensive, butrenders the procedure much simpler. And since the feature extraction is initially trained with the classification task, the detection fine-tuning is not as long anyway."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2914
                },
                {
                    "x": 2107,
                    "y": 2914
                },
                {
                    "x": 2107,
                    "y": 3054
                },
                {
                    "x": 442,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:16px'>In Fig. 11, we report the results of the ILSVRC 2013 competition where our detection system ranked<br>3rd with 19.4% mean average precision (mAP). We later established a new detection state of the art<br>with 24.3% mAP. Note that there is a large gap between the top 3 methods and other teams (the 4th</p>",
            "id": 103,
            "page": 12,
            "text": "In Fig. 11, we report the results of the ILSVRC 2013 competition where our detection system ranked 3rd with 19.4% mean average precision (mAP). We later established a new detection state of the art with 24.3% mAP. Note that there is a large gap between the top 3 methods and other teams (the 4th"
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1254,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='104' style='font-size:14px'>12</footer>",
            "id": 104,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 398
                },
                {
                    "x": 2053,
                    "y": 398
                },
                {
                    "x": 2053,
                    "y": 1264
                },
                {
                    "x": 515,
                    "y": 1264
                }
            ],
            "category": "figure",
            "html": "<figure><img id='105' style='font-size:14px' alt=\"29.9%\nOverFeat\nImageNet Fall11 pre-training\n33.5%\nSuperVision\n34.2%\n46.4%\nOxford VGG\n50.0%\nISI\n53.6%\n0% 10% 20% 30% 40% 50% 60%\nTop 5 error rate\nILSVRC12 ILSVRC13\" data-coord=\"top-left:(515,398); bottom-right:(2053,1264)\" /></figure>",
            "id": 105,
            "page": 13,
            "text": "29.9% OverFeat ImageNet Fall11 pre-training 33.5% SuperVision 34.2% 46.4% Oxford VGG 50.0% ISI 53.6% 0% 10% 20% 30% 40% 50% 60% Top 5 error rate ILSVRC12 ILSVRC13"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1324
                },
                {
                    "x": 2109,
                    "y": 1324
                },
                {
                    "x": 2109,
                    "y": 1464
                },
                {
                    "x": 441,
                    "y": 1464
                }
            ],
            "category": "caption",
            "html": "<caption id='106' style='font-size:20px'>Figure 10: ILSVRC12 and ILSVRC13 competitions results (test set). Our entry is the winner of<br>the ILSVRC13 localization competition with 29.9% error (top 5). Note that training and testing data<br>is the same for both years. The OverFeat entry uses 4 scales and a single-class regression approach.</caption>",
            "id": 106,
            "page": 13,
            "text": "Figure 10: ILSVRC12 and ILSVRC13 competitions results (test set). Our entry is the winner of the ILSVRC13 localization competition with 29.9% error (top 5). Note that training and testing data is the same for both years. The OverFeat entry uses 4 scales and a single-class regression approach."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1569
                },
                {
                    "x": 2038,
                    "y": 1569
                },
                {
                    "x": 2038,
                    "y": 2441
                },
                {
                    "x": 513,
                    "y": 2441
                }
            ],
            "category": "figure",
            "html": "<figure><img id='107' style='font-size:14px' alt=\"OverFeat* 24.3%\nUvA\n22.6%\nNEC\n20.9%\nOverFeat*\n19.4%\nToronto A\n11.5%\nSYSU_ Vision\n10.5%\nGPU_ UCLA\n9.8%\nDelta\n6.1%\nUIUC-IFP\n1.0%\n0% 5% 10% 15% 20% 25% 30%\nMean Average Precision (mAP)\nILSVRC13 Post competition\" data-coord=\"top-left:(513,1569); bottom-right:(2038,2441)\" /></figure>",
            "id": 107,
            "page": 13,
            "text": "OverFeat* 24.3% UvA 22.6% NEC 20.9% OverFeat* 19.4% Toronto A 11.5% SYSU_ Vision 10.5% GPU_ UCLA 9.8% Delta 6.1% UIUC-IFP 1.0% 0% 5% 10% 15% 20% 25% 30% Mean Average Precision (mAP) ILSVRC13 Post competition"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2500
                },
                {
                    "x": 2107,
                    "y": 2500
                },
                {
                    "x": 2107,
                    "y": 2642
                },
                {
                    "x": 440,
                    "y": 2642
                }
            ],
            "category": "caption",
            "html": "<caption id='108' style='font-size:18px'>Figure 11: ILSVRC13 test set Detection results. During the competition, UvA ranked first with<br>22.6% mAP. In post competition work, we establish a new state of the art with 24.3% mAP. Systems<br>marked with * were pre-trained with the ILSVRC12 classification data.</caption>",
            "id": 108,
            "page": 13,
            "text": "Figure 11: ILSVRC13 test set Detection results. During the competition, UvA ranked first with 22.6% mAP. In post competition work, we establish a new state of the art with 24.3% mAP. Systems marked with * were pre-trained with the ILSVRC12 classification data."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2730
                },
                {
                    "x": 2108,
                    "y": 2730
                },
                {
                    "x": 2108,
                    "y": 3056
                },
                {
                    "x": 441,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:20px'>method yields 11.5% mAP). Additionally, our approach is considerably different from the top 2 other<br>systems which use an initial segmentation step to reduce candidate windows from approximately<br>200,000 to 2,000. This technique speeds up inference and substantially reduces the number of<br>potential false positives. [29, 1] suggest that detection accuracy drops when using dense sliding<br>window as opposed to selective search which discards unlikely object locations hence reducing<br>false positives. Combined with our method, we may observe similar improvements as seen here<br>between traditional dense methods and segmentation based methods. It should also be noted that</p>",
            "id": 109,
            "page": 13,
            "text": "method yields 11.5% mAP). Additionally, our approach is considerably different from the top 2 other systems which use an initial segmentation step to reduce candidate windows from approximately 200,000 to 2,000. This technique speeds up inference and substantially reduces the number of potential false positives.  suggest that detection accuracy drops when using dense sliding window as opposed to selective search which discards unlikely object locations hence reducing false positives. Combined with our method, we may observe similar improvements as seen here between traditional dense methods and segmentation based methods. It should also be noted that"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1253,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='110' style='font-size:16px'>13</footer>",
            "id": 110,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 345
                },
                {
                    "x": 2108,
                    "y": 345
                },
                {
                    "x": 2108,
                    "y": 578
                },
                {
                    "x": 441,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:14px'>we did not fine tune on the detection validation set as NEC and UvA did. The validation and test<br>set distributions differ significantly enough from the training set that this alone improves results by<br>approximately 1 point. The improvement between the two OverFeat results in Fig. 11 are due to<br>longer training times and the use of context, i.e. each scale also uses lower resolution scales as<br>input.</p>",
            "id": 111,
            "page": 14,
            "text": "we did not fine tune on the detection validation set as NEC and UvA did. The validation and test set distributions differ significantly enough from the training set that this alone improves results by approximately 1 point. The improvement between the two OverFeat results in Fig. 11 are due to longer training times and the use of context, i.e. each scale also uses lower resolution scales as input."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 647
                },
                {
                    "x": 755,
                    "y": 647
                },
                {
                    "x": 755,
                    "y": 698
                },
                {
                    "x": 444,
                    "y": 698
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:20px'>6 Discussion</p>",
            "id": 112,
            "page": 14,
            "text": "6 Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 747
                },
                {
                    "x": 2107,
                    "y": 747
                },
                {
                    "x": 2107,
                    "y": 1299
                },
                {
                    "x": 442,
                    "y": 1299
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>We have presented a multi-scale, sliding window approach that can be used for classification, 1o-<br>calization and detection. We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in<br>classification, 1st in localization and 1st in detection. A second important contribution of our paper<br>is explaining how ConvNets can be effectively used for detection and localization tasks. These were<br>never addressed in [15] and thus we are the first to explain how this can be done in the context of Im-<br>ageNet 2012. The scheme we propose involves substantial modifications to networks designed for<br>classification, but clearly demonstrate that ConvNets are capable of these more challenging tasks.<br>Our localization approach won the 2013 ILSVRC competition and significantly outperformed all<br>2012 and 2013 approaches. The detection model was among the top performers during the compe-<br>tition, and ranks first in post-competition results. We have proposed an integrated pipeline that can<br>perform different tasks while sharing a common feature extraction base, entirely learned directly<br>from the pixels.</p>",
            "id": 113,
            "page": 14,
            "text": "We have presented a multi-scale, sliding window approach that can be used for classification, 1ocalization and detection. We applied it to the ILSVRC 2013 datasets, and it currently ranks 4th in classification, 1st in localization and 1st in detection. A second important contribution of our paper is explaining how ConvNets can be effectively used for detection and localization tasks. These were never addressed in  and thus we are the first to explain how this can be done in the context of ImageNet 2012. The scheme we propose involves substantial modifications to networks designed for classification, but clearly demonstrate that ConvNets are capable of these more challenging tasks. Our localization approach won the 2013 ILSVRC competition and significantly outperformed all 2012 and 2013 approaches. The detection model was among the top performers during the competition, and ranks first in post-competition results. We have proposed an integrated pipeline that can perform different tasks while sharing a common feature extraction base, entirely learned directly from the pixels."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1322
                },
                {
                    "x": 2108,
                    "y": 1322
                },
                {
                    "x": 2108,
                    "y": 1600
                },
                {
                    "x": 441,
                    "y": 1600
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:18px'>Our approach might still be improved in several ways. (i) For localization, we are not currently<br>back-propping through the whole network; doing so is likely to improve performance. (ii) We are<br>using l2 loss, rather than directly optimizing the intersection-over-union (IOU) criterion on which<br>performance is measured. Swapping the loss to this should be possible since IOU is still differen-<br>tiable, provided there is some overlap. (iii) Alternate parameterizations of the bounding box may<br>help to decorrelate the outputs, which will aid network training.</p>",
            "id": 114,
            "page": 14,
            "text": "Our approach might still be improved in several ways. (i) For localization, we are not currently back-propping through the whole network; doing so is likely to improve performance. (ii) We are using l2 loss, rather than directly optimizing the intersection-over-union (IOU) criterion on which performance is measured. Swapping the loss to this should be possible since IOU is still differentiable, provided there is some overlap. (iii) Alternate parameterizations of the bounding box may help to decorrelate the outputs, which will aid network training."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1662
                },
                {
                    "x": 686,
                    "y": 1662
                },
                {
                    "x": 686,
                    "y": 1714
                },
                {
                    "x": 445,
                    "y": 1714
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:22px'>References</p>",
            "id": 115,
            "page": 14,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 451,
                    "y": 1734
                },
                {
                    "x": 2113,
                    "y": 1734
                },
                {
                    "x": 2113,
                    "y": 3065
                },
                {
                    "x": 451,
                    "y": 3065
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:14px'>[1] J. Carreira, F. Li, and C. Sminchisescu. Object recognition by sequential figure-ground ranking. Interna-<br>tional journal of computervision, 98(3):243-262, 2012.<br>[2] J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation,<br>release 1. http://sminctikexcu.ins.uni-bon.de/code/cpmo/<br>[3] D. C. Ciresan, J. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.<br>In CVPR, 2012.<br>[4] M. Delakis and C. Garcia. Text detection with convolutional neural networks. In International Conference<br>on Computer Vision Theory and Applications (VISAPP 2008), 2008.<br>[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical<br>Image Database. In CVPR09, 2009.<br>[6] I. Endres and D. Hoiem. Category independent object proposals. In Computer Vision-ECCV 2010, pages<br>575-588. Springer, 2010.<br>[7] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE<br>Transactions on Pattern Analysis and Machine Intelligence, 2013. in press.<br>[8] C. Garcia and M. Delakis. Convolutional face finder: A neural architecture for fast and robust face<br>detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004.<br>[9] A. Giusti, D. C. Ciresan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep<br>max-pooling convolutional neural networks. In International Conference on Image Processing (ICIP),<br>2013.<br>[10] R. Hadsell, P. Sermanet, M. Scoffier, A. Erkan, K. Kavackuoglu, U. Muller, and Y. LeCun. Learning<br>long-range vision for autonomous off-road driving. Journal of Field Robotics, 26(2):120-144, February<br>2009.<br>[11] G. Hinton, N. Srivastave, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural net-<br>works by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.<br>[12] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In Artificial Neural Networks<br>and Machine Learning-ICANN 2011, pages 44-51. Springer Berlin Heidelberg, 2011.<br>[13] V. Jain, J. F. Murray, F. Roth, S. Turaga, V. Zhigulin, K. Briggman, M. Helmstaedter, W. Denk, and H. S.<br>Seung. Supervised learning of image restoration with convolutional networks. In ICCV'07.<br>[14] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for<br>object recognition? In Proc. International Conference on Computer Vision (ICCV'09). IEEE, 2009.</p>",
            "id": 116,
            "page": 14,
            "text": " J. Carreira, F. Li, and C. Sminchisescu. Object recognition by sequential figure-ground ranking. International journal of computervision, 98(3):243-262, 2012.  J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation, release 1. http://sminctikexcu.ins.uni-bon.de/code/cpmo/  D. C. Ciresan, J. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. In CVPR, 2012.  M. Delakis and C. Garcia. Text detection with convolutional neural networks. In International Conference on Computer Vision Theory and Applications (VISAPP 2008), 2008.  J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.  I. Endres and D. Hoiem. Category independent object proposals. In Computer Vision-ECCV 2010, pages 575-588. Springer, 2010.  C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013. in press.  C. Garcia and M. Delakis. Convolutional face finder: A neural architecture for fast and robust face detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004.  A. Giusti, D. C. Ciresan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In International Conference on Image Processing (ICIP), 2013.  R. Hadsell, P. Sermanet, M. Scoffier, A. Erkan, K. Kavackuoglu, U. Muller, and Y. LeCun. Learning long-range vision for autonomous off-road driving. Journal of Field Robotics, 26(2):120-144, February 2009.  G. Hinton, N. Srivastave, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.  G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In Artificial Neural Networks and Machine Learning-ICANN 2011, pages 44-51. Springer Berlin Heidelberg, 2011.  V. Jain, J. F. Murray, F. Roth, S. Turaga, V. Zhigulin, K. Briggman, M. Helmstaedter, W. Denk, and H. S. Seung. Supervised learning of image restoration with convolutional networks. In ICCV'07.  K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV'09). IEEE, 2009."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3131
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1252,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='117' style='font-size:14px'>14</footer>",
            "id": 117,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 437,
                    "y": 330
                },
                {
                    "x": 2118,
                    "y": 330
                },
                {
                    "x": 2118,
                    "y": 1878
                },
                {
                    "x": 437,
                    "y": 1878
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:14px'>[15] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural net-<br>works. In NIPS, 2012.<br>[16] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Hand-<br>written digit recognition with a back-propagation network. In D. Touretzky, editor, Advances in Neural<br>Information Processing Systems (NIPS 1989), volume 2, Denver, CO, 1990. Morgan Kaufman.<br>[17] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.<br>Proceedings of the IEEE, 86(11):2278-2324, November 1998.<br>[18] Y. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance<br>to pose and lighting. In Proceedings of CVPR'04. IEEE Press, 2004.<br>[19] S. Manen, M. Guillaumin, and L. Van Gool. Prime object proposals with randomized prims algorithm. In<br>International Conference on Computer Vision (ICCV), 2013.<br>[20] 0. Matan, J. Bromley, C. Burges, J. Denker, L. Jackel, Y. LeCun, E. Pednault, W. Satterfield, C. Stenard,<br>and T. Thompson. Reading handwritten digits: A zip code recognition system. IEEE Computer, 25(7):59-<br>63, July 1992.<br>[21] F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. Barbano. Toward automatic phenotyping of<br>developing embryos from videos. IEEE Transactions on Image Processing, 14(9): 1360-1371, September<br>2005. Special issue on Molecular and Cellular Bioimaging.<br>[22] S. Nowlan and J. Platt. A convolutional neural network hand tracker. pages 901-908, San Mateo, CA,<br>1995. Morgan Kaufmann.<br>[23] M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and pose estimation with energy-based<br>models. Journal of Machine Learning Research, 8:1197-1215, May 2007.<br>[24] P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit<br>classification. In International Conference on Pattern Recognition (ICPR 2012), 2012.<br>[25] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-<br>stage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition<br>(CVPR'13). IEEE, June 2013.<br>[26] P. Sermanet and Y. LeCun. Traffic sign recognition with multi-scale convolutional networks. In Proceed-<br>ings of International Joint Conference on Neural Networks (IJCNN'11), 2011.<br>[27] G. Taylor, R. Fergus, G. Williams, I. Spiro, and C. Bregler. Pose-sensitive embedding by nonlinear nca<br>regression. In NIPS, 2011.<br>[28] G. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invarance through imitation. In CVPR, 2011.<br>[29] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object<br>recognition. International Journal of Computer Vision, 104(2):154-171, 2013.<br>[30] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE<br>Proc on Vision, Image, and Signal Processing, 141(4):245-250, August 1994.</p>",
            "id": 118,
            "page": 15,
            "text": " A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.  Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Handwritten digit recognition with a back-propagation network. In D. Touretzky, editor, Advances in Neural Information Processing Systems (NIPS 1989), volume 2, Denver, CO, 1990. Morgan Kaufman.  Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998.  Y. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Proceedings of CVPR'04. IEEE Press, 2004.  S. Manen, M. Guillaumin, and L. Van Gool. Prime object proposals with randomized prims algorithm. In International Conference on Computer Vision (ICCV), 2013.  0. Matan, J. Bromley, C. Burges, J. Denker, L. Jackel, Y. LeCun, E. Pednault, W. Satterfield, C. Stenard, and T. Thompson. Reading handwritten digits: A zip code recognition system. IEEE Computer, 25(7):5963, July 1992.  F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. Barbano. Toward automatic phenotyping of developing embryos from videos. IEEE Transactions on Image Processing, 14(9): 1360-1371, September 2005. Special issue on Molecular and Cellular Bioimaging.  S. Nowlan and J. Platt. A convolutional neural network hand tracker. pages 901-908, San Mateo, CA, 1995. Morgan Kaufmann.  M. Osadchy, Y. LeCun, and M. Miller. Synergistic face detection and pose estimation with energy-based models. Journal of Machine Learning Research, 8:1197-1215, May 2007.  P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit classification. In International Conference on Pattern Recognition (ICPR 2012), 2012.  P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multistage feature learning. In Proc. International Conference on Computer Vision and Pattern Recognition (CVPR'13). IEEE, June 2013.  P. Sermanet and Y. LeCun. Traffic sign recognition with multi-scale convolutional networks. In Proceedings of International Joint Conference on Neural Networks (IJCNN'11), 2011.  G. Taylor, R. Fergus, G. Williams, I. Spiro, and C. Bregler. Pose-sensitive embedding by nonlinear nca regression. In NIPS, 2011.  G. Taylor, I. Spiro, C. Bregler, and R. Fergus. Learning invarance through imitation. In CVPR, 2011.  J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object recognition. International Journal of Computer Vision, 104(2):154-171, 2013.  R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE Proc on Vision, Image, and Signal Processing, 141(4):245-250, August 1994."
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 3131
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1254,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='119' style='font-size:18px'>15</footer>",
            "id": 119,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 339
                },
                {
                    "x": 1229,
                    "y": 339
                },
                {
                    "x": 1229,
                    "y": 395
                },
                {
                    "x": 443,
                    "y": 395
                }
            ],
            "category": "caption",
            "html": "<caption id='120' style='font-size:22px'>Appendix: Additional Model Details</caption>",
            "id": 120,
            "page": 16,
            "text": "Appendix: Additional Model Details"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 460
                },
                {
                    "x": 2105,
                    "y": 460
                },
                {
                    "x": 2105,
                    "y": 833
                },
                {
                    "x": 446,
                    "y": 833
                }
            ],
            "category": "table",
            "html": "<table id='121' style='font-size:14px'><tr><td>Layer</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>Output 9</td></tr><tr><td>Stage</td><td>conv + max</td><td>conv + max</td><td>conv</td><td>conv</td><td>conv</td><td>conv + max</td><td>full</td><td>full</td><td>full</td></tr><tr><td># channels</td><td>96</td><td>256</td><td>512</td><td>512</td><td>1024</td><td>1024</td><td>4096</td><td>4096</td><td>1000</td></tr><tr><td>Filter size</td><td>7x7</td><td>7x7</td><td>3x3</td><td>3x3</td><td>3x3</td><td>3x3</td><td></td><td></td><td>-</td></tr><tr><td>Conv. stride</td><td>2x2</td><td>1x1</td><td>1x1</td><td>1x1</td><td>1x1</td><td>1x1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Pooling size</td><td>3x3</td><td>2x2</td><td>-</td><td>-</td><td>-</td><td>3x3</td><td>-</td><td>-</td><td>、</td></tr><tr><td>Pooling stride</td><td>3x3</td><td>2x2</td><td>-</td><td>-</td><td>-</td><td>3x3</td><td></td><td></td><td>-</td></tr><tr><td>Zero-Padding size</td><td>-</td><td>-</td><td>lxlxlx1</td><td>lxlxlxl</td><td>lxlxlxl</td><td>lxlxlxl</td><td></td><td></td><td>-</td></tr><tr><td>Spatial input size</td><td>221x221</td><td>36x36</td><td>15x15</td><td>15x15</td><td>15x15</td><td>15x15</td><td>5x5</td><td>1x1</td><td>1x1</td></tr></table>",
            "id": 121,
            "page": 16,
            "text": "Layer 1 2 3 4 5 6 7 8 Output 9  Stage conv + max conv + max conv conv conv conv + max full full full  # channels 96 256 512 512 1024 1024 4096 4096 1000  Filter size 7x7 7x7 3x3 3x3 3x3 3x3    Conv. stride 2x2 1x1 1x1 1x1 1x1 1x1 - -  Pooling size 3x3 2x2 - - - 3x3 - - 、  Pooling stride 3x3 2x2 - - - 3x3    Zero-Padding size - - lxlxlx1 lxlxlxl lxlxlxl lxlxlxl    Spatial input size 221x221 36x36 15x15 15x15 15x15 15x15 5x5 1x1"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 906
                },
                {
                    "x": 2110,
                    "y": 906
                },
                {
                    "x": 2110,
                    "y": 1003
                },
                {
                    "x": 439,
                    "y": 1003
                }
            ],
            "category": "caption",
            "html": "<caption id='122' style='font-size:18px'>Table 3: Architecture specifics for accurate model. It differs from the fast model mainly in the<br>stride of the first convolution, the number of stages and the number of feature maps.</caption>",
            "id": 122,
            "page": 16,
            "text": "Table 3: Architecture specifics for accurate model. It differs from the fast model mainly in the stride of the first convolution, the number of stages and the number of feature maps."
        },
        {
            "bounding_box": [
                {
                    "x": 810,
                    "y": 1088
                },
                {
                    "x": 1727,
                    "y": 1088
                },
                {
                    "x": 1727,
                    "y": 1240
                },
                {
                    "x": 810,
                    "y": 1240
                }
            ],
            "category": "table",
            "html": "<table id='123' style='font-size:14px'><tr><td>model</td><td># parameters (in millions)</td><td># connections (in millions)</td></tr><tr><td>Krizhevsky</td><td>60</td><td></td></tr><tr><td>fast</td><td>145</td><td>2810</td></tr><tr><td>accurate</td><td>144</td><td>5369</td></tr></table>",
            "id": 123,
            "page": 16,
            "text": "model # parameters (in millions) # connections (in millions)  Krizhevsky 60   fast 145 2810  accurate 144"
        },
        {
            "bounding_box": [
                {
                    "x": 669,
                    "y": 1295
                },
                {
                    "x": 1876,
                    "y": 1295
                },
                {
                    "x": 1876,
                    "y": 1346
                },
                {
                    "x": 669,
                    "y": 1346
                }
            ],
            "category": "caption",
            "html": "<caption id='124' style='font-size:18px'>Table 4: Number of parameters and connections for different models.</caption>",
            "id": 124,
            "page": 16,
            "text": "Table 4: Number of parameters and connections for different models."
        },
        {
            "bounding_box": [
                {
                    "x": 621,
                    "y": 1430
                },
                {
                    "x": 1918,
                    "y": 1430
                },
                {
                    "x": 1918,
                    "y": 1798
                },
                {
                    "x": 621,
                    "y": 1798
                }
            ],
            "category": "table",
            "html": "<table id='125' style='font-size:16px'><tr><td>Scale</td><td>Input size</td><td>Layer 5 pre-pool</td><td>Layer 5 post-pool</td><td>Classifier map (pre-reshape)</td><td>Classifier map size</td></tr><tr><td>1</td><td>245x245</td><td>17x17</td><td>(5x5)x(3x3)</td><td>(1x1)x(3x3)xC</td><td>3x3xC</td></tr><tr><td>2</td><td>281x317</td><td>20x23</td><td>(6x7)x(3x3)</td><td>(2x3)x(3x3)xC</td><td>6x9xC</td></tr><tr><td>3</td><td>317x389</td><td>23x29</td><td>(7x9)x(3x3)</td><td>(3x5)x(3x3)xC</td><td>9x15xC</td></tr><tr><td>4</td><td>389x461</td><td>29x35</td><td>(9x11)x(3x3)</td><td>(5x7)x(3x3)xC</td><td>15x21xC</td></tr><tr><td>5</td><td>425x497</td><td>32x35</td><td>(10x11)x(3x3)</td><td>(6x7)x(3x3)xC</td><td>18x24xC</td></tr><tr><td>6</td><td>461x569</td><td>35x44</td><td>(11x14)x(3x3)</td><td>(7x10)x(3x3)xC</td><td>21x30xC</td></tr></table>",
            "id": 125,
            "page": 16,
            "text": "Scale Input size Layer 5 pre-pool Layer 5 post-pool Classifier map (pre-reshape) Classifier map size  1 245x245 17x17 (5x5)x(3x3) (1x1)x(3x3)xC 3x3xC  2 281x317 20x23 (6x7)x(3x3) (2x3)x(3x3)xC 6x9xC  3 317x389 23x29 (7x9)x(3x3) (3x5)x(3x3)xC 9x15xC  4 389x461 29x35 (9x11)x(3x3) (5x7)x(3x3)xC 15x21xC  5 425x497 32x35 (10x11)x(3x3) (6x7)x(3x3)xC 18x24xC  6 461x569 35x44 (11x14)x(3x3) (7x10)x(3x3)xC"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1848
                },
                {
                    "x": 2110,
                    "y": 1848
                },
                {
                    "x": 2110,
                    "y": 2085
                },
                {
                    "x": 440,
                    "y": 2085
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:20px'>Table 5: Spatial dimensions of our multi-scale approach. 6 different sizes of input images are<br>used, resulting in layer 5 unpooled feature maps of differing spatial resolution (although not indi-<br>cated in the table, all have 256 feature channels). The (3x3) results from our dense pooling operation<br>with (△x, △y) = {0,1,2}. See text and Fig. 3 for details for how these are converted into output<br>maps.</p>",
            "id": 126,
            "page": 16,
            "text": "Table 5: Spatial dimensions of our multi-scale approach. 6 different sizes of input images are used, resulting in layer 5 unpooled feature maps of differing spatial resolution (although not indicated in the table, all have 256 feature channels). The (3x3) results from our dense pooling operation with (△x, △y) = {0,1,2}. See text and Fig. 3 for details for how these are converted into output maps."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3131
                },
                {
                    "x": 1301,
                    "y": 3131
                },
                {
                    "x": 1301,
                    "y": 3171
                },
                {
                    "x": 1252,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='127' style='font-size:18px'>16</footer>",
            "id": 127,
            "page": 16,
            "text": "16"
        }
    ]
}