{
    "id": "32bead72-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2105.05233v4.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 518,
                    "y": 410
                },
                {
                    "x": 2028,
                    "y": 410
                },
                {
                    "x": 2028,
                    "y": 493
                },
                {
                    "x": 518,
                    "y": 493
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Diffusion Models Beat GANs on Image Synthesis</p>",
            "id": 0,
            "page": 1,
            "text": "Diffusion Models Beat GANs on Image Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 736,
                    "y": 665
                },
                {
                    "x": 1166,
                    "y": 665
                },
                {
                    "x": 1166,
                    "y": 807
                },
                {
                    "x": 736,
                    "y": 807
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:14px'>Prafulla Dhariwal*<br>OpenAI<br>prafulla@openai · com</p>",
            "id": 1,
            "page": 1,
            "text": "Prafulla Dhariwal* OpenAI prafulla@openai · com"
        },
        {
            "bounding_box": [
                {
                    "x": 1475,
                    "y": 666
                },
                {
                    "x": 1813,
                    "y": 666
                },
                {
                    "x": 1813,
                    "y": 807
                },
                {
                    "x": 1475,
                    "y": 807
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:18px'>Alex Nichol*<br>OpenAI<br>al ex@openai com</p>",
            "id": 2,
            "page": 1,
            "text": "Alex Nichol* OpenAI al ex@openai com"
        },
        {
            "bounding_box": [
                {
                    "x": 1175,
                    "y": 923
                },
                {
                    "x": 1373,
                    "y": 923
                },
                {
                    "x": 1373,
                    "y": 977
                },
                {
                    "x": 1175,
                    "y": 977
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 589,
                    "y": 1027
                },
                {
                    "x": 1962,
                    "y": 1027
                },
                {
                    "x": 1962,
                    "y": 1583
                },
                {
                    "x": 589,
                    "y": 1583
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>We show that diffusion models can achieve image sample quality superior to the<br>current state-of-the-art generative models. We achieve this on unconditional im-<br>age synthesis by finding a better architecture through a series of ablations. For<br>conditional image synthesis, we further improve sample quality with classifier guid-<br>ance: a simple, compute-efficient method for trading off diversity for fidelity using<br>gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128 x 128,<br>4.59 on ImageNet 256x256, and 7.72 on ImageNet 512x512, and we match<br>BigGAN-deep even with as few as 25 forward passes per sample, all while main-<br>taining better coverage of the distribution. Finally, we find that classifier guidance<br>combines well with upsampling diffusion models, further improving FID to 3.94<br>on ImageNet 256x256 and 3.85 on ImageNet 512x512. We release our code at<br>https : //github · com/ openai / guided-diffusion.</p>",
            "id": 4,
            "page": 1,
            "text": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128 x 128, 4.59 on ImageNet 256x256, and 7.72 on ImageNet 512x512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256x256 and 3.85 on ImageNet 512x512. We release our code at https : //github · com/ openai / guided-diffusion."
        },
        {
            "bounding_box": [
                {
                    "x": 448,
                    "y": 1665
                },
                {
                    "x": 799,
                    "y": 1665
                },
                {
                    "x": 799,
                    "y": 1721
                },
                {
                    "x": 448,
                    "y": 1721
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1 Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1782
                },
                {
                    "x": 2104,
                    "y": 1782
                },
                {
                    "x": 2104,
                    "y": 2618
                },
                {
                    "x": 443,
                    "y": 2618
                }
            ],
            "category": "figure",
            "html": "<figure><img id='6' alt=\"\" data-coord=\"top-left:(443,1782); bottom-right:(2104,2618)\" /></figure>",
            "id": 6,
            "page": 1,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 605,
                    "y": 2639
                },
                {
                    "x": 1943,
                    "y": 2639
                },
                {
                    "x": 1943,
                    "y": 2690
                },
                {
                    "x": 605,
                    "y": 2690
                }
            ],
            "category": "caption",
            "html": "<br><caption id='7' style='font-size:18px'>Figure 1: Selected samples from our best ImageNet 512x512 model (FID 3.85)</caption>",
            "id": 7,
            "page": 1,
            "text": "Figure 1: Selected samples from our best ImageNet 512x512 model (FID 3.85)"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2746
                },
                {
                    "x": 2109,
                    "y": 2746
                },
                {
                    "x": 2109,
                    "y": 2940
                },
                {
                    "x": 442,
                    "y": 2940
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:16px'>Over the few years, generative models have gained the ability to generate human-like natural<br>past<br>language [6], infinite high-quality synthetic images [5, 28, 51] and highly diverse human speech and<br>music [64, 13]. These models can be used in a variety of ways, such as generating images from text<br>prompts [72, 50] or learning useful feature representations [14, 7]. While these models are already</p>",
            "id": 8,
            "page": 1,
            "text": "Over the few years, generative models have gained the ability to generate human-like natural past language , infinite high-quality synthetic images  and highly diverse human speech and music . These models can be used in a variety of ways, such as generating images from text prompts  or learning useful feature representations . While these models are already"
        },
        {
            "bounding_box": [
                {
                    "x": 498,
                    "y": 2966
                },
                {
                    "x": 801,
                    "y": 2966
                },
                {
                    "x": 801,
                    "y": 3012
                },
                {
                    "x": 498,
                    "y": 3012
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>Equal contribution</p>",
            "id": 9,
            "page": 1,
            "text": "Equal contribution"
        },
        {
            "bounding_box": [
                {
                    "x": 65,
                    "y": 915
                },
                {
                    "x": 148,
                    "y": 915
                },
                {
                    "x": 148,
                    "y": 2320
                },
                {
                    "x": 65,
                    "y": 2320
                }
            ],
            "category": "footer",
            "html": "<br><footer id='10' style='font-size:14px'>2021<br>Jun<br>1<br>[cs.LG]<br>arXiv:2105.05233v4</footer>",
            "id": 10,
            "page": 1,
            "text": "2021 Jun 1 [cs.LG] arXiv:2105.05233v4"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 309
                },
                {
                    "x": 2107,
                    "y": 309
                },
                {
                    "x": 2107,
                    "y": 443
                },
                {
                    "x": 442,
                    "y": 443
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:18px'>capable of producing realistic images and sound, there is still much room for improvement beyond<br>the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic<br>design, games, music production, and countless other fields.</p>",
            "id": 11,
            "page": 2,
            "text": "capable of producing realistic images and sound, there is still much room for improvement beyond the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic design, games, music production, and countless other fields."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 465
                },
                {
                    "x": 2109,
                    "y": 465
                },
                {
                    "x": 2109,
                    "y": 695
                },
                {
                    "x": 440,
                    "y": 695
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:20px'>GANs [19] currently hold the state-of-the-art on most image generation tasks [5, 68, 28] as measured<br>by sample quality metrics such as FID [23], Inception Score [54] and Precision [32]. However, some<br>of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity<br>than state-of-the-art likelihood-based models [51, 43, 42]. Furthermore, GANs are often difficult to<br>train, collapsing without carefully selected hyperparameters and regularizers [5, 41, 4].</p>",
            "id": 12,
            "page": 2,
            "text": "GANs  currently hold the state-of-the-art on most image generation tasks  as measured by sample quality metrics such as FID , Inception Score  and Precision . However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models . Furthermore, GANs are often difficult to train, collapsing without carefully selected hyperparameters and regularizers ."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 715
                },
                {
                    "x": 2109,
                    "y": 715
                },
                {
                    "x": 2109,
                    "y": 945
                },
                {
                    "x": 441,
                    "y": 945
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:16px'>While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to<br>new domains. As a result, much work has been done to achieve GAN-like sample quality with<br>likelihood-based models [51, 25, 42, 9]. While these models capture more diversity and are typically<br>easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore,<br>except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time.</p>",
            "id": 13,
            "page": 2,
            "text": "While GANs hold the state-of-the-art, their drawbacks make them difficult to scale and apply to new domains. As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models . While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual sample quality. Furthermore, except for VAEs, sampling from these models is slower than GANs in terms of wall-clock time."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 965
                },
                {
                    "x": 2108,
                    "y": 965
                },
                {
                    "x": 2108,
                    "y": 1421
                },
                {
                    "x": 441,
                    "y": 1421
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:18px'>Diffusion models are a class of likelihood-based models which have recently been shown to produce<br>high-quality images [56, 59, 25] while offering desirable properties such as distribution coverage,<br>a stationary training objective, and easy scalability. These models generate samples by gradually<br>removing noise from a signal, and their training objective can be expressed as a reweighted variational<br>lower-bound [25]. This class of models already holds the state-of-the-art [60] on CIFAR-10 [31], but<br>still lags behind GANs on difficult generation datasets like LSUN and ImageNet. Nichol and Dhariwal<br>[43] found that these models improve reliably with increased compute, and can produce high-quality<br>samples even on the difficult ImageNet 256x256 dataset using an upsampling stack. However, the<br>FID of this model is still not competitive with BigGAN-deep [5], the current state-of-the-art on this<br>dataset.</p>",
            "id": 14,
            "page": 2,
            "text": "Diffusion models are a class of likelihood-based models which have recently been shown to produce high-quality images  while offering desirable properties such as distribution coverage, a stationary training objective, and easy scalability. These models generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound . This class of models already holds the state-of-the-art  on CIFAR-10 , but still lags behind GANs on difficult generation datasets like LSUN and ImageNet. Nichol and Dhariwal  found that these models improve reliably with increased compute, and can produce high-quality samples even on the difficult ImageNet 256x256 dataset using an upsampling stack. However, the FID of this model is still not competitive with BigGAN-deep , the current state-of-the-art on this dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1441
                },
                {
                    "x": 2108,
                    "y": 1441
                },
                {
                    "x": 2108,
                    "y": 1763
                },
                {
                    "x": 441,
                    "y": 1763
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:18px'>We hypothesize that the gap between diffusion models and GANs stems from at least two factors:<br>first, that the model architectures used by recent GAN literature have been heavily explored and<br>refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples<br>but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by<br>improving model architecture and then by devising a scheme for trading off diversity for fidelity.<br>With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different<br>metrics and datasets.</p>",
            "id": 15,
            "page": 2,
            "text": "We hypothesize that the gap between diffusion models and GANs stems from at least two factors: first, that the model architectures used by recent GAN literature have been heavily explored and refined; second, that GANs are able to trade off diversity for fidelity, producing high quality samples but not covering the whole distribution. We aim to bring these benefits to diffusion models, first by improving model architecture and then by devising a scheme for trading off diversity for fidelity. With these improvements, we achieve a new state-of-the-art, surpassing GANs on several different metrics and datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1782
                },
                {
                    "x": 2108,
                    "y": 1782
                },
                {
                    "x": 2108,
                    "y": 2379
                },
                {
                    "x": 441,
                    "y": 2379
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:16px'>The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion<br>models based on Ho et al. [25] and the improvements from Nichol and Dhariwal [43] and Song<br>et al. [57], and we describe our evaluation setup. In Section 3, we introduce simple architecture<br>improvements that give a substantial boost to FID. In Section 4, we describe a method for using<br>gradients from a classifier to guide a diffusion model during sampling. We find that a single<br>hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity,<br>and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial<br>examples [61]. Finally, in Section 5 we show that models with our improved architecture achieve<br>state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-of-<br>the-art on conditional image synthesis. When using classifier guidance, we find that we can sample<br>with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare<br>our improved models to upsampling stacks, finding that the two approaches give complementary<br>improvements and that combining them gives the best results on ImageNet 256x256 and 512x512.</p>",
            "id": 16,
            "page": 2,
            "text": "The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho   and the improvements from Nichol and Dhariwal  and Song  , and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classifier to guide a diffusion model during sampling. We find that a single hyperparameter, the scale of the classifier gradients, can be tuned to trade off diversity for fidelity, and we can increase this gradient scale factor by an order of magnitude without obtaining adversarial examples . Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classifier guidance achieve state-ofthe-art on conditional image synthesis. When using classifier guidance, we find that we can sample with as few as 25 forward passes while maintaining FIDs comparable to BigGAN. We also compare our improved models to upsampling stacks, finding that the two approaches give complementary improvements and that combining them gives the best results on ImageNet 256x256 and 512x512."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2491
                },
                {
                    "x": 791,
                    "y": 2491
                },
                {
                    "x": 791,
                    "y": 2546
                },
                {
                    "x": 445,
                    "y": 2546
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:22px'>2 Background</p>",
            "id": 17,
            "page": 2,
            "text": "2 Background"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2626
                },
                {
                    "x": 2104,
                    "y": 2626
                },
                {
                    "x": 2104,
                    "y": 2716
                },
                {
                    "x": 442,
                    "y": 2716
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:14px'>In this section, we provide a brief overview of diffusion models. For a more detailed mathematical<br>description, we refer the reader to Appendix B.</p>",
            "id": 18,
            "page": 2,
            "text": "In this section, we provide a brief overview of diffusion models. For a more detailed mathematical description, we refer the reader to Appendix B."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2738
                },
                {
                    "x": 2109,
                    "y": 2738
                },
                {
                    "x": 2109,
                    "y": 3014
                },
                {
                    "x": 441,
                    "y": 3014
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:16px'>On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In<br>particular, sampling starts with noise XT and produces gradually less-noisy samples XT-1, XT-2, ...<br>until reaching a final sample xo. Each timestep t corresponds to a certain noise level, and Xt can be<br>thought of as a mixture of a signal xo with some noise E where the signal to noise ratio is determined<br>by the timestep t. For the remainder of this paper, we assume that the noise E is drawn from a diagonal<br>Gaussian distribution, which works well for natural images and simplifies various derivations.</p>",
            "id": 19,
            "page": 2,
            "text": "On a high level, diffusion models sample from a distribution by reversing a gradual noising process. In particular, sampling starts with noise XT and produces gradually less-noisy samples XT-1, XT-2, ... until reaching a final sample xo. Each timestep t corresponds to a certain noise level, and Xt can be thought of as a mixture of a signal xo with some noise E where the signal to noise ratio is determined by the timestep t. For the remainder of this paper, we assume that the noise E is drawn from a diagonal Gaussian distribution, which works well for natural images and simplifies various derivations."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3090
                },
                {
                    "x": 1290,
                    "y": 3090
                },
                {
                    "x": 1290,
                    "y": 3131
                },
                {
                    "x": 1259,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='20' style='font-size:14px'>2</footer>",
            "id": 20,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 305
                },
                {
                    "x": 2108,
                    "y": 305
                },
                {
                    "x": 2108,
                    "y": 580
                },
                {
                    "x": 441,
                    "y": 580
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:14px'>A diffusion model learns to produce a slightly more \"denoised\" Xt-1 from Xt. Ho et al. [25]<br>parameterize this model as a function ea(xt, t) which predicts the noise component of a noisy sample<br>Xt. To train these models, each sample in a minibatch is produced by randomly drawing a data sample<br>xo, a timestep t, and noise E, which together give rise to a noised sample Xt (Equation 17). The<br>training objective is then IIE� (xt, t) - €||2 , i.e. a simple mean-squared error loss between the true<br>noise and the predicted noise (Equation 26).</p>",
            "id": 21,
            "page": 3,
            "text": "A diffusion model learns to produce a slightly more \"denoised\" Xt-1 from Xt. Ho   parameterize this model as a function ea(xt, t) which predicts the noise component of a noisy sample Xt. To train these models, each sample in a minibatch is produced by randomly drawing a data sample xo, a timestep t, and noise E, which together give rise to a noised sample Xt (Equation 17). The training objective is then IIE� (xt, t) - €||2 , i.e. a simple mean-squared error loss between the true noise and the predicted noise (Equation 26)."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 602
                },
                {
                    "x": 2107,
                    "y": 602
                },
                {
                    "x": 2107,
                    "y": 923
                },
                {
                    "x": 440,
                    "y": 923
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:16px'>Itis not immediately obvious how to sample from a noise predictor ea (xt, t). Recall that diffusion<br>sampling proceeds by repeatedly predicting Xt-1 from Xt, starting from XT. Ho et al. [25] show<br>that, under reasonable assumptions, we can model the distribution pe(xt-1|xt) of Xt-1 given Xt as<br>a diagonal Gaussian N (xt-1 ; ou (xt, t), Eo(xt, t)), where the mean on (xt, t) can be calculated as a<br>function of E0(xt, t) (Equation 27). The variance �� (xt, t) of this Gaussian distribution can be fixed<br>to a known constant [25] or learned with a separate neural network head [43], and both approaches<br>yield high-quality samples when the total number of diffusion steps T is large enough.</p>",
            "id": 22,
            "page": 3,
            "text": "Itis not immediately obvious how to sample from a noise predictor ea (xt, t). Recall that diffusion sampling proceeds by repeatedly predicting Xt-1 from Xt, starting from XT. Ho   show that, under reasonable assumptions, we can model the distribution pe(xt-1|xt) of Xt-1 given Xt as a diagonal Gaussian N (xt-1 ; ou (xt, t), Eo(xt, t)), where the mean on (xt, t) can be calculated as a function of E0(xt, t) (Equation 27). The variance �� (xt, t) of this Gaussian distribution can be fixed to a known constant  or learned with a separate neural network head , and both approaches yield high-quality samples when the total number of diffusion steps T is large enough."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 942
                },
                {
                    "x": 2109,
                    "y": 942
                },
                {
                    "x": 2109,
                    "y": 1264
                },
                {
                    "x": 441,
                    "y": 1264
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:16px'>Ho et al. [25] observe that the simple mean-sqaured error objective, Lsimple, works better in practice<br>than the actual variational lower bound Lvlb that can be derived from interpreting the denoising diffu-<br>sion model as a VAE. They also note that training with this objective and using their corresponding<br>sampling procedure is equivalent to the denoising score matching model from Song and Ermon [58],<br>who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to<br>produce high quality image samples. We often use \"diffusion models\" as shorthand to refer to both<br>classes of models.</p>",
            "id": 23,
            "page": 3,
            "text": "Ho   observe that the simple mean-sqaured error objective, Lsimple, works better in practice than the actual variational lower bound Lvlb that can be derived from interpreting the denoising diffusion model as a VAE. They also note that training with this objective and using their corresponding sampling procedure is equivalent to the denoising score matching model from Song and Ermon , who use Langevin dynamics to sample from a denoising model trained with multiple noise levels to produce high quality image samples. We often use \"diffusion models\" as shorthand to refer to both classes of models."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1315
                },
                {
                    "x": 804,
                    "y": 1315
                },
                {
                    "x": 804,
                    "y": 1364
                },
                {
                    "x": 444,
                    "y": 1364
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>2.1 Improvements</p>",
            "id": 24,
            "page": 3,
            "text": "2.1 Improvements"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1399
                },
                {
                    "x": 2108,
                    "y": 1399
                },
                {
                    "x": 2108,
                    "y": 1537
                },
                {
                    "x": 442,
                    "y": 1537
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:14px'>Following the breakthrough work of Song and Ermon [58] and Ho et al. [25], several recent papers<br>have proposed improvements to diffusion models. Here we describe a few of these improvements,<br>which we employ for our models.</p>",
            "id": 25,
            "page": 3,
            "text": "Following the breakthrough work of Song and Ermon  and Ho  , several recent papers have proposed improvements to diffusion models. Here we describe a few of these improvements, which we employ for our models."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1558
                },
                {
                    "x": 2110,
                    "y": 1558
                },
                {
                    "x": 2110,
                    "y": 1697
                },
                {
                    "x": 442,
                    "y": 1697
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:14px'>Nichol and Dhariwal [43] find that fixing the variance E�(xt, t) to a constant as done in Ho et al.<br>[25] is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize Eo(xt, t) as<br>a neural network whose output v is interpolated as:</p>",
            "id": 26,
            "page": 3,
            "text": "Nichol and Dhariwal  find that fixing the variance E�(xt, t) to a constant as done in Ho   is sub-optimal for sampling with fewer diffusion steps, and propose to parameterize Eo(xt, t) as a neural network whose output v is interpolated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1818
                },
                {
                    "x": 2107,
                    "y": 1818
                },
                {
                    "x": 2107,
                    "y": 2100
                },
                {
                    "x": 441,
                    "y": 2100
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:18px'>Here, Bt and Bt (Equation 19) are the variances in Ho et al. [25] corresponding to upper and lower<br>bounds for the reverse process variances. Additionally, Nichol and Dhariwal [43] propose a hybrid<br>objective for training both ea (xt, t) and Eo(xt, t) using the weighted sum Lsimple + 入Lvlb. Learning<br>the reverse process variances with their hybrid objective allows sampling with fewer steps without<br>much drop in sample quality. We adopt this objective and parameterization, and use it throughout our<br>experiments.</p>",
            "id": 27,
            "page": 3,
            "text": "Here, Bt and Bt (Equation 19) are the variances in Ho   corresponding to upper and lower bounds for the reverse process variances. Additionally, Nichol and Dhariwal  propose a hybrid objective for training both ea (xt, t) and Eo(xt, t) using the weighted sum Lsimple + 入Lvlb. Learning the reverse process variances with their hybrid objective allows sampling with fewer steps without much drop in sample quality. We adopt this objective and parameterization, and use it throughout our experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2122
                },
                {
                    "x": 2108,
                    "y": 2122
                },
                {
                    "x": 2108,
                    "y": 2398
                },
                {
                    "x": 441,
                    "y": 2398
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:16px'>Song et al. [57] propose DDIM, which formulates an alternative non-Markovian noising process<br>that has the same forward marginals as DDPM, but allows producing different reverse samplers by<br>changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any<br>model eo (xt, t) into a deterministic mapping from latents to images, and find that this provides an<br>alternative way to sample with fewer steps. We adopt this sampling approach when using fewer than<br>50 sampling steps, since Nichol and Dhariwal [43] found it to be beneficial in this regime.</p>",
            "id": 28,
            "page": 3,
            "text": "Song   propose DDIM, which formulates an alternative non-Markovian noising process that has the same forward marginals as DDPM, but allows producing different reverse samplers by changing the variance of the reverse noise. By setting this noise to 0, they provide a way to turn any model eo (xt, t) into a deterministic mapping from latents to images, and find that this provides an alternative way to sample with fewer steps. We adopt this sampling approach when using fewer than 50 sampling steps, since Nichol and Dhariwal  found it to be beneficial in this regime."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2448
                },
                {
                    "x": 971,
                    "y": 2448
                },
                {
                    "x": 971,
                    "y": 2497
                },
                {
                    "x": 443,
                    "y": 2497
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:20px'>2.2 Sample Quality Metrics</p>",
            "id": 29,
            "page": 3,
            "text": "2.2 Sample Quality Metrics"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2532
                },
                {
                    "x": 2109,
                    "y": 2532
                },
                {
                    "x": 2109,
                    "y": 2714
                },
                {
                    "x": 441,
                    "y": 2714
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>For comparing sample quality across models, we perform quantitative evaluations using the following<br>metrics. While these metrics are often used in practice and correspond well with human judgement,<br>they are not a perfect proxy, and finding better metrics for sample quality evaluation is still an open<br>problem.</p>",
            "id": 30,
            "page": 3,
            "text": "For comparing sample quality across models, we perform quantitative evaluations using the following metrics. While these metrics are often used in practice and correspond well with human judgement, they are not a perfect proxy, and finding better metrics for sample quality evaluation is still an open problem."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2738
                },
                {
                    "x": 2108,
                    "y": 2738
                },
                {
                    "x": 2108,
                    "y": 3014
                },
                {
                    "x": 441,
                    "y": 3014
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:16px'>Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures<br>the full ImageNet class distribution while still producing individual samples that are convincing<br>examples of a single class. One drawback of this metric is that it does not reward covering the<br>whole distribution or capturing diversity within a class, and models which memorize a small subset<br>of the full dataset will still have high IS [3]. To better capture diversity than IS, Frechet Inception<br>Distance (FID) was proposed by Heusel et al. [23], who argued that it is more consistent with human</p>",
            "id": 31,
            "page": 3,
            "text": "Inception Score (IS) was proposed by Salimans  , and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS . To better capture diversity than IS, Frechet Inception Distance (FID) was proposed by Heusel  , who argued that it is more consistent with human"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3094
                },
                {
                    "x": 1289,
                    "y": 3094
                },
                {
                    "x": 1289,
                    "y": 3129
                },
                {
                    "x": 1260,
                    "y": 3129
                }
            ],
            "category": "footer",
            "html": "<footer id='32' style='font-size:14px'>3</footer>",
            "id": 32,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 289
                },
                {
                    "x": 1997,
                    "y": 289
                },
                {
                    "x": 1997,
                    "y": 755
                },
                {
                    "x": 551,
                    "y": 755
                }
            ],
            "category": "table",
            "html": "<table id='33' style='font-size:14px'><tr><td>Channels</td><td>Depth</td><td>Heads</td><td>Attention resolutions</td><td>BigGAN up/downsample</td><td>Rescale resblock</td><td>FID 700K</td><td>FID 1200K</td></tr><tr><td>160</td><td>2</td><td>1</td><td>16</td><td>X</td><td>X</td><td>15.33</td><td>13.21</td></tr><tr><td>128</td><td>4</td><td></td><td></td><td></td><td></td><td>-0.21</td><td>-0.48</td></tr><tr><td></td><td></td><td>4</td><td></td><td></td><td></td><td>-0.54</td><td>-0.82</td></tr><tr><td></td><td></td><td></td><td>32,16,8</td><td></td><td></td><td>-0.72</td><td>-0.66</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>-1.20</td><td>-1.21</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>V</td><td>0.16</td><td>0.25</td></tr><tr><td>160</td><td>2</td><td>4</td><td>32,16,8</td><td>V</td><td>X</td><td>-3.14</td><td>-3.00</td></tr></table>",
            "id": 33,
            "page": 4,
            "text": "Channels Depth Heads Attention resolutions BigGAN up/downsample Rescale resblock FID 700K FID 1200K  160 2 1 16 X X 15.33 13.21  128 4     -0.21 -0.48    4    -0.54 -0.82     32,16,8   -0.72 -0.66        -1.20 -1.21       V 0.16 0.25  160 2 4 32,16,8 V X -3.14"
        },
        {
            "bounding_box": [
                {
                    "x": 523,
                    "y": 770
                },
                {
                    "x": 2029,
                    "y": 770
                },
                {
                    "x": 2029,
                    "y": 817
                },
                {
                    "x": 523,
                    "y": 817
                }
            ],
            "category": "caption",
            "html": "<br><caption id='34' style='font-size:18px'>Table 1: Ablation of various architecture changes, evaluated at 700K and 1200K iterations</caption>",
            "id": 34,
            "page": 4,
            "text": "Table 1: Ablation of various architecture changes, evaluated at 700K and 1200K iterations"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 879
                },
                {
                    "x": 2108,
                    "y": 879
                },
                {
                    "x": 2108,
                    "y": 1242
                },
                {
                    "x": 442,
                    "y": 1242
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>judgement than Inception Score. FID provides a symmetric measure of the distance between two<br>image distributions in the Inception- V3 [62] latent space. Recently, sFID was proposed by Nash<br>et al. [42] as a version of FID that uses spatial features rather than the standard pooled features.<br>They find that this metric better captures spatial relationships, rewarding image distributions with<br>coherent high-level structure. Finally, Kynkaanniemi et al. [32] proposed Improved Precision and<br>Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into<br>the data manifold (precision), and diversity as the fraction of data samples which fall into the sample<br>manifold (recall).</p>",
            "id": 35,
            "page": 4,
            "text": "judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception- V3  latent space. Recently, sFID was proposed by Nash   as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure. Finally, Kynkaanniemi   proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1264
                },
                {
                    "x": 2109,
                    "y": 1264
                },
                {
                    "x": 2109,
                    "y": 1678
                },
                {
                    "x": 442,
                    "y": 1678
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:16px'>We use FID as our default metric for overall sample quality comparisons as it captures both diversity<br>and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work<br>[27, 28, 5, 25]. We use Precision or IS to measure fidelity, and Recall to measure diversity or<br>distribution coverage. When comparing against other methods, we re-compute these metrics using<br>public samples or models whenever possible. This is for two reasons: first, some papers [27, 28,<br>25] compare against arbitrary subsets of the training set which are not readily available; and second,<br>subtle implementation differences can affect the resulting FID values [45]. To ensure consistent<br>comparisons, we use the entire training set as the reference batch [23, 5], and evaluate metrics for all<br>models using the same codebase.</p>",
            "id": 36,
            "page": 4,
            "text": "We use FID as our default metric for overall sample quality comparisons as it captures both diversity and fidelity and has been the de facto standard metric for state-of-the-art generative modeling work . We use Precision or IS to measure fidelity, and Recall to measure diversity or distribution coverage. When comparing against other methods, we re-compute these metrics using public samples or models whenever possible. This is for two reasons: first, some papers  compare against arbitrary subsets of the training set which are not readily available; and second, subtle implementation differences can affect the resulting FID values . To ensure consistent comparisons, we use the entire training set as the reference batch , and evaluate metrics for all models using the same codebase."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1742
                },
                {
                    "x": 1119,
                    "y": 1742
                },
                {
                    "x": 1119,
                    "y": 1797
                },
                {
                    "x": 444,
                    "y": 1797
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:22px'>3 Architecture Improvements</p>",
            "id": 37,
            "page": 4,
            "text": "3 Architecture Improvements"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1844
                },
                {
                    "x": 2105,
                    "y": 1844
                },
                {
                    "x": 2105,
                    "y": 1936
                },
                {
                    "x": 442,
                    "y": 1936
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:16px'>In this section we conduct several architecture ablations to find the model architecture that provides<br>the best sample quality for diffusion models.</p>",
            "id": 38,
            "page": 4,
            "text": "In this section we conduct several architecture ablations to find the model architecture that provides the best sample quality for diffusion models."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1956
                },
                {
                    "x": 2108,
                    "y": 1956
                },
                {
                    "x": 2108,
                    "y": 2415
                },
                {
                    "x": 441,
                    "y": 2415
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:18px'>Ho et al. [25] introduced the UNet architecture for diffusion models, which Jolicoeur-Martineau<br>et al. [26] found to substantially improve sample quality over the previous architectures [58, 33] used<br>for denoising score matching. The UNet model uses a stack of residual layers and downsampling<br>convolutions, followed by a stack of residual layers with upsampling colvolutions, with skip con-<br>nections connecting the layers with the same spatial size. In addition, they use a global attention<br>layer at the 16x 16 resolution with a single head, and add a projection of the timestep embedding into<br>each residual block. Song et al. [60] found that further changes to the UNet architecture improved<br>performance on the CIFAR-10 [31] and CelebA-64 [34] datasets. We show the same result on<br>ImageNet 128x 128, finding that architecture can indeed give a substantial boost to sample quality on<br>much larger and more diverse datasets at a higher resolution.</p>",
            "id": 39,
            "page": 4,
            "text": "Ho   introduced the UNet architecture for diffusion models, which Jolicoeur-Martineau   found to substantially improve sample quality over the previous architectures  used for denoising score matching. The UNet model uses a stack of residual layers and downsampling convolutions, followed by a stack of residual layers with upsampling colvolutions, with skip connections connecting the layers with the same spatial size. In addition, they use a global attention layer at the 16x 16 resolution with a single head, and add a projection of the timestep embedding into each residual block. Song   found that further changes to the UNet architecture improved performance on the CIFAR-10  and CelebA-64  datasets. We show the same result on ImageNet 128x 128, finding that architecture can indeed give a substantial boost to sample quality on much larger and more diverse datasets at a higher resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2437
                },
                {
                    "x": 1245,
                    "y": 2437
                },
                {
                    "x": 1245,
                    "y": 2484
                },
                {
                    "x": 445,
                    "y": 2484
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:20px'>We explore the following architectural changes:</p>",
            "id": 40,
            "page": 4,
            "text": "We explore the following architectural changes:"
        },
        {
            "bounding_box": [
                {
                    "x": 557,
                    "y": 2524
                },
                {
                    "x": 2104,
                    "y": 2524
                },
                {
                    "x": 2104,
                    "y": 2885
                },
                {
                    "x": 557,
                    "y": 2885
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>· Increasing depth versus width, holding model size relatively constant.<br>· Increasing the number of attention heads.<br>· Using attention at 32x32, 16x 16, and 8x8 resolutions rather than only at 16x16.<br>· Using the BigGAN [5] residual block for upsampling and downsampling the activations,<br>following [60].<br>· Rescaling residual connections with V, following [60, 27, 28].</p>",
            "id": 41,
            "page": 4,
            "text": "· Increasing depth versus width, holding model size relatively constant. · Increasing the number of attention heads. · Using attention at 32x32, 16x 16, and 8x8 resolutions rather than only at 16x16. · Using the BigGAN  residual block for upsampling and downsampling the activations, following . · Rescaling residual connections with V, following ."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2919
                },
                {
                    "x": 2107,
                    "y": 2919
                },
                {
                    "x": 2107,
                    "y": 3014
                },
                {
                    "x": 442,
                    "y": 3014
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>For all comparisons in this section, we train models on ImageNet 128x 128 with batch size 256, and<br>sample using 250 sampling steps. We train models with the above architecture changes and compare</p>",
            "id": 42,
            "page": 4,
            "text": "For all comparisons in this section, we train models on ImageNet 128x 128 with batch size 256, and sample using 250 sampling steps. We train models with the above architecture changes and compare"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3094
                },
                {
                    "x": 1288,
                    "y": 3094
                },
                {
                    "x": 1288,
                    "y": 3129
                },
                {
                    "x": 1259,
                    "y": 3129
                }
            ],
            "category": "footer",
            "html": "<footer id='43' style='font-size:14px'>4</footer>",
            "id": 43,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 879,
                    "y": 293
                },
                {
                    "x": 1667,
                    "y": 293
                },
                {
                    "x": 1667,
                    "y": 716
                },
                {
                    "x": 879,
                    "y": 716
                }
            ],
            "category": "table",
            "html": "<table id='44' style='font-size:16px'><tr><td>Number of heads</td><td>Channels per head</td><td>FID</td></tr><tr><td>1</td><td></td><td>14.08</td></tr><tr><td>2</td><td></td><td>-0.50</td></tr><tr><td>4</td><td></td><td>-0.97</td></tr><tr><td>8</td><td></td><td>-1.17</td></tr><tr><td></td><td>32</td><td>-1.36</td></tr><tr><td></td><td>64</td><td>-1.03</td></tr><tr><td></td><td>128</td><td>-1.08</td></tr></table>",
            "id": 44,
            "page": 5,
            "text": "Number of heads Channels per head FID  1  14.08  2  -0.50  4  -0.97  8  -1.17   32 -1.36   64 -1.03   128"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 729
                },
                {
                    "x": 2107,
                    "y": 729
                },
                {
                    "x": 2107,
                    "y": 821
                },
                {
                    "x": 442,
                    "y": 821
                }
            ],
            "category": "caption",
            "html": "<br><caption id='45' style='font-size:20px'>Table 2: Ablation of various attention configurations. More heads or lower channels per heads both<br>lead to improved FID.</caption>",
            "id": 45,
            "page": 5,
            "text": "Table 2: Ablation of various attention configurations. More heads or lower channels per heads both lead to improved FID."
        },
        {
            "bounding_box": [
                {
                    "x": 507,
                    "y": 918
                },
                {
                    "x": 2050,
                    "y": 918
                },
                {
                    "x": 2050,
                    "y": 1662
                },
                {
                    "x": 507,
                    "y": 1662
                }
            ],
            "category": "figure",
            "html": "<figure><img id='46' style='font-size:14px' alt=\"ch=128, res=4 1 head\nch=160, res=2 2 heads\nch=160, res=2, heads=4 4 heads\nch=160, res=2, multi-res attn 8 heads\nch=160, res=2, biggan up/down 32 head channels\nch=160, res=2, skip rescale 64 head channels\nch=160, res=2, heads=4, multi-res attn, biggan up/down 128 head channels\n28\n26\n26\n24\n24\n22\n22\nFID 20 FID\n20\n18\n18\n16\n16\n14\n14\n40 60 80 100 120 140 160 180 20 40 60 80 100\ntime (hrs) time (hrs)\" data-coord=\"top-left:(507,918); bottom-right:(2050,1662)\" /></figure>",
            "id": 46,
            "page": 5,
            "text": "ch=128, res=4 1 head ch=160, res=2 2 heads ch=160, res=2, heads=4 4 heads ch=160, res=2, multi-res attn 8 heads ch=160, res=2, biggan up/down 32 head channels ch=160, res=2, skip rescale 64 head channels ch=160, res=2, heads=4, multi-res attn, biggan up/down 128 head channels 28 26 26 24 24 22 22 FID 20 FID 20 18 18 16 16 14 14 40 60 80 100 120 140 160 180 20 40 60 80 100 time (hrs) time (hrs)"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1714
                },
                {
                    "x": 2108,
                    "y": 1714
                },
                {
                    "x": 2108,
                    "y": 1808
                },
                {
                    "x": 443,
                    "y": 1808
                }
            ],
            "category": "caption",
            "html": "<caption id='47' style='font-size:22px'>Figure 2: Ablation of various architecture changes, showing FID as a function of wall-clock time.<br>FID evaluated over 10k samples instead of 50k for efficiency.</caption>",
            "id": 47,
            "page": 5,
            "text": "Figure 2: Ablation of various architecture changes, showing FID as a function of wall-clock time. FID evaluated over 10k samples instead of 50k for efficiency."
        },
        {
            "bounding_box": [
                {
                    "x": 1006,
                    "y": 1935
                },
                {
                    "x": 1546,
                    "y": 1935
                },
                {
                    "x": 1546,
                    "y": 2102
                },
                {
                    "x": 1006,
                    "y": 2102
                }
            ],
            "category": "table",
            "html": "<table id='48' style='font-size:18px'><tr><td>Operation</td><td>FID</td></tr><tr><td>AdaGN</td><td>13.06</td></tr><tr><td>Addition + GroupNorm</td><td>15.08</td></tr></table>",
            "id": 48,
            "page": 5,
            "text": "Operation FID  AdaGN 13.06  Addition + GroupNorm"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2123
                },
                {
                    "x": 2109,
                    "y": 2123
                },
                {
                    "x": 2109,
                    "y": 2262
                },
                {
                    "x": 442,
                    "y": 2262
                }
            ],
            "category": "caption",
            "html": "<br><caption id='49' style='font-size:22px'>Table 3: Ablating the element-wise operation used when projecting timestep and class embeddings<br>into each residual block. Replacing AdaGN with the Addition + GroupNorm layer from Ho et al.<br>[25] makes FID worse.</caption>",
            "id": 49,
            "page": 5,
            "text": "Table 3: Ablating the element-wise operation used when projecting timestep and class embeddings into each residual block. Replacing AdaGN with the Addition + GroupNorm layer from Ho   makes FID worse."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2397
                },
                {
                    "x": 2107,
                    "y": 2397
                },
                {
                    "x": 2107,
                    "y": 2625
                },
                {
                    "x": 440,
                    "y": 2625
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:20px'>them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual<br>connections, all of the other modifications improve performance and have a positive compounding<br>effect. We observe in Figure 2 that while increased depth helps performance, it increases training<br>time and takes longer to reach the same performance as a wider model, SO we opt not to use this<br>change in further experiments.</p>",
            "id": 50,
            "page": 5,
            "text": "them on FID, evaluated at two different points of training, in Table 1. Aside from rescaling residual connections, all of the other modifications improve performance and have a positive compounding effect. We observe in Figure 2 that while increased depth helps performance, it increases training time and takes longer to reach the same performance as a wider model, SO we opt not to use this change in further experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2645
                },
                {
                    "x": 2108,
                    "y": 2645
                },
                {
                    "x": 2108,
                    "y": 3013
                },
                {
                    "x": 441,
                    "y": 3013
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:20px'>We also study other attention configurations that better match the Transformer architecture [66]. To<br>this end, we experimented with either fixing attention heads to a constant, or fixing the number of<br>channels per head. For the rest of the architecture, we use 128 base channels, 2 residual blocks<br>per resolution, multi-resolution attention, and BigGAN up/downsampling, and we train the models<br>for 700K iterations. Table 2 shows our results, indicating that more heads or fewer channels per<br>head improves FID. In Figure 2, we see 64 channels is best for wall-clock time, SO we opt to use 64<br>channels per head as our default. We note that this choice also better matches modern transformer<br>architectures, and is on par with our other configurations in terms of final FID.</p>",
            "id": 51,
            "page": 5,
            "text": "We also study other attention configurations that better match the Transformer architecture . To this end, we experimented with either fixing attention heads to a constant, or fixing the number of channels per head. For the rest of the architecture, we use 128 base channels, 2 residual blocks per resolution, multi-resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. Table 2 shows our results, indicating that more heads or fewer channels per head improves FID. In Figure 2, we see 64 channels is best for wall-clock time, SO we opt to use 64 channels per head as our default. We note that this choice also better matches modern transformer architectures, and is on par with our other configurations in terms of final FID."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3095
                },
                {
                    "x": 1289,
                    "y": 3095
                },
                {
                    "x": 1289,
                    "y": 3128
                },
                {
                    "x": 1260,
                    "y": 3128
                }
            ],
            "category": "footer",
            "html": "<footer id='52' style='font-size:18px'>5</footer>",
            "id": 52,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 306
                },
                {
                    "x": 1101,
                    "y": 306
                },
                {
                    "x": 1101,
                    "y": 353
                },
                {
                    "x": 443,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>3.1 Adaptive Group Normalization</p>",
            "id": 53,
            "page": 6,
            "text": "3.1 Adaptive Group Normalization"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 389
                },
                {
                    "x": 2109,
                    "y": 389
                },
                {
                    "x": 2109,
                    "y": 663
                },
                {
                    "x": 442,
                    "y": 663
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>We also experiment with a layer [43] that we refer to as adaptive group normalization (AdaGN), which<br>incorporates the timestep and class embedding into each residual block after a group normalization<br>operation [69], similar to adaptive instance norm [27] and FiLM [48]. We define this layer as<br>AdaGN(h, y) = Ys GroupNorm (h) + yb, where h is the intermediate activations of the residual block<br>following the first convolution, and y = [Ys, y6] is obtained from a linear projection of the timestep<br>and class embedding.</p>",
            "id": 54,
            "page": 6,
            "text": "We also experiment with a layer  that we refer to as adaptive group normalization (AdaGN), which incorporates the timestep and class embedding into each residual block after a group normalization operation , similar to adaptive instance norm  and FiLM . We define this layer as AdaGN(h, y) = Ys GroupNorm (h) + yb, where h is the intermediate activations of the residual block following the first convolution, and y = [Ys, y6] is obtained from a linear projection of the timestep and class embedding."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 685
                },
                {
                    "x": 2109,
                    "y": 685
                },
                {
                    "x": 2109,
                    "y": 912
                },
                {
                    "x": 441,
                    "y": 912
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:16px'>We had already seen AdaGN improve our earliest diffusion models, and SO had it included by<br>default in all our runs. In Table 3, we explicitly ablate this choice, and find that the adaptive group<br>normalization layer indeed improved FID. Both models use 128 base channels and 2 residual blocks<br>per resolution, multi-resolution attention with 64 channels per head, and BigGAN up/downsampling,<br>and were trained for 700K iterations.</p>",
            "id": 55,
            "page": 6,
            "text": "We had already seen AdaGN improve our earliest diffusion models, and SO had it included by default in all our runs. In Table 3, we explicitly ablate this choice, and find that the adaptive group normalization layer indeed improved FID. Both models use 128 base channels and 2 residual blocks per resolution, multi-resolution attention with 64 channels per head, and BigGAN up/downsampling, and were trained for 700K iterations."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 935
                },
                {
                    "x": 2108,
                    "y": 935
                },
                {
                    "x": 2108,
                    "y": 1120
                },
                {
                    "x": 442,
                    "y": 1120
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:16px'>In the rest of the paper, we use this final improved model architecture as our default: variable width<br>with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and<br>8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization<br>for injecting timestep and class embeddings into residual blocks.</p>",
            "id": 56,
            "page": 6,
            "text": "In the rest of the paper, we use this final improved model architecture as our default: variable width with 2 residual blocks per resolution, multiple heads with 64 channels per head, attention at 32, 16 and 8 resolutions, BigGAN residual blocks for up and downsampling, and adaptive group normalization for injecting timestep and class embeddings into residual blocks."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1183
                },
                {
                    "x": 947,
                    "y": 1183
                },
                {
                    "x": 947,
                    "y": 1238
                },
                {
                    "x": 442,
                    "y": 1238
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:22px'>4 Classifier Guidance</p>",
            "id": 57,
            "page": 6,
            "text": "4 Classifier Guidance"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1284
                },
                {
                    "x": 2108,
                    "y": 1284
                },
                {
                    "x": 2108,
                    "y": 1561
                },
                {
                    "x": 441,
                    "y": 1561
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>In addition to employing well designed architectures, GANs for conditional image synthesis [39, 5]<br>make heavy use of class labels. This often takes the form of class-conditional normalization statistics<br>[16, 11] as well as discriminators with heads that are explicitly designed to behave like classifiers<br>p(y|x) [40]. As further evidence that class information is crucial to the success of these models,<br>Lucic et al. [36] find that it is helpful to generate synthetic labels when working in a label-limited<br>regime.</p>",
            "id": 58,
            "page": 6,
            "text": "In addition to employing well designed architectures, GANs for conditional image synthesis  make heavy use of class labels. This often takes the form of class-conditional normalization statistics  as well as discriminators with heads that are explicitly designed to behave like classifiers p(y|x) . As further evidence that class information is crucial to the success of these models, Lucic   find that it is helpful to generate synthetic labels when working in a label-limited regime."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1582
                },
                {
                    "x": 2107,
                    "y": 1582
                },
                {
                    "x": 2107,
                    "y": 1903
                },
                {
                    "x": 442,
                    "y": 1903
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:18px'>Given this observation for GANs, it makes sense to explore different ways to condition diffusion<br>models on class labels. We already incorporate class information into normalization layers (Section<br>3.1). Here, we explore a different approach: exploiting a classifier p(y|x) to improve a diffusion<br>generator. Sohl-Dickstein et al. [56] and Song et al. [60] show one way to achieve this, wherein a<br>pre-trained diffusion model can be conditioned using the gradients of a classifier. In particular, we<br>can train a classifier p⌀(y|xt, t) on noisy images Xt, and then use gradients Vxt logp⌀(y|xt, t) to<br>guide the diffusion sampling process towards an arbitrary class label y.</p>",
            "id": 59,
            "page": 6,
            "text": "Given this observation for GANs, it makes sense to explore different ways to condition diffusion models on class labels. We already incorporate class information into normalization layers (Section 3.1). Here, we explore a different approach: exploiting a classifier p(y|x) to improve a diffusion generator. Sohl-Dickstein   and Song   show one way to achieve this, wherein a pre-trained diffusion model can be conditioned using the gradients of a classifier. In particular, we can train a classifier p⌀(y|xt, t) on noisy images Xt, and then use gradients Vxt logp⌀(y|xt, t) to guide the diffusion sampling process towards an arbitrary class label y."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1923
                },
                {
                    "x": 2108,
                    "y": 1923
                },
                {
                    "x": 2108,
                    "y": 2108
                },
                {
                    "x": 442,
                    "y": 2108
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:18px'>In this section, we first review two ways of deriving conditional sampling processes using classifiers.<br>We then describe how we use such classifiers in practice to improve sample quality. We choose the<br>notation P⌀ (y|xt, t) = p⌀(y|xt) and E0(xt, t) = E0(xt) for brevity, noting that they refer to separate<br>functions for each timestep t and at training time the models must be conditioned on the input t.</p>",
            "id": 60,
            "page": 6,
            "text": "In this section, we first review two ways of deriving conditional sampling processes using classifiers. We then describe how we use such classifiers in practice to improve sample quality. We choose the notation P⌀ (y|xt, t) = p⌀(y|xt) and E0(xt, t) = E0(xt) for brevity, noting that they refer to separate functions for each timestep t and at training time the models must be conditioned on the input t."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2160
                },
                {
                    "x": 1195,
                    "y": 2160
                },
                {
                    "x": 1195,
                    "y": 2208
                },
                {
                    "x": 444,
                    "y": 2208
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:18px'>4.1 Conditional Reverse Noising Process</p>",
            "id": 61,
            "page": 6,
            "text": "4.1 Conditional Reverse Noising Process"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2242
                },
                {
                    "x": 2104,
                    "y": 2242
                },
                {
                    "x": 2104,
                    "y": 2336
                },
                {
                    "x": 443,
                    "y": 2336
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:16px'>We start with a diffusion model with an unconditional reverse noising process po (xt|xt+1). To<br>condition this on a label y, it suffices to sample each transition2 according to</p>",
            "id": 62,
            "page": 6,
            "text": "We start with a diffusion model with an unconditional reverse noising process po (xt|xt+1). To condition this on a label y, it suffices to sample each transition2 according to"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2460
                },
                {
                    "x": 2105,
                    "y": 2460
                },
                {
                    "x": 2105,
                    "y": 2596
                },
                {
                    "x": 442,
                    "y": 2596
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:16px'>where Z is a normalizing constant (proof in Appendix H). It is typically intractable to sample from<br>this distribution exactly, but Sohl-Dickstein et al. [56] show that it can be approximated as a perturbed<br>Gaussian distribution. Here, we review this derivation.</p>",
            "id": 63,
            "page": 6,
            "text": "where Z is a normalizing constant (proof in Appendix H). It is typically intractable to sample from this distribution exactly, but Sohl-Dickstein   show that it can be approximated as a perturbed Gaussian distribution. Here, we review this derivation."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2622
                },
                {
                    "x": 2106,
                    "y": 2622
                },
                {
                    "x": 2106,
                    "y": 2710
                },
                {
                    "x": 443,
                    "y": 2710
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:18px'>Recall that our diffusion model predicts the previous timestep Xt from timestep Xt+1 using a Gaussian<br>distribution:</p>",
            "id": 64,
            "page": 6,
            "text": "Recall that our diffusion model predicts the previous timestep Xt from timestep Xt+1 using a Gaussian distribution:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2926
                },
                {
                    "x": 2105,
                    "y": 2926
                },
                {
                    "x": 2105,
                    "y": 3013
                },
                {
                    "x": 444,
                    "y": 3013
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:14px'>2We must also sample XT conditioned on y, but a noisy enough diffusion process causes XT to be nearly<br>Gaussian even in the conditional case.</p>",
            "id": 65,
            "page": 6,
            "text": "2We must also sample XT conditioned on y, but a noisy enough diffusion process causes XT to be nearly Gaussian even in the conditional case."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3095
                },
                {
                    "x": 1289,
                    "y": 3095
                },
                {
                    "x": 1289,
                    "y": 3130
                },
                {
                    "x": 1259,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='66' style='font-size:14px'>6</footer>",
            "id": 66,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 309
                },
                {
                    "x": 2106,
                    "y": 309
                },
                {
                    "x": 2106,
                    "y": 401
                },
                {
                    "x": 444,
                    "y": 401
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>Algorithm 1 Classifier guided diffusion sampling, given a diffusion model (��(xt), ��(xt)), classi-<br>fier p⌀(y|xt), and gradient scale s.</p>",
            "id": 67,
            "page": 7,
            "text": "Algorithm 1 Classifier guided diffusion sampling, given a diffusion model (��(xt), ��(xt)), classifier p⌀(y|xt), and gradient scale s."
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 422
                },
                {
                    "x": 1090,
                    "y": 422
                },
                {
                    "x": 1090,
                    "y": 534
                },
                {
                    "x": 485,
                    "y": 534
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:16px'>Input: class label y, gradient scale s<br>XT ← sample from N(0,I)<br>for all from T 1 do</p>",
            "id": 68,
            "page": 7,
            "text": "Input: class label y, gradient scale s XT ← sample from N(0,I) for all from T 1 do"
        },
        {
            "bounding_box": [
                {
                    "x": 604,
                    "y": 516
                },
                {
                    "x": 802,
                    "y": 516
                },
                {
                    "x": 802,
                    "y": 552
                },
                {
                    "x": 604,
                    "y": 552
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:14px'>t to</p>",
            "id": 69,
            "page": 7,
            "text": "t to"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 651
                },
                {
                    "x": 621,
                    "y": 651
                },
                {
                    "x": 621,
                    "y": 690
                },
                {
                    "x": 486,
                    "y": 690
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:16px'>end for</p>",
            "id": 70,
            "page": 7,
            "text": "end for"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 699
                },
                {
                    "x": 671,
                    "y": 699
                },
                {
                    "x": 671,
                    "y": 738
                },
                {
                    "x": 486,
                    "y": 738
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:14px'>return xo</p>",
            "id": 71,
            "page": 7,
            "text": "return xo"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 823
                },
                {
                    "x": 2104,
                    "y": 823
                },
                {
                    "x": 2104,
                    "y": 914
                },
                {
                    "x": 444,
                    "y": 914
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>Algorithm 2 Classifier guided DDIM sampling, given a diffusion model e0(xt), classifier p⌀(y|xt),<br>and gradient scale s.</p>",
            "id": 72,
            "page": 7,
            "text": "Algorithm 2 Classifier guided DDIM sampling, given a diffusion model e0(xt), classifier p⌀(y|xt), and gradient scale s."
        },
        {
            "bounding_box": [
                {
                    "x": 487,
                    "y": 933
                },
                {
                    "x": 1089,
                    "y": 933
                },
                {
                    "x": 1089,
                    "y": 1021
                },
                {
                    "x": 487,
                    "y": 1021
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:16px'>Input: class label y, gradient scale s<br>XT ← sample from N(0,I)</p>",
            "id": 73,
            "page": 7,
            "text": "Input: class label y, gradient scale s XT ← sample from N(0,I)"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 1023
                },
                {
                    "x": 889,
                    "y": 1023
                },
                {
                    "x": 889,
                    "y": 1065
                },
                {
                    "x": 486,
                    "y": 1065
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:14px'>for all t from T to 1 do</p>",
            "id": 74,
            "page": 7,
            "text": "for all t from T to 1 do"
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 1192
                },
                {
                    "x": 673,
                    "y": 1192
                },
                {
                    "x": 673,
                    "y": 1282
                },
                {
                    "x": 485,
                    "y": 1282
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:14px'>end for<br>return xo</p>",
            "id": 75,
            "page": 7,
            "text": "end for return xo"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1398
                },
                {
                    "x": 2106,
                    "y": 1398
                },
                {
                    "x": 2106,
                    "y": 1541
                },
                {
                    "x": 443,
                    "y": 1541
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>We can assume that log⌀ p(y|xt) has low curvature compared to �-1. This assumption is reasonable<br>in the limit ofinfinite diffusion steps, where �� → 0. In this case, we can approximate logp⌀(y|xt)<br>using a Taylor expansion around Xt = H as</p>",
            "id": 76,
            "page": 7,
            "text": "We can assume that log⌀ p(y|xt) has low curvature compared to �-1. This assumption is reasonable in the limit ofinfinite diffusion steps, where �� → 0. In this case, we can approximate logp⌀(y|xt) using a Taylor expansion around Xt = H as"
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 1750
                },
                {
                    "x": 1547,
                    "y": 1750
                },
                {
                    "x": 1547,
                    "y": 1798
                },
                {
                    "x": 447,
                    "y": 1798
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:16px'>Here, g = Vxt logpo(y|xt)|xt=� and C1 is a constant. This gives</p>",
            "id": 77,
            "page": 7,
            "text": "Here, g = Vxt logpo(y|xt)|xt=� and C1 is a constant. This gives"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2247
                },
                {
                    "x": 2106,
                    "y": 2247
                },
                {
                    "x": 2106,
                    "y": 2476
                },
                {
                    "x": 443,
                    "y": 2476
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:16px'>We can safely ignore the constant term C4, since it corresponds to the normalizing coefficient Z in<br>Equation 2. We have thus found that the conditional transition operator can be approximated by a<br>Gaussian similar to the unconditional transition operator, but with its mean shifted by �g. Algorithm<br>1 summaries the corresponding sampling algorithm. We include an optional scale factor s for the<br>gradients, which we describe in more detail in Section 4.3.</p>",
            "id": 78,
            "page": 7,
            "text": "We can safely ignore the constant term C4, since it corresponds to the normalizing coefficient Z in Equation 2. We have thus found that the conditional transition operator can be approximated by a Gaussian similar to the unconditional transition operator, but with its mean shifted by �g. Algorithm 1 summaries the corresponding sampling algorithm. We include an optional scale factor s for the gradients, which we describe in more detail in Section 4.3."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2546
                },
                {
                    "x": 1122,
                    "y": 2546
                },
                {
                    "x": 1122,
                    "y": 2592
                },
                {
                    "x": 444,
                    "y": 2592
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:16px'>4.2 Conditional Sampling for DDIM</p>",
            "id": 79,
            "page": 7,
            "text": "4.2 Conditional Sampling for DDIM"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2635
                },
                {
                    "x": 2107,
                    "y": 2635
                },
                {
                    "x": 2107,
                    "y": 2863
                },
                {
                    "x": 443,
                    "y": 2863
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:16px'>The above derivation for conditional sampling is only valid for the stochastic diffusion sampling<br>process, and cannot be applied to deterministic sampling methods like DDIM [57]. To this end, we<br>use a score-based conditioning trick adapted from Song et al. [60], which leverages the connection<br>between diffusion models and score matching [59]. In particular, if we have a model E0(xt) that<br>predicts the noise added to a sample, then this can be used to derive a score function:</p>",
            "id": 80,
            "page": 7,
            "text": "The above derivation for conditional sampling is only valid for the stochastic diffusion sampling process, and cannot be applied to deterministic sampling methods like DDIM . To this end, we use a score-based conditioning trick adapted from Song  , which leverages the connection between diffusion models and score matching . In particular, if we have a model E0(xt) that predicts the noise added to a sample, then this can be used to derive a score function:"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3091
                },
                {
                    "x": 1289,
                    "y": 3091
                },
                {
                    "x": 1289,
                    "y": 3128
                },
                {
                    "x": 1260,
                    "y": 3128
                }
            ],
            "category": "footer",
            "html": "<footer id='81' style='font-size:14px'>7</footer>",
            "id": 81,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 293
                },
                {
                    "x": 2101,
                    "y": 293
                },
                {
                    "x": 2101,
                    "y": 695
                },
                {
                    "x": 446,
                    "y": 695
                }
            ],
            "category": "figure",
            "html": "<figure><img id='82' alt=\"\" data-coord=\"top-left:(446,293); bottom-right:(2101,695)\" /></figure>",
            "id": 82,
            "page": 8,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 716
                },
                {
                    "x": 2108,
                    "y": 716
                },
                {
                    "x": 2108,
                    "y": 902
                },
                {
                    "x": 441,
                    "y": 902
                }
            ],
            "category": "caption",
            "html": "<br><caption id='83' style='font-size:18px'>Figure 3: Samples from an unconditional diffusion model with classifier guidance to condition<br>on the class \"Pembroke Welsh corgi\" Using classifier scale 1.0 (left; FID: 33.0) does not produce<br>convincing samples in this class, whereas classifier scale 10.0 (right; FID: 12.0) produces much more<br>class-consistent images.</caption>",
            "id": 83,
            "page": 8,
            "text": "Figure 3: Samples from an unconditional diffusion model with classifier guidance to condition on the class \"Pembroke Welsh corgi\" Using classifier scale 1.0 (left; FID: 33.0) does not produce convincing samples in this class, whereas classifier scale 10.0 (right; FID: 12.0) produces much more class-consistent images."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1003
                },
                {
                    "x": 1574,
                    "y": 1003
                },
                {
                    "x": 1574,
                    "y": 1052
                },
                {
                    "x": 445,
                    "y": 1052
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:14px'>We can now substitute this into the score function for p(xt)p(y|xt):</p>",
            "id": 84,
            "page": 8,
            "text": "We can now substitute this into the score function for p(xt)p(y|xt):"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1266
                },
                {
                    "x": 2104,
                    "y": 1266
                },
                {
                    "x": 2104,
                    "y": 1356
                },
                {
                    "x": 443,
                    "y": 1356
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:18px'>Finally, we can define a new epsilon prediction �(xt) which corresponds to the score of the joint<br>distribution:</p>",
            "id": 85,
            "page": 8,
            "text": "Finally, we can define a new epsilon prediction �(xt) which corresponds to the score of the joint distribution:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1462
                },
                {
                    "x": 2104,
                    "y": 1462
                },
                {
                    "x": 2104,
                    "y": 1600
                },
                {
                    "x": 443,
                    "y": 1600
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>We can then use the exact same sampling procedure as used for regular DDIM, but with the modified<br>noise predictions E0(xt) instead of E0(xt). Algorithm 2 summaries the corresponding sampling<br>algorithm.</p>",
            "id": 86,
            "page": 8,
            "text": "We can then use the exact same sampling procedure as used for regular DDIM, but with the modified noise predictions E0(xt) instead of E0(xt). Algorithm 2 summaries the corresponding sampling algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1655
                },
                {
                    "x": 1042,
                    "y": 1655
                },
                {
                    "x": 1042,
                    "y": 1700
                },
                {
                    "x": 443,
                    "y": 1700
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:20px'>4.3 Scaling Classifier Gradients</p>",
            "id": 87,
            "page": 8,
            "text": "4.3 Scaling Classifier Gradients"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1736
                },
                {
                    "x": 2106,
                    "y": 1736
                },
                {
                    "x": 2106,
                    "y": 2012
                },
                {
                    "x": 442,
                    "y": 2012
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:18px'>To apply classifier guidance to a large scale generative task, we train classification models on<br>ImageNet. Our classifier architecture is simply the downsampling trunk of the UNet model with<br>an attention pool [49] at the 8x8 layer to produce the final output. We train these classifiers on the<br>same noising distribution as the corresponding diffusion model, and also add random crops to reduce<br>overfitting. After training, we incorporate the classifier into the sampling process of the diffusion<br>model using Equation 10, as outlined by Algorithm 1.</p>",
            "id": 88,
            "page": 8,
            "text": "To apply classifier guidance to a large scale generative task, we train classification models on ImageNet. Our classifier architecture is simply the downsampling trunk of the UNet model with an attention pool  at the 8x8 layer to produce the final output. We train these classifiers on the same noising distribution as the corresponding diffusion model, and also add random crops to reduce overfitting. After training, we incorporate the classifier into the sampling process of the diffusion model using Equation 10, as outlined by Algorithm 1."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2033
                },
                {
                    "x": 2108,
                    "y": 2033
                },
                {
                    "x": 2108,
                    "y": 2307
                },
                {
                    "x": 442,
                    "y": 2307
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='89' style='font-size:18px'>In initial experiments with unconditional ImageNet models, we found it necessary to scale the<br>classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the<br>classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples,<br>but these samples did not match the intended classes upon visual inspection. Scaling up the classifier<br>gradients remedied this problem, and the class probabilities from the classifier increased to nearly<br>100%. Figure 3 shows an example of this effect.</p>",
            "id": 89,
            "page": 8,
            "text": "In initial experiments with unconditional ImageNet models, we found it necessary to scale the classifier gradients by a constant factor larger than 1. When using a scale of 1, we observed that the classifier assigned reasonable probabilities (around 50%) to the desired classes for the final samples, but these samples did not match the intended classes upon visual inspection. Scaling up the classifier gradients remedied this problem, and the class probabilities from the classifier increased to nearly 100%. Figure 3 shows an example of this effect."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2326
                },
                {
                    "x": 2107,
                    "y": 2326
                },
                {
                    "x": 2107,
                    "y": 2604
                },
                {
                    "x": 441,
                    "y": 2604
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:18px'>To understand the effect of scaling classifier gradients, note that s · Vx log p(y|x) = Vx log �p(y|x)s,<br>where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded<br>in a re-normalized classifier distribution proportional to p(y|x)s. When s > 1, this distribution<br>becomes sharper than p(yx), since larger values are amplified by the exponent. In other words, using<br>a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for<br>producing higher fidelity (but less diverse) samples.</p>",
            "id": 90,
            "page": 8,
            "text": "To understand the effect of scaling classifier gradients, note that s · Vx log p(y|x) = Vx log �p(y|x)s, where Z is an arbitrary constant. As a result, the conditioning process is still theoretically grounded in a re-normalized classifier distribution proportional to p(y|x)s. When s > 1, this distribution becomes sharper than p(yx), since larger values are amplified by the exponent. In other words, using a larger gradient scale focuses more on the modes of the classifier, which is potentially desirable for producing higher fidelity (but less diverse) samples."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2624
                },
                {
                    "x": 2107,
                    "y": 2624
                },
                {
                    "x": 2107,
                    "y": 2898
                },
                {
                    "x": 442,
                    "y": 2898
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling<br>p(x). It is also possible to train conditional diffusion models, p(xy), and use classifier guidance in<br>the exact same way. Table 4 shows that the sample quality of both unconditional and conditional<br>models can be greatly improved by classifier guidance. We see that, with a high enough scale, the<br>guided unconditional model can get quite close to the FID of an unguided conditional model, although<br>training directly with the class labels still helps. Guiding a conditional model further improves FID.</p>",
            "id": 91,
            "page": 8,
            "text": "In the above derivations, we assumed that the underlying diffusion model was unconditional, modeling p(x). It is also possible to train conditional diffusion models, p(xy), and use classifier guidance in the exact same way. Table 4 shows that the sample quality of both unconditional and conditional models can be greatly improved by classifier guidance. We see that, with a high enough scale, the guided unconditional model can get quite close to the FID of an unguided conditional model, although training directly with the class labels still helps. Guiding a conditional model further improves FID."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2920
                },
                {
                    "x": 2105,
                    "y": 2920
                },
                {
                    "x": 2105,
                    "y": 3014
                },
                {
                    "x": 442,
                    "y": 3014
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:16px'>Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing<br>a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with</p>",
            "id": 92,
            "page": 8,
            "text": "Table 4 also shows that classifier guidance improves precision at the cost of recall, thus introducing a trade-off in sample fidelity versus diversity. We explicitly evaluate how this trade-off varies with"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3095
                },
                {
                    "x": 1288,
                    "y": 3095
                },
                {
                    "x": 1288,
                    "y": 3130
                },
                {
                    "x": 1260,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='93' style='font-size:14px'>8</footer>",
            "id": 93,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 617,
                    "y": 295
                },
                {
                    "x": 1932,
                    "y": 295
                },
                {
                    "x": 1932,
                    "y": 642
                },
                {
                    "x": 617,
                    "y": 642
                }
            ],
            "category": "table",
            "html": "<table id='94' style='font-size:16px'><tr><td>Conditional</td><td>Guidance</td><td>Scale</td><td>FID</td><td>sFID</td><td>IS</td><td>Precision</td><td>Recall</td></tr><tr><td>X</td><td></td><td></td><td>26.21</td><td>6.35</td><td>39.70</td><td>0.61</td><td>0.63</td></tr><tr><td>X</td><td></td><td>1.0</td><td>33.03</td><td>6.99</td><td>32.92</td><td>0.56</td><td>0.65</td></tr><tr><td></td><td></td><td>10.0</td><td>12.00</td><td>10.40</td><td>95.41</td><td>0.76</td><td>0.44</td></tr><tr><td></td><td></td><td></td><td>10.94</td><td>6.02</td><td>100.98</td><td>0.69</td><td>0.63</td></tr><tr><td></td><td></td><td>1.0</td><td>4.59</td><td>5.25</td><td>186.70</td><td>0.82</td><td>0.52</td></tr><tr><td></td><td></td><td>10.0</td><td>9.11</td><td>10.93</td><td>283.92</td><td>0.88</td><td>0.32</td></tr></table>",
            "id": 94,
            "page": 9,
            "text": "Conditional Guidance Scale FID sFID IS Precision Recall  X   26.21 6.35 39.70 0.61 0.63  X  1.0 33.03 6.99 32.92 0.56 0.65    10.0 12.00 10.40 95.41 0.76 0.44     10.94 6.02 100.98 0.69 0.63    1.0 4.59 5.25 186.70 0.82 0.52    10.0 9.11 10.93 283.92 0.88"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 657
                },
                {
                    "x": 2107,
                    "y": 657
                },
                {
                    "x": 2107,
                    "y": 750
                },
                {
                    "x": 442,
                    "y": 750
                }
            ],
            "category": "caption",
            "html": "<br><caption id='95' style='font-size:18px'>Table 4: Effect of classifier guidance on sample quality. Both conditional and unconditional models<br>were trained for 2M iterations on ImageNet 256x256 with batch size 256.</caption>",
            "id": 95,
            "page": 9,
            "text": "Table 4: Effect of classifier guidance on sample quality. Both conditional and unconditional models were trained for 2M iterations on ImageNet 256x256 with batch size 256."
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 809
                },
                {
                    "x": 2079,
                    "y": 809
                },
                {
                    "x": 2079,
                    "y": 1241
                },
                {
                    "x": 466,
                    "y": 1241
                }
            ],
            "category": "figure",
            "html": "<figure><img id='96' style='font-size:14px' alt=\"FID sFID IS precision recall\n16 300\n0.9\n14\n0.8\n250\n12\n0.7\n10\n200\n0.6\n8\n0.5\n150\n6\n0.4\n4\n100 0.3\n0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10\ngradient scale gradient scale gradient scale\" data-coord=\"top-left:(466,809); bottom-right:(2079,1241)\" /></figure>",
            "id": 96,
            "page": 9,
            "text": "FID sFID IS precision recall 16 300 0.9 14 0.8 250 12 0.7 10 200 0.6 8 0.5 150 6 0.4 4 100 0.3 0 2 4 6 8 10 0 2 4 6 8 10 0 2 4 6 8 10 gradient scale gradient scale gradient scale"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1287
                },
                {
                    "x": 2105,
                    "y": 1287
                },
                {
                    "x": 2105,
                    "y": 1381
                },
                {
                    "x": 442,
                    "y": 1381
                }
            ],
            "category": "caption",
            "html": "<caption id='97' style='font-size:18px'>Figure 4: Change in sample quality as we vary scale of the classifier gradients for a class-conditional<br>ImageNet 128x 128 model.</caption>",
            "id": 97,
            "page": 9,
            "text": "Figure 4: Change in sample quality as we vary scale of the classifier gradients for a class-conditional ImageNet 128x 128 model."
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 1472
                },
                {
                    "x": 2082,
                    "y": 1472
                },
                {
                    "x": 2082,
                    "y": 2006
                },
                {
                    "x": 469,
                    "y": 2006
                }
            ],
            "category": "figure",
            "html": "<figure><img id='98' style='font-size:14px' alt=\"30\nBigGAN-deep BigGAN-deep\n0.6 Classifier guidance (ours) Classifier guidance (ours)\n25\n0.5\n20\n0.4\nRecall\nFID\n0.3 15\n0.2\n10\n0.1\n5\n0.0\n0.70 0.75 0.80 0.85 0.90 0.95 100 125 150 175 200 225 250 275\nPrecision IS\" data-coord=\"top-left:(469,1472); bottom-right:(2082,2006)\" /></figure>",
            "id": 98,
            "page": 9,
            "text": "30 BigGAN-deep BigGAN-deep 0.6 Classifier guidance (ours) Classifier guidance (ours) 25 0.5 20 0.4 Recall FID 0.3 15 0.2 10 0.1 5 0.0 0.70 0.75 0.80 0.85 0.90 0.95 100 125 150 175 200 225 250 275 Precision IS"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2042
                },
                {
                    "x": 2109,
                    "y": 2042
                },
                {
                    "x": 2109,
                    "y": 2186
                },
                {
                    "x": 440,
                    "y": 2186
                }
            ],
            "category": "caption",
            "html": "<caption id='99' style='font-size:20px'>Figure 5: Trade-offs when varying truncation for BigGAN-deep and gradient scale for classifier<br>guidance. Models are evaluated on ImageNet 128x 128. The BigGAN-deep results were produced<br>using the TFHub model [12] at truncation levels [0.1, 0.2, 0.3, ..., 1.0].</caption>",
            "id": 99,
            "page": 9,
            "text": "Figure 5: Trade-offs when varying truncation for BigGAN-deep and gradient scale for classifier guidance. Models are evaluated on ImageNet 128x 128. The BigGAN-deep results were produced using the TFHub model  at truncation levels [0.1, 0.2, 0.3, ..., 1.0]."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2269
                },
                {
                    "x": 2109,
                    "y": 2269
                },
                {
                    "x": 2109,
                    "y": 2594
                },
                {
                    "x": 441,
                    "y": 2594
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:18px'>the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off<br>recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID<br>depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also<br>compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier<br>guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear<br>cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up<br>until a certain precision threshold, after which point it cannot achieve better precision.</p>",
            "id": 100,
            "page": 9,
            "text": "the gradient scale in Figure 4. We see that scaling the gradients beyond 1.0 smoothly trades off recall (a measure of diversity) for higher precision and IS (measures of fidelity). Since FID and sFID depend on both diversity and fidelity, their best values are obtained at an intermediate point. We also compare our guidance with the truncation trick from BigGAN in Figure 5. We find that classifier guidance is strictly better than BigGAN-deep when trading off FID for Inception Score. Less clear cut is the precision/recall trade-off, which shows that classifier guidance is only a better choice up until a certain precision threshold, after which point it cannot achieve better precision."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2699
                },
                {
                    "x": 686,
                    "y": 2699
                },
                {
                    "x": 686,
                    "y": 2753
                },
                {
                    "x": 442,
                    "y": 2753
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:22px'>5 Results</p>",
            "id": 101,
            "page": 9,
            "text": "5 Results"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2827
                },
                {
                    "x": 2110,
                    "y": 2827
                },
                {
                    "x": 2110,
                    "y": 3012
                },
                {
                    "x": 441,
                    "y": 3012
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:18px'>To evaluate our improved model architecture on unconditional image generation, we train separate<br>diffusion models on three LSUN [71] classes: bedroom, horse, and cat. To evaluate classifier<br>guidance, we train conditional diffusion models on the ImageNet [52] dataset at 128x 128, 256x256,<br>and 512x512 resolution.</p>",
            "id": 102,
            "page": 9,
            "text": "To evaluate our improved model architecture on unconditional image generation, we train separate diffusion models on three LSUN  classes: bedroom, horse, and cat. To evaluate classifier guidance, we train conditional diffusion models on the ImageNet  dataset at 128x 128, 256x256, and 512x512 resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3093
                },
                {
                    "x": 1289,
                    "y": 3093
                },
                {
                    "x": 1289,
                    "y": 3128
                },
                {
                    "x": 1259,
                    "y": 3128
                }
            ],
            "category": "footer",
            "html": "<footer id='103' style='font-size:16px'>9</footer>",
            "id": 103,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 437,
                    "y": 284
                },
                {
                    "x": 2073,
                    "y": 284
                },
                {
                    "x": 2073,
                    "y": 1385
                },
                {
                    "x": 437,
                    "y": 1385
                }
            ],
            "category": "table",
            "html": "<table id='104' style='font-size:14px'><tr><td>Model</td><td>FID</td><td>sFID</td><td>Prec</td><td>Rec</td><td>Model</td><td>FID</td><td>sFID</td><td>Prec</td><td>Rec</td></tr><tr><td colspan=\"5\">LSUN Bedrooms 256x256</td><td>ImageNet 128x 128</td><td></td><td></td><td></td><td></td></tr><tr><td>DCTransformer† [42]</td><td>6.40</td><td>6.66</td><td>0.44</td><td>0.56</td><td>BigGAN-deep [5]</td><td>6.02</td><td>7.18</td><td>0.86</td><td>0.35</td></tr><tr><td>DDPM [25]</td><td>4.89</td><td>9.07</td><td>0.60</td><td>0.45</td><td>LOGAN� [68]</td><td>3.36</td><td></td><td></td><td></td></tr><tr><td>IDDPM [43]</td><td>4.24</td><td>8.21</td><td>0.62</td><td>0.46</td><td>ADM</td><td>5.91</td><td>5.09</td><td>0.70</td><td>0.65</td></tr><tr><td>StyleGAN [27]</td><td>2.35</td><td>6.62</td><td>0.59</td><td>0.48</td><td>ADM-G (25 steps)</td><td>5.98</td><td>7.04</td><td>0.78</td><td>0.51</td></tr><tr><td>ADM (dropout)</td><td>1.90</td><td>5.59</td><td>0.66</td><td>0.51</td><td>ADM-G</td><td>2.97</td><td>5.09</td><td>0.78</td><td>0.59</td></tr><tr><td colspan=\"5\">LSUN Horses 256x256</td><td>ImageNet 256x256</td><td></td><td></td><td></td><td></td></tr><tr><td>StyleGAN2 [28]</td><td>3.84</td><td>6.46</td><td>0.63</td><td>0.48</td><td>DCTransformer† [42]</td><td>36.51</td><td>8.24</td><td>0.36</td><td>0.67</td></tr><tr><td>ADM</td><td>2.95</td><td>5.94</td><td>0.69</td><td>0.55</td><td>VQ-VAE-2+± [51]</td><td>31.11</td><td>17.38</td><td>0.36</td><td>0.57</td></tr><tr><td>ADM (dropout)</td><td>2.57</td><td>6.81</td><td>0.71</td><td>0.55</td><td>IDDPM‡ [43]</td><td>12.26</td><td>5.42</td><td>0.70</td><td>0.62</td></tr><tr><td rowspan=\"2\" colspan=\"5\">LSUN Cats 256x256</td><td>SR3+‡ [53]</td><td>11.30</td><td></td><td></td><td></td></tr><tr><td>BigGAN-deep [5]</td><td>6.95</td><td>7.36</td><td>0.87</td><td>0.28</td></tr><tr><td>DDPM [25]</td><td>17.1</td><td>12.4</td><td>0.53</td><td>0.48</td><td>ADM</td><td>10.94</td><td>6.02</td><td>0.69</td><td>0.63</td></tr><tr><td>StyleGAN2 [28]</td><td>7.25</td><td>6.33</td><td>0.58</td><td>0.43</td><td>ADM-G (25 steps)</td><td>5.44</td><td>5.32</td><td>0.81</td><td>0.49</td></tr><tr><td>ADM (dropout)</td><td>5.57</td><td>6.69</td><td>0.63</td><td>0.52</td><td>ADM-G</td><td>4.59</td><td>5.25</td><td>0.82</td><td>0.52</td></tr><tr><td colspan=\"5\">ImageNet 64x 64</td><td>ImageNet 512x512</td><td></td><td></td><td></td><td></td></tr><tr><td>BigGAN-deep* [5]</td><td>4.06</td><td>3.96</td><td>0.79</td><td>0.48</td><td>BigGAN-deep [5]</td><td>8.43</td><td>8.13</td><td>0.88</td><td>0.29</td></tr><tr><td>IDDPM [43]</td><td>2.92</td><td>3.79</td><td>0.74</td><td>0.62</td><td>ADM</td><td>23.24</td><td>10.19</td><td>0.73</td><td>0.60</td></tr><tr><td>ADM</td><td>2.61</td><td>3.77</td><td>0.73</td><td>0.63</td><td>ADM-G (25 steps)</td><td>8.41</td><td>9.67</td><td>0.83</td><td>0.47</td></tr><tr><td>ADM (dropout)</td><td>2.07</td><td>4.29</td><td>0.74</td><td>0.63</td><td>ADM-G</td><td>7.72</td><td>6.57</td><td>0.87</td><td>0.42</td></tr></table>",
            "id": 104,
            "page": 10,
            "text": "Model FID sFID Prec Rec Model FID sFID Prec Rec  LSUN Bedrooms 256x256 ImageNet 128x 128      DCTransformer†  6.40 6.66 0.44 0.56 BigGAN-deep  6.02 7.18 0.86 0.35  DDPM  4.89 9.07 0.60 0.45 LOGAN�  3.36     IDDPM  4.24 8.21 0.62 0.46 ADM 5.91 5.09 0.70 0.65  StyleGAN  2.35 6.62 0.59 0.48 ADM-G (25 steps) 5.98 7.04 0.78 0.51  ADM (dropout) 1.90 5.59 0.66 0.51 ADM-G 2.97 5.09 0.78 0.59  LSUN Horses 256x256 ImageNet 256x256      StyleGAN2  3.84 6.46 0.63 0.48 DCTransformer†  36.51 8.24 0.36 0.67  ADM 2.95 5.94 0.69 0.55 VQ-VAE-2+±  31.11 17.38 0.36 0.57  ADM (dropout) 2.57 6.81 0.71 0.55 IDDPM‡  12.26 5.42 0.70 0.62  LSUN Cats 256x256 SR3+‡  11.30     BigGAN-deep  6.95 7.36 0.87 0.28  DDPM  17.1 12.4 0.53 0.48 ADM 10.94 6.02 0.69 0.63  StyleGAN2  7.25 6.33 0.58 0.43 ADM-G (25 steps) 5.44 5.32 0.81 0.49  ADM (dropout) 5.57 6.69 0.63 0.52 ADM-G 4.59 5.25 0.82 0.52  ImageNet 64x 64 ImageNet 512x512      BigGAN-deep*  4.06 3.96 0.79 0.48 BigGAN-deep  8.43 8.13 0.88 0.29  IDDPM  2.92 3.79 0.74 0.62 ADM 23.24 10.19 0.73 0.60  ADM 2.61 3.77 0.73 0.63 ADM-G (25 steps) 8.41 9.67 0.83 0.47  ADM (dropout) 2.07 4.29 0.74 0.63 ADM-G 7.72 6.57 0.87"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1396
                },
                {
                    "x": 2107,
                    "y": 1396
                },
                {
                    "x": 2107,
                    "y": 1673
                },
                {
                    "x": 441,
                    "y": 1673
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:16px'>Table 5: Sample quality comparison with state-of-the-art generative models for each task. ADM<br>refers to our ablated diffusion model, and ADM-G additionally uses classifier guidance. LSUN<br>diffusion models are sampled using 1000 steps (see Appendix J). ImageNet diffusion models are<br>sampled using 250 steps, except when we use the DDIM sampler with 25 steps. *No BigGAN-deep<br>model was available at this resolution, SO we trained our own. �Values are taken from a previous<br>paper, due to lack of public models or samples. ‡Results use two-resolution stacks.</p>",
            "id": 105,
            "page": 10,
            "text": "Table 5: Sample quality comparison with state-of-the-art generative models for each task. ADM refers to our ablated diffusion model, and ADM-G additionally uses classifier guidance. LSUN diffusion models are sampled using 1000 steps (see Appendix J). ImageNet diffusion models are sampled using 250 steps, except when we use the DDIM sampler with 25 steps. *No BigGAN-deep model was available at this resolution, SO we trained our own. �Values are taken from a previous paper, due to lack of public models or samples. ‡Results use two-resolution stacks."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1787
                },
                {
                    "x": 1118,
                    "y": 1787
                },
                {
                    "x": 1118,
                    "y": 1834
                },
                {
                    "x": 445,
                    "y": 1834
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:22px'>5.1 State-of-the-art Image Synthesis</p>",
            "id": 106,
            "page": 10,
            "text": "5.1 State-of-the-art Image Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1870
                },
                {
                    "x": 2107,
                    "y": 1870
                },
                {
                    "x": 2107,
                    "y": 2146
                },
                {
                    "x": 442,
                    "y": 2146
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and<br>the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art<br>image generation on LSUN and ImageNet 64 x64. For higher resolution ImageNet, we observe that<br>classifier guidance allows our models to substantially outperform the best GANs. These models<br>obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as<br>measured by recall, and can even do SO using only 25 diffusion steps.</p>",
            "id": 107,
            "page": 10,
            "text": "Table 5 summarizes our results. Our diffusion models can obtain the best FID on each task, and the best sFID on all but one task. With the improved architecture, we already obtain state-of-the-art image generation on LSUN and ImageNet 64 x64. For higher resolution ImageNet, we observe that classifier guidance allows our models to substantially outperform the best GANs. These models obtain perceptual quality similar to GANs, while maintaining a higher coverage of the distribution as measured by recall, and can even do SO using only 25 diffusion steps."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2166
                },
                {
                    "x": 2107,
                    "y": 2166
                },
                {
                    "x": 2107,
                    "y": 2399
                },
                {
                    "x": 440,
                    "y": 2399
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:16px'>Figure 6 compares random samples from the best BigGAN-deep model to our best diffusion model.<br>While the samples are of similar perceptual quality, the diffusion model contains more modes than the<br>GAN, such as zoomed ostrich heads, single flamingos, different orientations of cheeseburgers, and a<br>tinca fish with no human holding it. We also check our generated samples for nearest neighbors in<br>the Inception- V3 feature space in Appendix C, and we show additional samples in Appendices K-M.</p>",
            "id": 108,
            "page": 10,
            "text": "Figure 6 compares random samples from the best BigGAN-deep model to our best diffusion model. While the samples are of similar perceptual quality, the diffusion model contains more modes than the GAN, such as zoomed ostrich heads, single flamingos, different orientations of cheeseburgers, and a tinca fish with no human holding it. We also check our generated samples for nearest neighbors in the Inception- V3 feature space in Appendix C, and we show additional samples in Appendices K-M."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2449
                },
                {
                    "x": 1037,
                    "y": 2449
                },
                {
                    "x": 1037,
                    "y": 2497
                },
                {
                    "x": 444,
                    "y": 2497
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:20px'>5.2 Comparison to Upsampling</p>",
            "id": 109,
            "page": 10,
            "text": "5.2 Comparison to Upsampling"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2531
                },
                {
                    "x": 2107,
                    "y": 2531
                },
                {
                    "x": 2107,
                    "y": 2899
                },
                {
                    "x": 442,
                    "y": 2899
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:16px'>We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal [43] and<br>Saharia et al. [53] train two-stage diffusion models by combining a low-resolution diffusion model<br>with a corresponding upsampling diffusion model. In this approach, the upsampling model is<br>trained to upsample images from the training set, and conditions on low-resolution images that are<br>concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During<br>sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned<br>on this sample. This greatly improves FID on ImageNet 256 x256, but does not reach the same<br>performance as state-of-the-art models like BigGAN-deep [43, 53], as seen in Table 5.</p>",
            "id": 110,
            "page": 10,
            "text": "We also compare guidance to using a two-stage upsampling stack. Nichol and Dhariwal  and Saharia   train two-stage diffusion models by combining a low-resolution diffusion model with a corresponding upsampling diffusion model. In this approach, the upsampling model is trained to upsample images from the training set, and conditions on low-resolution images that are concatenated channel-wise to the model input using a simple interpolation (e.g. bilinear). During sampling, the low-resolution model produces a sample, and then the upsampling model is conditioned on this sample. This greatly improves FID on ImageNet 256 x256, but does not reach the same performance as state-of-the-art models like BigGAN-deep , as seen in Table 5."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2921
                },
                {
                    "x": 2107,
                    "y": 2921
                },
                {
                    "x": 2107,
                    "y": 3013
                },
                {
                    "x": 442,
                    "y": 3013
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:18px'>In Table 6, we show that guidance and upsampling improve sample quality along different axes.<br>While upsampling improves precision while keeping a high recall, guidance provides a knob to trade</p>",
            "id": 111,
            "page": 10,
            "text": "In Table 6, we show that guidance and upsampling improve sample quality along different axes. While upsampling improves precision while keeping a high recall, guidance provides a knob to trade"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='112' style='font-size:14px'>10</footer>",
            "id": 112,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 456,
                    "y": 293
                },
                {
                    "x": 2097,
                    "y": 293
                },
                {
                    "x": 2097,
                    "y": 1355
                },
                {
                    "x": 456,
                    "y": 1355
                }
            ],
            "category": "figure",
            "html": "<figure><img id='113' alt=\"\" data-coord=\"top-left:(456,293); bottom-right:(2097,1355)\" /></figure>",
            "id": 113,
            "page": 11,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1378
                },
                {
                    "x": 2107,
                    "y": 1378
                },
                {
                    "x": 2107,
                    "y": 1471
                },
                {
                    "x": 440,
                    "y": 1471
                }
            ],
            "category": "caption",
            "html": "<br><caption id='114' style='font-size:20px'>Figure 6: Samples from BigGAN-deep with truncation 1.0 (FID 6.95, left) VS samples from our<br>diffusion model with guidance (FID 4.59, middle) and samples from the training set (right).</caption>",
            "id": 114,
            "page": 11,
            "text": "Figure 6: Samples from BigGAN-deep with truncation 1.0 (FID 6.95, left) VS samples from our diffusion model with guidance (FID 4.59, middle) and samples from the training set (right)."
        },
        {
            "bounding_box": [
                {
                    "x": 583,
                    "y": 1481
                },
                {
                    "x": 1977,
                    "y": 1481
                },
                {
                    "x": 1977,
                    "y": 2161
                },
                {
                    "x": 583,
                    "y": 2161
                }
            ],
            "category": "table",
            "html": "<br><table id='115' style='font-size:14px'><tr><td>Model</td><td>Sbase</td><td>Supsample</td><td>FID</td><td>sFID</td><td>IS</td><td>Precision</td><td>Recall</td></tr><tr><td colspan=\"8\">ImageNet 256x256</td></tr><tr><td>ADM</td><td>250</td><td></td><td>10.94</td><td>6.02</td><td>100.98</td><td>0.69</td><td>0.63</td></tr><tr><td>ADM-U</td><td>250</td><td>250</td><td>7.49</td><td>5.13</td><td>127.49</td><td>0.72</td><td>0.63</td></tr><tr><td>ADM-G</td><td>250</td><td></td><td>4.59</td><td>5.25</td><td>186.70</td><td>0.82</td><td>0.52</td></tr><tr><td>ADM-G, ADM-U</td><td>250</td><td>250</td><td>3.94</td><td>6.14</td><td>215.84</td><td>0.83</td><td>0.53</td></tr><tr><td colspan=\"8\">ImageNet 512x512</td></tr><tr><td>ADM</td><td>250</td><td></td><td>23.24</td><td>10.19</td><td>58.06</td><td>0.73</td><td>0.60</td></tr><tr><td>ADM-U</td><td>250</td><td>250</td><td>9.96</td><td>5.62</td><td>121.78</td><td>0.75</td><td>0.64</td></tr><tr><td>ADM-G</td><td>250</td><td></td><td>7.72</td><td>6.57</td><td>172.71</td><td>0.87</td><td>0.42</td></tr><tr><td>ADM-G, ADM-U</td><td>25</td><td>25</td><td>5.96</td><td>12.10</td><td>187.87</td><td>0.81</td><td>0.54</td></tr><tr><td>ADM-G, ADM-U</td><td>250</td><td>25</td><td>4.11</td><td>9.57</td><td>219.29</td><td>0.83</td><td>0.55</td></tr><tr><td>ADM-G, ADM-U</td><td>250</td><td>250</td><td>3.85</td><td>5.86</td><td>221.72</td><td>0.84</td><td>0.53</td></tr></table>",
            "id": 115,
            "page": 11,
            "text": "Model Sbase Supsample FID sFID IS Precision Recall  ImageNet 256x256  ADM 250  10.94 6.02 100.98 0.69 0.63  ADM-U 250 250 7.49 5.13 127.49 0.72 0.63  ADM-G 250  4.59 5.25 186.70 0.82 0.52  ADM-G, ADM-U 250 250 3.94 6.14 215.84 0.83 0.53  ImageNet 512x512  ADM 250  23.24 10.19 58.06 0.73 0.60  ADM-U 250 250 9.96 5.62 121.78 0.75 0.64  ADM-G 250  7.72 6.57 172.71 0.87 0.42  ADM-G, ADM-U 25 25 5.96 12.10 187.87 0.81 0.54  ADM-G, ADM-U 250 25 4.11 9.57 219.29 0.83 0.55  ADM-G, ADM-U 250 250 3.85 5.86 221.72 0.84"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2174
                },
                {
                    "x": 2110,
                    "y": 2174
                },
                {
                    "x": 2110,
                    "y": 2405
                },
                {
                    "x": 440,
                    "y": 2405
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:16px'>Table 6: Comparing our single, upsampling and classifier guided models. For upsampling, we use<br>the upsampling stack from Nichol and Dhariwal [43] combined with our architecture improvements,<br>which we refer to as ADM-U. The base resolution for the two-stage upsampling models is 64 and<br>128 for the 256 and 512 models, respectively. When combining classifier guidance with upsampling,<br>we only guide the lower resolution model.</p>",
            "id": 116,
            "page": 11,
            "text": "Table 6: Comparing our single, upsampling and classifier guided models. For upsampling, we use the upsampling stack from Nichol and Dhariwal  combined with our architecture improvements, which we refer to as ADM-U. The base resolution for the two-stage upsampling models is 64 and 128 for the 256 and 512 models, respectively. When combining classifier guidance with upsampling, we only guide the lower resolution model."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2469
                },
                {
                    "x": 2106,
                    "y": 2469
                },
                {
                    "x": 2106,
                    "y": 2607
                },
                {
                    "x": 441,
                    "y": 2607
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>off diversity for much higher precision. We achieve the best FIDs by using guidance at a lower<br>resolution before upsampling to a higher resolution, indicating that these approaches complement<br>one another.</p>",
            "id": 117,
            "page": 11,
            "text": "off diversity for much higher precision. We achieve the best FIDs by using guidance at a lower resolution before upsampling to a higher resolution, indicating that these approaches complement one another."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2677
                },
                {
                    "x": 827,
                    "y": 2677
                },
                {
                    "x": 827,
                    "y": 2731
                },
                {
                    "x": 442,
                    "y": 2731
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:22px'>6 Related Work</p>",
            "id": 118,
            "page": 11,
            "text": "6 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2782
                },
                {
                    "x": 2110,
                    "y": 2782
                },
                {
                    "x": 2110,
                    "y": 3014
                },
                {
                    "x": 441,
                    "y": 3014
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>Score based generative models were introduced by Song and Ermon [59] as a way of modeling a<br>data distribution using its gradients, and then sampling using Langevin dynamics [67]. Ho et al. [25]<br>found a connection between this method and diffusion models [56], and achieved excellent sample<br>quality by leveraging this connection. After this breakthrough work, many works followed up with<br>more promising results: Kong et al. [30] and Chen et al. [8] demonstrated that diffusion models</p>",
            "id": 119,
            "page": 11,
            "text": "Score based generative models were introduced by Song and Ermon  as a way of modeling a data distribution using its gradients, and then sampling using Langevin dynamics . Ho   found a connection between this method and diffusion models , and achieved excellent sample quality by leveraging this connection. After this breakthrough work, many works followed up with more promising results: Kong   and Chen   demonstrated that diffusion models"
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3093
                },
                {
                    "x": 1296,
                    "y": 3093
                },
                {
                    "x": 1296,
                    "y": 3131
                },
                {
                    "x": 1251,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='120' style='font-size:14px'>11</footer>",
            "id": 120,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 307
                },
                {
                    "x": 2107,
                    "y": 307
                },
                {
                    "x": 2107,
                    "y": 717
                },
                {
                    "x": 442,
                    "y": 717
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:20px'>work well for audio; Jolicoeur-Martineau et al. [26] found that a GAN-like setup could improve<br>samples from these models; Song et al. [60] explored ways to leverage techniques from stochastic<br>differential equations to improve the sample quality obtained by score-based models; Song et al. [57]<br>and Nichol and Dhariwal [43] proposed methods to improve sampling speed; Nichol and Dhariwal<br>[43] and Saharia et al. [53] demonstrated promising results on the difficult ImageNet generation task<br>using upsampling diffusion models. Also related to diffusion models, and following the work of<br>Sohl-Dickstein et al. [56], Goyal et al. [21] described a technique for learning a model with learned<br>iterative generation steps, and found that it could achieve good image samples when trained with a<br>likelihood objective.</p>",
            "id": 121,
            "page": 12,
            "text": "work well for audio; Jolicoeur-Martineau   found that a GAN-like setup could improve samples from these models; Song   explored ways to leverage techniques from stochastic differential equations to improve the sample quality obtained by score-based models; Song   and Nichol and Dhariwal  proposed methods to improve sampling speed; Nichol and Dhariwal  and Saharia   demonstrated promising results on the difficult ImageNet generation task using upsampling diffusion models. Also related to diffusion models, and following the work of Sohl-Dickstein  , Goyal   described a technique for learning a model with learned iterative generation steps, and found that it could achieve good image samples when trained with a likelihood objective."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 738
                },
                {
                    "x": 2108,
                    "y": 738
                },
                {
                    "x": 2108,
                    "y": 1103
                },
                {
                    "x": 442,
                    "y": 1103
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>One missing element from previous work on diffusion models is a way to trade off diversity for fidelity.<br>Other generative techniques provide natural levers for this trade-off. Brock et al. [5] introduced the<br>truncation trick for GANs, wherein the latent vector is sampled from a truncated normal distribution.<br>They found that increasing truncation naturally led to a decrease in diversity but an increase in fidelity.<br>More recently, Razavi et al. [51] proposed to use classifier rejection sampling to filter out bad samples<br>from an autoregressive likelihood-based model, and found that this technique improved FID. Most<br>likelihood-based models also allow for low-temperature sampling [1], which provides a natural way<br>to emphasize modes of the data distribution (see Appendix G).</p>",
            "id": 122,
            "page": 12,
            "text": "One missing element from previous work on diffusion models is a way to trade off diversity for fidelity. Other generative techniques provide natural levers for this trade-off. Brock   introduced the truncation trick for GANs, wherein the latent vector is sampled from a truncated normal distribution. They found that increasing truncation naturally led to a decrease in diversity but an increase in fidelity. More recently, Razavi   proposed to use classifier rejection sampling to filter out bad samples from an autoregressive likelihood-based model, and found that this technique improved FID. Most likelihood-based models also allow for low-temperature sampling , which provides a natural way to emphasize modes of the data distribution (see Appendix G)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1125
                },
                {
                    "x": 2107,
                    "y": 1125
                },
                {
                    "x": 2107,
                    "y": 1718
                },
                {
                    "x": 441,
                    "y": 1718
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:20px'>Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE<br>[65] and VQ-VAE-2 [51] are autoregressive models trained on top of quantized latent codes, greatly<br>reducing the computational resources required to train these models on large images. These models<br>produce diverse and high quality images, but still fall short of GANs without expensive rejection<br>sampling and special metrics to compensate for blurriness. DCTransformer [42] is a related method<br>which relies on a more intelligent compression scheme. VAEs are another promising class of<br>likelihood-based models, and recent methods such as NVAE [63] and VDVAE [9] have successfully<br>been applied to difficult image generation domains. Energy-based models are another class of<br>likelihood-based models with a rich history [1, 10, 24]. Sampling from the EBM distribution is<br>challenging, and Xie et al. [70] demonstrate that Langevin dynamics can be used to sample coherent<br>images from these models. Du and Mordatch [15] further improve upon this approach, obtaining<br>high quality images. More recently, Gao et al. [18] incorporate diffusion steps into an energy-based<br>model, and find that doing SO improves image samples from these models.</p>",
            "id": 123,
            "page": 12,
            "text": "Other likelihood-based models have been shown to produce high-fidelity image samples. VQ-VAE  and VQ-VAE-2  are autoregressive models trained on top of quantized latent codes, greatly reducing the computational resources required to train these models on large images. These models produce diverse and high quality images, but still fall short of GANs without expensive rejection sampling and special metrics to compensate for blurriness. DCTransformer  is a related method which relies on a more intelligent compression scheme. VAEs are another promising class of likelihood-based models, and recent methods such as NVAE  and VDVAE  have successfully been applied to difficult image generation domains. Energy-based models are another class of likelihood-based models with a rich history . Sampling from the EBM distribution is challenging, and Xie   demonstrate that Langevin dynamics can be used to sample coherent images from these models. Du and Mordatch  further improve upon this approach, obtaining high quality images. More recently, Gao   incorporate diffusion steps into an energy-based model, and find that doing SO improves image samples from these models."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1739
                },
                {
                    "x": 2107,
                    "y": 1739
                },
                {
                    "x": 2107,
                    "y": 2060
                },
                {
                    "x": 442,
                    "y": 2060
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='124' style='font-size:16px'>Other works have controlled generative models with a pre-trained classifier. For example, an emerging<br>body of work [17, 47, 2] aims to optimize GAN latent spaces for text prompts using pre-trained CLIP<br>[49] models. More similar to our work, Song et al. [60] uses a classifier to generate class-conditional<br>CIFAR-10 images with a diffusion model. In some cases, classifiers can act as stand-alone generative<br>models. For example, Santurkar et al. [55] demonstrate that a robust image classifier can be used as a<br>stand-alone generative model, and Grathwohl et al. [22] train a model which is jointly a classifier and<br>an energy-based model.</p>",
            "id": 124,
            "page": 12,
            "text": "Other works have controlled generative models with a pre-trained classifier. For example, an emerging body of work  aims to optimize GAN latent spaces for text prompts using pre-trained CLIP  models. More similar to our work, Song   uses a classifier to generate class-conditional CIFAR-10 images with a diffusion model. In some cases, classifiers can act as stand-alone generative models. For example, Santurkar   demonstrate that a robust image classifier can be used as a stand-alone generative model, and Grathwohl   train a model which is jointly a classifier and an energy-based model."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2130
                },
                {
                    "x": 1158,
                    "y": 2130
                },
                {
                    "x": 1158,
                    "y": 2184
                },
                {
                    "x": 444,
                    "y": 2184
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:22px'>7 Limitations and Future Work</p>",
            "id": 125,
            "page": 12,
            "text": "7 Limitations and Future Work"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2236
                },
                {
                    "x": 2108,
                    "y": 2236
                },
                {
                    "x": 2108,
                    "y": 2559
                },
                {
                    "x": 443,
                    "y": 2559
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:20px'>While we believe diffusion models are an extremely promising direction for generative modeling,<br>they are still slower than GANs at sampling time due to the use of multiple denoising steps (and<br>therefore forward passes). One promising work in this direction is from Luhman and Luhman [37],<br>who explore a way to distill the DDIM sampling process into a single step model. The samples<br>from the single step model are not yet competitive with GANs, but are much better than previous<br>single-step likelihood-based models. Future work in this direction might be able to completely close<br>the sampling speed gap between diffusion models and GANs without sacrificing image quality.</p>",
            "id": 126,
            "page": 12,
            "text": "While we believe diffusion models are an extremely promising direction for generative modeling, they are still slower than GANs at sampling time due to the use of multiple denoising steps (and therefore forward passes). One promising work in this direction is from Luhman and Luhman , who explore a way to distill the DDIM sampling process into a single step model. The samples from the single step model are not yet competitive with GANs, but are much better than previous single-step likelihood-based models. Future work in this direction might be able to completely close the sampling speed gap between diffusion models and GANs without sacrificing image quality."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2580
                },
                {
                    "x": 2108,
                    "y": 2580
                },
                {
                    "x": 2108,
                    "y": 2806
                },
                {
                    "x": 442,
                    "y": 2806
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='127' style='font-size:20px'>Our proposed classifier guidance technique is currently limited to labeled datasets, and we have<br>provided no effective strategy for trading off diversity for fidelity on unlabeled datasets. In the future,<br>our method could be extended to unlabeled data by clustering samples to produce synthetic labels<br>[36] or by training discriminative models to predict when samples are in the true data distribution or<br>from the sampling distribution.</p>",
            "id": 127,
            "page": 12,
            "text": "Our proposed classifier guidance technique is currently limited to labeled datasets, and we have provided no effective strategy for trading off diversity for fidelity on unlabeled datasets. In the future, our method could be extended to unlabeled data by clustering samples to produce synthetic labels  or by training discriminative models to predict when samples are in the true data distribution or from the sampling distribution."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2830
                },
                {
                    "x": 2109,
                    "y": 2830
                },
                {
                    "x": 2109,
                    "y": 3013
                },
                {
                    "x": 443,
                    "y": 3013
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:18px'>The effectiveness of classifier guidance demonstrates that we can obtain powerful generative models<br>from the gradients of a classification function. This could be used to condition pre-trained models<br>in a plethora of ways, for example by conditioning an image generator with a text caption using a<br>noisy version of CLIP [49], similar to recent methods that guide GANs using text prompts [17, 47,</p>",
            "id": 128,
            "page": 12,
            "text": "The effectiveness of classifier guidance demonstrates that we can obtain powerful generative models from the gradients of a classification function. This could be used to condition pre-trained models in a plethora of ways, for example by conditioning an image generator with a text caption using a noisy version of CLIP , similar to recent methods that guide GANs using text prompts [17, 47,"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='129' style='font-size:14px'>12</footer>",
            "id": 129,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 305
                },
                {
                    "x": 2106,
                    "y": 305
                },
                {
                    "x": 2106,
                    "y": 400
                },
                {
                    "x": 441,
                    "y": 400
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:14px'>2]. It also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful<br>diffusion models that can later be improved by using a classifier with desirable properties.</p>",
            "id": 130,
            "page": 13,
            "text": "2]. It also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful diffusion models that can later be improved by using a classifier with desirable properties."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 465
                },
                {
                    "x": 768,
                    "y": 465
                },
                {
                    "x": 768,
                    "y": 516
                },
                {
                    "x": 443,
                    "y": 516
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>8 Conclusion</p>",
            "id": 131,
            "page": 13,
            "text": "8 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 563
                },
                {
                    "x": 2109,
                    "y": 563
                },
                {
                    "x": 2109,
                    "y": 936
                },
                {
                    "x": 442,
                    "y": 936
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>We have shown that diffusion models, a class of likelihood-based models with a stationary training<br>objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture<br>is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance<br>technique allows us to do SO on class-conditional tasks. In the latter case, we find that the scale<br>of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion<br>models can reduce the sampling time gap between GANs and diffusion models, although diffusion<br>models still require multiple forward passes during sampling. Finally, by combining guidance with<br>upsampling, we can further improve sample quality on high-resolution conditional image synthesis.</p>",
            "id": 132,
            "page": 13,
            "text": "We have shown that diffusion models, a class of likelihood-based models with a stationary training objective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture is sufficient to achieve this on unconditional image generation tasks, and our classifier guidance technique allows us to do SO on class-conditional tasks. In the latter case, we find that the scale of the classifier gradients can be adjusted to trade off diversity for fidelity. These guided diffusion models can reduce the sampling time gap between GANs and diffusion models, although diffusion models still require multiple forward passes during sampling. Finally, by combining guidance with upsampling, we can further improve sample quality on high-resolution conditional image synthesis."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 998
                },
                {
                    "x": 941,
                    "y": 998
                },
                {
                    "x": 941,
                    "y": 1049
                },
                {
                    "x": 443,
                    "y": 1049
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:22px'>9 Acknowledgements</p>",
            "id": 133,
            "page": 13,
            "text": "9 Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1097
                },
                {
                    "x": 2104,
                    "y": 1097
                },
                {
                    "x": 2104,
                    "y": 1189
                },
                {
                    "x": 441,
                    "y": 1189
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:16px'>We thank Alec Radford, Mark Chen, Pranav Shyam and Raul Puri for providing feedback on this<br>work.</p>",
            "id": 134,
            "page": 13,
            "text": "We thank Alec Radford, Mark Chen, Pranav Shyam and Raul Puri for providing feedback on this work."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1257
                },
                {
                    "x": 686,
                    "y": 1257
                },
                {
                    "x": 686,
                    "y": 1307
                },
                {
                    "x": 444,
                    "y": 1307
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:20px'>References</p>",
            "id": 135,
            "page": 13,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 457,
                    "y": 1322
                },
                {
                    "x": 2117,
                    "y": 1322
                },
                {
                    "x": 2117,
                    "y": 3025
                },
                {
                    "x": 457,
                    "y": 3025
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>[1] David Ackley, Geoffrey Hinton, and Terrence Sejnowski. A learning algorithm for boltzmann<br>machines. Cognitive science, 9(1):147-169, 1985.<br>[2] Adverb. The big sleep. https : / /twitter · com/advadnoun/ status/<br>1351038053033406468, 2021.<br>[3] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv:1801.01973, 2018.<br>[4] Andrew Brock, Theodore Lim, J. M. Ritchie, and Nick Weston. Neural photo editing with<br>introspective adversarial networks. arXiv:1609.07093, 2016.<br>[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity<br>natural image synthesis. arXiv:1809.11096, 2018.<br>[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-<br>wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,<br>Ariel Herbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.<br>Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz<br>Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,<br>Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.<br>arXiv:2005.14165, 2020.<br>[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya<br>Sutskever. Generative pretraining from pixels. In International Conference on Machine<br>Learning, pages 1691-1703. PMLR, 2020.<br>[8] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan.<br>Wavegrad: Estimating gradients for waveform generation. arXiv:2009.00713, 2020.<br>[9] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on<br>images. arXiv:2011.10650, 2021.<br>[10] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz<br>machine. Neural computation, 7(5):889-904, 1995.<br>[11] Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron<br>Courville. Modulating early visual processing by language. arXiv:1707.00683, 2017.<br>[12] DeepMind. Biggan-deep 128x128 on tensorflow hub. https: //tfhub · dev/ deepmind/<br>biggan-deep-128/1, 2018.</p>",
            "id": 136,
            "page": 13,
            "text": " David Ackley, Geoffrey Hinton, and Terrence Sejnowski. A learning algorithm for boltzmann machines. Cognitive science, 9(1):147-169, 1985.  Adverb. The big sleep. https : / /twitter · com/advadnoun/ status/ 1351038053033406468, 2021.  Shane Barratt and Rishi Sharma. A note on the inception score. arXiv:1801.01973, 2018.  Andrew Brock, Theodore Lim, J. M. Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv:1609.07093, 2016.  Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv:1809.11096, 2018.  Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv:2005.14165, 2020.  Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pages 1691-1703. PMLR, 2020.  Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. arXiv:2009.00713, 2020.  Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv:2011.10650, 2021.  Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural computation, 7(5):889-904, 1995.  Harm de Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron Courville. Modulating early visual processing by language. arXiv:1707.00683, 2017.  DeepMind. Biggan-deep 128x128 on tensorflow hub. https: //tfhub · dev/ deepmind/ biggan-deep-128/1, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='137' style='font-size:14px'>13</footer>",
            "id": 137,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 290
                },
                {
                    "x": 2119,
                    "y": 290
                },
                {
                    "x": 2119,
                    "y": 3010
                },
                {
                    "x": 444,
                    "y": 3010
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:18px'>[13] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya<br>Sutskever. Jukebox: A generative model for music. arXiv:2005.00341, 2020.<br>[14] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning.<br>arXiv:1907.02544, 2019.<br>[15] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models.<br>arXiv:1903.08689, 2019.<br>[16] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for<br>artistic style. arXiv:1610.07629, 2017.<br>[17] Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from<br>caption and vice versa via clip-guided generative latent space search. arXiv:2102.01645, 2021.<br>[18] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P. Kingma. Learning energy-<br>based models by diffusion recovery likelihood. arXiv:2012.08125, 2020.<br>[19] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil<br>Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv:1406.2661,<br>2014.<br>[20] Google. Cloud tpus. https : / / cloud . google · com/tpu/, 2018.<br>[21] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback:<br>Learning a transition operator as a stochastic recurrent net. arXiv:1711.02282, 2017.<br>[22] Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad<br>Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should<br>treat it like one. arXiv:1912.03263, 2019.<br>[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.<br>Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in<br>Neural Information Processing Systems 30 (NIPS 2017), 2017.<br>[24] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural<br>computation, 14(8):1771-1800, 2002.<br>[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.<br>arXiv:2006.11239, 2020.<br>[26] Alexia Jolicoeur-Martineau, Remi Piche-Taillefer, Remi Tachet des Combes, and Ioan-<br>nis Mitliagkas. Adversarial score matching and improved sampling for image generation.<br>arXiv:2009.05475, 2020.<br>[27] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative<br>adversarial networks. arXiv:arXiv:1812.04948, 2019.<br>[28] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.<br>Analyzing and improving the image quality of stylegan. arXiv:1912.04958, 2019.<br>[29] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.<br>arXiv:1412.6980, 2014.<br>[30] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile<br>diffusion model for audio synthesis. arXiv:2009.09761, 2020.<br>[31] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced<br>Research), 2009. URL http : // www. cs . toronto · edu/~kriz/cifar html.<br>[32] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved<br>precision and recall metric for assessing generative models. arXiv:1904.06991, 2019.<br>[33] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement<br>networks for high-resolution semantic segmentation. arXiv:1611.06612, 2016.</p>",
            "id": 138,
            "page": 14,
            "text": " Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. arXiv:2005.00341, 2020.  Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. arXiv:1907.02544, 2019.  Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv:1903.08689, 2019.  Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur. A learned representation for artistic style. arXiv:1610.07629, 2017.  Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from caption and vice versa via clip-guided generative latent space search. arXiv:2102.01645, 2021.  Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P. Kingma. Learning energybased models by diffusion recovery likelihood. arXiv:2012.08125, 2020.  Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv:1406.2661, 2014.  Google. Cloud tpus. https : / / cloud . google · com/tpu/, 2018.  Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. arXiv:1711.02282, 2017.  Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. arXiv:1912.03263, 2019.  Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017.  Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771-1800, 2002.  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv:2006.11239, 2020.  Alexia Jolicoeur-Martineau, Remi Piche-Taillefer, Remi Tachet des Combes, and Ioannis Mitliagkas. Adversarial score matching and improved sampling for image generation. arXiv:2009.05475, 2020.  Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. arXiv:arXiv:1812.04948, 2019.  Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. arXiv:1912.04958, 2019.  Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.  Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. arXiv:2009.09761, 2020.  Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced Research), 2009. URL http : // www. cs . toronto · edu/~kriz/cifar html.  Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. arXiv:1904.06991, 2019.  Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation. arXiv:1611.06612, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='139' style='font-size:14px'>14</footer>",
            "id": 139,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 288
                },
                {
                    "x": 2119,
                    "y": 288
                },
                {
                    "x": 2119,
                    "y": 3010
                },
                {
                    "x": 445,
                    "y": 3010
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>[34] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the<br>wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.<br>[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101,<br>2017.<br>[36] Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain<br>Gelly. High-fidelity image generation with fewer labels. arXiv:1903.02271, 2019.<br>[37] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for<br>improved sampling speed. arXiv:2101.02388, 2021.<br>[38] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,<br>Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed<br>precision training. arXiv:1710.03740, 2017.<br>[39] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784,<br>2014.<br>[40] Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv:1802.05637,<br>2018.<br>[41] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization<br>for generative adversarial networks. arXiv:1802.05957, 2018.<br>[42] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with<br>sparse representations. arXiv:2103.03841, 2021.<br>[43] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.<br>arXiv:2102.09672, 2021.<br>[44] NVIDIA. Stylegan2. https : / /github · com/NVlabs/stylegan2, 2019.<br>[45] Gaurav Parmar, Richard Zhang, and Jun- Yan Zhu. On buggy resizing libraries and surprising<br>subtleties in fid calculation. arXiv:2104.11222, 2021.<br>[46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,<br>Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative<br>style, high-performance deep learning library. arXiv:1912.01703, 2019.<br>[47] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip:<br>Text-driven manipulation of stylegan imagery. arXiv:2103.17249, 2021.<br>[48] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film:<br>Visual reasoning with a general conditioning layer. arXiv:1709.07871, 2017.<br>[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini<br>Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger,<br>and Ilya Sutskever. Learning transferable visual models from natural language supervision.<br>arXiv:2103.00020, 2021.<br>[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark<br>Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv:2102.12092, 2021.<br>[51] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images<br>with VQ-VAE-2. arXiv:1906.00446, 2019.<br>[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng<br>Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.<br>Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.<br>[53] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad<br>Norouzi. Image super-resolution via iterative refinement. arXiv:arXiv:2104.07636, 2021.</p>",
            "id": 140,
            "page": 15,
            "text": " Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.  Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101, 2017.  Mario Lucic, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain Gelly. High-fidelity image generation with fewer labels. arXiv:1903.02271, 2019.  Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv:2101.02388, 2021.  Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. arXiv:1710.03740, 2017.  Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014.  Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv:1802.05637, 2018.  Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv:1802.05957, 2018.  Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with sparse representations. arXiv:2103.03841, 2021.  Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv:2102.09672, 2021.  NVIDIA. Stylegan2. https : / /github · com/NVlabs/stylegan2, 2019.  Gaurav Parmar, Richard Zhang, and Jun- Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv:2104.11222, 2021.  Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga,  Pytorch: An imperative style, high-performance deep learning library. arXiv:1912.01703, 2019.  Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. arXiv:2103.17249, 2021.  Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. arXiv:1709.07871, 2017.  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv:2103.00020, 2021.  Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv:2102.12092, 2021.  Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. arXiv:1906.00446, 2019.  Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.  Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. arXiv:arXiv:2104.07636, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='141' style='font-size:14px'>15</footer>",
            "id": 141,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 291
                },
                {
                    "x": 2118,
                    "y": 291
                },
                {
                    "x": 2118,
                    "y": 2897
                },
                {
                    "x": 443,
                    "y": 2897
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>[54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.<br>Improved techniques for training gans. arXiv:1606.03498, 2016.<br>[55] Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and<br>Aleksander Madry. Image synthesis with a single (robust) classifier. arXiv:1906.09453, 2019.<br>[56] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep<br>unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585, 2015.<br>[57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models.<br>arXiv:2010.02502, 2020.<br>[58] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.<br>arXiv:2006.09011, 2020.<br>[59] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data<br>distribution. arXiv:arXiv:1907.05600, 2020.<br>[60] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,<br>and Ben Poole. Score-based generative modeling through stochastic differential equations.<br>arXiv:2011.13456, 2020.<br>[61] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-<br>fellow, and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199, 2013.<br>[62] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.<br>Rethinking the inception architecture for computer vision. arXiv:1512.00567, 2015.<br>[63] Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder.<br>arXiv:2007.03898, 2020.<br>[64] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex<br>Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative<br>model for raw audio. arXiv:1609.03499, 2016.<br>[65] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation<br>learning. arXiv:1711.00937, 2017.<br>[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,<br>Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017.<br>[67] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.<br>In Proceedings of the 28th international conference on machine learning (ICML-11), pages<br>681-688. Citeseer, 2011.<br>[68] Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. Logan: Latent<br>optimisation for generative adversarial networks. arXiv:1912.00953, 2019.<br>[69] Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494, 2018.<br>[70] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet.<br>arXiv:1602.03264, 2016.<br>[71] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.<br>Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop.<br>arXiv:1506.03365, 2015.<br>[72] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and<br>Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative<br>adversarial networks. arXiv:1612.03242, 2016.<br>[73] Ligeng Zhu. Thop. https : / /gi thub · com/Lyken1 7 /pytorch-OpCounter, 2018.</p>",
            "id": 142,
            "page": 16,
            "text": " Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. arXiv:1606.03498, 2016.  Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Image synthesis with a single (robust) classifier. arXiv:1906.09453, 2019.  Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585, 2015.  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, 2020.  Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv:2006.09011, 2020.  Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. arXiv:arXiv:1907.05600, 2020.  Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv:2011.13456, 2020.  Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199, 2013.  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. arXiv:1512.00567, 2015.  Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv:2007.03898, 2020.  Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv:1609.03499, 2016.  Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. arXiv:1711.00937, 2017.  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv:1706.03762, 2017.  Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681-688. Citeseer, 2011.  Yan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. Logan: Latent optimisation for generative adversarial networks. arXiv:1912.00953, 2019.  Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494, 2018.  Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet. arXiv:1602.03264, 2016.  Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv:1506.03365, 2015.  Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. arXiv:1612.03242, 2016.  Ligeng Zhu. Thop. https : / /gi thub · com/Lyken1 7 /pytorch-OpCounter, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1300,
                    "y": 3091
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1252,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='143' style='font-size:14px'>16</footer>",
            "id": 143,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 300
                },
                {
                    "x": 1173,
                    "y": 300
                },
                {
                    "x": 1173,
                    "y": 356
                },
                {
                    "x": 445,
                    "y": 356
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:22px'>A Computational Requirements</p>",
            "id": 144,
            "page": 17,
            "text": "A Computational Requirements"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 404
                },
                {
                    "x": 2109,
                    "y": 404
                },
                {
                    "x": 2109,
                    "y": 589
                },
                {
                    "x": 442,
                    "y": 589
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:16px'>Compute is essential to modern machine learning applications, and more compute typically yields<br>better results. It is thus important to compare our method's compute requirements to competing<br>methods. In this section, we demonstrate that we can achieve results better than StyleGAN2 and<br>BigGAN-deep with the same or lower compute budget.</p>",
            "id": 145,
            "page": 17,
            "text": "Compute is essential to modern machine learning applications, and more compute typically yields better results. It is thus important to compare our method's compute requirements to competing methods. In this section, we demonstrate that we can achieve results better than StyleGAN2 and BigGAN-deep with the same or lower compute budget."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 646
                },
                {
                    "x": 773,
                    "y": 646
                },
                {
                    "x": 773,
                    "y": 696
                },
                {
                    "x": 444,
                    "y": 696
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:20px'>A.1 Throughput</p>",
            "id": 146,
            "page": 17,
            "text": "A.1 Throughput"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 730
                },
                {
                    "x": 2109,
                    "y": 730
                },
                {
                    "x": 2109,
                    "y": 961
                },
                {
                    "x": 442,
                    "y": 961
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:16px'>We first benchmark the throughput of our models in Table 7. For the theoretical throughput, we<br>measure the theoretical FLOPs for our model using THOP [73], and assume 100% utilization of an<br>NVIDIA Tesla V100 (120 TFLOPs), while for the actual throughput we use measured wall-clock<br>time. We include communication time across two machines whenever our training batch size doesn't<br>fit on a single machine, where each of our machines has 8 V100s.</p>",
            "id": 147,
            "page": 17,
            "text": "We first benchmark the throughput of our models in Table 7. For the theoretical throughput, we measure the theoretical FLOPs for our model using THOP , and assume 100% utilization of an NVIDIA Tesla V100 (120 TFLOPs), while for the actual throughput we use measured wall-clock time. We include communication time across two machines whenever our training batch size doesn't fit on a single machine, where each of our machines has 8 V100s."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 981
                },
                {
                    "x": 2109,
                    "y": 981
                },
                {
                    "x": 2109,
                    "y": 1256
                },
                {
                    "x": 442,
                    "y": 1256
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:16px'>We find that a naive implementation of our models in PyTorch 1.7 is very inefficient, utilizing only<br>20-30% of the hardware. We also benchmark our optimized version, which use larger per-GPU batch<br>sizes, fused GroupNorm-Swish and fused Adam CUDA ops. For our ImageNet 128x 128 model in<br>particular, we find that we can increase the per-GPU batch size from 4 to 32 while still fitting in GPU<br>memory, and this makes a large utilization difference. Our implementation is still far from optimal,<br>and further optimizations should allow us to reach higher levels of utilization.</p>",
            "id": 148,
            "page": 17,
            "text": "We find that a naive implementation of our models in PyTorch 1.7 is very inefficient, utilizing only 20-30% of the hardware. We also benchmark our optimized version, which use larger per-GPU batch sizes, fused GroupNorm-Swish and fused Adam CUDA ops. For our ImageNet 128x 128 model in particular, we find that we can increase the per-GPU batch size from 4 to 32 while still fitting in GPU memory, and this makes a large utilization difference. Our implementation is still far from optimal, and further optimizations should allow us to reach higher levels of utilization."
        },
        {
            "bounding_box": [
                {
                    "x": 635,
                    "y": 1298
                },
                {
                    "x": 1914,
                    "y": 1298
                },
                {
                    "x": 1914,
                    "y": 2150
                },
                {
                    "x": 635,
                    "y": 2150
                }
            ],
            "category": "table",
            "html": "<table id='149' style='font-size:14px'><tr><td>Model</td><td>Implementation</td><td>Batch Size per GPU</td><td>Throughput Imgs per V100-sec</td><td>Utilization</td></tr><tr><td rowspan=\"3\">64x64</td><td>Theoretical</td><td>-</td><td>182.3</td><td>100%</td></tr><tr><td>Naive</td><td>32</td><td>37.0</td><td>20%</td></tr><tr><td>Optimized</td><td>96</td><td>74.1</td><td>41%</td></tr><tr><td rowspan=\"3\">128x 128</td><td>Theoretical</td><td>-</td><td>65.2</td><td>100%</td></tr><tr><td>Naive</td><td>4</td><td>11.5</td><td>18%</td></tr><tr><td>Optimized</td><td>32</td><td>24.8</td><td>38%</td></tr><tr><td rowspan=\"3\">256x256</td><td>Theoretical</td><td>-</td><td>17.9</td><td>100%</td></tr><tr><td>Naive</td><td>4</td><td>4.4</td><td>25%</td></tr><tr><td>Optimized</td><td>8</td><td>6.4</td><td>36%</td></tr><tr><td rowspan=\"3\">64 → 256</td><td>Theoretical</td><td>-</td><td>31.7</td><td>100%</td></tr><tr><td>Naive</td><td>4</td><td>6.3</td><td>20%</td></tr><tr><td>Optimized</td><td>12</td><td>9.5</td><td>30%</td></tr><tr><td rowspan=\"3\">128 → 512</td><td>Theoretical</td><td>-</td><td>8.0</td><td>100%</td></tr><tr><td>Naive</td><td>2</td><td>1.9</td><td>24%</td></tr><tr><td>Optimized</td><td>2</td><td>2.3</td><td>29%</td></tr></table>",
            "id": 149,
            "page": 17,
            "text": "Model Implementation Batch Size per GPU Throughput Imgs per V100-sec Utilization  64x64 Theoretical - 182.3 100%  Naive 32 37.0 20%  Optimized 96 74.1 41%  128x 128 Theoretical - 65.2 100%  Naive 4 11.5 18%  Optimized 32 24.8 38%  256x256 Theoretical - 17.9 100%  Naive 4 4.4 25%  Optimized 8 6.4 36%  64 → 256 Theoretical - 31.7 100%  Naive 4 6.3 20%  Optimized 12 9.5 30%  128 → 512 Theoretical - 8.0 100%  Naive 2 1.9 24%  Optimized 2 2.3"
        },
        {
            "bounding_box": [
                {
                    "x": 595,
                    "y": 2163
                },
                {
                    "x": 1950,
                    "y": 2163
                },
                {
                    "x": 1950,
                    "y": 2211
                },
                {
                    "x": 595,
                    "y": 2211
                }
            ],
            "category": "caption",
            "html": "<br><caption id='150' style='font-size:18px'>Table 7: Throughput of our ImageNet models, measured in Images per V100-sec.</caption>",
            "id": 150,
            "page": 17,
            "text": "Table 7: Throughput of our ImageNet models, measured in Images per V100-sec."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2273
                },
                {
                    "x": 820,
                    "y": 2273
                },
                {
                    "x": 820,
                    "y": 2325
                },
                {
                    "x": 445,
                    "y": 2325
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:22px'>A.2 Early stopping</p>",
            "id": 151,
            "page": 17,
            "text": "A.2 Early stopping"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2358
                },
                {
                    "x": 2107,
                    "y": 2358
                },
                {
                    "x": 2107,
                    "y": 2588
                },
                {
                    "x": 442,
                    "y": 2588
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>In addition, we can train for many fewer iterations while maintaining sample quality superior to<br>BigGAN-deep. Table 8 and 9 evaluate our ImageNet 128x128 and 256x256 models throughout<br>training. We can see that the ImageNet 128x 128 model beats BigGAN-deep's FID (6.02) after 500K<br>training iterations, only one eighth of the way through training. Similarly, the ImageNet 256x256<br>model beats BigGAN-deep after 750K iterations, roughly a third of the way through training.</p>",
            "id": 152,
            "page": 17,
            "text": "In addition, we can train for many fewer iterations while maintaining sample quality superior to BigGAN-deep. Table 8 and 9 evaluate our ImageNet 128x128 and 256x256 models throughout training. We can see that the ImageNet 128x 128 model beats BigGAN-deep's FID (6.02) after 500K training iterations, only one eighth of the way through training. Similarly, the ImageNet 256x256 model beats BigGAN-deep after 750K iterations, roughly a third of the way through training."
        },
        {
            "bounding_box": [
                {
                    "x": 886,
                    "y": 2633
                },
                {
                    "x": 1666,
                    "y": 2633
                },
                {
                    "x": 1666,
                    "y": 2938
                },
                {
                    "x": 886,
                    "y": 2938
                }
            ],
            "category": "table",
            "html": "<table id='153' style='font-size:14px'><tr><td>Iterations</td><td>FID</td><td>sFID</td><td>Precision</td><td>Recall</td></tr><tr><td>250K</td><td>7.97</td><td>6.48</td><td>0.80</td><td>0.50</td></tr><tr><td>500K</td><td>5.31</td><td>5.97</td><td>0.83</td><td>0.49</td></tr><tr><td>1000K</td><td>4.10</td><td>5.80</td><td>0.81</td><td>0.51</td></tr><tr><td>2000K</td><td>3.42</td><td>5.69</td><td>0.83</td><td>0.53</td></tr><tr><td>4360K</td><td>3.09</td><td>5.59</td><td>0.82</td><td>0.54</td></tr></table>",
            "id": 153,
            "page": 17,
            "text": "Iterations FID sFID Precision Recall  250K 7.97 6.48 0.80 0.50  500K 5.31 5.97 0.83 0.49  1000K 4.10 5.80 0.81 0.51  2000K 3.42 5.69 0.83 0.53  4360K 3.09 5.59 0.82"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2952
                },
                {
                    "x": 2031,
                    "y": 2952
                },
                {
                    "x": 2031,
                    "y": 2999
                },
                {
                    "x": 512,
                    "y": 2999
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:20px'>Table 8: Evaluating an ImageNet 128 x 128 model throughout training (classifier scale 1.0).</p>",
            "id": 154,
            "page": 17,
            "text": "Table 8: Evaluating an ImageNet 128 x 128 model throughout training (classifier scale 1.0)."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1252,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='155' style='font-size:18px'>17</footer>",
            "id": 155,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 880,
                    "y": 297
                },
                {
                    "x": 1670,
                    "y": 297
                },
                {
                    "x": 1670,
                    "y": 642
                },
                {
                    "x": 880,
                    "y": 642
                }
            ],
            "category": "table",
            "html": "<table id='156' style='font-size:14px'><tr><td>Iterations</td><td>FID</td><td>sFID</td><td>Precision</td><td>Recall</td></tr><tr><td>250K</td><td>12.21</td><td>6.15</td><td>0.78</td><td>0.50</td></tr><tr><td>500K</td><td>7.95</td><td>5.51</td><td>0.81</td><td>0.50</td></tr><tr><td>750K</td><td>6.49</td><td>5.39</td><td>0.81</td><td>0.50</td></tr><tr><td>1000K</td><td>5.74</td><td>5.29</td><td>0.81</td><td>0.52</td></tr><tr><td>1500K</td><td>5.01</td><td>5.20</td><td>0.82</td><td>0.52</td></tr><tr><td>1980K</td><td>4.59</td><td>5.25</td><td>0.82</td><td>0.52</td></tr></table>",
            "id": 156,
            "page": 18,
            "text": "Iterations FID sFID Precision Recall  250K 12.21 6.15 0.78 0.50  500K 7.95 5.51 0.81 0.50  750K 6.49 5.39 0.81 0.50  1000K 5.74 5.29 0.81 0.52  1500K 5.01 5.20 0.82 0.52  1980K 4.59 5.25 0.82"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 656
                },
                {
                    "x": 2032,
                    "y": 656
                },
                {
                    "x": 2032,
                    "y": 702
                },
                {
                    "x": 512,
                    "y": 702
                }
            ],
            "category": "caption",
            "html": "<br><caption id='157' style='font-size:20px'>Table 9: Evaluating an ImageNet 256x256 model throughout training (classifier scale 1.0).</caption>",
            "id": 157,
            "page": 18,
            "text": "Table 9: Evaluating an ImageNet 256x256 model throughout training (classifier scale 1.0)."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 762
                },
                {
                    "x": 939,
                    "y": 762
                },
                {
                    "x": 939,
                    "y": 810
                },
                {
                    "x": 445,
                    "y": 810
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:22px'>A.3 Compute comparison</p>",
            "id": 158,
            "page": 18,
            "text": "A.3 Compute comparison"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 844
                },
                {
                    "x": 2108,
                    "y": 844
                },
                {
                    "x": 2108,
                    "y": 1167
                },
                {
                    "x": 444,
                    "y": 1167
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:16px'>Finally, in Table 10 we compare the compute of our models with StyleGAN2 and BigGAN-deep, and<br>show we can obtain better FIDs with a similar compute budget. For BigGAN-deep, Brock et al. [5] do<br>not explicitly describe the compute requirements for training their models, but rather provide rough<br>estimates in terms of days on a Google TPUv3 pod [20]. We convert their TPU-v3 estimates to V100<br>days according to 2 TPU-v3 day = 1 V100 day. For StyleGAN2, we use the reported throughput of<br>25M images over 32 days 13 hour on one V100 for config-f [44]. We note that our classifier training<br>is relatively lightweight compared to training the generative model.</p>",
            "id": 159,
            "page": 18,
            "text": "Finally, in Table 10 we compare the compute of our models with StyleGAN2 and BigGAN-deep, and show we can obtain better FIDs with a similar compute budget. For BigGAN-deep, Brock   do not explicitly describe the compute requirements for training their models, but rather provide rough estimates in terms of days on a Google TPUv3 pod . We convert their TPU-v3 estimates to V100 days according to 2 TPU-v3 day = 1 V100 day. For StyleGAN2, we use the reported throughput of 25M images over 32 days 13 hour on one V100 for config-f . We note that our classifier training is relatively lightweight compared to training the generative model."
        },
        {
            "bounding_box": [
                {
                    "x": 481,
                    "y": 1205
                },
                {
                    "x": 2076,
                    "y": 1205
                },
                {
                    "x": 2076,
                    "y": 2593
                },
                {
                    "x": 481,
                    "y": 2593
                }
            ],
            "category": "table",
            "html": "<table id='160' style='font-size:14px'><tr><td>Model</td><td>Generator Compute</td><td>Classifier Compute</td><td>Total Compute</td><td>FID</td><td>sFID</td><td>Precision</td><td>Recall</td></tr><tr><td colspan=\"8\">LSUN Horse 256 x 256</td></tr><tr><td>StyleGAN2 [28]</td><td></td><td></td><td>130</td><td>3.84</td><td>6.46</td><td>0.63</td><td>0.48</td></tr><tr><td>ADM (250K)</td><td>116</td><td></td><td>116</td><td>2.95</td><td>5.94</td><td>0.69</td><td>0.55</td></tr><tr><td>ADM (dropout, 250K)</td><td>116</td><td></td><td>116</td><td>2.57</td><td>6.81</td><td>0.71</td><td>0.55</td></tr><tr><td colspan=\"8\">LSUN Cat 256x256</td></tr><tr><td>StyleGAN2 [28]</td><td></td><td></td><td>115</td><td>7.25</td><td>6.33</td><td>0.58</td><td>0.43</td></tr><tr><td>ADM (dropout, 200K)</td><td>92</td><td></td><td>92</td><td>5.57</td><td>6.69</td><td>0.63</td><td>0.52</td></tr><tr><td colspan=\"8\">ImageNet 128x 128</td></tr><tr><td>BigGAN-deep [5]</td><td></td><td></td><td>64-128</td><td>6.02</td><td>7.18</td><td>0.86</td><td>0.35</td></tr><tr><td>ADM-G (4360K)</td><td>521</td><td>9</td><td>530</td><td>3.09</td><td>5.59</td><td>0.82</td><td>0.54</td></tr><tr><td>ADM-G (450K)</td><td>54</td><td>9</td><td>63</td><td>5.67</td><td>6.19</td><td>0.82</td><td>0.49</td></tr><tr><td colspan=\"8\">ImageNet 256x256</td></tr><tr><td>BigGAN-deep [5]</td><td></td><td></td><td>128-256</td><td>6.95</td><td>7.36</td><td>0.87</td><td>0.28</td></tr><tr><td>ADM-G (1980K)</td><td>916</td><td>46</td><td>962</td><td>4.59</td><td>5.25</td><td>0.82</td><td>0.52</td></tr><tr><td>ADM-G (750K)</td><td>347</td><td>46</td><td>393</td><td>6.49</td><td>5.39</td><td>0.81</td><td>0.50</td></tr><tr><td>ADM-G (750K)</td><td>347</td><td>14t</td><td>361</td><td>6.68</td><td>5.34</td><td>0.81</td><td>0.51</td></tr><tr><td>ADM-G (540K), ADM-U (500K)</td><td>329</td><td>30</td><td>359</td><td>3.85</td><td>5.86</td><td>0.84</td><td>0.53</td></tr><tr><td>ADM-G (540K), ADM-U (150K)</td><td>219</td><td>30</td><td>249</td><td>4.15</td><td>6.14</td><td>0.82</td><td>0.54</td></tr><tr><td>ADM-G (200K), ADM-U (150K)</td><td>110</td><td>10‡</td><td>126</td><td>4.93</td><td>5.82</td><td>0.82</td><td>0.52</td></tr><tr><td colspan=\"8\">ImageNet 512x512</td></tr><tr><td>BigGAN-deep [5]</td><td></td><td></td><td>256-512</td><td>8.43</td><td>8.13</td><td>0.88</td><td>0.29</td></tr><tr><td>ADM-G (4360K), ADM-U (1050K)</td><td>1878</td><td>36</td><td>1914</td><td>3.85</td><td>5.86</td><td>0.84</td><td>0.53</td></tr><tr><td>ADM-G (500K), ADM-U (100K)</td><td>189</td><td>9*</td><td>198</td><td>7.59</td><td>6.84</td><td>0.84</td><td>0.53</td></tr></table>",
            "id": 160,
            "page": 18,
            "text": "Model Generator Compute Classifier Compute Total Compute FID sFID Precision Recall  LSUN Horse 256 x 256  StyleGAN2    130 3.84 6.46 0.63 0.48  ADM (250K) 116  116 2.95 5.94 0.69 0.55  ADM (dropout, 250K) 116  116 2.57 6.81 0.71 0.55  LSUN Cat 256x256  StyleGAN2    115 7.25 6.33 0.58 0.43  ADM (dropout, 200K) 92  92 5.57 6.69 0.63 0.52  ImageNet 128x 128  BigGAN-deep    64-128 6.02 7.18 0.86 0.35  ADM-G (4360K) 521 9 530 3.09 5.59 0.82 0.54  ADM-G (450K) 54 9 63 5.67 6.19 0.82 0.49  ImageNet 256x256  BigGAN-deep    128-256 6.95 7.36 0.87 0.28  ADM-G (1980K) 916 46 962 4.59 5.25 0.82 0.52  ADM-G (750K) 347 46 393 6.49 5.39 0.81 0.50  ADM-G (750K) 347 14t 361 6.68 5.34 0.81 0.51  ADM-G (540K), ADM-U (500K) 329 30 359 3.85 5.86 0.84 0.53  ADM-G (540K), ADM-U (150K) 219 30 249 4.15 6.14 0.82 0.54  ADM-G (200K), ADM-U (150K) 110 10‡ 126 4.93 5.82 0.82 0.52  ImageNet 512x512  BigGAN-deep    256-512 8.43 8.13 0.88 0.29  ADM-G (4360K), ADM-U (1050K) 1878 36 1914 3.85 5.86 0.84 0.53  ADM-G (500K), ADM-U (100K) 189 9* 198 7.59 6.84 0.84"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2602
                },
                {
                    "x": 2109,
                    "y": 2602
                },
                {
                    "x": 2109,
                    "y": 2833
                },
                {
                    "x": 442,
                    "y": 2833
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:18px'>Table 10: Training compute requirements for our diffusion models compared to StyleGAN2 and<br>BigGAN-deep. Training iterations for each diffusion model are mentioned in parenthesis. Compute<br>is measured in V100-days. +ImageNet 256x 256 classifier with 150K iterations (instead of 500K).<br>‡ImageNet 64x64 classifier with batch size 256 (instead of 1024). *ImageNet 128x 128 classifier<br>with batch size 256 (instead of 1024).</p>",
            "id": 161,
            "page": 18,
            "text": "Table 10: Training compute requirements for our diffusion models compared to StyleGAN2 and BigGAN-deep. Training iterations for each diffusion model are mentioned in parenthesis. Compute is measured in V100-days. +ImageNet 256x 256 classifier with 150K iterations (instead of 500K). ‡ImageNet 64x64 classifier with batch size 256 (instead of 1024). *ImageNet 128x 128 classifier with batch size 256 (instead of 1024)."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3129
                },
                {
                    "x": 1252,
                    "y": 3129
                }
            ],
            "category": "footer",
            "html": "<footer id='162' style='font-size:14px'>18</footer>",
            "id": 162,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 300
                },
                {
                    "x": 1209,
                    "y": 300
                },
                {
                    "x": 1209,
                    "y": 352
                },
                {
                    "x": 444,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:20px'>B Detailed Formulation of DDPM</p>",
            "id": 163,
            "page": 19,
            "text": "B Detailed Formulation of DDPM"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 400
                },
                {
                    "x": 2107,
                    "y": 400
                },
                {
                    "x": 2107,
                    "y": 583
                },
                {
                    "x": 444,
                    "y": 583
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:14px'>Here, we provide a detailed review of the formulation of Gaussian diffusion models from Ho et al.<br>[25]. We start by defining our data distribution xo ~ q(xo) and a Markovian noising process q which<br>gradually adds noise to the data to produce noised samples X1 through XT. In particular, each step of<br>the noising process adds Gaussian noise according to some variance schedule given by Bt:</p>",
            "id": 164,
            "page": 19,
            "text": "Here, we provide a detailed review of the formulation of Gaussian diffusion models from Ho  . We start by defining our data distribution xo ~ q(xo) and a Markovian noising process q which gradually adds noise to the data to produce noised samples X1 through XT. In particular, each step of the noising process adds Gaussian noise according to some variance schedule given by Bt:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 696
                },
                {
                    "x": 2105,
                    "y": 696
                },
                {
                    "x": 2105,
                    "y": 796
                },
                {
                    "x": 444,
                    "y": 796
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:14px'>Ho et al. [25] note that we need not apply q repeatedly to sample from Xt ~ q(xt|xo). Instead,<br>q(xt|xo) can be expressed as a Gaussian distribution. With at := 1 - Bt and �t := IIs=o as</p>",
            "id": 165,
            "page": 19,
            "text": "Ho   note that we need not apply q repeatedly to sample from Xt ~ q(xt|xo). Instead, q(xt|xo) can be expressed as a Gaussian distribution. With at := 1 - Bt and �t := IIs=o as"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 973
                },
                {
                    "x": 2103,
                    "y": 973
                },
                {
                    "x": 2103,
                    "y": 1062
                },
                {
                    "x": 443,
                    "y": 1062
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:14px'>Here, 1 - �t tells us the variance of the noise for an arbitrary timestep, and we could equivalently<br>use this to define the noise schedule instead of Bt.</p>",
            "id": 166,
            "page": 19,
            "text": "Here, 1 - �t tells us the variance of the noise for an arbitrary timestep, and we could equivalently use this to define the noise schedule instead of Bt."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1086
                },
                {
                    "x": 2105,
                    "y": 1086
                },
                {
                    "x": 2105,
                    "y": 1185
                },
                {
                    "x": 443,
                    "y": 1185
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:16px'>Using Bayes theorem, one finds that the posterior q(xt-1|xt, xo) is also a Gaussian with mean<br>ut(xt, xo) and variance Bt defined as follows:</p>",
            "id": 167,
            "page": 19,
            "text": "Using Bayes theorem, one finds that the posterior q(xt-1|xt, xo) is also a Gaussian with mean ut(xt, xo) and variance Bt defined as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1573
                },
                {
                    "x": 2104,
                    "y": 1573
                },
                {
                    "x": 2104,
                    "y": 1889
                },
                {
                    "x": 444,
                    "y": 1889
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:14px'>If we wish to sample from the data distribution q(xo), we can first sample from q(xT) and then sample<br>reverse steps q(xt-1|xt) until we reach xo. Under reasonable settings for Bt and T, the distribution<br>q(xT) is nearly an isotropic Gaussian distribution, SO sampling XT is trivial. All that is left is to<br>approximate q(xt-1|xt) using a neural network, since it cannot be computed exactly when the data<br>distribution is unknown. To this end, Sohl-Dickstein et al. [56] note that q(xt-1|xt) approaches a<br>diagonal Gaussian distribution as T → 8 and correspondingly Bt → 0, so it is sufficient to train a<br>neural network to predict a mean ou and a diagonal covariance matrix �o:</p>",
            "id": 168,
            "page": 19,
            "text": "If we wish to sample from the data distribution q(xo), we can first sample from q(xT) and then sample reverse steps q(xt-1|xt) until we reach xo. Under reasonable settings for Bt and T, the distribution q(xT) is nearly an isotropic Gaussian distribution, SO sampling XT is trivial. All that is left is to approximate q(xt-1|xt) using a neural network, since it cannot be computed exactly when the data distribution is unknown. To this end, Sohl-Dickstein   note that q(xt-1|xt) approaches a diagonal Gaussian distribution as T → 8 and correspondingly Bt → 0, so it is sufficient to train a neural network to predict a mean ou and a diagonal covariance matrix �o:"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1996
                },
                {
                    "x": 2104,
                    "y": 1996
                },
                {
                    "x": 2104,
                    "y": 2085
                },
                {
                    "x": 445,
                    "y": 2085
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:16px'>To train this model such that p(xo) learns the true data distribution q(xo), we can optimize the<br>following variational lower-bound Lvlb for pe(xo):</p>",
            "id": 169,
            "page": 19,
            "text": "To train this model such that p(xo) learns the true data distribution q(xo), we can optimize the following variational lower-bound Lvlb for pe(xo):"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2363
                },
                {
                    "x": 2104,
                    "y": 2363
                },
                {
                    "x": 2104,
                    "y": 2543
                },
                {
                    "x": 443,
                    "y": 2543
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:16px'>While the above objective is well-justified, Ho et al. [25] found that a different objective produces<br>better samples in practice. In particular, they do not directly parameterize ��(xt,t) as a neural<br>network, but instead train a model E0(xt, t) to predict E from Equation 17. This simplified objective<br>is defined as follows:</p>",
            "id": 170,
            "page": 19,
            "text": "While the above objective is well-justified, Ho   found that a different objective produces better samples in practice. In particular, they do not directly parameterize ��(xt,t) as a neural network, but instead train a model E0(xt, t) to predict E from Equation 17. This simplified objective is defined as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2655
                },
                {
                    "x": 1706,
                    "y": 2655
                },
                {
                    "x": 1706,
                    "y": 2702
                },
                {
                    "x": 444,
                    "y": 2702
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:16px'>During sampling, we can use substitution to derive mo(xt, t) from E0(xt, t):</p>",
            "id": 171,
            "page": 19,
            "text": "During sampling, we can use substitution to derive mo(xt, t) from E0(xt, t):"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2866
                },
                {
                    "x": 2105,
                    "y": 2866
                },
                {
                    "x": 2105,
                    "y": 3011
                },
                {
                    "x": 442,
                    "y": 3011
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:16px'>Note that Lsimple does not provide any learning signal for Eo(xt, t). Ho et al. [25] find that instead of<br>learning E�(xt, t), they can fix it to a constant, choosing either BtI or BtI. These values correspond<br>to upper and lower bounds for the true reverse step variance.</p>",
            "id": 172,
            "page": 19,
            "text": "Note that Lsimple does not provide any learning signal for Eo(xt, t). Ho   find that instead of learning E�(xt, t), they can fix it to a constant, choosing either BtI or BtI. These values correspond to upper and lower bounds for the true reverse step variance."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1253,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='173' style='font-size:14px'>19</footer>",
            "id": 173,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 300
                },
                {
                    "x": 1199,
                    "y": 300
                },
                {
                    "x": 1199,
                    "y": 355
                },
                {
                    "x": 444,
                    "y": 355
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:22px'>C Nearest Neighbors for Samples</p>",
            "id": 174,
            "page": 20,
            "text": "C Nearest Neighbors for Samples"
        },
        {
            "bounding_box": [
                {
                    "x": 524,
                    "y": 421
                },
                {
                    "x": 2023,
                    "y": 421
                },
                {
                    "x": 2023,
                    "y": 1322
                },
                {
                    "x": 524,
                    "y": 1322
                }
            ],
            "category": "figure",
            "html": "<figure><img id='175' style='font-size:14px' alt=\"최대\nSITESEN\" data-coord=\"top-left:(524,421); bottom-right:(2023,1322)\" /></figure>",
            "id": 175,
            "page": 20,
            "text": "최대 SITESEN"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1339
                },
                {
                    "x": 2109,
                    "y": 1339
                },
                {
                    "x": 2109,
                    "y": 1530
                },
                {
                    "x": 441,
                    "y": 1530
                }
            ],
            "category": "caption",
            "html": "<br><caption id='176' style='font-size:18px'>Figure 7: Nearest neighbors for samples from a classifier guided model on ImageNet 256x256. For<br>each image, the top row is a sample, and the remaining rows are the top 3 nearest neighbors from the<br>dataset. The top samples were generated with classifier scale 1 and 250 diffusion sampling steps (FID<br>4.59). The bottom samples were generated with classifier scale 2.5 and 25 DDIM steps (FID 5.44).</caption>",
            "id": 176,
            "page": 20,
            "text": "Figure 7: Nearest neighbors for samples from a classifier guided model on ImageNet 256x256. For each image, the top row is a sample, and the remaining rows are the top 3 nearest neighbors from the dataset. The top samples were generated with classifier scale 1 and 250 diffusion sampling steps (FID 4.59). The bottom samples were generated with classifier scale 2.5 and 25 DDIM steps (FID 5.44)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1597
                },
                {
                    "x": 2111,
                    "y": 1597
                },
                {
                    "x": 2111,
                    "y": 1829
                },
                {
                    "x": 441,
                    "y": 1829
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:18px'>Our models achieve their best FID when using a classifier to reduce the diversity of the generations.<br>One might fear that such a process could cause the model to recall existing images from the training<br>dataset, especially as the classifier scale is increased. To test this, we looked at the nearest neighbors<br>(in InceptionV3 [62] feature space) for a handful of samples. Figure 7 shows our results, revealing<br>that the samples are indeed unique and not stored in the training set.</p>",
            "id": 177,
            "page": 20,
            "text": "Our models achieve their best FID when using a classifier to reduce the diversity of the generations. One might fear that such a process could cause the model to recall existing images from the training dataset, especially as the classifier scale is increased. To test this, we looked at the nearest neighbors (in InceptionV3  feature space) for a handful of samples. Figure 7 shows our results, revealing that the samples are indeed unique and not stored in the training set."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1893
                },
                {
                    "x": 1318,
                    "y": 1893
                },
                {
                    "x": 1318,
                    "y": 1948
                },
                {
                    "x": 443,
                    "y": 1948
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:20px'>D Effect of Varying the Classifier Scale</p>",
            "id": 178,
            "page": 20,
            "text": "D Effect of Varying the Classifier Scale"
        },
        {
            "bounding_box": [
                {
                    "x": 525,
                    "y": 2015
                },
                {
                    "x": 2020,
                    "y": 2015
                },
                {
                    "x": 2020,
                    "y": 2755
                },
                {
                    "x": 525,
                    "y": 2755
                }
            ],
            "category": "figure",
            "html": "<figure><img id='179' alt=\"\" data-coord=\"top-left:(525,2015); bottom-right:(2020,2755)\" /></figure>",
            "id": 179,
            "page": 20,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2786
                },
                {
                    "x": 2111,
                    "y": 2786
                },
                {
                    "x": 2111,
                    "y": 2932
                },
                {
                    "x": 442,
                    "y": 2932
                }
            ],
            "category": "caption",
            "html": "<caption id='180' style='font-size:18px'>Figure 8: Samples when increasing the classifier scale from 0.0 (left) to 5.5 (right). Each row<br>corresponds to a fixed noise seed. We observe that the classifier drastically changes some images,<br>while leaving others relatively unaffected.</caption>",
            "id": 180,
            "page": 20,
            "text": "Figure 8: Samples when increasing the classifier scale from 0.0 (left) to 5.5 (right). Each row corresponds to a fixed noise seed. We observe that the classifier drastically changes some images, while leaving others relatively unaffected."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1249,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='181' style='font-size:16px'>20</footer>",
            "id": 181,
            "page": 20,
            "text": "20"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 298
                },
                {
                    "x": 1152,
                    "y": 298
                },
                {
                    "x": 1152,
                    "y": 355
                },
                {
                    "x": 444,
                    "y": 355
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:20px'>E LSUN Diversity Comparison</p>",
            "id": 182,
            "page": 21,
            "text": "E LSUN Diversity Comparison"
        },
        {
            "bounding_box": [
                {
                    "x": 454,
                    "y": 418
                },
                {
                    "x": 2095,
                    "y": 418
                },
                {
                    "x": 2095,
                    "y": 1618
                },
                {
                    "x": 454,
                    "y": 1618
                }
            ],
            "category": "figure",
            "html": "<figure><img id='183' style='font-size:14px' alt=\"Rosesarered\nolets ared - Tulips arere\nshes are red Trees are red\nHevery setyour gardenon fire\nILVOLIEW\" data-coord=\"top-left:(454,418); bottom-right:(2095,1618)\" /></figure>",
            "id": 183,
            "page": 21,
            "text": "Rosesarered olets ared - Tulips arere shes are red Trees are red Hevery setyour gardenon fire ILVOLIEW"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1636
                },
                {
                    "x": 2109,
                    "y": 1636
                },
                {
                    "x": 2109,
                    "y": 1731
                },
                {
                    "x": 443,
                    "y": 1731
                }
            ],
            "category": "caption",
            "html": "<br><caption id='184' style='font-size:18px'>Figure 9: Samples from StyleGAN2 (or StyleGAN for bedrooms) with truncation 1.0 (left) VS<br>samples from our diffusion models (middle) and samples from the training set (right).</caption>",
            "id": 184,
            "page": 21,
            "text": "Figure 9: Samples from StyleGAN2 (or StyleGAN for bedrooms) with truncation 1.0 (left) VS samples from our diffusion models (middle) and samples from the training set (right)."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3091
                },
                {
                    "x": 1296,
                    "y": 3091
                },
                {
                    "x": 1296,
                    "y": 3131
                },
                {
                    "x": 1248,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='185' style='font-size:16px'>21</footer>",
            "id": 185,
            "page": 21,
            "text": "21"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 299
                },
                {
                    "x": 1629,
                    "y": 299
                },
                {
                    "x": 1629,
                    "y": 355
                },
                {
                    "x": 442,
                    "y": 355
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:22px'>F Interpolating Between Dataset Images Using DDIM</p>",
            "id": 186,
            "page": 22,
            "text": "F Interpolating Between Dataset Images Using DDIM"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 400
                },
                {
                    "x": 2109,
                    "y": 400
                },
                {
                    "x": 2109,
                    "y": 586
                },
                {
                    "x": 441,
                    "y": 586
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:18px'>The DDIM [57] sampling process is deterministic given the initial noise XT, thus giving rise to an<br>implicit latent space. It corresponds to integrating an ODE in the forward direction, and we can run<br>the process in reverse to get the latents that produce a given real image. Here, we experiment with<br>encoding real images into this latent space and then interpolating between them.</p>",
            "id": 187,
            "page": 22,
            "text": "The DDIM  sampling process is deterministic given the initial noise XT, thus giving rise to an implicit latent space. It corresponds to integrating an ODE in the forward direction, and we can run the process in reverse to get the latents that produce a given real image. Here, we experiment with encoding real images into this latent space and then interpolating between them."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 605
                },
                {
                    "x": 1378,
                    "y": 605
                },
                {
                    "x": 1378,
                    "y": 654
                },
                {
                    "x": 442,
                    "y": 654
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='188' style='font-size:18px'>Equation 13 for the generative pass in DDIM looks like</p>",
            "id": 188,
            "page": 22,
            "text": "Equation 13 for the generative pass in DDIM looks like"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 806
                },
                {
                    "x": 2106,
                    "y": 806
                },
                {
                    "x": 2106,
                    "y": 898
                },
                {
                    "x": 442,
                    "y": 898
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:16px'>Thus, in the limit of small steps, we can expect the reversal of this ODE in the forward direction<br>looks like</p>",
            "id": 189,
            "page": 22,
            "text": "Thus, in the limit of small steps, we can expect the reversal of this ODE in the forward direction looks like"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1042
                },
                {
                    "x": 2108,
                    "y": 1042
                },
                {
                    "x": 2108,
                    "y": 1276
                },
                {
                    "x": 441,
                    "y": 1276
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:18px'>We found that this reverse ODE approximation gives latents with reasonable reconstructions, even<br>with as few as 250 reverse steps. However, we noticed some noise artifacts when reversing all 250<br>steps, and find that reversing the first 249 steps gives much better reconstructions. To interpolate<br>the latents, class embeddings, and classifier log probabilities, we use cos(0)xo + sin(0)x1 where 0<br>sweeps linearly from 0 to 플.</p>",
            "id": 190,
            "page": 22,
            "text": "We found that this reverse ODE approximation gives latents with reasonable reconstructions, even with as few as 250 reverse steps. However, we noticed some noise artifacts when reversing all 250 steps, and find that reversing the first 249 steps gives much better reconstructions. To interpolate the latents, class embeddings, and classifier log probabilities, we use cos(0)xo + sin(0)x1 where 0 sweeps linearly from 0 to 플."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1295
                },
                {
                    "x": 2107,
                    "y": 1295
                },
                {
                    "x": 2107,
                    "y": 1527
                },
                {
                    "x": 441,
                    "y": 1527
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='191' style='font-size:18px'>Figures 10a through 10c show DDIM latent space interpolations on a class-conditional 256x256<br>model, while varying the classifier scale. The left and rightmost images are ground truth dataset<br>examples, and between them are reconstructed interpolations in DDIM latent space (including both<br>endpoints). We see that the model with no guidance has almost perfect reconstructions due to its high<br>recall, whereas raising the guidance scale to 2.5 only finds approximately similar reconstructions.</p>",
            "id": 191,
            "page": 22,
            "text": "Figures 10a through 10c show DDIM latent space interpolations on a class-conditional 256x256 model, while varying the classifier scale. The left and rightmost images are ground truth dataset examples, and between them are reconstructed interpolations in DDIM latent space (including both endpoints). We see that the model with no guidance has almost perfect reconstructions due to its high recall, whereas raising the guidance scale to 2.5 only finds approximately similar reconstructions."
        },
        {
            "bounding_box": [
                {
                    "x": 568,
                    "y": 1563
                },
                {
                    "x": 1972,
                    "y": 1563
                },
                {
                    "x": 1972,
                    "y": 2774
                },
                {
                    "x": 568,
                    "y": 2774
                }
            ],
            "category": "figure",
            "html": "<figure><img id='192' style='font-size:14px' alt=\"SEED\n***Doing *** 140 외에서는 RAN-AN\nRUI) 1005/301/\" data-coord=\"top-left:(568,1563); bottom-right:(1972,2774)\" /></figure>",
            "id": 192,
            "page": 22,
            "text": "SEED ***Doing *** 140 외에서는 RAN-AN RUI) 1005/301/"
        },
        {
            "bounding_box": [
                {
                    "x": 491,
                    "y": 2787
                },
                {
                    "x": 2055,
                    "y": 2787
                },
                {
                    "x": 2055,
                    "y": 2833
                },
                {
                    "x": 491,
                    "y": 2833
                }
            ],
            "category": "caption",
            "html": "<br><caption id='193' style='font-size:14px'>Figure 10a: DDIM latent reconstructions and interpolations on real images with no classifier guidance.</caption>",
            "id": 193,
            "page": 22,
            "text": "Figure 10a: DDIM latent reconstructions and interpolations on real images with no classifier guidance."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3089
                },
                {
                    "x": 1300,
                    "y": 3089
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1248,
                    "y": 3132
                }
            ],
            "category": "footer",
            "html": "<footer id='194' style='font-size:14px'>22</footer>",
            "id": 194,
            "page": 22,
            "text": "22"
        },
        {
            "bounding_box": [
                {
                    "x": 562,
                    "y": 351
                },
                {
                    "x": 1976,
                    "y": 351
                },
                {
                    "x": 1976,
                    "y": 1568
                },
                {
                    "x": 562,
                    "y": 1568
                }
            ],
            "category": "figure",
            "html": "<figure><img id='195' style='font-size:16px' alt=\"FacGbu\n**Daving **Dawning 14.D 重复 00100 盟\" data-coord=\"top-left:(562,351); bottom-right:(1976,1568)\" /></figure>",
            "id": 195,
            "page": 23,
            "text": "FacGbu **Daving **Dawning 14.D 重复 00100 盟"
        },
        {
            "bounding_box": [
                {
                    "x": 522,
                    "y": 1579
                },
                {
                    "x": 2023,
                    "y": 1579
                },
                {
                    "x": 2023,
                    "y": 1624
                },
                {
                    "x": 522,
                    "y": 1624
                }
            ],
            "category": "caption",
            "html": "<br><caption id='196' style='font-size:16px'>Figure 10b: DDIM latent reconstructions and interpolations on real images with classifier scale 1.0.</caption>",
            "id": 196,
            "page": 23,
            "text": "Figure 10b: DDIM latent reconstructions and interpolations on real images with classifier scale 1.0."
        },
        {
            "bounding_box": [
                {
                    "x": 569,
                    "y": 1647
                },
                {
                    "x": 1971,
                    "y": 1647
                },
                {
                    "x": 1971,
                    "y": 2859
                },
                {
                    "x": 569,
                    "y": 2859
                }
            ],
            "category": "figure",
            "html": "<figure><img id='197' style='font-size:14px' alt=\"PIPHO\nSWIPS9\n**Daving X*Doumma\" data-coord=\"top-left:(569,1647); bottom-right:(1971,2859)\" /></figure>",
            "id": 197,
            "page": 23,
            "text": "PIPHO SWIPS9 **Daving X*Doumma"
        },
        {
            "bounding_box": [
                {
                    "x": 518,
                    "y": 2868
                },
                {
                    "x": 2027,
                    "y": 2868
                },
                {
                    "x": 2027,
                    "y": 2916
                },
                {
                    "x": 518,
                    "y": 2916
                }
            ],
            "category": "caption",
            "html": "<br><caption id='198' style='font-size:18px'>Figure 10c: DDIM latent reconstructions and interpolations on real images with classifier scale 2.5.</caption>",
            "id": 198,
            "page": 23,
            "text": "Figure 10c: DDIM latent reconstructions and interpolations on real images with classifier scale 2.5."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3091
                },
                {
                    "x": 1297,
                    "y": 3091
                },
                {
                    "x": 1297,
                    "y": 3130
                },
                {
                    "x": 1249,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='199' style='font-size:20px'>23</footer>",
            "id": 199,
            "page": 23,
            "text": "23"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 298
                },
                {
                    "x": 1234,
                    "y": 298
                },
                {
                    "x": 1234,
                    "y": 358
                },
                {
                    "x": 442,
                    "y": 358
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:22px'>G Reduced Temperature Sampling</p>",
            "id": 200,
            "page": 24,
            "text": "G Reduced Temperature Sampling"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 400
                },
                {
                    "x": 2111,
                    "y": 400
                },
                {
                    "x": 2111,
                    "y": 769
                },
                {
                    "x": 442,
                    "y": 769
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:16px'>We achieved our best ImageNet samples by reducing the diversity of our models using classifier<br>guidance. For many classes of generative models, there is a much simpler way to reduce diversity:<br>reducing the temperature [1]. The temperature parameter T is typically setup SO that T = 1.0 corre-<br>sponds to standard sampling, and T < 1.0 focuses more on high-density samples. We experimented<br>with two ways of implementing this for diffusion models: first, by scaling the Gaussian noise used<br>for each transition by T, and second by dividing E0(xt) by T. The latter implementation makes<br>sense when thinking about 6 as a re-scaled score function (see Section 4.2), and scaling up the score<br>function is similar to scaling up classifier gradients.</p>",
            "id": 201,
            "page": 24,
            "text": "We achieved our best ImageNet samples by reducing the diversity of our models using classifier guidance. For many classes of generative models, there is a much simpler way to reduce diversity: reducing the temperature . The temperature parameter T is typically setup SO that T = 1.0 corresponds to standard sampling, and T < 1.0 focuses more on high-density samples. We experimented with two ways of implementing this for diffusion models: first, by scaling the Gaussian noise used for each transition by T, and second by dividing E0(xt) by T. The latter implementation makes sense when thinking about 6 as a re-scaled score function (see Section 4.2), and scaling up the score function is similar to scaling up classifier gradients."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 787
                },
                {
                    "x": 2112,
                    "y": 787
                },
                {
                    "x": 2112,
                    "y": 1067
                },
                {
                    "x": 441,
                    "y": 1067
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='202' style='font-size:18px'>To measure how temperature scaling affects samples, we experimented with our ImageNet 128x 128<br>model, evaluating FID, Precision, and Recall across different temperatures (Figure 11). We find<br>that two techniques behave similarly, and neither technique provides any substantial improvement in<br>our evaluation metrics. We also find that low temperatures have both low precision and low recall,<br>indicating that the model is not focusing on modes of the real data distribution. Figure 12 highlights<br>this effect, indicating that reducing temperature produces blurry, smooth images.</p>",
            "id": 202,
            "page": 24,
            "text": "To measure how temperature scaling affects samples, we experimented with our ImageNet 128x 128 model, evaluating FID, Precision, and Recall across different temperatures (Figure 11). We find that two techniques behave similarly, and neither technique provides any substantial improvement in our evaluation metrics. We also find that low temperatures have both low precision and low recall, indicating that the model is not focusing on modes of the real data distribution. Figure 12 highlights this effect, indicating that reducing temperature produces blurry, smooth images."
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1110
                },
                {
                    "x": 2081,
                    "y": 1110
                },
                {
                    "x": 2081,
                    "y": 1479
                },
                {
                    "x": 467,
                    "y": 1479
                }
            ],
            "category": "figure",
            "html": "<figure><img id='203' style='font-size:14px' alt=\"20\n0.7\nnoise temperature 0.65\nepsilon temperature\n15 0.60\n0.6\nPrecision\n0.55\n문 10\n0.5 Recall\n0.50\n5 noise temperature noise temperature\n0.45\n0.4 epsilon temperature epsilon temperature\n0\n10-3 10-2 10-3 10-2 10-3 10-2\n1 - temperature 1 - temperature 1 - temperature\" data-coord=\"top-left:(467,1110); bottom-right:(2081,1479)\" /></figure>",
            "id": 203,
            "page": 24,
            "text": "20 0.7 noise temperature 0.65 epsilon temperature 15 0.60 0.6 Precision 0.55 문 10 0.5 Recall 0.50 5 noise temperature noise temperature 0.45 0.4 epsilon temperature epsilon temperature 0 10-3 10-2 10-3 10-2 10-3 10-2 1 - temperature 1 - temperature 1 - temperature"
        },
        {
            "bounding_box": [
                {
                    "x": 595,
                    "y": 1529
                },
                {
                    "x": 1952,
                    "y": 1529
                },
                {
                    "x": 1952,
                    "y": 1580
                },
                {
                    "x": 595,
                    "y": 1580
                }
            ],
            "category": "caption",
            "html": "<caption id='204' style='font-size:18px'>Figure 11: The effect of changing temperature for an ImageNet 128x 128 model.</caption>",
            "id": 204,
            "page": 24,
            "text": "Figure 11: The effect of changing temperature for an ImageNet 128x 128 model."
        },
        {
            "bounding_box": [
                {
                    "x": 475,
                    "y": 1649
                },
                {
                    "x": 2075,
                    "y": 1649
                },
                {
                    "x": 2075,
                    "y": 2403
                },
                {
                    "x": 475,
                    "y": 2403
                }
            ],
            "category": "figure",
            "html": "<figure><img id='205' style='font-size:14px' alt=\"♥RIFD♥ 2017\n- HER\n11\nd\" data-coord=\"top-left:(475,1649); bottom-right:(2075,2403)\" /></figure>",
            "id": 205,
            "page": 24,
            "text": "♥RIFD♥ 2017 - HER 11 d"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2445
                },
                {
                    "x": 2036,
                    "y": 2445
                },
                {
                    "x": 2036,
                    "y": 2495
                },
                {
                    "x": 512,
                    "y": 2495
                }
            ],
            "category": "caption",
            "html": "<caption id='206' style='font-size:20px'>Figure 12: Samples at temperature 0.98 with epsilon scaling (left) and noise scaling (right).</caption>",
            "id": 206,
            "page": 24,
            "text": "Figure 12: Samples at temperature 0.98 with epsilon scaling (left) and noise scaling (right)."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1248,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='207' style='font-size:16px'>24</footer>",
            "id": 207,
            "page": 24,
            "text": "24"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 299
                },
                {
                    "x": 1175,
                    "y": 299
                },
                {
                    "x": 1175,
                    "y": 354
                },
                {
                    "x": 444,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:18px'>H Conditional Diffusion Process</p>",
            "id": 208,
            "page": 25,
            "text": "H Conditional Diffusion Process"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 431
                },
                {
                    "x": 2107,
                    "y": 431
                },
                {
                    "x": 2107,
                    "y": 568
                },
                {
                    "x": 443,
                    "y": 568
                }
            ],
            "category": "paragraph",
            "html": "<p id='209' style='font-size:14px'>In this section, we show that conditional sampling can be achieved with a transition operator<br>proportional to pe(xt|xt+1)po(y|xt), where pe(xt|xt+1) approximates q(xt|xt+1) and p⌀(y|xt)<br>approximates the label distribution for a noised sample xt.</p>",
            "id": 209,
            "page": 25,
            "text": "In this section, we show that conditional sampling can be achieved with a transition operator proportional to pe(xt|xt+1)po(y|xt), where pe(xt|xt+1) approximates q(xt|xt+1) and p⌀(y|xt) approximates the label distribution for a noised sample xt."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 591
                },
                {
                    "x": 2106,
                    "y": 591
                },
                {
                    "x": 2106,
                    "y": 681
                },
                {
                    "x": 442,
                    "y": 681
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='210' style='font-size:14px'>We start by defining a conditional Markovian noising process q similar to 9, and assume that q(y|xo)<br>is a known and readily available label distribution for each sample.</p>",
            "id": 210,
            "page": 25,
            "text": "We start by defining a conditional Markovian noising process q similar to 9, and assume that q(y|xo) is a known and readily available label distribution for each sample."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1150
                },
                {
                    "x": 2106,
                    "y": 1150
                },
                {
                    "x": 2106,
                    "y": 1291
                },
                {
                    "x": 442,
                    "y": 1291
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:14px'>While we defined the noising process 9 conditioned on y, we can prove that q behaves exactly like<br>q when not conditioned on y. Along these lines, we first derive the unconditional noising operator<br>q(xt+1|xt):</p>",
            "id": 211,
            "page": 25,
            "text": "While we defined the noising process 9 conditioned on y, we can prove that q behaves exactly like q when not conditioned on y. Along these lines, we first derive the unconditional noising operator q(xt+1|xt):"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2021
                },
                {
                    "x": 1536,
                    "y": 2021
                },
                {
                    "x": 1536,
                    "y": 2071
                },
                {
                    "x": 444,
                    "y": 2071
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:14px'>Following similar logic, we find the joint distribution q(x1:T|x0):</p>",
            "id": 212,
            "page": 25,
            "text": "Following similar logic, we find the joint distribution q(x1:T|x0):"
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3092
                },
                {
                    "x": 1298,
                    "y": 3092
                },
                {
                    "x": 1298,
                    "y": 3129
                },
                {
                    "x": 1249,
                    "y": 3129
                }
            ],
            "category": "footer",
            "html": "<footer id='213' style='font-size:14px'>25</footer>",
            "id": 213,
            "page": 25,
            "text": "25"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 308
                },
                {
                    "x": 1208,
                    "y": 308
                },
                {
                    "x": 1208,
                    "y": 355
                },
                {
                    "x": 443,
                    "y": 355
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:16px'>Using Equation 44, we can now derive q(xt):</p>",
            "id": 214,
            "page": 26,
            "text": "Using Equation 44, we can now derive q(xt):"
        },
        {
            "bounding_box": [
                {
                    "x": 2024,
                    "y": 913
                },
                {
                    "x": 2106,
                    "y": 913
                },
                {
                    "x": 2106,
                    "y": 963
                },
                {
                    "x": 2024,
                    "y": 963
                }
            ],
            "category": "paragraph",
            "html": "<p id='215' style='font-size:20px'>(50)</p>",
            "id": 215,
            "page": 26,
            "text": "(50)"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1010
                },
                {
                    "x": 2103,
                    "y": 1010
                },
                {
                    "x": 2103,
                    "y": 1105
                },
                {
                    "x": 443,
                    "y": 1105
                }
            ],
            "category": "paragraph",
            "html": "<p id='216' style='font-size:16px'>Using the identities q(xt) = q(xt) and q(xt+1|xt) = q(xt+1|xt), it is trivial to show via Bayes rule<br>that the unconditional reverse process q(xt|xt+1) = q(xt|xt+1).</p>",
            "id": 216,
            "page": 26,
            "text": "Using the identities q(xt) = q(xt) and q(xt+1|xt) = q(xt+1|xt), it is trivial to show via Bayes rule that the unconditional reverse process q(xt|xt+1) = q(xt|xt+1)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1127
                },
                {
                    "x": 2106,
                    "y": 1127
                },
                {
                    "x": 2106,
                    "y": 1261
                },
                {
                    "x": 442,
                    "y": 1261
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='217' style='font-size:14px'>One observation about �is that it gives rise to a noisy classification function, q(y|xt). We can show<br>that this classification distribution does not depend on Xt+1 (a noisier version of xt), a fact which we<br>will later use:</p>",
            "id": 217,
            "page": 26,
            "text": "One observation about �is that it gives rise to a noisy classification function, q(y|xt). We can show that this classification distribution does not depend on Xt+1 (a noisier version of xt), a fact which we will later use:"
        },
        {
            "bounding_box": [
                {
                    "x": 2025,
                    "y": 1569
                },
                {
                    "x": 2106,
                    "y": 1569
                },
                {
                    "x": 2106,
                    "y": 1617
                },
                {
                    "x": 2025,
                    "y": 1617
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:18px'>(54)</p>",
            "id": 218,
            "page": 26,
            "text": "(54)"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1667
                },
                {
                    "x": 1299,
                    "y": 1667
                },
                {
                    "x": 1299,
                    "y": 1711
                },
                {
                    "x": 445,
                    "y": 1711
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:14px'>We can now derive the conditional reverse process:</p>",
            "id": 219,
            "page": 26,
            "text": "We can now derive the conditional reverse process:"
        },
        {
            "bounding_box": [
                {
                    "x": 2024,
                    "y": 2428
                },
                {
                    "x": 2107,
                    "y": 2428
                },
                {
                    "x": 2107,
                    "y": 2477
                },
                {
                    "x": 2024,
                    "y": 2477
                }
            ],
            "category": "paragraph",
            "html": "<p id='220' style='font-size:20px'>(61)</p>",
            "id": 220,
            "page": 26,
            "text": "(61)"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2525
                },
                {
                    "x": 2105,
                    "y": 2525
                },
                {
                    "x": 2105,
                    "y": 2756
                },
                {
                    "x": 441,
                    "y": 2756
                }
            ],
            "category": "paragraph",
            "html": "<p id='221' style='font-size:14px'>The q(y|xt+1) term can be treated as a constant since it does not depend on xt. We thus want to<br>sample from the distribution Zq(xt|xt+1)q(y|xt) where Z is a normalizing constant. We already<br>have a neural network approximation of q(xt|xt+1), called pe(xt|xt+1), SO all that is left is an<br>approximation of q(y|xt). This can be obtained by training a classifier p⌀(y|xt) on noised images Xt<br>derived by sampling from q(xt).</p>",
            "id": 221,
            "page": 26,
            "text": "The q(y|xt+1) term can be treated as a constant since it does not depend on xt. We thus want to sample from the distribution Zq(xt|xt+1)q(y|xt) where Z is a normalizing constant. We already have a neural network approximation of q(xt|xt+1), called pe(xt|xt+1), SO all that is left is an approximation of q(y|xt). This can be obtained by training a classifier p⌀(y|xt) on noised images Xt derived by sampling from q(xt)."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3093
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1249,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='222' style='font-size:14px'>26</footer>",
            "id": 222,
            "page": 26,
            "text": "26"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 301
                },
                {
                    "x": 903,
                    "y": 301
                },
                {
                    "x": 903,
                    "y": 356
                },
                {
                    "x": 445,
                    "y": 356
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:22px'>I Hyperparameters</p>",
            "id": 223,
            "page": 27,
            "text": "I Hyperparameters"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 402
                },
                {
                    "x": 2110,
                    "y": 402
                },
                {
                    "x": 2110,
                    "y": 584
                },
                {
                    "x": 443,
                    "y": 584
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:20px'>When choosing optimal classifier scales for our sampler, we swept over [0.5, 1, 2] for ImageNet<br>128 x 128 and ImageNet 256x256, and [1, 2, 3, 3.5, 4, 4.5, 5] for ImageNet 512x512. For DDIM,<br>we swept over values [0.5, 0.75, 1.0, 1.25, 2] for ImageNet 128x 128, [0.5, 1, 1.5, 2, 2.5, 3, 3.5] for<br>ImageNet 256x256, and [3, 4, 5, 6, 7, 9, 11] for ImageNet 512x512.</p>",
            "id": 224,
            "page": 27,
            "text": "When choosing optimal classifier scales for our sampler, we swept over [0.5, 1, 2] for ImageNet 128 x 128 and ImageNet 256x256, and [1, 2, 3, 3.5, 4, 4.5, 5] for ImageNet 512x512. For DDIM, we swept over values [0.5, 0.75, 1.0, 1.25, 2] for ImageNet 128x 128, [0.5, 1, 1.5, 2, 2.5, 3, 3.5] for ImageNet 256x256, and  for ImageNet 512x512."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 607
                },
                {
                    "x": 2109,
                    "y": 607
                },
                {
                    "x": 2109,
                    "y": 881
                },
                {
                    "x": 442,
                    "y": 881
                }
            ],
            "category": "paragraph",
            "html": "<p id='225' style='font-size:18px'>Hyperparameters for training the diffusion and classification models are in Table 11 and Table 12<br>respectively. Hyperparameters for guided sampling are in Table 14. Hyperparameters used to train<br>upsampling models are in Table 13. We train all of our models using Adam [29] or Adam W [35]<br>with B1 = 0.9 and B2 = 0.999. We train in 16-bit precision using loss-scaling [38], but maintain<br>32-bit weights, EMA, and optimizer state. We use an EMA rate of 0.9999 for all experiments. We<br>use PyTorch [46], and train on NVIDIA Tesla V100s.</p>",
            "id": 225,
            "page": 27,
            "text": "Hyperparameters for training the diffusion and classification models are in Table 11 and Table 12 respectively. Hyperparameters for guided sampling are in Table 14. Hyperparameters used to train upsampling models are in Table 13. We train all of our models using Adam  or Adam W  with B1 = 0.9 and B2 = 0.999. We train in 16-bit precision using loss-scaling , but maintain 32-bit weights, EMA, and optimizer state. We use an EMA rate of 0.9999 for all experiments. We use PyTorch , and train on NVIDIA Tesla V100s."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 902
                },
                {
                    "x": 2112,
                    "y": 902
                },
                {
                    "x": 2112,
                    "y": 1132
                },
                {
                    "x": 441,
                    "y": 1132
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='226' style='font-size:18px'>For all architecture ablations, we train with batch size 256, and sample using 250 sampling steps.<br>For our attention heads ablations, we use 128 base channels, 2 residual blocks per resolution, multi-<br>resolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. By<br>default, all of our experiments use adaptive group normalization, except when explicitly ablating for<br>it.</p>",
            "id": 226,
            "page": 27,
            "text": "For all architecture ablations, we train with batch size 256, and sample using 250 sampling steps. For our attention heads ablations, we use 128 base channels, 2 residual blocks per resolution, multiresolution attention, and BigGAN up/downsampling, and we train the models for 700K iterations. By default, all of our experiments use adaptive group normalization, except when explicitly ablating for it."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1152
                },
                {
                    "x": 2108,
                    "y": 1152
                },
                {
                    "x": 2108,
                    "y": 1291
                },
                {
                    "x": 443,
                    "y": 1291
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='227' style='font-size:16px'>When sampling with 1000 timesteps, we use the same noise schedule as for training. On ImageNet,<br>we use the uniform stride from Nichol and Dhariwal [43] for 250 step samples and the slightly<br>different uniform stride from Song et al. [57] for 25 step DDIM.</p>",
            "id": 227,
            "page": 27,
            "text": "When sampling with 1000 timesteps, we use the same noise schedule as for training. On ImageNet, we use the uniform stride from Nichol and Dhariwal  for 250 step samples and the slightly different uniform stride from Song   for 25 step DDIM."
        },
        {
            "bounding_box": [
                {
                    "x": 484,
                    "y": 1334
                },
                {
                    "x": 2069,
                    "y": 1334
                },
                {
                    "x": 2069,
                    "y": 2007
                },
                {
                    "x": 484,
                    "y": 2007
                }
            ],
            "category": "table",
            "html": "<table id='228' style='font-size:14px'><tr><td></td><td>LSUN</td><td>ImageNet 64</td><td>ImageNet 128</td><td>ImageNet 256</td><td>ImageNet 512</td></tr><tr><td>Diffusion steps</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Noise Schedule</td><td>linear</td><td>cosine</td><td>linear</td><td>linear</td><td>linear</td></tr><tr><td>Model size</td><td>552M</td><td>296M</td><td>422M</td><td>554M</td><td>559M</td></tr><tr><td>Channels</td><td>256</td><td>192</td><td>256</td><td>256</td><td>256</td></tr><tr><td>Depth</td><td>2</td><td>3</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Channels multiple</td><td>1,1,2,2,4,4</td><td>1,2,3,4</td><td>1,1,2,3,4</td><td>1,1,2,2,4,4</td><td>0.5,1,1,2,2,4,4</td></tr><tr><td>Heads</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Heads Channels</td><td>64</td><td>64</td><td></td><td>64</td><td>64</td></tr><tr><td>Attention resolution</td><td>32,16,8</td><td>32,16,8</td><td>32,16,8</td><td>32,16,8</td><td>32,16,8</td></tr><tr><td>BigGAN up/downsample</td><td>V</td><td>V</td><td>V</td><td>V</td><td>V</td></tr><tr><td>Dropout</td><td>0.1</td><td>0.1</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Batch size</td><td>256</td><td>2048</td><td>256</td><td>256</td><td>256</td></tr><tr><td>Iterations</td><td>varies*</td><td>540K</td><td>4360K</td><td>1980K</td><td>1940K</td></tr><tr><td>Learning Rate</td><td>1e-4</td><td>3e-4</td><td>1e-4</td><td>1e-4</td><td>1e-4</td></tr></table>",
            "id": 228,
            "page": 27,
            "text": "LSUN ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512  Diffusion steps 1000 1000 1000 1000 1000  Noise Schedule linear cosine linear linear linear  Model size 552M 296M 422M 554M 559M  Channels 256 192 256 256 256  Depth 2 3 2 2 2  Channels multiple 1,1,2,2,4,4 1,2,3,4 1,1,2,3,4 1,1,2,2,4,4 0.5,1,1,2,2,4,4  Heads   4    Heads Channels 64 64  64 64  Attention resolution 32,16,8 32,16,8 32,16,8 32,16,8 32,16,8  BigGAN up/downsample V V V V V  Dropout 0.1 0.1 0.0 0.0 0.0  Batch size 256 2048 256 256 256  Iterations varies* 540K 4360K 1980K 1940K  Learning Rate 1e-4 3e-4 1e-4 1e-4"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2027
                },
                {
                    "x": 2103,
                    "y": 2027
                },
                {
                    "x": 2103,
                    "y": 2117
                },
                {
                    "x": 443,
                    "y": 2117
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='229' style='font-size:18px'>Table 11: Hyperparameters for diffusion models. *We used 200K iterations for LSUN cat, 250K for<br>LSUN horse, and 500K for LSUN bedroom.</p>",
            "id": 229,
            "page": 27,
            "text": "Table 11: Hyperparameters for diffusion models. *We used 200K iterations for LSUN cat, 250K for LSUN horse, and 500K for LSUN bedroom."
        },
        {
            "bounding_box": [
                {
                    "x": 532,
                    "y": 2163
                },
                {
                    "x": 2020,
                    "y": 2163
                },
                {
                    "x": 2020,
                    "y": 2835
                },
                {
                    "x": 532,
                    "y": 2835
                }
            ],
            "category": "table",
            "html": "<table id='230' style='font-size:14px'><tr><td></td><td>ImageNet 64</td><td>ImageNet 128</td><td>ImageNet 256</td><td>ImageNet 512</td></tr><tr><td>Diffusion steps</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Noise Schedule</td><td>cosine</td><td>linear</td><td>linear</td><td>linear</td></tr><tr><td>Model size</td><td>65M</td><td>43M</td><td>54M</td><td>54M</td></tr><tr><td>Channels</td><td>128</td><td>128</td><td>128</td><td>128</td></tr><tr><td>Depth</td><td>4</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Channels multiple</td><td>1,2,3,4</td><td>1,1,2,3,4</td><td>1,1,2,2,4,4</td><td>0.5,1,1,2,2,4,4</td></tr><tr><td>Heads Channels</td><td>64</td><td>64</td><td>64</td><td>64</td></tr><tr><td>Attention resolution</td><td>32,16,8</td><td>32,16,8</td><td>32,16,8</td><td>32,16,8</td></tr><tr><td>BigGAN up/downsample</td><td>V</td><td>V</td><td>V</td><td>V</td></tr><tr><td>Attention pooling</td><td>V</td><td>V</td><td>V</td><td>V</td></tr><tr><td>Weight decay</td><td>0.2</td><td>0.05</td><td>0.05</td><td>0.05</td></tr><tr><td>Batch size</td><td>1024</td><td>256*</td><td>256</td><td>256</td></tr><tr><td>Iterations</td><td>300K</td><td>300K</td><td>500K</td><td>500K</td></tr><tr><td>Learning rate</td><td>6e-4</td><td>3e-4*</td><td>3e-4</td><td>3e-4</td></tr></table>",
            "id": 230,
            "page": 27,
            "text": "ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512  Diffusion steps 1000 1000 1000 1000  Noise Schedule cosine linear linear linear  Model size 65M 43M 54M 54M  Channels 128 128 128 128  Depth 4 2 2 2  Channels multiple 1,2,3,4 1,1,2,3,4 1,1,2,2,4,4 0.5,1,1,2,2,4,4  Heads Channels 64 64 64 64  Attention resolution 32,16,8 32,16,8 32,16,8 32,16,8  BigGAN up/downsample V V V V  Attention pooling V V V V  Weight decay 0.2 0.05 0.05 0.05  Batch size 1024 256* 256 256  Iterations 300K 300K 500K 500K  Learning rate 6e-4 3e-4* 3e-4"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2854
                },
                {
                    "x": 2108,
                    "y": 2854
                },
                {
                    "x": 2108,
                    "y": 2991
                },
                {
                    "x": 441,
                    "y": 2991
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='231' style='font-size:16px'>Table 12: Hyperparameters for classification models. *For our ImageNet 128 x 128 → 512x512<br>upsamples, we used a different classifier for the base model, with batch size 1024 and learning rate<br>6e-5.</p>",
            "id": 231,
            "page": 27,
            "text": "Table 12: Hyperparameters for classification models. *For our ImageNet 128 x 128 → 512x512 upsamples, we used a different classifier for the base model, with batch size 1024 and learning rate 6e-5."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3092
                },
                {
                    "x": 1297,
                    "y": 3092
                },
                {
                    "x": 1297,
                    "y": 3130
                },
                {
                    "x": 1249,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='232' style='font-size:14px'>27</footer>",
            "id": 232,
            "page": 27,
            "text": "27"
        },
        {
            "bounding_box": [
                {
                    "x": 686,
                    "y": 719
                },
                {
                    "x": 1864,
                    "y": 719
                },
                {
                    "x": 1864,
                    "y": 1395
                },
                {
                    "x": 686,
                    "y": 1395
                }
            ],
            "category": "table",
            "html": "<table id='233' style='font-size:14px'><tr><td></td><td>ImageNet 64 → 256</td><td>ImageNet 128 → 512</td></tr><tr><td>Diffusion steps</td><td>1000</td><td>1000</td></tr><tr><td>Noise Schedule</td><td>linear</td><td>linear</td></tr><tr><td>Model size</td><td>312M</td><td>309M</td></tr><tr><td>Channels</td><td>192</td><td>192</td></tr><tr><td>Depth</td><td>2</td><td>2</td></tr><tr><td>Channels multiple</td><td>1,1,2,2,4,4</td><td>1,1,2,2,4,4*</td></tr><tr><td>Heads</td><td>4</td><td></td></tr><tr><td>Heads Channels</td><td></td><td>64</td></tr><tr><td>Attention resolution</td><td>32,16,8</td><td>32,16,8</td></tr><tr><td>BigGAN up/downsample</td><td>V</td><td>V</td></tr><tr><td>Dropout</td><td>0.0</td><td>0.0</td></tr><tr><td>Batch size</td><td>256</td><td>256</td></tr><tr><td>Iterations</td><td>500K</td><td>1050K</td></tr><tr><td>Learning Rate</td><td>1e-4</td><td>1e-4</td></tr></table>",
            "id": 233,
            "page": 28,
            "text": "ImageNet 64 → 256 ImageNet 128 → 512  Diffusion steps 1000 1000  Noise Schedule linear linear  Model size 312M 309M  Channels 192 192  Depth 2 2  Channels multiple 1,1,2,2,4,4 1,1,2,2,4,4*  Heads 4   Heads Channels  64  Attention resolution 32,16,8 32,16,8  BigGAN up/downsample V V  Dropout 0.0 0.0  Batch size 256 256  Iterations 500K 1050K  Learning Rate 1e-4"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1414
                },
                {
                    "x": 2106,
                    "y": 1414
                },
                {
                    "x": 2106,
                    "y": 1510
                },
                {
                    "x": 440,
                    "y": 1510
                }
            ],
            "category": "caption",
            "html": "<br><caption id='234' style='font-size:18px'>Table 13: Hyperparameters for upsampling diffusion models. *We chose this as an optimization, with<br>the intuition that a lower-resolution path should be unnecessary for upsampling 128x128 images.</caption>",
            "id": 234,
            "page": 28,
            "text": "Table 13: Hyperparameters for upsampling diffusion models. *We chose this as an optimization, with the intuition that a lower-resolution path should be unnecessary for upsampling 128x128 images."
        },
        {
            "bounding_box": [
                {
                    "x": 480,
                    "y": 2365
                },
                {
                    "x": 2067,
                    "y": 2365
                },
                {
                    "x": 2067,
                    "y": 2537
                },
                {
                    "x": 480,
                    "y": 2537
                }
            ],
            "category": "table",
            "html": "<table id='235' style='font-size:14px'><tr><td></td><td>ImageNet 64</td><td>ImageNet 128</td><td>ImageNet 256</td><td>ImageNet 512</td></tr><tr><td>Gradient Scale (250 steps)</td><td>1.0</td><td>0.5</td><td>1.0</td><td>4.0</td></tr><tr><td>Gradient Scale (DDIM, 25 steps)</td><td>-</td><td>1.25</td><td>2.5</td><td>9.0</td></tr></table>",
            "id": 235,
            "page": 28,
            "text": "ImageNet 64 ImageNet 128 ImageNet 256 ImageNet 512  Gradient Scale (250 steps) 1.0 0.5 1.0 4.0  Gradient Scale (DDIM, 25 steps) - 1.25 2.5"
        },
        {
            "bounding_box": [
                {
                    "x": 785,
                    "y": 2555
                },
                {
                    "x": 1760,
                    "y": 2555
                },
                {
                    "x": 1760,
                    "y": 2603
                },
                {
                    "x": 785,
                    "y": 2603
                }
            ],
            "category": "caption",
            "html": "<br><caption id='236' style='font-size:20px'>Table 14: Hyperparameters for classifier-guided sampling.</caption>",
            "id": 236,
            "page": 28,
            "text": "Table 14: Hyperparameters for classifier-guided sampling."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1249,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='237' style='font-size:16px'>28</footer>",
            "id": 237,
            "page": 28,
            "text": "28"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 299
                },
                {
                    "x": 1344,
                    "y": 299
                },
                {
                    "x": 1344,
                    "y": 356
                },
                {
                    "x": 443,
                    "y": 356
                }
            ],
            "category": "paragraph",
            "html": "<p id='238' style='font-size:20px'>J Using Fewer Sampling Steps on LSUN</p>",
            "id": 238,
            "page": 29,
            "text": "J Using Fewer Sampling Steps on LSUN"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 400
                },
                {
                    "x": 2109,
                    "y": 400
                },
                {
                    "x": 2109,
                    "y": 678
                },
                {
                    "x": 441,
                    "y": 678
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:16px'>We initially found that our LSUN models achieved much better results when sampling with 1000<br>steps rather than 250 steps, contrary to previous results from Nichol and Dhariwal [43]. To address<br>this, we conducted a sweep over sampling-time noise schedules, finding that an improved schedule<br>can largely close the gap. We swept over schedules on LSUN bedrooms, and selected the schedule<br>with the best FID for use on the other two datasets. Table 15 details the findings of this sweep, and<br>Table 16 applies this schedule to three LSUN datasets.</p>",
            "id": 239,
            "page": 29,
            "text": "We initially found that our LSUN models achieved much better results when sampling with 1000 steps rather than 250 steps, contrary to previous results from Nichol and Dhariwal . To address this, we conducted a sweep over sampling-time noise schedules, finding that an improved schedule can largely close the gap. We swept over schedules on LSUN bedrooms, and selected the schedule with the best FID for use on the other two datasets. Table 15 details the findings of this sweep, and Table 16 applies this schedule to three LSUN datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 697
                },
                {
                    "x": 2107,
                    "y": 697
                },
                {
                    "x": 2107,
                    "y": 836
                },
                {
                    "x": 441,
                    "y": 836
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='240' style='font-size:16px'>While sweeping over sampling schedules is not as expensive as re-training models from scratch, it<br>does require a significant amount of sampling compute. As a result, we did not conduct an exhaustive<br>sweep, and superior schedules are likely to exist.</p>",
            "id": 240,
            "page": 29,
            "text": "While sweeping over sampling schedules is not as expensive as re-training models from scratch, it does require a significant amount of sampling compute. As a result, we did not conduct an exhaustive sweep, and superior schedules are likely to exist."
        },
        {
            "bounding_box": [
                {
                    "x": 1042,
                    "y": 877
                },
                {
                    "x": 1499,
                    "y": 877
                },
                {
                    "x": 1499,
                    "y": 1305
                },
                {
                    "x": 1042,
                    "y": 1305
                }
            ],
            "category": "table",
            "html": "<table id='241' style='font-size:18px'><tr><td>Schedule</td><td>FID</td></tr><tr><td>50, 50, 50, 50, 50</td><td>2.31</td></tr><tr><td>70, 60, 50, 40, 30</td><td>2.17</td></tr><tr><td>90, 50, 40, 40, 30</td><td>2.10</td></tr><tr><td>90, 60, 50, 30, 20</td><td>2.09</td></tr><tr><td>80, 60, 50, 30, 30</td><td>2.09</td></tr><tr><td>90, 50, 50, 30, 30</td><td>2.07</td></tr><tr><td>100, 50, 40, 30, 30</td><td>2.03</td></tr><tr><td>90, 60, 60, 20, 20</td><td>2.02</td></tr></table>",
            "id": 241,
            "page": 29,
            "text": "Schedule FID  50, 50, 50, 50, 50 2.31  70, 60, 50, 40, 30 2.17  90, 50, 40, 40, 30 2.10  90, 60, 50, 30, 20 2.09  80, 60, 50, 30, 30 2.09  90, 50, 50, 30, 30 2.07  100, 50, 40, 30, 30 2.03  90, 60, 60, 20, 20"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1320
                },
                {
                    "x": 2109,
                    "y": 1320
                },
                {
                    "x": 2109,
                    "y": 1552
                },
                {
                    "x": 440,
                    "y": 1552
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:16px'>Table 15: Results of sweeping over 250 step sampling schedules on LSUN bedrooms. The schedule<br>is expressed as a sequence of five integers, where each integer is the number of steps allocated to<br>one fifth of the diffusion process. The first integer corresponding to t E [0, 199] and the last to<br>t E [T - 200, T - 1]. Thus, 50, 50, 50, 50, 50 is a uniform schedule, and 250, 0,0,0, 0 is a schedule<br>where all timesteps are spent near t = 0.</p>",
            "id": 242,
            "page": 29,
            "text": "Table 15: Results of sweeping over 250 step sampling schedules on LSUN bedrooms. The schedule is expressed as a sequence of five integers, where each integer is the number of steps allocated to one fifth of the diffusion process. The first integer corresponding to t E  and the last to t E [T - 200, T - 1]. Thus, 50, 50, 50, 50, 50 is a uniform schedule, and 250, 0,0,0, 0 is a schedule where all timesteps are spent near t = 0."
        },
        {
            "bounding_box": [
                {
                    "x": 870,
                    "y": 1653
                },
                {
                    "x": 1683,
                    "y": 1653
                },
                {
                    "x": 1683,
                    "y": 2366
                },
                {
                    "x": 870,
                    "y": 2366
                }
            ],
            "category": "table",
            "html": "<table id='243' style='font-size:14px'><tr><td>Schedule</td><td>FID</td><td>sFID</td><td>Prec</td><td>Rec</td></tr><tr><td colspan=\"5\">LSUN Bedrooms 256x 256</td></tr><tr><td>1000 steps</td><td>1.90</td><td>5.59</td><td>0.66</td><td>0.51</td></tr><tr><td>250 steps (uniform)</td><td>2.31</td><td>6.12</td><td>0.65</td><td>0.50</td></tr><tr><td>250 steps (sweep)</td><td>2.02</td><td>6.12</td><td>0.67</td><td>0.50</td></tr><tr><td colspan=\"5\">LSUN Horses 256x256</td></tr><tr><td>1000 steps</td><td>2.57</td><td>6.81</td><td>0.71</td><td>0.55</td></tr><tr><td>250 steps (uniform)</td><td>3.45</td><td>7.55</td><td>0.68</td><td>0.56</td></tr><tr><td>250 steps (sweep)</td><td>2.83</td><td>7.08</td><td>0.69</td><td>0.56</td></tr><tr><td colspan=\"5\">LSUN Cat 256x256</td></tr><tr><td>1000 steps</td><td>5.57</td><td>6.69</td><td>0.63</td><td>0.52</td></tr><tr><td>250 steps (uniform)</td><td>7.03</td><td>8.24</td><td>0.60</td><td>0.53</td></tr><tr><td>250 steps (sweep)</td><td>5.94</td><td>7.43</td><td>0.62</td><td>0.52</td></tr></table>",
            "id": 243,
            "page": 29,
            "text": "Schedule FID sFID Prec Rec  LSUN Bedrooms 256x 256  1000 steps 1.90 5.59 0.66 0.51  250 steps (uniform) 2.31 6.12 0.65 0.50  250 steps (sweep) 2.02 6.12 0.67 0.50  LSUN Horses 256x256  1000 steps 2.57 6.81 0.71 0.55  250 steps (uniform) 3.45 7.55 0.68 0.56  250 steps (sweep) 2.83 7.08 0.69 0.56  LSUN Cat 256x256  1000 steps 5.57 6.69 0.63 0.52  250 steps (uniform) 7.03 8.24 0.60 0.53  250 steps (sweep) 5.94 7.43 0.62"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2379
                },
                {
                    "x": 2108,
                    "y": 2379
                },
                {
                    "x": 2108,
                    "y": 2524
                },
                {
                    "x": 440,
                    "y": 2524
                }
            ],
            "category": "caption",
            "html": "<br><caption id='244' style='font-size:18px'>Table 16: Evaluations on LSUN bedrooms, horses, and cats using different sampling schedules. We<br>find that the sweep schedule produces better results than the uniform 250 step schedule on all three<br>datasets, and mostly bridges the gap to the 1000 step schedule.</caption>",
            "id": 244,
            "page": 29,
            "text": "Table 16: Evaluations on LSUN bedrooms, horses, and cats using different sampling schedules. We find that the sweep schedule produces better results than the uniform 250 step schedule on all three datasets, and mostly bridges the gap to the 1000 step schedule."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3129
                },
                {
                    "x": 1249,
                    "y": 3129
                }
            ],
            "category": "footer",
            "html": "<footer id='245' style='font-size:18px'>29</footer>",
            "id": 245,
            "page": 29,
            "text": "29"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 299
                },
                {
                    "x": 1260,
                    "y": 299
                },
                {
                    "x": 1260,
                    "y": 354
                },
                {
                    "x": 445,
                    "y": 354
                }
            ],
            "category": "caption",
            "html": "<caption id='246' style='font-size:20px'>K Samples from ImageNet 512x512</caption>",
            "id": 246,
            "page": 30,
            "text": "K Samples from ImageNet 512x512"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 411
                },
                {
                    "x": 2109,
                    "y": 411
                },
                {
                    "x": 2109,
                    "y": 2917
                },
                {
                    "x": 440,
                    "y": 2917
                }
            ],
            "category": "figure",
            "html": "<figure><img id='247' style='font-size:14px' alt=\"ALK 6 시 VAYYDIS 18 imcorri\nBOTIO\n나 691 FIZIII\nwinders\nacter\" data-coord=\"top-left:(440,411); bottom-right:(2109,2917)\" /></figure>",
            "id": 247,
            "page": 30,
            "text": "ALK 6 시 VAYYDIS 18 imcorri BOTIO 나 691 FIZIII winders acter"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2929
                },
                {
                    "x": 2105,
                    "y": 2929
                },
                {
                    "x": 2105,
                    "y": 3024
                },
                {
                    "x": 444,
                    "y": 3024
                }
            ],
            "category": "caption",
            "html": "<br><caption id='248' style='font-size:18px'>Figure 13: Samples from our best 512x512 model (FID: 3.85). Classes are 1: goldfish, 279: arctic<br>fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball.</caption>",
            "id": 248,
            "page": 30,
            "text": "Figure 13: Samples from our best 512x512 model (FID: 3.85). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1250,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='249' style='font-size:16px'>30</footer>",
            "id": 249,
            "page": 30,
            "text": "30"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 286
                },
                {
                    "x": 2111,
                    "y": 286
                },
                {
                    "x": 2111,
                    "y": 2787
                },
                {
                    "x": 441,
                    "y": 2787
                }
            ],
            "category": "figure",
            "html": "<figure><img id='250' style='font-size:14px' alt=\"师超声 이번가 $ 光\n018:90p\n사람들\" data-coord=\"top-left:(441,286); bottom-right:(2111,2787)\" /></figure>",
            "id": 250,
            "page": 31,
            "text": "师超声 이번가 $ 光 018:90p 사람들"
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 2803
                },
                {
                    "x": 2105,
                    "y": 2803
                },
                {
                    "x": 2105,
                    "y": 2891
                },
                {
                    "x": 447,
                    "y": 2891
                }
            ],
            "category": "caption",
            "html": "<br><caption id='251' style='font-size:20px'>Figure 14: Samples from our best 512x512 model (FID: 3.85). Classes are 933: cheeseburger, 562:<br>fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.</caption>",
            "id": 251,
            "page": 31,
            "text": "Figure 14: Samples from our best 512x512 model (FID: 3.85). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3092
                },
                {
                    "x": 1297,
                    "y": 3092
                },
                {
                    "x": 1297,
                    "y": 3131
                },
                {
                    "x": 1250,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='252' style='font-size:16px'>31</footer>",
            "id": 252,
            "page": 31,
            "text": "31"
        },
        {
            "bounding_box": [
                {
                    "x": 437,
                    "y": 283
                },
                {
                    "x": 2112,
                    "y": 283
                },
                {
                    "x": 2112,
                    "y": 2791
                },
                {
                    "x": 437,
                    "y": 2791
                }
            ],
            "category": "figure",
            "html": "<figure><img id='253' style='font-size:14px' alt=\"MAINBURE\nFINIHE INI ANOAM OWNG A.M �LITK ONIN BeHtHz FND\nGIN raMMA JN~O.UCT EISK\nTAFMENE\nhellinDsg\nLAMANDBASKI\nPOWERTS\nSTR Aisj\nCU+\nXIAR\n소 ���\nINVATIVITION,\n1309FANOOSTVUL\nrespenning\" data-coord=\"top-left:(437,283); bottom-right:(2112,2791)\" /></figure>",
            "id": 253,
            "page": 32,
            "text": "MAINBURE FINIHE INI ANOAM OWNG A.M �LITK ONIN BeHtHz FND GIN raMMA JN~O.UCT EISK TAFMENE hellinDsg LAMANDBASKI POWERTS STR Aisj CU+ XIAR 소 ��� INVATIVITION, 1309FANOOSTVUL respenning"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2801
                },
                {
                    "x": 2106,
                    "y": 2801
                },
                {
                    "x": 2106,
                    "y": 2893
                },
                {
                    "x": 444,
                    "y": 2893
                }
            ],
            "category": "caption",
            "html": "<br><caption id='254' style='font-size:20px'>Figure 15: Difficult class samples from our best 512x512 model (FID: 3.85). Classes are 432:<br>bassoon, 468: cab, 424: barbershop, 444: bicycle-built-for-two, 981: ballplayer, 550: espresso maker.</caption>",
            "id": 254,
            "page": 32,
            "text": "Figure 15: Difficult class samples from our best 512x512 model (FID: 3.85). Classes are 432: bassoon, 468: cab, 424: barbershop, 444: bicycle-built-for-two, 981: ballplayer, 550: espresso maker."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1250,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='255' style='font-size:16px'>32</footer>",
            "id": 255,
            "page": 32,
            "text": "32"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 312
                },
                {
                    "x": 2110,
                    "y": 312
                },
                {
                    "x": 2110,
                    "y": 2812
                },
                {
                    "x": 440,
                    "y": 2812
                }
            ],
            "category": "figure",
            "html": "<figure><img id='256' style='font-size:14px' alt=\"CESTRIAL\ndis\" data-coord=\"top-left:(440,312); bottom-right:(2110,2812)\" /></figure>",
            "id": 256,
            "page": 33,
            "text": "CESTRIAL dis"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2830
                },
                {
                    "x": 2107,
                    "y": 2830
                },
                {
                    "x": 2107,
                    "y": 2969
                },
                {
                    "x": 445,
                    "y": 2969
                }
            ],
            "category": "caption",
            "html": "<br><caption id='257' style='font-size:20px'>Figure 16: Samples from our guided 512x512 model using 250 steps with classifier scale 4.0 (FID<br>7.72). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130:<br>flamingo, 852: tennis ball.</caption>",
            "id": 257,
            "page": 33,
            "text": "Figure 16: Samples from our guided 512x512 model using 250 steps with classifier scale 4.0 (FID 7.72). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3130
                },
                {
                    "x": 1250,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='258' style='font-size:16px'>33</footer>",
            "id": 258,
            "page": 33,
            "text": "33"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 314
                },
                {
                    "x": 2109,
                    "y": 314
                },
                {
                    "x": 2109,
                    "y": 2813
                },
                {
                    "x": 441,
                    "y": 2813
                }
            ],
            "category": "figure",
            "html": "<figure><img id='259' style='font-size:14px' alt=\"THE\nunsping\nPHISING\" data-coord=\"top-left:(441,314); bottom-right:(2109,2813)\" /></figure>",
            "id": 259,
            "page": 34,
            "text": "THE unsping PHISING"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2831
                },
                {
                    "x": 2106,
                    "y": 2831
                },
                {
                    "x": 2106,
                    "y": 2969
                },
                {
                    "x": 443,
                    "y": 2969
                }
            ],
            "category": "caption",
            "html": "<br><caption id='260' style='font-size:20px'>Figure 17: Samples from our guided 512x512 model using 250 steps with classifier scale 4.0 (FID<br>7.72). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992:<br>agaric.</caption>",
            "id": 260,
            "page": 34,
            "text": "Figure 17: Samples from our guided 512x512 model using 250 steps with classifier scale 4.0 (FID 7.72). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3130
                },
                {
                    "x": 1250,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='261' style='font-size:16px'>34</footer>",
            "id": 261,
            "page": 34,
            "text": "34"
        },
        {
            "bounding_box": [
                {
                    "x": 438,
                    "y": 921
                },
                {
                    "x": 2110,
                    "y": 921
                },
                {
                    "x": 2110,
                    "y": 2605
                },
                {
                    "x": 438,
                    "y": 2605
                }
            ],
            "category": "figure",
            "html": "<figure><img id='262' style='font-size:14px' alt=\"TWO\nSacity\nbacusugi RIMPOTTEN] 10g nocer\n新eans TEPPYZA\nededla\nton\nXIAR\nを 끊 「 5 分 著 9 S S 3 입\n> る み 2 6 お 8 法 [ お 5 お € 准\n0 � e ※ ( 5 9 ( 粥 0 제 *** 0 79\" data-coord=\"top-left:(438,921); bottom-right:(2110,2605)\" /></figure>",
            "id": 262,
            "page": 35,
            "text": "TWO Sacity bacusugi RIMPOTTEN] 10g nocer 新eans TEPPYZA ededla ton XIAR を 끊 「 5 分 著 9 S S 3 입 > る み 2 6 お 8 法 [ お 5 お € 准 0 � e ※ ( 5 9 ( 粥 0 제 *** 0 79"
        },
        {
            "bounding_box": [
                {
                    "x": 591,
                    "y": 2614
                },
                {
                    "x": 1954,
                    "y": 2614
                },
                {
                    "x": 1954,
                    "y": 2662
                },
                {
                    "x": 591,
                    "y": 2662
                }
            ],
            "category": "caption",
            "html": "<br><caption id='263' style='font-size:20px'>Figure 18: Random samples from our best ImageNet 512x512 model (FID 3.85).</caption>",
            "id": 263,
            "page": 35,
            "text": "Figure 18: Random samples from our best ImageNet 512x512 model (FID 3.85)."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3090
                },
                {
                    "x": 1299,
                    "y": 3090
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1249,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='264' style='font-size:16px'>35</footer>",
            "id": 264,
            "page": 35,
            "text": "35"
        },
        {
            "bounding_box": [
                {
                    "x": 438,
                    "y": 899
                },
                {
                    "x": 2111,
                    "y": 899
                },
                {
                    "x": 2111,
                    "y": 2575
                },
                {
                    "x": 438,
                    "y": 2575
                }
            ],
            "category": "figure",
            "html": "<figure><img id='265' style='font-size:14px' alt=\"822,01\" data-coord=\"top-left:(438,899); bottom-right:(2111,2575)\" /></figure>",
            "id": 265,
            "page": 36,
            "text": "822,01"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2591
                },
                {
                    "x": 2103,
                    "y": 2591
                },
                {
                    "x": 2103,
                    "y": 2681
                },
                {
                    "x": 443,
                    "y": 2681
                }
            ],
            "category": "caption",
            "html": "<br><caption id='266' style='font-size:20px'>Figure 19: Random samples from our guided 512x512 model using 250 steps with classifier scale<br>4.0 (FID 7.72).</caption>",
            "id": 266,
            "page": 36,
            "text": "Figure 19: Random samples from our guided 512x512 model using 250 steps with classifier scale 4.0 (FID 7.72)."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1249,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='267' style='font-size:16px'>36</footer>",
            "id": 267,
            "page": 36,
            "text": "36"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 298
                },
                {
                    "x": 1254,
                    "y": 298
                },
                {
                    "x": 1254,
                    "y": 354
                },
                {
                    "x": 444,
                    "y": 354
                }
            ],
            "category": "caption",
            "html": "<caption id='268' style='font-size:20px'>L Samples from ImageNet 256 x 256</caption>",
            "id": 268,
            "page": 37,
            "text": "L Samples from ImageNet 256 x 256"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 409
                },
                {
                    "x": 2110,
                    "y": 409
                },
                {
                    "x": 2110,
                    "y": 2916
                },
                {
                    "x": 440,
                    "y": 2916
                }
            ],
            "category": "figure",
            "html": "<figure><img id='269' alt=\"\" data-coord=\"top-left:(440,409); bottom-right:(2110,2916)\" /></figure>",
            "id": 269,
            "page": 37,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2928
                },
                {
                    "x": 2111,
                    "y": 2928
                },
                {
                    "x": 2111,
                    "y": 3070
                },
                {
                    "x": 442,
                    "y": 3070
                }
            ],
            "category": "caption",
            "html": "<br><caption id='270' style='font-size:16px'>Figure 20: Samples using our best 256x256 model (FID 3.94). Classes are 1: goldfish, 279: arctic<br>fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger,<br>562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric</caption>",
            "id": 270,
            "page": 37,
            "text": "Figure 20: Samples using our best 256x256 model (FID 3.94). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric"
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3131
                },
                {
                    "x": 1250,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<br><footer id='271' style='font-size:14px'>37</footer>",
            "id": 271,
            "page": 37,
            "text": "37"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 296
                },
                {
                    "x": 2111,
                    "y": 296
                },
                {
                    "x": 2111,
                    "y": 2797
                },
                {
                    "x": 440,
                    "y": 2797
                }
            ],
            "category": "figure",
            "html": "<figure><img id='272' style='font-size:14px' alt=\"WHIRMARMAN\nFIE\nyou\n스케\nUs Conliscula\nMCORATED.\nJUFOR HEMONE\nOratner\n(인)\" data-coord=\"top-left:(440,296); bottom-right:(2111,2797)\" /></figure>",
            "id": 272,
            "page": 38,
            "text": "WHIRMARMAN FIE you 스케 Us Conliscula MCORATED. JUFOR HEMONE Oratner (인)"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2807
                },
                {
                    "x": 2110,
                    "y": 2807
                },
                {
                    "x": 2110,
                    "y": 2991
                },
                {
                    "x": 443,
                    "y": 2991
                }
            ],
            "category": "caption",
            "html": "<br><caption id='273' style='font-size:20px'>Figure 21: Samples from our guided 256x256 model using 250 steps with classifier scale 1.0 (FID<br>4.59). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130:<br>flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90:<br>lorikeet, 992: agaric</caption>",
            "id": 273,
            "page": 38,
            "text": "Figure 21: Samples from our guided 256x256 model using 250 steps with classifier scale 1.0 (FID 4.59). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric"
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3093
                },
                {
                    "x": 1298,
                    "y": 3130
                },
                {
                    "x": 1250,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='274' style='font-size:16px'>38</footer>",
            "id": 274,
            "page": 38,
            "text": "38"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 293
                },
                {
                    "x": 2109,
                    "y": 293
                },
                {
                    "x": 2109,
                    "y": 2795
                },
                {
                    "x": 441,
                    "y": 2795
                }
            ],
            "category": "figure",
            "html": "<figure><img id='275' style='font-size:14px' alt=\"fEatich Pparnation\nLayie anomin\nSUS\n0,00\nING[LIAT\nAngs\n山市\" data-coord=\"top-left:(441,293); bottom-right:(2109,2795)\" /></figure>",
            "id": 275,
            "page": 39,
            "text": "fEatich Pparnation Layie anomin SUS 0,00 ING[LIAT Angs 山市"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2808
                },
                {
                    "x": 2107,
                    "y": 2808
                },
                {
                    "x": 2107,
                    "y": 2990
                },
                {
                    "x": 444,
                    "y": 2990
                }
            ],
            "category": "caption",
            "html": "<br><caption id='276' style='font-size:20px'>Figure 22: Samples from our guided 256x256 model using 25 DDIM steps with classifier scale 2.5<br>(FID 5.44). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant,<br>130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90:<br>lorikeet, 992: agaric</caption>",
            "id": 276,
            "page": 39,
            "text": "Figure 22: Samples from our guided 256x256 model using 25 DDIM steps with classifier scale 2.5 (FID 5.44). Classes are 1: goldfish, 279: arctic fox, 323: monarch butterfly, 386: african elephant, 130: flamingo, 852: tennis ball, 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric"
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3092
                },
                {
                    "x": 1299,
                    "y": 3130
                },
                {
                    "x": 1250,
                    "y": 3130
                }
            ],
            "category": "footer",
            "html": "<footer id='277' style='font-size:16px'>39</footer>",
            "id": 277,
            "page": 39,
            "text": "39"
        },
        {
            "bounding_box": [
                {
                    "x": 438,
                    "y": 918
                },
                {
                    "x": 2110,
                    "y": 918
                },
                {
                    "x": 2110,
                    "y": 2602
                },
                {
                    "x": 438,
                    "y": 2602
                }
            ],
            "category": "figure",
            "html": "<figure><img id='278' style='font-size:14px' alt=\"331 6 thin\nnoncy\nNUHURAL\nMich\n\nA\n5\" data-coord=\"top-left:(438,918); bottom-right:(2110,2602)\" /></figure>",
            "id": 278,
            "page": 40,
            "text": "331 6 thin noncy NUHURAL Mich  A 5"
        },
        {
            "bounding_box": [
                {
                    "x": 676,
                    "y": 2612
                },
                {
                    "x": 1868,
                    "y": 2612
                },
                {
                    "x": 1868,
                    "y": 2664
                },
                {
                    "x": 676,
                    "y": 2664
                }
            ],
            "category": "caption",
            "html": "<br><caption id='279' style='font-size:20px'>Figure 23: Random samples from our best 256x256 model (FID 3.94).</caption>",
            "id": 279,
            "page": 40,
            "text": "Figure 23: Random samples from our best 256x256 model (FID 3.94)."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3090
                },
                {
                    "x": 1300,
                    "y": 3090
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1248,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='280' style='font-size:16px'>40</footer>",
            "id": 280,
            "page": 40,
            "text": "40"
        },
        {
            "bounding_box": [
                {
                    "x": 437,
                    "y": 899
                },
                {
                    "x": 2111,
                    "y": 899
                },
                {
                    "x": 2111,
                    "y": 2581
                },
                {
                    "x": 437,
                    "y": 2581
                }
            ],
            "category": "figure",
            "html": "<figure><img id='281' style='font-size:14px' alt=\"aloor\nCHARMA\n\nCHOMARS CHATH.COMPA\" data-coord=\"top-left:(437,899); bottom-right:(2111,2581)\" /></figure>",
            "id": 281,
            "page": 41,
            "text": "aloor CHARMA  CHOMARS CHATH.COMPA"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2592
                },
                {
                    "x": 2103,
                    "y": 2592
                },
                {
                    "x": 2103,
                    "y": 2682
                },
                {
                    "x": 444,
                    "y": 2682
                }
            ],
            "category": "caption",
            "html": "<br><caption id='282' style='font-size:20px'>Figure 24: Random samples from our guided 256x256 model using 250 steps with classifier scale<br>1.0 (FID 4.59).</caption>",
            "id": 282,
            "page": 41,
            "text": "Figure 24: Random samples from our guided 256x256 model using 250 steps with classifier scale 1.0 (FID 4.59)."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3092
                },
                {
                    "x": 1297,
                    "y": 3092
                },
                {
                    "x": 1297,
                    "y": 3131
                },
                {
                    "x": 1248,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='283' style='font-size:16px'>41</footer>",
            "id": 283,
            "page": 41,
            "text": "41"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 298
                },
                {
                    "x": 991,
                    "y": 298
                },
                {
                    "x": 991,
                    "y": 353
                },
                {
                    "x": 443,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<p id='284' style='font-size:20px'>M Samples from LSUN</p>",
            "id": 284,
            "page": 42,
            "text": "M Samples from LSUN"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 412
                },
                {
                    "x": 2109,
                    "y": 412
                },
                {
                    "x": 2109,
                    "y": 2091
                },
                {
                    "x": 439,
                    "y": 2091
                }
            ],
            "category": "figure",
            "html": "<figure><img id='285' style='font-size:14px' alt=\"SORM\nILOGOLATOLITI\nPA\n♥\" data-coord=\"top-left:(439,412); bottom-right:(2109,2091)\" /></figure>",
            "id": 285,
            "page": 42,
            "text": "SORM ILOGOLATOLITI PA ♥"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2101
                },
                {
                    "x": 2104,
                    "y": 2101
                },
                {
                    "x": 2104,
                    "y": 2157
                },
                {
                    "x": 442,
                    "y": 2157
                }
            ],
            "category": "caption",
            "html": "<br><caption id='286' style='font-size:16px'>Figure 25: Random samples from our LSUN bedroom model using 1000 sampling steps. (FID 1.90)</caption>",
            "id": 286,
            "page": 42,
            "text": "Figure 25: Random samples from our LSUN bedroom model using 1000 sampling steps. (FID 1.90)"
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3091
                },
                {
                    "x": 1300,
                    "y": 3091
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1248,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='287' style='font-size:16px'>42</footer>",
            "id": 287,
            "page": 42,
            "text": "42"
        },
        {
            "bounding_box": [
                {
                    "x": 437,
                    "y": 919
                },
                {
                    "x": 2109,
                    "y": 919
                },
                {
                    "x": 2109,
                    "y": 2602
                },
                {
                    "x": 437,
                    "y": 2602
                }
            ],
            "category": "figure",
            "html": "<figure><img id='288' style='font-size:14px' alt=\"WILIQL\n이\nTIAOI\nnehos/\nSNU\nwww.shutte sttock.com 9189693\n002\n\n!!!!! P\n±coodHmneyluukino 08\" data-coord=\"top-left:(437,919); bottom-right:(2109,2602)\" /></figure>",
            "id": 288,
            "page": 43,
            "text": "WILIQL 이 TIAOI nehos/ SNU www.shutte sttock.com 9189693 002  !!!!! P ±coodHmneyluukino 08"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 2612
                },
                {
                    "x": 2083,
                    "y": 2612
                },
                {
                    "x": 2083,
                    "y": 2664
                },
                {
                    "x": 466,
                    "y": 2664
                }
            ],
            "category": "caption",
            "html": "<br><caption id='289' style='font-size:20px'>Figure 26: Random samples from our LSUN horse model using 1000 sampling steps. (FID 2.57)</caption>",
            "id": 289,
            "page": 43,
            "text": "Figure 26: Random samples from our LSUN horse model using 1000 sampling steps. (FID 2.57)"
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3091
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1248,
                    "y": 3131
                }
            ],
            "category": "footer",
            "html": "<footer id='290' style='font-size:16px'>43</footer>",
            "id": 290,
            "page": 43,
            "text": "43"
        },
        {
            "bounding_box": [
                {
                    "x": 436,
                    "y": 918
                },
                {
                    "x": 2111,
                    "y": 918
                },
                {
                    "x": 2111,
                    "y": 2604
                },
                {
                    "x": 436,
                    "y": 2604
                }
            ],
            "category": "figure",
            "html": "<figure><img id='291' style='font-size:14px' alt=\"100.9ms\nDaur\nMum\nDD)\nnannolatin StMin.ocla\nshutterstock\nwww. shutterstok .com · 138583927\nVOU\nSCU D ⊙\nSO? wn sitannunctamin yubinJoaJiabo\\/L18\" data-coord=\"top-left:(436,918); bottom-right:(2111,2604)\" /></figure>",
            "id": 291,
            "page": 44,
            "text": "100.9ms Daur Mum DD) nannolatin StMin.ocla shutterstock www. shutterstok .com · 138583927 VOU SCU D ⊙ SO? wn sitannunctamin yubinJoaJiabo\\/L18"
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 2612
                },
                {
                    "x": 2061,
                    "y": 2612
                },
                {
                    "x": 2061,
                    "y": 2666
                },
                {
                    "x": 485,
                    "y": 2666
                }
            ],
            "category": "caption",
            "html": "<br><caption id='292' style='font-size:20px'>Figure 27: Random samples from our LSUN cat model using 1000 sampling steps. (FID 5.57)</caption>",
            "id": 292,
            "page": 44,
            "text": "Figure 27: Random samples from our LSUN cat model using 1000 sampling steps. (FID 5.57)"
        },
        {
            "bounding_box": [
                {
                    "x": 1247,
                    "y": 3090
                },
                {
                    "x": 1300,
                    "y": 3090
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1247,
                    "y": 3132
                }
            ],
            "category": "footer",
            "html": "<footer id='293' style='font-size:16px'>44</footer>",
            "id": 293,
            "page": 44,
            "text": "44"
        }
    ]
}