{
    "id": "32c1969a-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2303.16634v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 285
                },
                {
                    "x": 2158,
                    "y": 285
                },
                {
                    "x": 2158,
                    "y": 366
                },
                {
                    "x": 333,
                    "y": 366
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment</p>",
            "id": 0,
            "page": 1,
            "text": "G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment"
        },
        {
            "bounding_box": [
                {
                    "x": 703,
                    "y": 455
                },
                {
                    "x": 1795,
                    "y": 455
                },
                {
                    "x": 1795,
                    "y": 573
                },
                {
                    "x": 703,
                    "y": 573
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Yang Liu Dan Iter Yichong Xu<br>Shuohang Wang Ruochen Xu Chenguang Zhu</p>",
            "id": 1,
            "page": 1,
            "text": "Yang Liu Dan Iter Yichong Xu Shuohang Wang Ruochen Xu Chenguang Zhu"
        },
        {
            "bounding_box": [
                {
                    "x": 607,
                    "y": 630
                },
                {
                    "x": 1901,
                    "y": 630
                },
                {
                    "x": 1901,
                    "y": 751
                },
                {
                    "x": 607,
                    "y": 751
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:20px'>Microsoft Cognitive Services Research<br>{yaliu10, iterdan, yicxu, shuowa, ruox, chezhu} @microsoft.com</p>",
            "id": 2,
            "page": 1,
            "text": "Microsoft Cognitive Services Research {yaliu10, iterdan, yicxu, shuowa, ruox, chezhu} @microsoft.com"
        },
        {
            "bounding_box": [
                {
                    "x": 655,
                    "y": 930
                },
                {
                    "x": 854,
                    "y": 930
                },
                {
                    "x": 854,
                    "y": 985
                },
                {
                    "x": 655,
                    "y": 985
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:18px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 359,
                    "y": 1035
                },
                {
                    "x": 1157,
                    "y": 1035
                },
                {
                    "x": 1157,
                    "y": 2444
                },
                {
                    "x": 359,
                    "y": 2444
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:14px'>The quality of texts generated by natural lan-<br>guage generation (NLG) systems is hard to<br>measure automatically. Conventional reference-<br>based metrics, such as BLEU and ROUGE,<br>have been shown to have relatively low cor-<br>relation with human judgments, especially for<br>tasks that require creativity and diversity. Re-<br>cent studies suggest using large language mod-<br>els (LLMs) as reference-free metrics for NLG<br>evaluation, which have the benefit of being ap-<br>plicable to new tasks that lack human refer-<br>ences. However, these LLM-based evaluators<br>still have lower human correspondence than<br>medium-size neural evaluators. In this work,<br>we present G-EVAL, a framework of using<br>large language models with chain-of-thoughts<br>(CoT) and a form-filling paradigm, to assess the<br>quality of NLG outputs. We experiment with<br>two generation tasks, text summarization and<br>dialogue generation. We show that G-EVAL<br>with GPT-4 as the backbone model achieves a<br>Spearman correlation of 0.514 with human on<br>summarization task, outperforming all previous<br>methods by a large margin. We also propose<br>analysis on the behavior of LLM-based eval-<br>uators, and highlight the potential concern of<br>LLM-based evaluators having a bias towards<br>the LLM-generated texts. 1</p>",
            "id": 4,
            "page": 1,
            "text": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional referencebased metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-EVAL, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-EVAL with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts. 1"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2498
                },
                {
                    "x": 648,
                    "y": 2498
                },
                {
                    "x": 648,
                    "y": 2550
                },
                {
                    "x": 294,
                    "y": 2550
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>1 Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2595
                },
                {
                    "x": 1222,
                    "y": 2595
                },
                {
                    "x": 1222,
                    "y": 3102
                },
                {
                    "x": 292,
                    "y": 3102
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:16px'>Evaluating the quality of natural language genera-<br>tion systems is a challenging problem even when<br>large language models can generate high-quality<br>and diverse texts that are often indistinguishable<br>from human-written texts (Ouyang et al., 2022).<br>Traditional automatic metrics, such as BLEU (Pap-<br>ineni et al., 2002), ROUGE (Lin, 2004), and ME-<br>TEOR (Banerjee and Lavie, 2005), are widely used<br>for NLG evaluation, but they have been shown to</p>",
            "id": 6,
            "page": 1,
            "text": "Evaluating the quality of natural language generation systems is a challenging problem even when large language models can generate high-quality and diverse texts that are often indistinguishable from human-written texts (Ouyang , 2022). Traditional automatic metrics, such as BLEU (Papineni , 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005), are widely used for NLG evaluation, but they have been shown to"
        },
        {
            "bounding_box": [
                {
                    "x": 344,
                    "y": 3141
                },
                {
                    "x": 1087,
                    "y": 3141
                },
                {
                    "x": 1087,
                    "y": 3196
                },
                {
                    "x": 344,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:14px'>1https: / / github · com/ nlpyang / geval</p>",
            "id": 7,
            "page": 1,
            "text": "1https: / / github · com/ nlpyang / geval"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 932
                },
                {
                    "x": 2202,
                    "y": 932
                },
                {
                    "x": 2202,
                    "y": 1156
                },
                {
                    "x": 1271,
                    "y": 1156
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:16px'>have relatively low correlation with human judg-<br>ments, especially for open-ended generation tasks.<br>Moreover, these metrics require associated refer-<br>ence output, which is costly to collect for new tasks.</p>",
            "id": 8,
            "page": 1,
            "text": "have relatively low correlation with human judgments, especially for open-ended generation tasks. Moreover, these metrics require associated reference output, which is costly to collect for new tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1162
                },
                {
                    "x": 2202,
                    "y": 1162
                },
                {
                    "x": 2202,
                    "y": 2005
                },
                {
                    "x": 1271,
                    "y": 2005
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>Recent studies propose directly using LLMs as<br>reference-free NLG evaluators (Fu et al., 2023;<br>Wang et al., 2023). The idea is to use the LLMs to<br>score the candidate output based on its generation<br>probability without any reference target, under the<br>assumption that the LLMs have learned to assign<br>higher probabilities to high-quality and fluent texts.<br>However, the validity and reliability of using LLMs<br>as NLG evaluators have not been systematically in-<br>vestigated. In addition, meta-evaluations show that<br>these LLM-based evaluators still have lower human<br>correspondence than medium-size neural evalua-<br>tors (Zhong et al., 2022). Thus, there is a need for<br>a more effective and reliable framework for using<br>LLMs for NLG evaluation.</p>",
            "id": 9,
            "page": 1,
            "text": "Recent studies propose directly using LLMs as reference-free NLG evaluators (Fu , 2023; Wang , 2023). The idea is to use the LLMs to score the candidate output based on its generation probability without any reference target, under the assumption that the LLMs have learned to assign higher probabilities to high-quality and fluent texts. However, the validity and reliability of using LLMs as NLG evaluators have not been systematically investigated. In addition, meta-evaluations show that these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators (Zhong , 2022). Thus, there is a need for a more effective and reliable framework for using LLMs for NLG evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2010
                },
                {
                    "x": 2203,
                    "y": 2010
                },
                {
                    "x": 2203,
                    "y": 3193
                },
                {
                    "x": 1272,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>In this paper, we propose G-EVAL, a framework<br>of using LLMs with chain-of-thoughts (CoT) (Wei<br>et al., 2022) to evaluate the quality of generated<br>texts in a form-filling paradigm. By only feeding<br>the Task Introduction and the Evaluation Criteria<br>as a prompt, we ask LLMs to generate a CoT of<br>detailed Evaluation Steps. Then we use the prompt<br>along with the generated CoT to evaluate the NLG<br>outputs. The evaluator output is formatted as a<br>form. Moreover, the probabilities of the output<br>rating tokens can be used to refine the final met-<br>ric. We conduct extensive experiments on three<br>meta-evaluation benchmarks of two NLG tasks:<br>text summarization and dialogue generation. The<br>results show that G-EVAL can outperform existing<br>NLG evaluators by a large margin in terms of corre-<br>lation with human evaluations. Finally, we conduct<br>analysis on the behavior of LLM-based evaluators,<br>and highlight the potential issue of LLM-based<br>evaluator having a bias towards the LLM-generated<br>texts.</p>",
            "id": 10,
            "page": 1,
            "text": "In this paper, we propose G-EVAL, a framework of using LLMs with chain-of-thoughts (CoT) (Wei , 2022) to evaluate the quality of generated texts in a form-filling paradigm. By only feeding the Task Introduction and the Evaluation Criteria as a prompt, we ask LLMs to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs. The evaluator output is formatted as a form. Moreover, the probabilities of the output rating tokens can be used to refine the final metric. We conduct extensive experiments on three meta-evaluation benchmarks of two NLG tasks: text summarization and dialogue generation. The results show that G-EVAL can outperform existing NLG evaluators by a large margin in terms of correlation with human evaluations. Finally, we conduct analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluator having a bias towards the LLM-generated texts."
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 1070
                },
                {
                    "x": 149,
                    "y": 1070
                },
                {
                    "x": 149,
                    "y": 2532
                },
                {
                    "x": 58,
                    "y": 2532
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2023<br>May<br>23<br>[cs.CL]<br>arXiv:2303.16634v3</footer>",
            "id": 11,
            "page": 1,
            "text": "2023 May 23 [cs.CL] arXiv:2303.16634v3"
        },
        {
            "bounding_box": [
                {
                    "x": 308,
                    "y": 269
                },
                {
                    "x": 2198,
                    "y": 269
                },
                {
                    "x": 2198,
                    "y": 1408
                },
                {
                    "x": 308,
                    "y": 1408
                }
            ],
            "category": "figure",
            "html": "<figure><img id='12' style='font-size:14px' alt=\"Input Context\nTask Introduction Article: Paul Merson has restarted his row with\nAndros Townsend after the Tottenham midfielder\nYou will be given one summary written for a news was brought on with only seven minutes remaining\narticle. Your task is to rate the summary on one in his team 's 0-0 draw with Burnley on\nmetric ......\nInput Target\nSummary: Paul merson was brought on with only\nseven minutes remaining in his team 's 0-0 draw\nEvaluation Criteria ......\nwith burnley\nCoherence (1-5) - the collective quality of all Evaluation Form (scores ONLY):\nsentences. We align this dimension with the DUC\nquality question of structure and coherence ...... - Coherence:\nAuto\nCoT\nEvaluation Steps\n1. Read the news article carefully and identify the 0.6\nmain topic and key points.\n0.4\n2. Read the summary and compare it to the news\nG-Eval\narticle. Check if the summary covers the main topic\n0.2\nand key points of the news article, and ifit presents\nthem in a clear and logical order. 0\n3. Assign a score for coherence on a scale of 1 to 1 2 3 4 5\n10, where 1 is the lowest and 5 is the highest based\non the Evaluation Criteria.\nWeighted Summed Score: 2.59\" data-coord=\"top-left:(308,269); bottom-right:(2198,1408)\" /></figure>",
            "id": 12,
            "page": 2,
            "text": "Input Context Task Introduction Article: Paul Merson has restarted his row with Andros Townsend after the Tottenham midfielder You will be given one summary written for a news was brought on with only seven minutes remaining article. Your task is to rate the summary on one in his team 's 0-0 draw with Burnley on metric ...... Input Target Summary: Paul merson was brought on with only seven minutes remaining in his team 's 0-0 draw Evaluation Criteria ...... with burnley Coherence (1-5) - the collective quality of all Evaluation Form (scores ONLY): sentences. We align this dimension with the DUC quality question of structure and coherence ...... - Coherence: Auto CoT Evaluation Steps 1. Read the news article carefully and identify the 0.6 main topic and key points. 0.4 2. Read the summary and compare it to the news G-Eval article. Check if the summary covers the main topic 0.2 and key points of the news article, and ifit presents them in a clear and logical order. 0 3. Assign a score for coherence on a scale of 1 to 1 2 3 4 5 10, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria. Weighted Summed Score: 2.59"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1453
                },
                {
                    "x": 2201,
                    "y": 1453
                },
                {
                    "x": 2201,
                    "y": 1657
                },
                {
                    "x": 290,
                    "y": 1657
                }
            ],
            "category": "caption",
            "html": "<caption id='13' style='font-size:14px'>Figure 1: The overall framework of G-EVAL. We first input Task Introduction and Evaluation Criteria to the LLM,<br>and ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to<br>evaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the<br>output scores as the final score.</caption>",
            "id": 13,
            "page": 2,
            "text": "Figure 1: The overall framework of G-EVAL. We first input Task Introduction and Evaluation Criteria to the LLM, and ask it to generate a CoT of detailed Evaluation Steps. Then we use the prompt along with the generated CoT to evaluate the NLG outputs in a form-filling paradigm. Finally, we use the probability-weighted summation of the output scores as the final score."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1746
                },
                {
                    "x": 1220,
                    "y": 1746
                },
                {
                    "x": 1220,
                    "y": 1858
                },
                {
                    "x": 292,
                    "y": 1858
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:16px'>To summarize, our main contributions in this<br>paper are:</p>",
            "id": 14,
            "page": 2,
            "text": "To summarize, our main contributions in this paper are:"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1888
                },
                {
                    "x": 1222,
                    "y": 1888
                },
                {
                    "x": 1222,
                    "y": 3196
                },
                {
                    "x": 328,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:20px'>1. LLM-based metrics generally outperform<br>reference-based and reference-free baseline<br>metrics in terms of correlation with human<br>quality judgments, especially for open-ended<br>and creative NLG tasks, such as dialogue re-<br>sponse generation.<br>2. LLM-based metrics are sensitive to the in-<br>structions and prompts, and chain-of-thought<br>can improve the performance of LLM-based<br>evaluators by providing more context and<br>guidance.<br>3. LLM-based metrics can provide a more fine-<br>grained continuous score by re-weighting the<br>discrete scores by their respective token prob-<br>abilities.<br>4. LLM-based metrics have a potential issue of<br>preferring LLM-generated texts over human-<br>written texts, which may lead to the self-<br>reinforcement of LLMs ifLLM-based metrics<br>are used as the reward signal for improving<br>themselves.</p>",
            "id": 15,
            "page": 2,
            "text": "1. LLM-based metrics generally outperform reference-based and reference-free baseline metrics in terms of correlation with human quality judgments, especially for open-ended and creative NLG tasks, such as dialogue response generation. 2. LLM-based metrics are sensitive to the instructions and prompts, and chain-of-thought can improve the performance of LLM-based evaluators by providing more context and guidance. 3. LLM-based metrics can provide a more finegrained continuous score by re-weighting the discrete scores by their respective token probabilities. 4. LLM-based metrics have a potential issue of preferring LLM-generated texts over humanwritten texts, which may lead to the selfreinforcement of LLMs ifLLM-based metrics are used as the reward signal for improving themselves."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1742
                },
                {
                    "x": 1524,
                    "y": 1742
                },
                {
                    "x": 1524,
                    "y": 1796
                },
                {
                    "x": 1273,
                    "y": 1796
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:22px'>2 Method</p>",
            "id": 16,
            "page": 2,
            "text": "2 Method"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1832
                },
                {
                    "x": 2200,
                    "y": 1832
                },
                {
                    "x": 2200,
                    "y": 2336
                },
                {
                    "x": 1272,
                    "y": 2336
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:20px'>G-EVAL is a prompt-based evaluator with three<br>main components: 1) a prompt that contains the def-<br>inition of the evaluation task and the desired evalu-<br>ation criteria, 2) a chain-of-thoughts (CoT) that is<br>a set of intermediate instructions generated by the<br>LLM describing the detailed evaluation steps, and<br>3) a scoring function that calls LLM and calculates<br>the score based on the probabilities of the return<br>tokens.</p>",
            "id": 17,
            "page": 2,
            "text": "G-EVAL is a prompt-based evaluator with three main components: 1) a prompt that contains the definition of the evaluation task and the desired evaluation criteria, 2) a chain-of-thoughts (CoT) that is a set of intermediate instructions generated by the LLM describing the detailed evaluation steps, and 3) a scoring function that calls LLM and calculates the score based on the probabilities of the return tokens."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2371
                },
                {
                    "x": 2201,
                    "y": 2371
                },
                {
                    "x": 2201,
                    "y": 2649
                },
                {
                    "x": 1271,
                    "y": 2649
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>Prompt for NLG Evaluation The prompt is a<br>natural language instruction that defines the evalu-<br>ation task and the desired evaluation criteria. For<br>example, for text summarization, the prompt can<br>be:</p>",
            "id": 18,
            "page": 2,
            "text": "Prompt for NLG Evaluation The prompt is a natural language instruction that defines the evaluation task and the desired evaluation criteria. For example, for text summarization, the prompt can be:"
        },
        {
            "bounding_box": [
                {
                    "x": 1355,
                    "y": 2687
                },
                {
                    "x": 2117,
                    "y": 2687
                },
                {
                    "x": 2117,
                    "y": 3102
                },
                {
                    "x": 1355,
                    "y": 3102
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:18px'>You will be given one summary written<br>for a news article. Your task is to rate<br>the summary on one metric.<br>Please make sure you read and under-<br>stand these instructions carefully. Please<br>keep this document open while reviewing,<br>and refer to it as needed.</p>",
            "id": 19,
            "page": 2,
            "text": "You will be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed."
        },
        {
            "bounding_box": [
                {
                    "x": 1319,
                    "y": 3139
                },
                {
                    "x": 2202,
                    "y": 3139
                },
                {
                    "x": 2202,
                    "y": 3192
                },
                {
                    "x": 1319,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>The prompt should also contain customized eval-</p>",
            "id": 20,
            "page": 2,
            "text": "The prompt should also contain customized eval-"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 269
                },
                {
                    "x": 1221,
                    "y": 269
                },
                {
                    "x": 1221,
                    "y": 492
                },
                {
                    "x": 291,
                    "y": 492
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:16px'>uation criteria for different NLG tasks and, such as<br>coherence, conciseness, or grammar. For example,<br>for evaluating coherence in text summarization, we<br>add the following content to the prompt:</p>",
            "id": 21,
            "page": 3,
            "text": "uation criteria for different NLG tasks and, such as coherence, conciseness, or grammar. For example, for evaluating coherence in text summarization, we add the following content to the prompt:"
        },
        {
            "bounding_box": [
                {
                    "x": 384,
                    "y": 537
                },
                {
                    "x": 766,
                    "y": 537
                },
                {
                    "x": 766,
                    "y": 584
                },
                {
                    "x": 384,
                    "y": 584
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:16px'>Evaluation Criteria:</p>",
            "id": 22,
            "page": 3,
            "text": "Evaluation Criteria:"
        },
        {
            "bounding_box": [
                {
                    "x": 381,
                    "y": 608
                },
                {
                    "x": 1128,
                    "y": 608
                },
                {
                    "x": 1128,
                    "y": 1171
                },
                {
                    "x": 381,
                    "y": 1171
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:18px'>Coherence (1-5) - the collective quality<br>of all sentences. We align this dimen-<br>sion with the DUC quality question of<br>structure and coherence whereby \"the<br>summary should be well-structured and<br>well-organized. The summary should not<br>just be a heap of related information, but<br>should build from sentence to sentence<br>to a coherent body of information about<br>a topic.\"</p>",
            "id": 23,
            "page": 3,
            "text": "Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.\""
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1215
                },
                {
                    "x": 1214,
                    "y": 1215
                },
                {
                    "x": 1214,
                    "y": 1265
                },
                {
                    "x": 294,
                    "y": 1265
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:20px'>Auto Chain-of-Thoughts for NLG Evaluation</p>",
            "id": 24,
            "page": 3,
            "text": "Auto Chain-of-Thoughts for NLG Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1273
                },
                {
                    "x": 1219,
                    "y": 1273
                },
                {
                    "x": 1219,
                    "y": 2116
                },
                {
                    "x": 291,
                    "y": 2116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:16px'>The chain-of-thoughts (CoT) is a sequence of in-<br>termediate representations that are generated by<br>the LLM during the text generation process. For<br>evaluation tasks, some criteria need a more detailed<br>evaluation instruction beyond the simple definition,<br>and it is time-consuming to manually design such<br>evaluation steps for each task. We find that LLM<br>can generate such evaluation steps by itself. The<br>CoT can provide more context and guidance for the<br>LLM to evaluate the generated text, and can also<br>help to explain the evaluation process and results.<br>For example, for evaluating coherence in text sum-<br>marization, we add a line of \"Evaluation Steps: \" to<br>the prompt and let LLM to generate the following<br>CoT automatically:</p>",
            "id": 25,
            "page": 3,
            "text": "The chain-of-thoughts (CoT) is a sequence of intermediate representations that are generated by the LLM during the text generation process. For evaluation tasks, some criteria need a more detailed evaluation instruction beyond the simple definition, and it is time-consuming to manually design such evaluation steps for each task. We find that LLM can generate such evaluation steps by itself. The CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. For example, for evaluating coherence in text summarization, we add a line of \"Evaluation Steps: \" to the prompt and let LLM to generate the following CoT automatically:"
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 2157
                },
                {
                    "x": 1125,
                    "y": 2157
                },
                {
                    "x": 1125,
                    "y": 2815
                },
                {
                    "x": 379,
                    "y": 2815
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:16px'>1. Read the news article carefully and<br>identify the main topic and key points.<br>2. Read the summary and compare it to<br>the news article. Check if the summary<br>covers the main topic and key points of<br>the news article, and if it presents them<br>in a clear and logical order.<br>3. Assign a score for coherence on a<br>scale of 1 to 5, where 1 is the lowest and<br>5 is the highest based on the Evaluation<br>Criteria.</p>",
            "id": 26,
            "page": 3,
            "text": "1. Read the news article carefully and identify the main topic and key points. 2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order. 3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2856
                },
                {
                    "x": 1219,
                    "y": 2856
                },
                {
                    "x": 1219,
                    "y": 3195
                },
                {
                    "x": 291,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>Scoring Function The scoring function calls the<br>LLM with the designed prompt, auto CoT, the input<br>context and the target text that needs to be evalu-<br>ated. Unlike GPTScore (Fu et al., 2023) which uses<br>the conditional probability of generating the tar-<br>get text as an evaluation metric, G-EVAL directly</p>",
            "id": 27,
            "page": 3,
            "text": "Scoring Function The scoring function calls the LLM with the designed prompt, auto CoT, the input context and the target text that needs to be evaluated. Unlike GPTScore (Fu , 2023) which uses the conditional probability of generating the target text as an evaluation metric, G-EVAL directly"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 268
                },
                {
                    "x": 2200,
                    "y": 268
                },
                {
                    "x": 2200,
                    "y": 658
                },
                {
                    "x": 1272,
                    "y": 658
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:16px'>performs the evaluation task with a form-filling<br>paradigm. For example, for evaluating coherence<br>in text summarization, we concatenate the prompt,<br>the CoT, the news article, and the summary, and<br>then call the LLM to output a score from 1 to 5<br>for each evaluation aspect, based on the defined<br>criteria.</p>",
            "id": 28,
            "page": 3,
            "text": "performs the evaluation task with a form-filling paradigm. For example, for evaluating coherence in text summarization, we concatenate the prompt, the CoT, the news article, and the summary, and then call the LLM to output a score from 1 to 5 for each evaluation aspect, based on the defined criteria."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 665
                },
                {
                    "x": 2194,
                    "y": 665
                },
                {
                    "x": 2194,
                    "y": 770
                },
                {
                    "x": 1273,
                    "y": 770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:16px'>However, we notice this direct scoring function<br>has two issues:</p>",
            "id": 29,
            "page": 3,
            "text": "However, we notice this direct scoring function has two issues:"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 816
                },
                {
                    "x": 2200,
                    "y": 816
                },
                {
                    "x": 2200,
                    "y": 1419
                },
                {
                    "x": 1310,
                    "y": 1419
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:14px'>1. For some evaluation tasks, one digit usually<br>dominates the distribution of the scores, such<br>as 3 for a 1 - 5 scale. This may lead to the low<br>variance of the scores and the low correlation<br>with human judgments.<br>2. LLMs usually only output integer scores, even<br>when the prompt explicitly requests decimal<br>values. This leads to many ties in evaluation<br>scores which do not capture the subtle differ-<br>ence between generated texts.</p>",
            "id": 30,
            "page": 3,
            "text": "1. For some evaluation tasks, one digit usually dominates the distribution of the scores, such as 3 for a 1 - 5 scale. This may lead to the low variance of the scores and the low correlation with human judgments. 2. LLMs usually only output integer scores, even when the prompt explicitly requests decimal values. This leads to many ties in evaluation scores which do not capture the subtle difference between generated texts."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1457
                },
                {
                    "x": 2202,
                    "y": 1457
                },
                {
                    "x": 2202,
                    "y": 1905
                },
                {
                    "x": 1271,
                    "y": 1905
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:16px'>To address these issues, we propose using the<br>probabilities of output tokens from LLMs to nor-<br>malize the scores and take their weighted summa-<br>tion as the final results. Formally, given a set of<br>scores (like from 1 to 5) predefined in the prompt<br>S = {S1, S2, ..., sn}, the probability of each score<br>p(si) is calculated by the LLM, and the final score<br>is:</p>",
            "id": 31,
            "page": 3,
            "text": "To address these issues, we propose using the probabilities of output tokens from LLMs to normalize the scores and take their weighted summation as the final results. Formally, given a set of scores (like from 1 to 5) predefined in the prompt S = {S1, S2, ..., sn}, the probability of each score p(si) is calculated by the LLM, and the final score is:"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2109
                },
                {
                    "x": 2203,
                    "y": 2109
                },
                {
                    "x": 2203,
                    "y": 2276
                },
                {
                    "x": 1271,
                    "y": 2276
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:14px'>This method obtains more fine-grained, continu-<br>ous scores that better reflect the quality and diver-<br>sity of the generated texts.</p>",
            "id": 32,
            "page": 3,
            "text": "This method obtains more fine-grained, continuous scores that better reflect the quality and diversity of the generated texts."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2318
                },
                {
                    "x": 1632,
                    "y": 2318
                },
                {
                    "x": 1632,
                    "y": 2375
                },
                {
                    "x": 1274,
                    "y": 2375
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:22px'>3 Experiments</p>",
            "id": 33,
            "page": 3,
            "text": "3 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2407
                },
                {
                    "x": 2202,
                    "y": 2407
                },
                {
                    "x": 2202,
                    "y": 2635
                },
                {
                    "x": 1272,
                    "y": 2635
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:18px'>Following Zhong et al. (2022), we meta-evaluate<br>our evaluator on three benchmarks, SummEval,<br>Topical-Chat and QAGS, of two NLG tasks, sum-<br>marization and dialogue response generation.</p>",
            "id": 34,
            "page": 3,
            "text": "Following Zhong  (2022), we meta-evaluate our evaluator on three benchmarks, SummEval, Topical-Chat and QAGS, of two NLG tasks, summarization and dialogue response generation."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2669
                },
                {
                    "x": 1844,
                    "y": 2669
                },
                {
                    "x": 1844,
                    "y": 2723
                },
                {
                    "x": 1272,
                    "y": 2723
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:20px'>3.1 Implementation Details</p>",
            "id": 35,
            "page": 3,
            "text": "3.1 Implementation Details"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2745
                },
                {
                    "x": 2202,
                    "y": 2745
                },
                {
                    "x": 2202,
                    "y": 3193
                },
                {
                    "x": 1272,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:14px'>We use OpenAI's GPT family as our LLMs, includ-<br>ing GPT-3.5 (text-davinci-003) and GPT-4. For<br>GPT-3.5, we set decoding temperature to 0 to in-<br>crease the model's determinism. For GPT-4, as it<br>does not support the output of token probabilities,<br>we set 'n = 20, temperature = 1, top-p = 1' to<br>sample 20 times to estimate the token probabilities.<br>We use G-EVAL-4 to indicate G-EVAL with GPT-4</p>",
            "id": 36,
            "page": 3,
            "text": "We use OpenAI's GPT family as our LLMs, including GPT-3.5 (text-davinci-003) and GPT-4. For GPT-3.5, we set decoding temperature to 0 to increase the model's determinism. For GPT-4, as it does not support the output of token probabilities, we set 'n = 20, temperature = 1, top-p = 1' to sample 20 times to estimate the token probabilities. We use G-EVAL-4 to indicate G-EVAL with GPT-4"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 250
                },
                {
                    "x": 2173,
                    "y": 250
                },
                {
                    "x": 2173,
                    "y": 1121
                },
                {
                    "x": 296,
                    "y": 1121
                }
            ],
            "category": "table",
            "html": "<table id='37' style='font-size:16px'><tr><td rowspan=\"2\">Metrics</td><td colspan=\"2\">Coherence</td><td colspan=\"2\">Consistency</td><td colspan=\"2\">Fluency</td><td colspan=\"2\">Relevance</td><td colspan=\"2\">AVG</td></tr><tr><td>P</td><td>T</td><td>P</td><td>T</td><td>P</td><td>T</td><td>P</td><td>T</td><td>P</td><td>T</td></tr><tr><td>ROUGE-1</td><td>0.167</td><td>0.126</td><td>0.160</td><td>0.130</td><td>0.115</td><td>0.094</td><td>0.326</td><td>0.252</td><td>0.192</td><td>0.150</td></tr><tr><td>ROUGE-2</td><td>0.184</td><td>0.139</td><td>0.187</td><td>0.155</td><td>0.159</td><td>0.128</td><td>0.290</td><td>0.219</td><td>0.205</td><td>0.161</td></tr><tr><td>ROUGE-L</td><td>0.128</td><td>0.099</td><td>0.115</td><td>0.092</td><td>0.105</td><td>0.084</td><td>0.311</td><td>0.237</td><td>0.165</td><td>0.128</td></tr><tr><td>BERTScore</td><td>0.284</td><td>0.211</td><td>0.110</td><td>0.090</td><td>0.193</td><td>0.158</td><td>0.312</td><td>0.243</td><td>0.225</td><td>0.175</td></tr><tr><td>MOVERSscore</td><td>0.159</td><td>0.118</td><td>0.157</td><td>0.127</td><td>0.129</td><td>0.105</td><td>0.318</td><td>0.244</td><td>0.191</td><td>0.148</td></tr><tr><td>BARTScore</td><td>0.448</td><td>0.342</td><td>0.382</td><td>0.315</td><td>0.356</td><td>0.292</td><td>0.356</td><td>0.273</td><td>0.385</td><td>0.305</td></tr><tr><td>UniEval</td><td>0.575</td><td>0.442</td><td>0.446</td><td>0.371</td><td>0.449</td><td>0.371</td><td>0.426</td><td>0.325</td><td>0.474</td><td>0.377</td></tr><tr><td>GPTScore</td><td>0.434</td><td>-</td><td>0.449</td><td>-</td><td>0.403</td><td>-</td><td>0.381</td><td>-</td><td>0.417</td><td>-</td></tr><tr><td>G-EVAL-3.5</td><td>0.440</td><td>0.335</td><td>0.386</td><td>0.318</td><td>0.424</td><td>0.347</td><td>0.385</td><td>0.293</td><td>0.401</td><td>0.320</td></tr><tr><td>- Probs</td><td>0.359</td><td>0.313</td><td>0.361</td><td>0.344</td><td>0.339</td><td>0.323</td><td>0.327</td><td>0.288</td><td>0.346</td><td>0.317</td></tr><tr><td>G-EVAL-4</td><td>0.582</td><td>0.457</td><td>0.507</td><td>0.425</td><td>0.455</td><td>0.378</td><td>0.547</td><td>0.433</td><td>0.514</td><td>0.418</td></tr><tr><td>- Probs</td><td>0.560</td><td>0.472</td><td>0.501</td><td>0.459</td><td>0.438</td><td>0.408</td><td>0.511</td><td>0.444</td><td>0.502</td><td>0.446</td></tr><tr><td>- CoT</td><td>0.564</td><td>0.454</td><td>0.493</td><td>0.413</td><td>0.403</td><td>0.334</td><td>0.538</td><td>0.427</td><td>0.500</td><td>0.407</td></tr></table>",
            "id": 37,
            "page": 4,
            "text": "Metrics Coherence Consistency Fluency Relevance AVG  P T P T P T P T P T  ROUGE-1 0.167 0.126 0.160 0.130 0.115 0.094 0.326 0.252 0.192 0.150  ROUGE-2 0.184 0.139 0.187 0.155 0.159 0.128 0.290 0.219 0.205 0.161  ROUGE-L 0.128 0.099 0.115 0.092 0.105 0.084 0.311 0.237 0.165 0.128  BERTScore 0.284 0.211 0.110 0.090 0.193 0.158 0.312 0.243 0.225 0.175  MOVERSscore 0.159 0.118 0.157 0.127 0.129 0.105 0.318 0.244 0.191 0.148  BARTScore 0.448 0.342 0.382 0.315 0.356 0.292 0.356 0.273 0.385 0.305  UniEval 0.575 0.442 0.446 0.371 0.449 0.371 0.426 0.325 0.474 0.377  GPTScore 0.434 - 0.449 - 0.403 - 0.381 - 0.417  G-EVAL-3.5 0.440 0.335 0.386 0.318 0.424 0.347 0.385 0.293 0.401 0.320  - Probs 0.359 0.313 0.361 0.344 0.339 0.323 0.327 0.288 0.346 0.317  G-EVAL-4 0.582 0.457 0.507 0.425 0.455 0.378 0.547 0.433 0.514 0.418  - Probs 0.560 0.472 0.501 0.459 0.438 0.408 0.511 0.444 0.502 0.446  - CoT 0.564 0.454 0.493 0.413 0.403 0.334 0.538 0.427 0.500"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1152
                },
                {
                    "x": 2203,
                    "y": 1152
                },
                {
                    "x": 2203,
                    "y": 1353
                },
                {
                    "x": 291,
                    "y": 1353
                }
            ],
            "category": "caption",
            "html": "<caption id='38' style='font-size:14px'>Table 1: Summary-level Spearman (p) and Kendall-Tau (T) correlations of different metrics on SummEval bench-<br>mark. G-EVAL without probabilities (italicized) should not be considered as a fair comparison to other metrics on T,<br>as it leads to many ties in the scores. This results in a higher Kendall-Tau correlation, but it does not fairly reflect<br>the true evaluation ability. More details are in Section 4.</caption>",
            "id": 38,
            "page": 4,
            "text": "Table 1: Summary-level Spearman (p) and Kendall-Tau (T) correlations of different metrics on SummEval benchmark. G-EVAL without probabilities (italicized) should not be considered as a fair comparison to other metrics on T, as it leads to many ties in the scores. This results in a higher Kendall-Tau correlation, but it does not fairly reflect the true evaluation ability. More details are in Section 4."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1444
                },
                {
                    "x": 1219,
                    "y": 1444
                },
                {
                    "x": 1219,
                    "y": 1666
                },
                {
                    "x": 291,
                    "y": 1666
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>as the backbone model, and G-EVAL-3.5 to indi-<br>cate G-EVAL with GPT-3.5 as the backbone model.<br>Example prompts for each task are provided in the<br>Appendix.</p>",
            "id": 39,
            "page": 4,
            "text": "as the backbone model, and G-EVAL-3.5 to indicate G-EVAL with GPT-3.5 as the backbone model. Example prompts for each task are provided in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1708
                },
                {
                    "x": 653,
                    "y": 1708
                },
                {
                    "x": 653,
                    "y": 1760
                },
                {
                    "x": 292,
                    "y": 1760
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:18px'>3.2 Benchmarks</p>",
            "id": 40,
            "page": 4,
            "text": "3.2 Benchmarks"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1784
                },
                {
                    "x": 1218,
                    "y": 1784
                },
                {
                    "x": 1218,
                    "y": 1951
                },
                {
                    "x": 292,
                    "y": 1951
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:20px'>We adopt three meta-evaluation benchmarks to<br>measure the correlation between G-EVAL and<br>human judgments.</p>",
            "id": 41,
            "page": 4,
            "text": "We adopt three meta-evaluation benchmarks to measure the correlation between G-EVAL and human judgments."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2008
                },
                {
                    "x": 1221,
                    "y": 2008
                },
                {
                    "x": 1221,
                    "y": 2401
                },
                {
                    "x": 291,
                    "y": 2401
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:20px'>SummEval (Fabbri et al., 2021) is a bench-<br>mark that compares different evaluation methods<br>for summarization. It gives human ratings<br>for four aspects of each summary: fluency,<br>coherence, consistency and relevance.<br>It is built on the CNN/DailyMail dataset (Hermann<br>et al., 2015)</p>",
            "id": 42,
            "page": 4,
            "text": "SummEval (Fabbri , 2021) is a benchmark that compares different evaluation methods for summarization. It gives human ratings for four aspects of each summary: fluency, coherence, consistency and relevance. It is built on the CNN/DailyMail dataset (Hermann , 2015)"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2460
                },
                {
                    "x": 1219,
                    "y": 2460
                },
                {
                    "x": 1219,
                    "y": 2855
                },
                {
                    "x": 291,
                    "y": 2855
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:22px'>Topical-Chat (Mehri and Eskenazi, 2020)<br>is a testbed for meta-evaluating different<br>evaluators on dialogue response generation<br>systems that use knowledge. We follow (Zhong<br>et al., 2022) to use its human ratings on<br>four aspects: naturalness, coherence,<br>engagingness and groundedness.</p>",
            "id": 43,
            "page": 4,
            "text": "Topical-Chat (Mehri and Eskenazi, 2020) is a testbed for meta-evaluating different evaluators on dialogue response generation systems that use knowledge. We follow (Zhong , 2022) to use its human ratings on four aspects: naturalness, coherence, engagingness and groundedness."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2910
                },
                {
                    "x": 1219,
                    "y": 2910
                },
                {
                    "x": 1219,
                    "y": 3193
                },
                {
                    "x": 290,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:18px'>QAGS (Wang et al., 2020) is a benchmark<br>for evaluating hallucinations in the summarization<br>task. It aims to measure the consistency<br>dimension of summaries on two different<br>summarization datasets.</p>",
            "id": 44,
            "page": 4,
            "text": "QAGS (Wang , 2020) is a benchmark for evaluating hallucinations in the summarization task. It aims to measure the consistency dimension of summaries on two different summarization datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1443
                },
                {
                    "x": 1567,
                    "y": 1443
                },
                {
                    "x": 1567,
                    "y": 1494
                },
                {
                    "x": 1272,
                    "y": 1494
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:20px'>3.3 Baselines</p>",
            "id": 45,
            "page": 4,
            "text": "3.3 Baselines"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1526
                },
                {
                    "x": 2195,
                    "y": 1526
                },
                {
                    "x": 2195,
                    "y": 1634
                },
                {
                    "x": 1274,
                    "y": 1634
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:20px'>We evaluate G-EVAL against various evaluators<br>that achieved state-of-the-art performance.</p>",
            "id": 46,
            "page": 4,
            "text": "We evaluate G-EVAL against various evaluators that achieved state-of-the-art performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1646
                },
                {
                    "x": 2201,
                    "y": 1646
                },
                {
                    "x": 2201,
                    "y": 1810
                },
                {
                    "x": 1273,
                    "y": 1810
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:22px'>BERTScore (Zhang et al., 2019) measures the<br>similarity between two texts based on the contextu-<br>alized embedding from BERT (Devlin et al., 2019).</p>",
            "id": 47,
            "page": 4,
            "text": "BERTScore (Zhang , 2019) measures the similarity between two texts based on the contextualized embedding from BERT (Devlin , 2019)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1821
                },
                {
                    "x": 2202,
                    "y": 1821
                },
                {
                    "x": 2202,
                    "y": 2041
                },
                {
                    "x": 1272,
                    "y": 2041
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:22px'>MoverScore (Zhao et al., 2019) improves<br>BERTScore by adding soft alignments and new<br>aggregation methods to obtain a more robust simi-<br>larity measure.</p>",
            "id": 48,
            "page": 4,
            "text": "MoverScore (Zhao , 2019) improves BERTScore by adding soft alignments and new aggregation methods to obtain a more robust similarity measure."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2052
                },
                {
                    "x": 2201,
                    "y": 2052
                },
                {
                    "x": 2201,
                    "y": 2329
                },
                {
                    "x": 1271,
                    "y": 2329
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:18px'>BARTScore (Yuan et al., 2021) is a unified eval-<br>uator which evaluate with the average likelihood<br>of the pretrained encoder-decoder model, BART<br>(Lewis et al., 2020). It can predict different scores<br>depending on the formats of source and target.</p>",
            "id": 49,
            "page": 4,
            "text": "BARTScore (Yuan , 2021) is a unified evaluator which evaluate with the average likelihood of the pretrained encoder-decoder model, BART (Lewis , 2020). It can predict different scores depending on the formats of source and target."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2337
                },
                {
                    "x": 2201,
                    "y": 2337
                },
                {
                    "x": 2201,
                    "y": 2842
                },
                {
                    "x": 1271,
                    "y": 2842
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:18px'>FactCC and QAGS (Krysci�ski et al., 2020;<br>Wang et al., 2020) are two evaluators that measure<br>the factual consistency of generated summaries.<br>FactCC is a BERT-based classifier that predicts<br>whether a summary is consistent with the source<br>document. QAGS is a question-answering based<br>evaluator that generates questions from the sum-<br>mary and checks if the answers can be found in the<br>source document.</p>",
            "id": 50,
            "page": 4,
            "text": "FactCC and QAGS (Krysci�ski , 2020; Wang , 2020) are two evaluators that measure the factual consistency of generated summaries. FactCC is a BERT-based classifier that predicts whether a summary is consistent with the source document. QAGS is a question-answering based evaluator that generates questions from the summary and checks if the answers can be found in the source document."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2852
                },
                {
                    "x": 2200,
                    "y": 2852
                },
                {
                    "x": 2200,
                    "y": 3074
                },
                {
                    "x": 1272,
                    "y": 3074
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:20px'>USR (Mehri and Eskenazi, 2020) is evaluator<br>that assess dialogue response generation from dif-<br>ferent perspectives. It has several versions that<br>assign different scores to each target response.</p>",
            "id": 51,
            "page": 4,
            "text": "USR (Mehri and Eskenazi, 2020) is evaluator that assess dialogue response generation from different perspectives. It has several versions that assign different scores to each target response."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 3083
                },
                {
                    "x": 2204,
                    "y": 3083
                },
                {
                    "x": 2204,
                    "y": 3194
                },
                {
                    "x": 1272,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:16px'>UniEval (Zhong et al., 2022) is a unified evalua-<br>tor that can evaluate different aspects of text gen-</p>",
            "id": 52,
            "page": 4,
            "text": "UniEval (Zhong , 2022) is a unified evaluator that can evaluate different aspects of text gen-"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 268
                },
                {
                    "x": 1220,
                    "y": 268
                },
                {
                    "x": 1220,
                    "y": 603
                },
                {
                    "x": 290,
                    "y": 603
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:14px'>eration as QA tasks. It uses a pretrained T5 model<br>(Raffel et al., 2020) to encode the evaluation task,<br>source and target texts as questions and answers,<br>and then computes the QA score as the evaluation<br>score. It can also handle different evaluation tasks<br>by changing the question format.</p>",
            "id": 53,
            "page": 5,
            "text": "eration as QA tasks. It uses a pretrained T5 model (Raffel , 2020) to encode the evaluation task, source and target texts as questions and answers, and then computes the QA score as the evaluation score. It can also handle different evaluation tasks by changing the question format."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 607
                },
                {
                    "x": 1220,
                    "y": 607
                },
                {
                    "x": 1220,
                    "y": 1058
                },
                {
                    "x": 292,
                    "y": 1058
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:18px'>GPTScore (Fu et al., 2023) is a new framework<br>that evaluates texts with generative pre-training<br>models like GPT-3. It assumes that a generative<br>pre-training model will assign a higher probability<br>of high-quality generated text following a given in-<br>struction and context. Unlike G-EVAL, GPTScore<br>formulates the evaluation task as a conditional gen-<br>eration problem instead of a form-filling problem.</p>",
            "id": 54,
            "page": 5,
            "text": "GPTScore (Fu , 2023) is a new framework that evaluates texts with generative pre-training models like GPT-3. It assumes that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context. Unlike G-EVAL, GPTScore formulates the evaluation task as a conditional generation problem instead of a form-filling problem."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1093
                },
                {
                    "x": 930,
                    "y": 1093
                },
                {
                    "x": 930,
                    "y": 1147
                },
                {
                    "x": 291,
                    "y": 1147
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>3.4 Results for Summarization</p>",
            "id": 55,
            "page": 5,
            "text": "3.4 Results for Summarization"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1166
                },
                {
                    "x": 1219,
                    "y": 1166
                },
                {
                    "x": 1219,
                    "y": 1898
                },
                {
                    "x": 291,
                    "y": 1898
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:16px'>We adopt the same approach as Zhong et al. (2022)<br>to evaluate different summarization metrics using<br>summary-level Spearman and Kendall-Tau corre-<br>lation. The first part of Table 1 shows the results<br>of metrics that compare the semantic similarity<br>between the model output and the reference text.<br>These metrics perform poorly on most dimensions.<br>The second part shows the results of metrics that<br>use neural networks to learn from human ratings of<br>summary quality. These metrics have much higher<br>correlations than the similarity-based metrics, sug-<br>gesting that they are more reliable for summariza-<br>tion evaluation.</p>",
            "id": 56,
            "page": 5,
            "text": "We adopt the same approach as Zhong  (2022) to evaluate different summarization metrics using summary-level Spearman and Kendall-Tau correlation. The first part of Table 1 shows the results of metrics that compare the semantic similarity between the model output and the reference text. These metrics perform poorly on most dimensions. The second part shows the results of metrics that use neural networks to learn from human ratings of summary quality. These metrics have much higher correlations than the similarity-based metrics, suggesting that they are more reliable for summarization evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1902
                },
                {
                    "x": 1218,
                    "y": 1902
                },
                {
                    "x": 1218,
                    "y": 2693
                },
                {
                    "x": 291,
                    "y": 2693
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:18px'>In the last part of Table 1 which corresponds to<br>GPT-based evaluators, GPTScore also uses GPTs<br>for evaluating summarization texts, but relies on<br>GPT's conditional probabilities of the given tar-<br>get. G-EVAL substantially surpasses all previous<br>state-of-the-art evaluators on the SummEval bench-<br>mark. G-EVAL-4 achieved much higher human<br>correspondence compared with G-EVAL-3.5 on<br>both Spearman and Kendall-Tau correlation, which<br>indicates that the larger model size of GPT-4 is<br>beneficial for summarization evaluation. G-EVAL<br>also outperforms GPTScore on several dimension,<br>demonstrating the effectiveness of the simple form-<br>filling paradigm.</p>",
            "id": 57,
            "page": 5,
            "text": "In the last part of Table 1 which corresponds to GPT-based evaluators, GPTScore also uses GPTs for evaluating summarization texts, but relies on GPT's conditional probabilities of the given target. G-EVAL substantially surpasses all previous state-of-the-art evaluators on the SummEval benchmark. G-EVAL-4 achieved much higher human correspondence compared with G-EVAL-3.5 on both Spearman and Kendall-Tau correlation, which indicates that the larger model size of GPT-4 is beneficial for summarization evaluation. G-EVAL also outperforms GPTScore on several dimension, demonstrating the effectiveness of the simple formfilling paradigm."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2727
                },
                {
                    "x": 1029,
                    "y": 2727
                },
                {
                    "x": 1029,
                    "y": 2779
                },
                {
                    "x": 292,
                    "y": 2779
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:20px'>3.5 Results for Dialogue Generation</p>",
            "id": 58,
            "page": 5,
            "text": "3.5 Results for Dialogue Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2802
                },
                {
                    "x": 1219,
                    "y": 2802
                },
                {
                    "x": 1219,
                    "y": 3194
                },
                {
                    "x": 291,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:16px'>We use the Topical-chat benchmark from Mehri<br>and Eskenazi (2020) to measure how well differ-<br>ent evaluators agree with human ratings on the<br>quality of dialogue responses. We calculate the<br>Pearson and Spearman correlation for each turn<br>of the dialogue. Table 2 shows that similarity-<br>based metrics have good agreement with humans</p>",
            "id": 59,
            "page": 5,
            "text": "We use the Topical-chat benchmark from Mehri and Eskenazi (2020) to measure how well different evaluators agree with human ratings on the quality of dialogue responses. We calculate the Pearson and Spearman correlation for each turn of the dialogue. Table 2 shows that similaritybased metrics have good agreement with humans"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 270
                },
                {
                    "x": 2201,
                    "y": 270
                },
                {
                    "x": 2201,
                    "y": 546
                },
                {
                    "x": 1272,
                    "y": 546
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:14px'>on how engaging and grounded the responses<br>are, but not on the other aspects. With respect<br>to the learning-based evaluators, before G-EVAL,<br>UniEval predicts scores that are most consistent<br>with human judgments across all aspects.</p>",
            "id": 60,
            "page": 5,
            "text": "on how engaging and grounded the responses are, but not on the other aspects. With respect to the learning-based evaluators, before G-EVAL, UniEval predicts scores that are most consistent with human judgments across all aspects."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 553
                },
                {
                    "x": 2202,
                    "y": 553
                },
                {
                    "x": 2202,
                    "y": 890
                },
                {
                    "x": 1272,
                    "y": 890
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:16px'>As shown in the last part, G-EVAL also substan-<br>tially surpasses all previous state-of-the-art eval-<br>uator on the Topical-Chat benchmark. Notably,<br>the G-EVAL-3.5 can achieve similar results with<br>G-EVAL-4. This indicates that this benchmark is<br>relatively easy for the G-EVAL model.</p>",
            "id": 61,
            "page": 5,
            "text": "As shown in the last part, G-EVAL also substantially surpasses all previous state-of-the-art evaluator on the Topical-Chat benchmark. Notably, the G-EVAL-3.5 can achieve similar results with G-EVAL-4. This indicates that this benchmark is relatively easy for the G-EVAL model."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 936
                },
                {
                    "x": 1883,
                    "y": 936
                },
                {
                    "x": 1883,
                    "y": 988
                },
                {
                    "x": 1272,
                    "y": 988
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:14px'>3.6 Results on Hallucinations</p>",
            "id": 62,
            "page": 5,
            "text": "3.6 Results on Hallucinations"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1014
                },
                {
                    "x": 2202,
                    "y": 1014
                },
                {
                    "x": 2202,
                    "y": 1915
                },
                {
                    "x": 1272,
                    "y": 1915
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:16px'>Advanced NLG models often produce text that<br>does not match the context input (Cao et al., 2018),<br>and recent studies find even powerful LLMs also<br>suffer from the problem of hallucination. This<br>motivates recent research to design evaluators for<br>measuring the consistency aspect in summa-<br>rization (Krysci�ski et al., 2020; Wang et al.,<br>2020; Cao et al., 2020; Durmus et al., 2020). We<br>test the QAGS meta-evaluation benchmark, which<br>includes two different summarization datasets:<br>CNN/DailyMail and XSum (Narayan et al., 2018)<br>Table 3 shows that BARTScore performs well on<br>the more extractive subset (QAGS-CNN), but has<br>low correlation on the more abstractive subset<br>(QAGS-Xsum). UniEval has good correlation on<br>both subsets of the data.</p>",
            "id": 63,
            "page": 5,
            "text": "Advanced NLG models often produce text that does not match the context input (Cao , 2018), and recent studies find even powerful LLMs also suffer from the problem of hallucination. This motivates recent research to design evaluators for measuring the consistency aspect in summarization (Krysci�ski , 2020; Wang , 2020; Cao , 2020; Durmus , 2020). We test the QAGS meta-evaluation benchmark, which includes two different summarization datasets: CNN/DailyMail and XSum (Narayan , 2018) Table 3 shows that BARTScore performs well on the more extractive subset (QAGS-CNN), but has low correlation on the more abstractive subset (QAGS-Xsum). UniEval has good correlation on both subsets of the data."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1922
                },
                {
                    "x": 2201,
                    "y": 1922
                },
                {
                    "x": 2201,
                    "y": 2312
                },
                {
                    "x": 1272,
                    "y": 2312
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:16px'>On average, G-EVAL-4 outperforms all state-of-<br>the-art evaluators on QAGS, with a large margin<br>on QAGS-Xsum. G-EVAL-3.5, on the other hand,<br>failed to perform well on this benchmark, which<br>indicates that the consistency aspect is sensitive to<br>the LLM's capacity. This result is consistent with<br>Table 1.</p>",
            "id": 64,
            "page": 5,
            "text": "On average, G-EVAL-4 outperforms all state-ofthe-art evaluators on QAGS, with a large margin on QAGS-Xsum. G-EVAL-3.5, on the other hand, failed to perform well on this benchmark, which indicates that the consistency aspect is sensitive to the LLM's capacity. This result is consistent with Table 1."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2365
                },
                {
                    "x": 1540,
                    "y": 2365
                },
                {
                    "x": 1540,
                    "y": 2420
                },
                {
                    "x": 1272,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:22px'>4 Analysis</p>",
            "id": 65,
            "page": 5,
            "text": "4 Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2459
                },
                {
                    "x": 2202,
                    "y": 2459
                },
                {
                    "x": 2202,
                    "y": 3196
                },
                {
                    "x": 1273,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:16px'>Will G-EVAL prefer LLM-based outputs? One<br>concern about using LLM as an evaluator is that it<br>may prefer the outputs generated by the LLM itself,<br>rather than the high-quality human-written texts.<br>To investigate this issue, we conduct an experi-<br>ment on the summarization task, where we com-<br>pare the evaluation scores of the LLM-generated<br>and the human-written summaries. We use the<br>dataset collected in Zhang et al. (2023), where they<br>first ask freelance writers to write high-quality sum-<br>maries for news articles, and then ask annotators<br>to compare human-written summaries and LLM-<br>generated summaries (using GPT-3.5, text-davinci-</p>",
            "id": 66,
            "page": 5,
            "text": "Will G-EVAL prefer LLM-based outputs? One concern about using LLM as an evaluator is that it may prefer the outputs generated by the LLM itself, rather than the high-quality human-written texts. To investigate this issue, we conduct an experiment on the summarization task, where we compare the evaluation scores of the LLM-generated and the human-written summaries. We use the dataset collected in Zhang  (2023), where they first ask freelance writers to write high-quality summaries for news articles, and then ask annotators to compare human-written summaries and LLMgenerated summaries (using GPT-3.5, text-davinci-"
        },
        {
            "bounding_box": [
                {
                    "x": 298,
                    "y": 251
                },
                {
                    "x": 2152,
                    "y": 251
                },
                {
                    "x": 2152,
                    "y": 841
                },
                {
                    "x": 298,
                    "y": 841
                }
            ],
            "category": "table",
            "html": "<table id='67' style='font-size:18px'><tr><td rowspan=\"2\">Metrics</td><td colspan=\"2\">Naturalness</td><td colspan=\"2\">Coherence</td><td colspan=\"2\">Engagingness</td><td colspan=\"2\">Groundedness</td><td colspan=\"2\">AVG</td></tr><tr><td>r</td><td>P</td><td>r</td><td>P</td><td>r</td><td>P</td><td>r</td><td>P</td><td>r</td><td>P</td></tr><tr><td>ROUGE-L</td><td>0.176</td><td>0.146</td><td>0.193</td><td>0.203</td><td>0.295</td><td>0.300</td><td>0.310</td><td>0.327</td><td>0.243</td><td>0.244</td></tr><tr><td>BLEU-4</td><td>0.180</td><td>0.175</td><td>0.131</td><td>0.235</td><td>0.232</td><td>0.316</td><td>0.213</td><td>0.310</td><td>0.189</td><td>0.259</td></tr><tr><td>METEOR</td><td>0.212</td><td>0.191</td><td>0.250</td><td>0.302</td><td>0.367</td><td>0.439</td><td>0.333</td><td>0.391</td><td>0.290</td><td>0.331</td></tr><tr><td>BERTScore</td><td>0.226</td><td>0.209</td><td>0.214</td><td>0.233</td><td>0.317</td><td>0.335</td><td>0.291</td><td>0.317</td><td>0.262</td><td>0.273</td></tr><tr><td>USR</td><td>0.337</td><td>0.325</td><td>0.416</td><td>0.377</td><td>0.456</td><td>0.465</td><td>0.222</td><td>0.447</td><td>0.358</td><td>0.403</td></tr><tr><td>UniEval</td><td>0.455</td><td>0.330</td><td>0.602</td><td>0.455</td><td>0.573</td><td>0.430</td><td>0.577</td><td>0.453</td><td>0.552</td><td>0.417</td></tr><tr><td>G-EVAL-3.5</td><td>0.532</td><td>0.539</td><td>0.519</td><td>0.544</td><td>0.660</td><td>0.691</td><td>0.586</td><td>0.567</td><td>0.574</td><td>0.585</td></tr><tr><td>G-EVAL-4</td><td>0.549</td><td>0.565</td><td>0.594</td><td>0.605</td><td>0.627</td><td>0.631</td><td>0.531</td><td>0.551</td><td>0.575</td><td>0.588</td></tr></table>",
            "id": 67,
            "page": 6,
            "text": "Metrics Naturalness Coherence Engagingness Groundedness AVG  r P r P r P r P r P  ROUGE-L 0.176 0.146 0.193 0.203 0.295 0.300 0.310 0.327 0.243 0.244  BLEU-4 0.180 0.175 0.131 0.235 0.232 0.316 0.213 0.310 0.189 0.259  METEOR 0.212 0.191 0.250 0.302 0.367 0.439 0.333 0.391 0.290 0.331  BERTScore 0.226 0.209 0.214 0.233 0.317 0.335 0.291 0.317 0.262 0.273  USR 0.337 0.325 0.416 0.377 0.456 0.465 0.222 0.447 0.358 0.403  UniEval 0.455 0.330 0.602 0.455 0.573 0.430 0.577 0.453 0.552 0.417  G-EVAL-3.5 0.532 0.539 0.519 0.544 0.660 0.691 0.586 0.567 0.574 0.585  G-EVAL-4 0.549 0.565 0.594 0.605 0.627 0.631 0.531 0.551 0.575"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 871
                },
                {
                    "x": 2190,
                    "y": 871
                },
                {
                    "x": 2190,
                    "y": 922
                },
                {
                    "x": 293,
                    "y": 922
                }
            ],
            "category": "caption",
            "html": "<caption id='68' style='font-size:18px'>Table 2: Turn-level Spearman (p) and Kendall-Tau (T) correlations of different metrics on Topical-Chat benchmark.</caption>",
            "id": 68,
            "page": 6,
            "text": "Table 2: Turn-level Spearman (p) and Kendall-Tau (T) correlations of different metrics on Topical-Chat benchmark."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1013
                },
                {
                    "x": 393,
                    "y": 1013
                },
                {
                    "x": 393,
                    "y": 1059
                },
                {
                    "x": 292,
                    "y": 1059
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:22px'>003).</p>",
            "id": 69,
            "page": 6,
            "text": "003)."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1068
                },
                {
                    "x": 1220,
                    "y": 1068
                },
                {
                    "x": 1220,
                    "y": 1571
                },
                {
                    "x": 292,
                    "y": 1571
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:20px'>The dataset can be divided in three categories:<br>1) human-written summaries that are rated higher<br>than GPT-3.5 summaries by human judges, 2)<br>human-written summaries that are rated lower<br>than GPT-3.5 summaries by human judges, and 3)<br>human-written summaries and GPT-3.5 summaries<br>are rated equally good by human judges. We use G-<br>EVAL-4 to evaluate the summaries in each category,<br>2<br>and compare the averaged scores.</p>",
            "id": 70,
            "page": 6,
            "text": "The dataset can be divided in three categories: 1) human-written summaries that are rated higher than GPT-3.5 summaries by human judges, 2) human-written summaries that are rated lower than GPT-3.5 summaries by human judges, and 3) human-written summaries and GPT-3.5 summaries are rated equally good by human judges. We use GEVAL-4 to evaluate the summaries in each category, 2 and compare the averaged scores."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1577
                },
                {
                    "x": 1219,
                    "y": 1577
                },
                {
                    "x": 1219,
                    "y": 2139
                },
                {
                    "x": 292,
                    "y": 2139
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:20px'>The results are shown in Figure 2. We can see<br>that, G-EVAL-4 assigns higher scores to human-<br>written summaries when human judges also pre-<br>fer human-written summaries, and assigns lower<br>scores when human judges prefer GPT-3.5 sum-<br>maries. However, G-EVAL-4 always gives higher<br>scores to GPT-3.5 summaries than human-written<br>summaries, even when human judges prefer human-<br>written summaries. We propose two potential rea-<br>sons for this phenomenon:</p>",
            "id": 71,
            "page": 6,
            "text": "The results are shown in Figure 2. We can see that, G-EVAL-4 assigns higher scores to humanwritten summaries when human judges also prefer human-written summaries, and assigns lower scores when human judges prefer GPT-3.5 summaries. However, G-EVAL-4 always gives higher scores to GPT-3.5 summaries than human-written summaries, even when human judges prefer humanwritten summaries. We propose two potential reasons for this phenomenon:"
        },
        {
            "bounding_box": [
                {
                    "x": 327,
                    "y": 2174
                },
                {
                    "x": 1217,
                    "y": 2174
                },
                {
                    "x": 1217,
                    "y": 2783
                },
                {
                    "x": 327,
                    "y": 2783
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:20px'>1. NLG outputs from high-quality systems are<br>in natural difficult to evaluate. The authors of<br>the original paper found that inter-annotator<br>agreement on judging human-written and<br>LLM-generated summaries is very low, with<br>Krippendorff's alpha at 0.07.<br>2. G-EVAL may have a bias towards the LLM-<br>generated summaries because the model could<br>share the same concept of evaluation criteria<br>during generation and evaluation.</p>",
            "id": 72,
            "page": 6,
            "text": "1. NLG outputs from high-quality systems are in natural difficult to evaluate. The authors of the original paper found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07. 2. G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2823
                },
                {
                    "x": 1217,
                    "y": 2823
                },
                {
                    "x": 1217,
                    "y": 2989
                },
                {
                    "x": 291,
                    "y": 2989
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:18px'>Our work should be considered as a preliminary<br>study on this issue, and more research is needed<br>to fully understand the behavior of LLM-based</p>",
            "id": 73,
            "page": 6,
            "text": "Our work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 3018
                },
                {
                    "x": 1217,
                    "y": 3018
                },
                {
                    "x": 1217,
                    "y": 3192
                },
                {
                    "x": 292,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:16px'>2We use G-EVAL-4 in this experiment, because its su-<br>periority in evaluating summarization tasks. Although it has<br>different distribution with with GPT-3.5, the two LLMs should<br>share similar behaviors in terms of text generation.</p>",
            "id": 74,
            "page": 6,
            "text": "2We use G-EVAL-4 in this experiment, because its superiority in evaluating summarization tasks. Although it has different distribution with with GPT-3.5, the two LLMs should share similar behaviors in terms of text generation."
        },
        {
            "bounding_box": [
                {
                    "x": 1335,
                    "y": 1008
                },
                {
                    "x": 1356,
                    "y": 1008
                },
                {
                    "x": 1356,
                    "y": 1032
                },
                {
                    "x": 1335,
                    "y": 1032
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:14px'>4</p>",
            "id": 75,
            "page": 6,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 1300,
                    "y": 1025
                },
                {
                    "x": 2176,
                    "y": 1025
                },
                {
                    "x": 2176,
                    "y": 1519
                },
                {
                    "x": 1300,
                    "y": 1519
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='76' style='font-size:14px' alt=\"3.95\n3.9\n3.85\n3.8\n3.75\nHuman GPT-3.5 Human GPT-3.5 Human GPT-3.5\nSummary Summary Summary Summary Summary Summary\nHuman Summary is Better LLM Summary is Better Equally Good\" data-coord=\"top-left:(1300,1025); bottom-right:(2176,1519)\" /></figure>",
            "id": 76,
            "page": 6,
            "text": "3.95 3.9 3.85 3.8 3.75 Human GPT-3.5 Human GPT-3.5 Human GPT-3.5 Summary Summary Summary Summary Summary Summary Human Summary is Better LLM Summary is Better Equally Good"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1561
                },
                {
                    "x": 2201,
                    "y": 1561
                },
                {
                    "x": 2201,
                    "y": 1714
                },
                {
                    "x": 1271,
                    "y": 1714
                }
            ],
            "category": "caption",
            "html": "<caption id='77' style='font-size:18px'>Figure 2: Averaged G-EVAL-4's scores for human-<br>written summaries and GPT-3.5 summaries, divided<br>by human judges' preference.</caption>",
            "id": 77,
            "page": 6,
            "text": "Figure 2: Averaged G-EVAL-4's scores for humanwritten summaries and GPT-3.5 summaries, divided by human judges' preference."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1812
                },
                {
                    "x": 2200,
                    "y": 1812
                },
                {
                    "x": 2200,
                    "y": 2258
                },
                {
                    "x": 1272,
                    "y": 2258
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>evaluators to reduce its inherent bias towards LLM-<br>generated text. We highlight this concern in the<br>context that LLM-based evaluators may lead to<br>self-reinforcement of LLMs if the evaluation score<br>is used as a reward signal for further tuning. And<br>this could result in the over-fitting of the LLMs to<br>their own evaluation criteria, rather than the true<br>evaluation criteria of the NLG tasks.</p>",
            "id": 78,
            "page": 6,
            "text": "evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2302
                },
                {
                    "x": 2199,
                    "y": 2302
                },
                {
                    "x": 2199,
                    "y": 2868
                },
                {
                    "x": 1272,
                    "y": 2868
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:20px'>The Effect of Chain-of-Thoughts We compare<br>the performance of G-EVAL with and without<br>chain-of-thoughts (CoT) on the SummEval bench-<br>mark. Table 1 shows that G-EVAL-4 with CoT has<br>higher correlation than G-EVAL-4 without CoT<br>on all dimensions, especially for fluency. This<br>suggests that CoT can provide more context and<br>guidance for the LLM to evaluate the generated<br>text, and can also help to explain the evaluation<br>process and results.</p>",
            "id": 79,
            "page": 6,
            "text": "The Effect of Chain-of-Thoughts We compare the performance of G-EVAL with and without chain-of-thoughts (CoT) on the SummEval benchmark. Table 1 shows that G-EVAL-4 with CoT has higher correlation than G-EVAL-4 without CoT on all dimensions, especially for fluency. This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2912
                },
                {
                    "x": 2200,
                    "y": 2912
                },
                {
                    "x": 2200,
                    "y": 3195
                },
                {
                    "x": 1271,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:20px'>The Effect of Probability Normalization We<br>compare the performance of G-EVAL with and<br>without probability normalization on the Sum-<br>mEval benchmark. Table 1 shows that, on Kendall-<br>Tau correlation, G-EVAL-4 with probabilities is</p>",
            "id": 80,
            "page": 6,
            "text": "The Effect of Probability Normalization We compare the performance of G-EVAL with and without probability normalization on the SummEval benchmark. Table 1 shows that, on KendallTau correlation, G-EVAL-4 with probabilities is"
        },
        {
            "bounding_box": [
                {
                    "x": 402,
                    "y": 253
                },
                {
                    "x": 2095,
                    "y": 253
                },
                {
                    "x": 2095,
                    "y": 1008
                },
                {
                    "x": 402,
                    "y": 1008
                }
            ],
            "category": "table",
            "html": "<table id='81' style='font-size:14px'><tr><td rowspan=\"2\">Metrics</td><td rowspan=\"2\" colspan=\"3\">QAGS-CNN r P</td><td rowspan=\"2\" colspan=\"3\">QAGS-XSUM r</td><td colspan=\"3\">Average</td></tr><tr><td>r</td><td>P</td><td>T</td></tr><tr><td>ROUGE-2</td><td>0.459</td><td>0.418</td><td>T 0.333</td><td>0.097</td><td>P 0.083</td><td>T 0.068</td><td>0.278</td><td>0.250</td><td>0.200</td></tr><tr><td>ROUGE-L</td><td>0.357</td><td>0.324</td><td>0.254</td><td>0.024</td><td>-0.011</td><td>-0.009</td><td>0.190</td><td>0.156</td><td>0.122</td></tr><tr><td>BERTScore</td><td>0.576</td><td>0.505</td><td>0.399</td><td>0.024</td><td>0.008</td><td>0.006</td><td>0.300</td><td>0.256</td><td>0.202</td></tr><tr><td>MoverScore</td><td>0.414</td><td>0.347</td><td>0.271</td><td>0.054</td><td>0.044</td><td>0.036</td><td>0.234</td><td>0.195</td><td>0.153</td></tr><tr><td>FactCC</td><td>0.416</td><td>0.484</td><td>0.376</td><td>0.297</td><td>0.259</td><td>0.212</td><td>0.356</td><td>0.371</td><td>0.294</td></tr><tr><td>QAGS</td><td>0.545</td><td>-</td><td>-</td><td>0.175</td><td>-</td><td>-</td><td>0.375</td><td>-</td><td>-</td></tr><tr><td>BARTScore</td><td>0.735</td><td>0.680</td><td>0.557</td><td>0.184</td><td>0.159</td><td>0.130</td><td>0.459</td><td>0.420</td><td>0.343</td></tr><tr><td>CTC</td><td>0.619</td><td>0.564</td><td>0.450</td><td>0.309</td><td>0.295</td><td>0.242</td><td>0.464</td><td>0.430</td><td>0.346</td></tr><tr><td>UniEval</td><td>0.682</td><td>0.662</td><td>0.532</td><td>0.461</td><td>0.488</td><td>0.399</td><td>0.571</td><td>0.575</td><td>0.465</td></tr><tr><td>G-EVAL-3.5</td><td>0.477</td><td>0.516</td><td>0.410</td><td>0.211</td><td>0.406</td><td>0.343</td><td>0.344</td><td>0.461</td><td>0.377</td></tr><tr><td>G-EVAL-4</td><td>0.631</td><td>0.685</td><td>0.591</td><td>0.558</td><td>0.537</td><td>0.472</td><td>0.599</td><td>0.611</td><td>0.525</td></tr></table>",
            "id": 81,
            "page": 7,
            "text": "Metrics QAGS-CNN r P QAGS-XSUM r Average  r P T  ROUGE-2 0.459 0.418 T 0.333 0.097 P 0.083 T 0.068 0.278 0.250 0.200  ROUGE-L 0.357 0.324 0.254 0.024 -0.011 -0.009 0.190 0.156 0.122  BERTScore 0.576 0.505 0.399 0.024 0.008 0.006 0.300 0.256 0.202  MoverScore 0.414 0.347 0.271 0.054 0.044 0.036 0.234 0.195 0.153  FactCC 0.416 0.484 0.376 0.297 0.259 0.212 0.356 0.371 0.294  QAGS 0.545 - - 0.175 - - 0.375 -  BARTScore 0.735 0.680 0.557 0.184 0.159 0.130 0.459 0.420 0.343  CTC 0.619 0.564 0.450 0.309 0.295 0.242 0.464 0.430 0.346  UniEval 0.682 0.662 0.532 0.461 0.488 0.399 0.571 0.575 0.465  G-EVAL-3.5 0.477 0.516 0.410 0.211 0.406 0.343 0.344 0.461 0.377  G-EVAL-4 0.631 0.685 0.591 0.558 0.537 0.472 0.599 0.611"
        },
        {
            "bounding_box": [
                {
                    "x": 313,
                    "y": 1039
                },
                {
                    "x": 2169,
                    "y": 1039
                },
                {
                    "x": 2169,
                    "y": 1091
                },
                {
                    "x": 313,
                    "y": 1091
                }
            ],
            "category": "caption",
            "html": "<caption id='82' style='font-size:14px'>Table 3: Pearson (r), Spearman (p) and Kendall-Tau (T) correlations of different metrics on QAGS benchmark.</caption>",
            "id": 82,
            "page": 7,
            "text": "Table 3: Pearson (r), Spearman (p) and Kendall-Tau (T) correlations of different metrics on QAGS benchmark."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1179
                },
                {
                    "x": 1220,
                    "y": 1179
                },
                {
                    "x": 1220,
                    "y": 2027
                },
                {
                    "x": 290,
                    "y": 2027
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:16px'>inferior to G-EVAL-4 without probabilities on Sum-<br>mEval. We believe this is related to the calculation<br>of Kendall-Tau correlation, which is based on the<br>number of concordant and discordant pairs. Direct<br>scoring without probabilities can lead to many ties,<br>which are not counted as either concordant or dis-<br>cordant. This may result in a higher Kendall-Tau<br>correlation, but it does not reflect the model's true<br>capacity of evaluating the generated texts. On the<br>other hand, probability normalization can obtain<br>more fine-grained, continuous scores that better<br>capture the subtle difference between generated<br>texts. This is reflected by the higher Spearman cor-<br>relation of G-EVAL-4 with probabilities, which is<br>based on the rank order of the scores.</p>",
            "id": 83,
            "page": 7,
            "text": "inferior to G-EVAL-4 without probabilities on SummEval. We believe this is related to the calculation of Kendall-Tau correlation, which is based on the number of concordant and discordant pairs. Direct scoring without probabilities can lead to many ties, which are not counted as either concordant or discordant. This may result in a higher Kendall-Tau correlation, but it does not reflect the model's true capacity of evaluating the generated texts. On the other hand, probability normalization can obtain more fine-grained, continuous scores that better capture the subtle difference between generated texts. This is reflected by the higher Spearman correlation of G-EVAL-4 with probabilities, which is based on the rank order of the scores."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2051
                },
                {
                    "x": 1220,
                    "y": 2051
                },
                {
                    "x": 1220,
                    "y": 2677
                },
                {
                    "x": 290,
                    "y": 2677
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:18px'>The Effect of Model Size We compare the per-<br>formance of G-EVAL with different model sizes<br>on the SummEval and QAGS benchmarks. Ta-<br>ble 1 and Table 3 show that G-EVAL-4 has higher<br>correlation than G-EVAL-3.5 on most dimensions<br>and datasets, except for engagingness and<br>groundedness on the Topical-Chat benchmark.<br>This demonstrates that larger model size can im-<br>prove the performance of G-EVAL, especially for<br>more challenging and complex evaluation tasks,<br>such as consistency and relevance.</p>",
            "id": 84,
            "page": 7,
            "text": "The Effect of Model Size We compare the performance of G-EVAL with different model sizes on the SummEval and QAGS benchmarks. Table 1 and Table 3 show that G-EVAL-4 has higher correlation than G-EVAL-3.5 on most dimensions and datasets, except for engagingness and groundedness on the Topical-Chat benchmark. This demonstrates that larger model size can improve the performance of G-EVAL, especially for more challenging and complex evaluation tasks, such as consistency and relevance."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2711
                },
                {
                    "x": 677,
                    "y": 2711
                },
                {
                    "x": 677,
                    "y": 2768
                },
                {
                    "x": 291,
                    "y": 2768
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:20px'>5 Related Work</p>",
            "id": 85,
            "page": 7,
            "text": "5 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2798
                },
                {
                    "x": 1221,
                    "y": 2798
                },
                {
                    "x": 1221,
                    "y": 3195
                },
                {
                    "x": 291,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>Ngram-based Metrics Ngram-based metrics re-<br>fer to the scores for evaluating the NLG models by<br>measuring the lexical overlap between a generated<br>text and a reference text. BLEU (Papineni et al.,<br>2002) is the most widely used metric for machine<br>translation evaluation, which calculates the geomet-<br>ric mean of modified n-gram precision and a brevity</p>",
            "id": 86,
            "page": 7,
            "text": "Ngram-based Metrics Ngram-based metrics refer to the scores for evaluating the NLG models by measuring the lexical overlap between a generated text and a reference text. BLEU (Papineni , 2002) is the most widely used metric for machine translation evaluation, which calculates the geometric mean of modified n-gram precision and a brevity"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1180
                },
                {
                    "x": 2202,
                    "y": 1180
                },
                {
                    "x": 2202,
                    "y": 1800
                },
                {
                    "x": 1271,
                    "y": 1800
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:16px'>penalty. ROUGE (Lin, 2004) is a recall-oriented<br>metric for summarization evaluation, which mea-<br>sures the n-gram overlap between a generated sum-<br>mary and a set of reference summaries. It has been<br>shown that more than 60% of recent papers on<br>NLG only rely on ROUGE or BLEU to evaluate<br>their systems (Kasai et al., 2021). However, these<br>metrics fail to measure content quality (Reiter and<br>Belz, 2009) or capture syntactic errors (Stent et al.,<br>2005), and therefore do not reflect the reliability of<br>NLG systems accurately.</p>",
            "id": 87,
            "page": 7,
            "text": "penalty. ROUGE (Lin, 2004) is a recall-oriented metric for summarization evaluation, which measures the n-gram overlap between a generated summary and a set of reference summaries. It has been shown that more than 60% of recent papers on NLG only rely on ROUGE or BLEU to evaluate their systems (Kasai , 2021). However, these metrics fail to measure content quality (Reiter and Belz, 2009) or capture syntactic errors (Stent , 2005), and therefore do not reflect the reliability of NLG systems accurately."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1845
                },
                {
                    "x": 2203,
                    "y": 1845
                },
                {
                    "x": 2203,
                    "y": 2810
                },
                {
                    "x": 1271,
                    "y": 2810
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:18px'>Embedding-based Metrics Embedding-based<br>metrics refer to the scores for evaluating the NLG<br>models by measuring the semantic similarity be-<br>tween a generated text and a reference text based<br>on the word or sentence embeddings. WMD (Kus-<br>ner et al., 2015) is a metric that measures the dis-<br>tance between two texts based on the word embed-<br>dings. BERTScore (Zhang et al., 2019) measures<br>the similarity between two texts based on the con-<br>textualized embedding from BERT (Devlin et al.,<br>2019). MoverScore (Zhao et al., 2019) improves<br>BERTScore by adding soft alignments and new<br>aggregation methods to obtain a more robust simi-<br>larity measure. (Clark et al., 2019) propose a metric<br>that evaluates multi-sentence texts by computing<br>the similarity between the generated text and the<br>reference text based on the sentence embeddings.</p>",
            "id": 88,
            "page": 7,
            "text": "Embedding-based Metrics Embedding-based metrics refer to the scores for evaluating the NLG models by measuring the semantic similarity between a generated text and a reference text based on the word or sentence embeddings. WMD (Kusner , 2015) is a metric that measures the distance between two texts based on the word embeddings. BERTScore (Zhang , 2019) measures the similarity between two texts based on the contextualized embedding from BERT (Devlin , 2019). MoverScore (Zhao , 2019) improves BERTScore by adding soft alignments and new aggregation methods to obtain a more robust similarity measure. (Clark , 2019) propose a metric that evaluates multi-sentence texts by computing the similarity between the generated text and the reference text based on the sentence embeddings."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2855
                },
                {
                    "x": 2204,
                    "y": 2855
                },
                {
                    "x": 2204,
                    "y": 3195
                },
                {
                    "x": 1271,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>Task-specific Evaluators Task-specific metrics<br>refer to the scores for evaluating the NLG mod-<br>els by measuring the quality of the generated<br>texts based on the specific task requirements.<br>For example, summarization tasks need to as-<br>sess the consistency of the generated sum-</p>",
            "id": 89,
            "page": 7,
            "text": "Task-specific Evaluators Task-specific metrics refer to the scores for evaluating the NLG models by measuring the quality of the generated texts based on the specific task requirements. For example, summarization tasks need to assess the consistency of the generated sum-"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 268
                },
                {
                    "x": 1219,
                    "y": 268
                },
                {
                    "x": 1219,
                    "y": 716
                },
                {
                    "x": 291,
                    "y": 716
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:16px'>maries (Krysci�ski et al., 2020; Wang et al., 2020;<br>Cao et al., 2020; Durmus et al., 2020), and di-<br>alogue response generation tasks need to assess<br>the coherence of the generated responses (Dziri<br>et al., 2019; Ye et al., 2021). However, these met-<br>rics are not generalizable to other NLG tasks, and<br>they are not able to measure the overall quality of<br>the generated texts.</p>",
            "id": 90,
            "page": 8,
            "text": "maries (Krysci�ski , 2020; Wang , 2020; Cao , 2020; Durmus , 2020), and dialogue response generation tasks need to assess the coherence of the generated responses (Dziri , 2019; Ye , 2021). However, these metrics are not generalizable to other NLG tasks, and they are not able to measure the overall quality of the generated texts."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 748
                },
                {
                    "x": 1218,
                    "y": 748
                },
                {
                    "x": 1218,
                    "y": 1254
                },
                {
                    "x": 291,
                    "y": 1254
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:16px'>Unified Evaluators Recently, some evaluators<br>have been developed to assess text quality from<br>multiple dimensions by varying the input and out-<br>put contents (Yuan et al., 2021) or the model vari-<br>ants (Mehri and Eskenazi, 2020) they use. UniEval<br>(Zhong et al., 2022) is a unified evaluator that can<br>evaluate different aspects of text generation as QA<br>tasks. By changing the question format, it can han-<br>dle different evaluation tasks.</p>",
            "id": 91,
            "page": 8,
            "text": "Unified Evaluators Recently, some evaluators have been developed to assess text quality from multiple dimensions by varying the input and output contents (Yuan , 2021) or the model variants (Mehri and Eskenazi, 2020) they use. UniEval (Zhong , 2022) is a unified evaluator that can evaluate different aspects of text generation as QA tasks. By changing the question format, it can handle different evaluation tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1285
                },
                {
                    "x": 1215,
                    "y": 1285
                },
                {
                    "x": 1215,
                    "y": 1852
                },
                {
                    "x": 291,
                    "y": 1852
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:18px'>LLM-based Evaluators Fu et al. (2023) propose<br>GPTScore, a new framework that evaluated texts<br>with generative pre-training models like GPT-3. It<br>assumes that a generative pre-training model will<br>assign a higher probability of high-quality gener-<br>ated text following a given instruction and context.<br>Wang et al. (2023) conduct a preliminary survey<br>of using ChatGPT as a NLG evaluator. Kocmi and<br>Federmann (2023) proposed to use GPT models<br>for evaluating machine translation tasks.</p>",
            "id": 92,
            "page": 8,
            "text": "LLM-based Evaluators Fu  (2023) propose GPTScore, a new framework that evaluated texts with generative pre-training models like GPT-3. It assumes that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context. Wang  (2023) conduct a preliminary survey of using ChatGPT as a NLG evaluator. Kocmi and Federmann (2023) proposed to use GPT models for evaluating machine translation tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1895
                },
                {
                    "x": 616,
                    "y": 1895
                },
                {
                    "x": 616,
                    "y": 1946
                },
                {
                    "x": 294,
                    "y": 1946
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:20px'>6 Conclusion</p>",
            "id": 93,
            "page": 8,
            "text": "6 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1984
                },
                {
                    "x": 1217,
                    "y": 1984
                },
                {
                    "x": 1217,
                    "y": 2828
                },
                {
                    "x": 290,
                    "y": 2828
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>In this paper, we propose G-EVAL, a framework of<br>using LLM with chain-of-thoughts (CoT) to eval-<br>uate the quality of generated texts. We conduct<br>extensive experiments on two NLG tasks, text sum-<br>marization and dialogue generation, and show that<br>G-EVAL can outperform state-of-the-art evaluators<br>and achieve higher human correspondence. We<br>also propose preliminary analysis on the behavior<br>of LLM-based evaluators, and highlight the poten-<br>tial issue of LLM-based evaluator having a bias<br>towards the LLM-generated texts. We hope our<br>work can inspire more research on using LLMs for<br>NLG evaluation, and also raise awareness of the<br>potential risks and challenges of using LLMs as<br>evaluators.</p>",
            "id": 94,
            "page": 8,
            "text": "In this paper, we propose G-EVAL, a framework of using LLM with chain-of-thoughts (CoT) to evaluate the quality of generated texts. We conduct extensive experiments on two NLG tasks, text summarization and dialogue generation, and show that G-EVAL can outperform state-of-the-art evaluators and achieve higher human correspondence. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluator having a bias towards the LLM-generated texts. We hope our work can inspire more research on using LLMs for NLG evaluation, and also raise awareness of the potential risks and challenges of using LLMs as evaluators."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 264
                },
                {
                    "x": 1516,
                    "y": 264
                },
                {
                    "x": 1516,
                    "y": 317
                },
                {
                    "x": 1274,
                    "y": 317
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:22px'>References</p>",
            "id": 95,
            "page": 8,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 347
                },
                {
                    "x": 2200,
                    "y": 347
                },
                {
                    "x": 2200,
                    "y": 624
                },
                {
                    "x": 1275,
                    "y": 624
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:14px'>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An<br>automatic metric for mt evaluation with improved cor-<br>relation with human judgments. In Proceedings of<br>the acl workshop on intrinsic and extrinsic evaluation<br>measures for machine translation and/or summariza-<br>tion, pages 65-72.</p>",
            "id": 96,
            "page": 8,
            "text": "Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 662
                },
                {
                    "x": 2199,
                    "y": 662
                },
                {
                    "x": 2199,
                    "y": 895
                },
                {
                    "x": 1274,
                    "y": 895
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:14px'>Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit<br>Cheung. 2020. Factual error correction for abstrac-<br>tive summarization models. In Proceedings of the<br>2020 Conference on Empirical Methods in Natural<br>Language Processing (EMNLP), pages 6251-6258.</p>",
            "id": 97,
            "page": 8,
            "text": "Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correction for abstractive summarization models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251-6258."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 932
                },
                {
                    "x": 2199,
                    "y": 932
                },
                {
                    "x": 2199,
                    "y": 1118
                },
                {
                    "x": 1275,
                    "y": 1118
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:16px'>Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.<br>Faithful to the original: Fact aware neural abstractive<br>summarization. In thirty-second AAAI conference on<br>artificial intelligence.</p>",
            "id": 98,
            "page": 8,
            "text": "Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In thirty-second AAAI conference on artificial intelligence."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1156
                },
                {
                    "x": 2200,
                    "y": 1156
                },
                {
                    "x": 2200,
                    "y": 1389
                },
                {
                    "x": 1275,
                    "y": 1389
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:16px'>Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith.<br>2019. Sentence mover's similarity: Automatic evalu-<br>ation for multi-sentence texts. In Proceedings of the<br>57th Annual Meeting of the Association for Compu-<br>tational Linguistics, pages 2748-2760.</p>",
            "id": 99,
            "page": 8,
            "text": "Elizabeth Clark, Asli Celikyilmaz, and Noah A Smith. 2019. Sentence mover's similarity: Automatic evaluation for multi-sentence texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2748-2760."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1426
                },
                {
                    "x": 2202,
                    "y": 1426
                },
                {
                    "x": 2202,
                    "y": 1794
                },
                {
                    "x": 1275,
                    "y": 1794
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:16px'>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and<br>Kristina Toutanova. 2019. Bert: Pre-training of deep<br>bidirectional transformers for language understand-<br>ing. In Proceedings of the 2019 Conference of the<br>North American Chapter of the Association for Com-<br>putational Linguistics: Human Language Technolo-<br>gies, Volume 1 (Long and Short Papers), pages 4171-<br>4186.</p>",
            "id": 100,
            "page": 8,
            "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1834
                },
                {
                    "x": 2201,
                    "y": 1834
                },
                {
                    "x": 2201,
                    "y": 2107
                },
                {
                    "x": 1275,
                    "y": 2107
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:14px'>Esin Durmus, He He, and Mona Diab. 2020. Feqa: A<br>question answering evaluation framework for faith-<br>fulness assessment in abstractive summarization. In<br>Proceedings of the 58th Annual Meeting of the Asso-<br>ciation for Computational Linguistics, pages 5055-<br>5070.</p>",
            "id": 101,
            "page": 8,
            "text": "Esin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 50555070."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2150
                },
                {
                    "x": 2201,
                    "y": 2150
                },
                {
                    "x": 2201,
                    "y": 2472
                },
                {
                    "x": 1276,
                    "y": 2472
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:16px'>Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and<br>Osmar R Zaiane. 2019. Evaluating coherence in di-<br>alogue systems using entailment. In Proceedings of<br>the 2019 Conference of the North American Chap-<br>ter of the Association for Computational Linguistics:<br>Human Language Technologies, Volume 1 (Long and<br>Short Papers), pages 3806-3812.</p>",
            "id": 102,
            "page": 8,
            "text": "Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar R Zaiane. 2019. Evaluating coherence in dialogue systems using entailment. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3806-3812."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2512
                },
                {
                    "x": 2201,
                    "y": 2512
                },
                {
                    "x": 2201,
                    "y": 2743
                },
                {
                    "x": 1276,
                    "y": 2743
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:18px'>Alexander R Fabbri, Wojciech Kryscinski, Bryan Mc-<br>Cann, Caiming Xiong, Richard Socher, and Dragomir<br>Radev. 2021. Summeval: Re-evaluating summariza-<br>tion evaluation. Transactions of the Association for<br>Computational Linguistics, 9:391-409.</p>",
            "id": 103,
            "page": 8,
            "text": "Alexander R Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2782
                },
                {
                    "x": 2198,
                    "y": 2782
                },
                {
                    "x": 2198,
                    "y": 2920
                },
                {
                    "x": 1274,
                    "y": 2920
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:16px'>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei<br>Liu. 2023. Gptscore: Evaluate as you desire. arXiv<br>preprint arXiv:2302.04166.</p>",
            "id": 104,
            "page": 8,
            "text": "Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2960
                },
                {
                    "x": 2200,
                    "y": 2960
                },
                {
                    "x": 2200,
                    "y": 3192
                },
                {
                    "x": 1276,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:14px'>Karl Moritz Hermann, Tomas Kocisky, Edward Grefen-<br>stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,<br>and Phil Blunsom. 2015. Teaching machines to read<br>and comprehend. Advances in neural information<br>processing systems, 28.</p>",
            "id": 105,
            "page": 8,
            "text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 270
                },
                {
                    "x": 1220,
                    "y": 270
                },
                {
                    "x": 1220,
                    "y": 502
                },
                {
                    "x": 292,
                    "y": 502
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:16px'>Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras,<br>Lavinia Dunagan, Jacob Morrison, Alexander R Fab-<br>bri, Yejin Choi, and Noah A Smith. 2021. Bidimen-<br>sional leaderboards: Generate and evaluate language<br>hand in hand. arXiv preprint arXiv:2112.04139.</p>",
            "id": 106,
            "page": 9,
            "text": "Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R Fabbri, Yejin Choi, and Noah A Smith. 2021. Bidimensional leaderboards: Generate and evaluate language hand in hand. arXiv preprint arXiv:2112.04139."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 534
                },
                {
                    "x": 1217,
                    "y": 534
                },
                {
                    "x": 1217,
                    "y": 675
                },
                {
                    "x": 293,
                    "y": 675
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:14px'>Tom Kocmi and Christian Federmann. 2023. Large<br>language models are state-of-the-art evaluators of<br>translation quality. arXiv preprint arXiv:2302.14520.</p>",
            "id": 107,
            "page": 9,
            "text": "Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 708
                },
                {
                    "x": 1217,
                    "y": 708
                },
                {
                    "x": 1217,
                    "y": 984
                },
                {
                    "x": 294,
                    "y": 984
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:18px'>Wojciech Krysci�ski, Bryan McCann, Caiming Xiong,<br>and Richard Socher. 2020. Evaluating the factual<br>consistency of abstractive text summarization. In<br>Proceedings of the 2020 Conference on Empirical<br>Methods in Natural Language Processing (EMNLP),<br>pages 9332-9346.</p>",
            "id": 108,
            "page": 9,
            "text": "Wojciech Krysci�ski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1018
                },
                {
                    "x": 1218,
                    "y": 1018
                },
                {
                    "x": 1218,
                    "y": 1204
                },
                {
                    "x": 293,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:14px'>Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Wein-<br>berger. 2015. From word embeddings to document<br>distances. In International conference on machine<br>learning, pages 957-966. PMLR.</p>",
            "id": 109,
            "page": 9,
            "text": "Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In International conference on machine learning, pages 957-966. PMLR."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1238
                },
                {
                    "x": 1218,
                    "y": 1238
                },
                {
                    "x": 1218,
                    "y": 1650
                },
                {
                    "x": 294,
                    "y": 1650
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan<br>Ghazvininejad, Abdelrahman Mohamed, Omer Levy,<br>Veselin Stoyanov, and Luke Zettlemoyer. 2020.<br>BART: denoising sequence-to-sequence pre-training<br>for natural language generation, translation, and com-<br>prehension. In Proceedings of the 58th Annual Meet-<br>ing of the Association for Computational Linguistics,<br>ACL 2020, Online, July 5-10, 2020, pages 7871-7880.<br>Association for Computational Linguistics.</p>",
            "id": 110,
            "page": 9,
            "text": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871-7880. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1683
                },
                {
                    "x": 1218,
                    "y": 1683
                },
                {
                    "x": 1218,
                    "y": 1824
                },
                {
                    "x": 292,
                    "y": 1824
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:14px'>Chin-Yew Lin. 2004. Rouge: A package for automatic<br>evaluation of summaries. In Text summarization<br>branches out, pages 74-81.</p>",
            "id": 111,
            "page": 9,
            "text": "Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1857
                },
                {
                    "x": 1218,
                    "y": 1857
                },
                {
                    "x": 1218,
                    "y": 1997
                },
                {
                    "x": 294,
                    "y": 1997
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:14px'>Shikib Mehri and Maxine Eskenazi. 2020. Usr: An<br>unsupervised and reference free evaluation metric for<br>dialog generation. arXiv preprint arXiv:2005.00456.</p>",
            "id": 112,
            "page": 9,
            "text": "Shikib Mehri and Maxine Eskenazi. 2020. Usr: An unsupervised and reference free evaluation metric for dialog generation. arXiv preprint arXiv:2005.00456."
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 2030
                },
                {
                    "x": 1217,
                    "y": 2030
                },
                {
                    "x": 1217,
                    "y": 2308
                },
                {
                    "x": 295,
                    "y": 2308
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>Shashi Narayan, Shay B Cohen, and Mirella Lapata.<br>2018. Don't give me the details, just the summary!<br>topic-aware convolutional neural networks for ex-<br>treme summarization. In Proceedings of the 2018<br>Conference on Empirical Methods in Natural Lan-<br>guage Processing, pages 1797-1807.</p>",
            "id": 113,
            "page": 9,
            "text": "Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2340
                },
                {
                    "x": 1219,
                    "y": 2340
                },
                {
                    "x": 1219,
                    "y": 2617
                },
                {
                    "x": 293,
                    "y": 2617
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:20px'>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,<br>Carroll Wainwright, Pamela Mishkin, Chong Zhang,<br>Sandhini Agarwal, Katarina Slama, Alex Ray, et al.<br>2022. Training language models to follow instruc-<br>tions with human feedback. Advances in Neural<br>Information Processing Systems, 35:27730-27744.</p>",
            "id": 114,
            "page": 9,
            "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,  2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2649
                },
                {
                    "x": 1219,
                    "y": 2649
                },
                {
                    "x": 1219,
                    "y": 2881
                },
                {
                    "x": 293,
                    "y": 2881
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:16px'>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-<br>Jing Zhu. 2002. Bleu: a method for automatic evalu-<br>ation of machine translation. In Proceedings of the<br>40th annual meeting of the Associationfor Computa-<br>tional Linguistics, pages 311-318.</p>",
            "id": 115,
            "page": 9,
            "text": "Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Associationfor Computational Linguistics, pages 311-318."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2915
                },
                {
                    "x": 1219,
                    "y": 2915
                },
                {
                    "x": 1219,
                    "y": 3188
                },
                {
                    "x": 292,
                    "y": 3188
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:16px'>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine<br>Lee, Sharan Narang, Michael Matena, Yanqi Zhou,<br>Wei Li, and Peter J Liu. 2020. Exploring the limits<br>of transfer learning with a unified text-to-text trans-<br>former. Journal of Machine Learning Research, 21:1-<br>67.</p>",
            "id": 116,
            "page": 9,
            "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:167."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 270
                },
                {
                    "x": 2202,
                    "y": 270
                },
                {
                    "x": 2202,
                    "y": 456
                },
                {
                    "x": 1275,
                    "y": 456
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:18px'>Ehud Reiter and Anja Belz. 2009. An investigation into<br>the validity of some metrics for automatically evalu-<br>ating natural language generation systems. Computa-<br>tional Linguistics, 35(4):529-558.</p>",
            "id": 117,
            "page": 9,
            "text": "Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529-558."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 492
                },
                {
                    "x": 2200,
                    "y": 492
                },
                {
                    "x": 2200,
                    "y": 722
                },
                {
                    "x": 1276,
                    "y": 722
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:18px'>Amanda Stent, Matthew Marge, and Mohit Singhai.<br>2005. Evaluating evaluation methods for generation<br>in the presence of variation. In Proceedings of the 6th<br>international conference on Computational Linguis-<br>tics and Intelligent Text Processing, pages 341-351.</p>",
            "id": 118,
            "page": 9,
            "text": "Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In Proceedings of the 6th international conference on Computational Linguistics and Intelligent Text Processing, pages 341-351."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 757
                },
                {
                    "x": 2200,
                    "y": 757
                },
                {
                    "x": 2200,
                    "y": 987
                },
                {
                    "x": 1276,
                    "y": 987
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:20px'>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.<br>Asking and answering questions to evaluate the fac-<br>tual consistency of summaries. In Proceedings of the<br>58th Annual Meeting of the Association for Compu-<br>tational Linguistics, pages 5008-5020.</p>",
            "id": 119,
            "page": 9,
            "text": "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1022
                },
                {
                    "x": 2198,
                    "y": 1022
                },
                {
                    "x": 2198,
                    "y": 1207
                },
                {
                    "x": 1276,
                    "y": 1207
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:20px'>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang<br>Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.<br>2023. Is chatgpt a good nlg evaluator? a preliminary<br>study. arXiv preprint arXiv:2303.04048.</p>",
            "id": 120,
            "page": 9,
            "text": "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1243
                },
                {
                    "x": 2198,
                    "y": 1243
                },
                {
                    "x": 2198,
                    "y": 1428
                },
                {
                    "x": 1275,
                    "y": 1428
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten<br>Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.<br>Chain of thought prompting elicits reasoning in large<br>language models. arXiv preprint arXiv:2201.11903.</p>",
            "id": 121,
            "page": 9,
            "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1462
                },
                {
                    "x": 2201,
                    "y": 1462
                },
                {
                    "x": 2201,
                    "y": 1785
                },
                {
                    "x": 1276,
                    "y": 1785
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and<br>Xiaodan Liang. 2021. Towards quantifiable dialogue<br>coherence evaluation. In Proceedings of the 59th An-<br>nual Meeting of the Associationfor Computational<br>Linguistics and the 11th International Joint Confer-<br>ence on Natural Language Processing (Volume 1:<br>Long Papers), pages 2718-2729.</p>",
            "id": 122,
            "page": 9,
            "text": "Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, and Xiaodan Liang. 2021. Towards quantifiable dialogue coherence evaluation. In Proceedings of the 59th Annual Meeting of the Associationfor Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2718-2729."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1820
                },
                {
                    "x": 2201,
                    "y": 1820
                },
                {
                    "x": 2201,
                    "y": 2003
                },
                {
                    "x": 1277,
                    "y": 2003
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:16px'>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.<br>Bartscore: Evaluating generated text as text gener-<br>ation. Advances in Neural Information Processing<br>Systems, 34.</p>",
            "id": 123,
            "page": 9,
            "text": "Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2040
                },
                {
                    "x": 2200,
                    "y": 2040
                },
                {
                    "x": 2200,
                    "y": 2222
                },
                {
                    "x": 1276,
                    "y": 2222
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:16px'>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q<br>Weinberger, and Yoav Artzi. 2019. Bertscore: Eval-<br>uating text generation with bert. arXiv preprint<br>arXiv:1904.09675.</p>",
            "id": 124,
            "page": 9,
            "text": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2259
                },
                {
                    "x": 2200,
                    "y": 2259
                },
                {
                    "x": 2200,
                    "y": 2442
                },
                {
                    "x": 1276,
                    "y": 2442
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,<br>Kathleen McKeown, and Tatsunori B. Hashimoto.<br>2023. Benchmarking large language models for news<br>summarization.</p>",
            "id": 125,
            "page": 9,
            "text": "Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2023. Benchmarking large language models for news summarization."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2479
                },
                {
                    "x": 2200,
                    "y": 2479
                },
                {
                    "x": 2200,
                    "y": 2847
                },
                {
                    "x": 1276,
                    "y": 2847
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:16px'>Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-<br>tian M Meyer, and Steffen Eger. 2019. Moverscore:<br>Text generation evaluating with contextualized em-<br>beddings and earth mover distance. In Proceedings<br>of the 2019 Conference on Empirical Methods in Nat-<br>ural Language Processing and the 9th International<br>Joint Conference on Natural Language Processing<br>(EMNLP-IJCNLP), pages 563-578.</p>",
            "id": 126,
            "page": 9,
            "text": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563-578."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2881
                },
                {
                    "x": 2200,
                    "y": 2881
                },
                {
                    "x": 2200,
                    "y": 3114
                },
                {
                    "x": 1276,
                    "y": 3114
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu<br>Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and<br>Jiawei Han. 2022. Towards a unified multi-<br>dimensional evaluator for text generation. arXiv<br>preprint arXiv:2210.07197.</p>",
            "id": 127,
            "page": 9,
            "text": "Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. arXiv preprint arXiv:2210.07197."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 263
                },
                {
                    "x": 775,
                    "y": 263
                },
                {
                    "x": 775,
                    "y": 323
                },
                {
                    "x": 291,
                    "y": 323
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:22px'>A Example Prompts</p>",
            "id": 128,
            "page": 10,
            "text": "A Example Prompts"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 351
                },
                {
                    "x": 1231,
                    "y": 351
                },
                {
                    "x": 1231,
                    "y": 408
                },
                {
                    "x": 291,
                    "y": 408
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:18px'>Evaluate Coherence in the Summarization Task</p>",
            "id": 129,
            "page": 10,
            "text": "Evaluate Coherence in the Summarization Task"
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 442
                },
                {
                    "x": 2108,
                    "y": 442
                },
                {
                    "x": 2108,
                    "y": 710
                },
                {
                    "x": 379,
                    "y": 710
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:14px'>You will be given one summary written for a news article.<br>Your task is to rate the summary on one metric.<br>Please make sure you read and understand these instructions carefully. Please keep this<br>document open while reviewing, and refer to it as needed.</p>",
            "id": 130,
            "page": 10,
            "text": "You will be given one summary written for a news article. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed."
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 770
                },
                {
                    "x": 2112,
                    "y": 770
                },
                {
                    "x": 2112,
                    "y": 1071
                },
                {
                    "x": 379,
                    "y": 1071
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>Evaluation Criteria:<br>Coherence (1-5) - the collective quality of all sentences. We align this dimension with<br>the DUC quality question of structure and coherence whereby \"the summary should be<br>well-structured and well-organized. The summary should not just be a heap of related informa-<br>tion, but should build from sentence to sentence to a coherent body ofinformation about a topic. 7</p>",
            "id": 131,
            "page": 10,
            "text": "Evaluation Criteria: Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby \"the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body ofinformation about a topic. 7"
        },
        {
            "bounding_box": [
                {
                    "x": 380,
                    "y": 1127
                },
                {
                    "x": 2109,
                    "y": 1127
                },
                {
                    "x": 2109,
                    "y": 1515
                },
                {
                    "x": 380,
                    "y": 1515
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>Evaluation Steps:<br>1. Read the news article carefully and identify the main topic and key points.<br>2. Read the summary and compare it to the news article. Check if the summary covers the main<br>topic and key points of the news article, and if it presents them in a clear and logical order.<br>3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest<br>based on the Evaluation Criteria.</p>",
            "id": 132,
            "page": 10,
            "text": "Evaluation Steps: 1. Read the news article carefully and identify the main topic and key points. 2. Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order. 3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria."
        },
        {
            "bounding_box": [
                {
                    "x": 378,
                    "y": 1567
                },
                {
                    "x": 1001,
                    "y": 1567
                },
                {
                    "x": 1001,
                    "y": 2114
                },
                {
                    "x": 378,
                    "y": 2114
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:20px'>Example:<br>Source Text:<br>{{Document}}<br>Summary:<br>{{Summary}}<br>Evaluation Form (scores ONLY):<br>- Coherence:</p>",
            "id": 133,
            "page": 10,
            "text": "Example: Source Text: {{Document}} Summary: {{Summary}} Evaluation Form (scores ONLY): - Coherence:"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2144
                },
                {
                    "x": 1393,
                    "y": 2144
                },
                {
                    "x": 1393,
                    "y": 2205
                },
                {
                    "x": 289,
                    "y": 2205
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:20px'>Evaluate Engagingness in the Dialogue Generation Task</p>",
            "id": 134,
            "page": 10,
            "text": "Evaluate Engagingness in the Dialogue Generation Task"
        },
        {
            "bounding_box": [
                {
                    "x": 381,
                    "y": 2236
                },
                {
                    "x": 2108,
                    "y": 2236
                },
                {
                    "x": 2108,
                    "y": 2403
                },
                {
                    "x": 381,
                    "y": 2403
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:14px'>You will be given a conversation between two individuals. You will then be given one potential<br>response for the next turn in the conversation. The response concerns an interesting fact, which<br>will be provided as well.</p>",
            "id": 135,
            "page": 10,
            "text": "You will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. The response concerns an interesting fact, which will be provided as well."
        },
        {
            "bounding_box": [
                {
                    "x": 381,
                    "y": 2424
                },
                {
                    "x": 2107,
                    "y": 2424
                },
                {
                    "x": 2107,
                    "y": 2618
                },
                {
                    "x": 381,
                    "y": 2618
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:14px'>Your task is to rate the responses on one metric.<br>Please make sure you read and understand these instructions carefully. Please keep this<br>document open while reviewing, and refer to it as needed.</p>",
            "id": 136,
            "page": 10,
            "text": "Your task is to rate the responses on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed."
        },
        {
            "bounding_box": [
                {
                    "x": 378,
                    "y": 2671
                },
                {
                    "x": 2114,
                    "y": 2671
                },
                {
                    "x": 2114,
                    "y": 3184
                },
                {
                    "x": 378,
                    "y": 3184
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>Evaluation Crieteria:<br>Engagingness (1-3) Is the response dull/interesting?<br>- A score ofl (dull) means that the response is generic and dull.<br>- A score of 2 (somewhat interesting) means the response is somewhat interesting and could<br>engage you in the conversation (e.g., an opinion, thought)<br>- A score of 3 (interesting) means the response is very interesting or presents an interesting fact<br>Evaluation Steps:</p>",
            "id": 137,
            "page": 10,
            "text": "Evaluation Crieteria: Engagingness (1-3) Is the response dull/interesting? - A score ofl (dull) means that the response is generic and dull. - A score of 2 (somewhat interesting) means the response is somewhat interesting and could engage you in the conversation (e.g., an opinion, thought) - A score of 3 (interesting) means the response is very interesting or presents an interesting fact Evaluation Steps:"
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 264
                },
                {
                    "x": 2111,
                    "y": 264
                },
                {
                    "x": 2111,
                    "y": 530
                },
                {
                    "x": 379,
                    "y": 530
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:14px'>1. Read the conversation, the corresponding fact and the response carefully.<br>2. Rate the response on a scale of1-3for engagingness, according to the criteria above.<br>3. Provide a brief explanation for your rating, referring to specific aspects of the response and<br>the conversation.</p>",
            "id": 138,
            "page": 11,
            "text": "1. Read the conversation, the corresponding fact and the response carefully. 2. Rate the response on a scale of1-3for engagingness, according to the criteria above. 3. Provide a brief explanation for your rating, referring to specific aspects of the response and the conversation."
        },
        {
            "bounding_box": [
                {
                    "x": 378,
                    "y": 582
                },
                {
                    "x": 822,
                    "y": 582
                },
                {
                    "x": 822,
                    "y": 1104
                },
                {
                    "x": 378,
                    "y": 1104
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:22px'>Example:<br>Conversation History:<br>{{Document}}<br>Corresponding Fact:<br>{{Fact}}<br>Response:<br>{{Response}}</p>",
            "id": 139,
            "page": 11,
            "text": "Example: Conversation History: {{Document}} Corresponding Fact: {{Fact}} Response: {{Response}}"
        },
        {
            "bounding_box": [
                {
                    "x": 381,
                    "y": 1151
                },
                {
                    "x": 994,
                    "y": 1151
                },
                {
                    "x": 994,
                    "y": 1282
                },
                {
                    "x": 381,
                    "y": 1282
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:14px'>Evaluation Form (scores ONLY):<br>- Engagingness:</p>",
            "id": 140,
            "page": 11,
            "text": "Evaluation Form (scores ONLY): - Engagingness:"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1317
                },
                {
                    "x": 772,
                    "y": 1317
                },
                {
                    "x": 772,
                    "y": 1374
                },
                {
                    "x": 290,
                    "y": 1374
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:14px'>Evaluate Hallucinations</p>",
            "id": 141,
            "page": 11,
            "text": "Evaluate Hallucinations"
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 1411
                },
                {
                    "x": 1328,
                    "y": 1411
                },
                {
                    "x": 1328,
                    "y": 1468
                },
                {
                    "x": 379,
                    "y": 1468
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Human Evaluation of Text Summarization Systems:</p>",
            "id": 142,
            "page": 11,
            "text": "Human Evaluation of Text Summarization Systems:"
        },
        {
            "bounding_box": [
                {
                    "x": 383,
                    "y": 1527
                },
                {
                    "x": 2110,
                    "y": 1527
                },
                {
                    "x": 2110,
                    "y": 1639
                },
                {
                    "x": 383,
                    "y": 1639
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:14px'>Factual Consistency: Does the summary untruthful or misleading facts that are not<br>supported by the source text?</p>",
            "id": 143,
            "page": 11,
            "text": "Factual Consistency: Does the summary untruthful or misleading facts that are not supported by the source text?"
        },
        {
            "bounding_box": [
                {
                    "x": 382,
                    "y": 1696
                },
                {
                    "x": 672,
                    "y": 1696
                },
                {
                    "x": 672,
                    "y": 1981
                },
                {
                    "x": 382,
                    "y": 1981
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:20px'>Source Text:<br>{{Document}}<br>Summary:<br>{{Summary}}</p>",
            "id": 144,
            "page": 11,
            "text": "Source Text: {{Document}} Summary: {{Summary}}"
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 2033
                },
                {
                    "x": 1301,
                    "y": 2033
                },
                {
                    "x": 1301,
                    "y": 2162
                },
                {
                    "x": 379,
                    "y": 2162
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:16px'>Does the summary contain factual inconsistency?<br>Answer:</p>",
            "id": 145,
            "page": 11,
            "text": "Does the summary contain factual inconsistency? Answer:"
        }
    ]
}