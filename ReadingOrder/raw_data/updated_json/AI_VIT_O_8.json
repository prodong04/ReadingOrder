{
    "id": "62a4edbe-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2103.14030v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 259,
                    "y": 435
                },
                {
                    "x": 2220,
                    "y": 435
                },
                {
                    "x": 2220,
                    "y": 504
                },
                {
                    "x": 259,
                    "y": 504
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</p>",
            "id": 0,
            "page": 1,
            "text": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
        },
        {
            "bounding_box": [
                {
                    "x": 574,
                    "y": 596
                },
                {
                    "x": 1890,
                    "y": 596
                },
                {
                    "x": 1890,
                    "y": 775
                },
                {
                    "x": 574,
                    "y": 775
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Ze Liut* Yutong Lin +* Yue Cao * *‡ Yixuan Weit<br>Han Hu<br>Zheng Zhang Stephen Lin Baining Guo<br>Microsoft Research Asia</p>",
            "id": 1,
            "page": 1,
            "text": "Ze Liut* Yutong Lin +* Yue Cao * *‡ Yixuan Weit\nHan Hu\nZheng Zhang Stephen Lin Baining Guo\nMicrosoft Research Asia"
        },
        {
            "bounding_box": [
                {
                    "x": 386,
                    "y": 786
                },
                {
                    "x": 2096,
                    "y": 786
                },
                {
                    "x": 2096,
                    "y": 835
                },
                {
                    "x": 386,
                    "y": 835
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:16px'>{v-zeliul, v-yutlin, yuecao , hanhu, v-yixwe, zhez, stevelin, bainguo}@microsoft · com</p>",
            "id": 2,
            "page": 1,
            "text": "{v-zeliul, v-yutlin, yuecao , hanhu, v-yixwe, zhez, stevelin, bainguo}@microsoft · com"
        },
        {
            "bounding_box": [
                {
                    "x": 601,
                    "y": 947
                },
                {
                    "x": 799,
                    "y": 947
                },
                {
                    "x": 799,
                    "y": 1003
                },
                {
                    "x": 601,
                    "y": 1003
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:22px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1053
                },
                {
                    "x": 1199,
                    "y": 1053
                },
                {
                    "x": 1199,
                    "y": 2457
                },
                {
                    "x": 200,
                    "y": 2457
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>This paper presents a new vision Transformer, called<br>Swin Transformer, that capably serves as a general-purpose<br>backbone for computer vision. Challenges in adapting<br>Transformer, from language to vision arise from differences<br>between the two domains, such as large variations in the<br>scale of visual entities and the high resolution of pixels<br>in images compared to words in text. To address these<br>differences, we propose a hierarchical Transformer whose<br>representation is computed with Shifted windows. The<br>shifted windowing scheme brings greater efficiency by lim-<br>iting self-attention computation to non-overlapping local<br>windows while also allowing for cross-window connection.<br>This hierarchical architecture has the flexibility to model<br>at various scales and has linear computational complexity<br>with respect to image size. These qualities of Swin Trans-<br>former make it compatible with a broad range of vision<br>tasks, including image classification (87.3 top-1 accuracy<br>on ImageNet-1K) and dense prediction tasks such as object<br>detection (58.7 box AP and 51.1 mask AP on COCO test-<br>dev) and semantic segmentation (53.5 mIoU on ADE20K<br>val). Its performance surpasses the previous state-of-the-<br>art by a large margin of +2.7 box AP and +2.6 mask AP on<br>COCO, and +3.2 mIoU on ADE20K, demonstrating the po-<br>tential of Transformer-based models as vision backbones.<br>The hierarchical design and the shifted window approach<br>also prove beneficial for all-MLP architectures. The code<br>and models are publicly available at https : / /github.<br>com/mi crosoft / Swin-Transformer.</p>",
            "id": 4,
            "page": 1,
            "text": "This paper presents a new vision Transformer, called\nSwin Transformer, that capably serves as a general-purpose\nbackbone for computer vision. Challenges in adapting\nTransformer, from language to vision arise from differences\nbetween the two domains, such as large variations in the\nscale of visual entities and the high resolution of pixels\nin images compared to words in text. To address these\ndifferences, we propose a hierarchical Transformer whose\nrepresentation is computed with Shifted windows. The\nshifted windowing scheme brings greater efficiency by lim-\niting self-attention computation to non-overlapping local\nwindows while also allowing for cross-window connection.\nThis hierarchical architecture has the flexibility to model\nat various scales and has linear computational complexity\nwith respect to image size. These qualities of Swin Trans-\nformer make it compatible with a broad range of vision\ntasks, including image classification (87.3 top-1 accuracy\non ImageNet-1K) and dense prediction tasks such as object\ndetection (58.7 box AP and 51.1 mask AP on COCO test-\ndev) and semantic segmentation (53.5 mIoU on ADE20K\nval). Its performance surpasses the previous state-of-the-\nart by a large margin of +2.7 box AP and +2.6 mask AP on\nCOCO, and +3.2 mIoU on ADE20K, demonstrating the po-\ntential of Transformer-based models as vision backbones.\nThe hierarchical design and the shifted window approach\nalso prove beneficial for all-MLP architectures. The code\nand models are publicly available at https : / /github.\ncom/mi crosoft / Swin-Transformer."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2554
                },
                {
                    "x": 532,
                    "y": 2554
                },
                {
                    "x": 532,
                    "y": 2607
                },
                {
                    "x": 205,
                    "y": 2607
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2640
                },
                {
                    "x": 1200,
                    "y": 2640
                },
                {
                    "x": 1200,
                    "y": 2892
                },
                {
                    "x": 203,
                    "y": 2892
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>Modeling in computer vision has long been dominated<br>by convolutional neural networks (CNNs). Beginning with<br>AlexNet [39] and its revolutionary performance on the<br>ImageNet image classification challenge, CNN architec-<br>tures have evolved to become increasingly powerful through</p>",
            "id": 6,
            "page": 1,
            "text": "Modeling in computer vision has long been dominated\nby convolutional neural networks (CNNs). Beginning with\nAlexNet [39] and its revolutionary performance on the\nImageNet image classification challenge, CNN architec-\ntures have evolved to become increasingly powerful through"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 966
                },
                {
                    "x": 1830,
                    "y": 966
                },
                {
                    "x": 1830,
                    "y": 1480
                },
                {
                    "x": 1279,
                    "y": 1480
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='7' style='font-size:14px' alt=\"segmentation\nclassification detection\n16x\n8x\n4x\" data-coord=\"top-left:(1279,966); bottom-right:(1830,1480)\" /></figure>",
            "id": 7,
            "page": 1,
            "text": "segmentation\nclassification detection\n16x\n8x\n4x"
        },
        {
            "bounding_box": [
                {
                    "x": 1306,
                    "y": 1466
                },
                {
                    "x": 2095,
                    "y": 1466
                },
                {
                    "x": 2095,
                    "y": 1520
                },
                {
                    "x": 1306,
                    "y": 1520
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>(a) Swin Transformer (ours) (b) ViT</p>",
            "id": 8,
            "page": 1,
            "text": "(a) Swin Transformer (ours) (b) ViT"
        },
        {
            "bounding_box": [
                {
                    "x": 1794,
                    "y": 972
                },
                {
                    "x": 2263,
                    "y": 972
                },
                {
                    "x": 2263,
                    "y": 1468
                },
                {
                    "x": 1794,
                    "y": 1468
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='9' style='font-size:14px' alt=\"classification\n...\n16x\n16x\n16x\" data-coord=\"top-left:(1794,972); bottom-right:(2263,1468)\" /></figure>",
            "id": 9,
            "page": 1,
            "text": "classification\n...\n16x\n16x\n16x"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1548
                },
                {
                    "x": 2277,
                    "y": 1548
                },
                {
                    "x": 2277,
                    "y": 2008
                },
                {
                    "x": 1279,
                    "y": 2008
                }
            ],
            "category": "caption",
            "html": "<caption id='10' style='font-size:16px'>Figure 1. (a) The proposed Swin Transformer builds hierarchical<br>feature maps by merging image patches (shown in gray) in deeper<br>layers and has linear computation complexity to input image size<br>due to computation of self-attention only within each local win-<br>dow (shown in red). It can thus serve as a general-purpose back-<br>bone for both image classification and dense recognition tasks.<br>(b) In contrast, previous vision Transformers [20] produce fea-<br>ture maps of a single low resolution and have quadratic compu-<br>tation complexity to input image size due to computation of self-<br>attention globally.</caption>",
            "id": 10,
            "page": 1,
            "text": "Figure 1. (a) The proposed Swin Transformer builds hierarchical\nfeature maps by merging image patches (shown in gray) in deeper\nlayers and has linear computation complexity to input image size\ndue to computation of self-attention only within each local win-\ndow (shown in red). It can thus serve as a general-purpose back-\nbone for both image classification and dense recognition tasks.\n(b) In contrast, previous vision Transformers [20] produce fea-\nture maps of a single low resolution and have quadratic compu-\ntation complexity to input image size due to computation of self-\nattention globally."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2066
                },
                {
                    "x": 2277,
                    "y": 2066
                },
                {
                    "x": 2277,
                    "y": 2312
                },
                {
                    "x": 1280,
                    "y": 2312
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:18px'>greater scale [30, 76], more extensive connections [34], and<br>more sophisticated forms of convolution [70, 18, 84]. With<br>CNNs serving as backbone networks for a variety of vision<br>tasks, these architectural advances have led to performance<br>improvements that have broadly lifted the entire field.</p>",
            "id": 11,
            "page": 1,
            "text": "greater scale [30, 76], more extensive connections [34], and\nmore sophisticated forms of convolution [70, 18, 84]. With\nCNNs serving as backbone networks for a variety of vision\ntasks, these architectural advances have led to performance\nimprovements that have broadly lifted the entire field."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2321
                },
                {
                    "x": 2277,
                    "y": 2321
                },
                {
                    "x": 2277,
                    "y": 2870
                },
                {
                    "x": 1278,
                    "y": 2870
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:18px'>On the other hand, the evolution of network architectures<br>in natural language processing (NLP) has taken a different<br>path, where the prevalent architecture today is instead the<br>Transformer [64]. Designed for sequence modeling and<br>transduction tasks, the Transformer is notable for its use<br>of attention to model long-range dependencies in the data.<br>Its tremendous success in the language domain has led re-<br>searchers to investigate its adaptation to computer vision,<br>where ithas recently demonstrated promising results on cer-<br>tain tasks, specifically image classification [20] and joint<br>vision-language modeling [47].</p>",
            "id": 12,
            "page": 1,
            "text": "On the other hand, the evolution of network architectures\nin natural language processing (NLP) has taken a different\npath, where the prevalent architecture today is instead the\nTransformer [64]. Designed for sequence modeling and\ntransduction tasks, the Transformer is notable for its use\nof attention to model long-range dependencies in the data.\nIts tremendous success in the language domain has led re-\nsearchers to investigate its adaptation to computer vision,\nwhere ithas recently demonstrated promising results on cer-\ntain tasks, specifically image classification [20] and joint\nvision-language modeling [47]."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2878
                },
                {
                    "x": 2278,
                    "y": 2878
                },
                {
                    "x": 2278,
                    "y": 2979
                },
                {
                    "x": 1280,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:16px'>In this paper, we seek to expand the applicability of<br>Transformer such that it can serve as a general-purpose</p>",
            "id": 13,
            "page": 1,
            "text": "In this paper, we seek to expand the applicability of\nTransformer such that it can serve as a general-purpose"
        },
        {
            "bounding_box": [
                {
                    "x": 254,
                    "y": 2930
                },
                {
                    "x": 1016,
                    "y": 2930
                },
                {
                    "x": 1016,
                    "y": 2973
                },
                {
                    "x": 254,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:16px'>*Equal contribution. �Interns at MSRA. *Contact person.</p>",
            "id": 14,
            "page": 1,
            "text": "*Equal contribution. �Interns at MSRA. *Contact person."
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 870
                },
                {
                    "x": 152,
                    "y": 870
                },
                {
                    "x": 152,
                    "y": 2342
                },
                {
                    "x": 58,
                    "y": 2342
                }
            ],
            "category": "footer",
            "html": "<br><footer id='15' style='font-size:14px'>2021<br>Aug<br>17<br>[cs.CV]<br>arXiv:2103.14030v2</footer>",
            "id": 15,
            "page": 1,
            "text": "2021\nAug\n17\n[cs.CV]\narXiv:2103.14030v2"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 304
                },
                {
                    "x": 1199,
                    "y": 304
                },
                {
                    "x": 1199,
                    "y": 2200
                },
                {
                    "x": 198,
                    "y": 2200
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:20px'>backbone for computer vision, as it does for NLP and<br>as CNNs do in vision. We observe that significant chal-<br>lenges in transferring its high performance in the language<br>domain to the visual domain can be explained by differ-<br>ences between the two modalities. One of these differ-<br>ences involves scale. Unlike the word tokens that serve<br>as the basic elements of processing in language Trans-<br>formers, visual elements can vary substantially in scale, a<br>problem that receives attention in tasks such as object de-<br>tection [42, 53, 54]. In existing Transformer-based mod-<br>els [64, 20], tokens are all of a fixed scale, a property un-<br>suitable for these vision applications. Another difference<br>is the much higher resolution of pixels in images com-<br>pared to words in passages of text. There exist many vi-<br>sion tasks such as semantic segmentation that require dense<br>prediction at the pixel level, and this would be intractable<br>for Transformer on high-resolution images, as the compu-<br>tational complexity of its self-attention is quadratic to im-<br>age size. To overcome these issues, we propose a general-<br>purpose Transformer backbone, called Swin Transformer,<br>which constructs hierarchical feature maps and has linear<br>computational complexity to image size. As illustrated in<br>Figure 1(a), Swin Transformer constructs a hierarchical rep-<br>resentation by starting from small-sized patches (outlined in<br>gray) and gradually merging neighboring patches in deeper<br>Transformer layers. With these hierarchical feature maps,<br>the Swin Transformer model can conveniently leverage ad-<br>vanced techniques for dense prediction such as feature pyra-<br>mid networks (FPN) [42] or U-Net [51]. The linear compu-<br>tational complexity is achieved by computing self-attention<br>locally within non-overlapping windows that partition an<br>image (outlined in red). The number of patches in each<br>window is fixed, and thus the complexity becomes linear<br>to image size. These merits make Swin Transformer suit-<br>able as a general-purpose backbone for various vision tasks,<br>in contrast to previous Transformer based architectures [20]<br>which produce feature maps of a single resolution and have<br>quadratic complexity.</p>",
            "id": 16,
            "page": 2,
            "text": "backbone for computer vision, as it does for NLP and\nas CNNs do in vision. We observe that significant chal-\nlenges in transferring its high performance in the language\ndomain to the visual domain can be explained by differ-\nences between the two modalities. One of these differ-\nences involves scale. Unlike the word tokens that serve\nas the basic elements of processing in language Trans-\nformers, visual elements can vary substantially in scale, a\nproblem that receives attention in tasks such as object de-\ntection [42, 53, 54]. In existing Transformer-based mod-\nels [64, 20], tokens are all of a fixed scale, a property un-\nsuitable for these vision applications. Another difference\nis the much higher resolution of pixels in images com-\npared to words in passages of text. There exist many vi-\nsion tasks such as semantic segmentation that require dense\nprediction at the pixel level, and this would be intractable\nfor Transformer on high-resolution images, as the compu-\ntational complexity of its self-attention is quadratic to im-\nage size. To overcome these issues, we propose a general-\npurpose Transformer backbone, called Swin Transformer,\nwhich constructs hierarchical feature maps and has linear\ncomputational complexity to image size. As illustrated in\nFigure 1(a), Swin Transformer constructs a hierarchical rep-\nresentation by starting from small-sized patches (outlined in\ngray) and gradually merging neighboring patches in deeper\nTransformer layers. With these hierarchical feature maps,\nthe Swin Transformer model can conveniently leverage ad-\nvanced techniques for dense prediction such as feature pyra-\nmid networks (FPN) [42] or U-Net [51]. The linear compu-\ntational complexity is achieved by computing self-attention\nlocally within non-overlapping windows that partition an\nimage (outlined in red). The number of patches in each\nwindow is fixed, and thus the complexity becomes linear\nto image size. These merits make Swin Transformer suit-\nable as a general-purpose backbone for various vision tasks,\nin contrast to previous Transformer based architectures [20]\nwhich produce feature maps of a single resolution and have\nquadratic complexity."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2205
                },
                {
                    "x": 1199,
                    "y": 2205
                },
                {
                    "x": 1199,
                    "y": 2804
                },
                {
                    "x": 199,
                    "y": 2804
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:18px'>A key design element of Swin Transformer is its shift<br>of the window partition between consecutive self-attention<br>layers, as illustrated in Figure 2. The shifted windows<br>bridge the windows of the preceding layer, providing con-<br>nections among them that significantly enhance modeling<br>power (see Table 4). This strategy is also efficient in re-<br>gards to real-world latency: all query patches within a win-<br>dow share the same key set1 which facilitates memory ac-<br>,<br>cess in hardware. In contrast, earlier sliding window based<br>self-attention approaches [33, 50] suffer from low latency<br>on general hardware due to different key sets for different<br>query pixels2. Our experiments show that the proposed</p>",
            "id": 17,
            "page": 2,
            "text": "A key design element of Swin Transformer is its shift\nof the window partition between consecutive self-attention\nlayers, as illustrated in Figure 2. The shifted windows\nbridge the windows of the preceding layer, providing con-\nnections among them that significantly enhance modeling\npower (see Table 4). This strategy is also efficient in re-\ngards to real-world latency: all query patches within a win-\ndow share the same key set1 which facilitates memory ac-\n,\ncess in hardware. In contrast, earlier sliding window based\nself-attention approaches [33, 50] suffer from low latency\non general hardware due to different key sets for different\nquery pixels2. Our experiments show that the proposed"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2854
                },
                {
                    "x": 1199,
                    "y": 2854
                },
                {
                    "x": 1199,
                    "y": 2972
                },
                {
                    "x": 200,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:14px'>1The query and key are projection vectors in a self-attention layer.<br>2While there are efficient methods to implement a sliding-window<br>based convolution layer on general hardware, thanks to its shared kernel</p>",
            "id": 18,
            "page": 2,
            "text": "1The query and key are projection vectors in a self-attention layer.\n2While there are efficient methods to implement a sliding-window\nbased convolution layer on general hardware, thanks to its shared kernel"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 307
                },
                {
                    "x": 2263,
                    "y": 307
                },
                {
                    "x": 2263,
                    "y": 686
                },
                {
                    "x": 1282,
                    "y": 686
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='19' style='font-size:14px' alt=\"Layer 1 Layer 1+1\nA local window to\nperform self-attention\nA patch\" data-coord=\"top-left:(1282,307); bottom-right:(2263,686)\" /></figure>",
            "id": 19,
            "page": 2,
            "text": "Layer 1 Layer 1+1\nA local window to\nperform self-attention\nA patch"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 694
                },
                {
                    "x": 2278,
                    "y": 694
                },
                {
                    "x": 2278,
                    "y": 1059
                },
                {
                    "x": 1279,
                    "y": 1059
                }
            ],
            "category": "caption",
            "html": "<br><caption id='20' style='font-size:16px'>Figure 2. An illustration of the shifted window approach for com-<br>puting self-attention in the proposed Swin Transformer architec-<br>ture. In layer l (left), a regular window partitioning scheme is<br>adopted, and self-attention is computed within each window. In<br>the next layer l + 1 (right), the window partitioning is shifted, re-<br>sulting in new windows. The self-attention computation in the new<br>windows crosses the boundaries of the previous windows in layer<br>l, providing connections among them.</caption>",
            "id": 20,
            "page": 2,
            "text": "Figure 2. An illustration of the shifted window approach for com-\nputing self-attention in the proposed Swin Transformer architec-\nture. In layer l (left), a regular window partitioning scheme is\nadopted, and self-attention is computed within each window. In\nthe next layer l + 1 (right), the window partitioning is shifted, re-\nsulting in new windows. The self-attention computation in the new\nwindows crosses the boundaries of the previous windows in layer\nl, providing connections among them."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1097
                },
                {
                    "x": 2277,
                    "y": 1097
                },
                {
                    "x": 2277,
                    "y": 1293
                },
                {
                    "x": 1280,
                    "y": 1293
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:20px'>shifted window approach has much lower latency than the<br>sliding window method, yet is similar in modeling power<br>(see Tables 5 and 6). The shifted window approach also<br>proves beneficial for all-MLP architectures [61].</p>",
            "id": 21,
            "page": 2,
            "text": "shifted window approach has much lower latency than the\nsliding window method, yet is similar in modeling power\n(see Tables 5 and 6). The shifted window approach also\nproves beneficial for all-MLP architectures [61]."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1299
                },
                {
                    "x": 2276,
                    "y": 1299
                },
                {
                    "x": 2276,
                    "y": 1940
                },
                {
                    "x": 1278,
                    "y": 1940
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:20px'>The proposed Swin Transformer achieves strong perfor-<br>mance on the recognition tasks of image classification, ob-<br>ject detection and semantic segmentation. It outperforms<br>the ViT / DeiT [20, 63] and ResNe(X)t models [30, 70] sig-<br>nificantly with similar latency on the three tasks. Its 58.7<br>box AP and 51.1 mask AP on the COCO test-dev set sur-<br>pass the previous state-of-the-art results by +2.7 box AP<br>(Copy-paste [26] without external data) and +2.6 mask AP<br>(DetectoRS [46]). On ADE20K semantic segmentation, it<br>obtains 53.5 mIoU on the val set, an improvement of +3.2<br>mIoU over the previous state-of-the-art (SETR [81]). It also<br>achieves a top-1 accuracy of 87.3% on ImageNet-1K image<br>classification.</p>",
            "id": 22,
            "page": 2,
            "text": "The proposed Swin Transformer achieves strong perfor-\nmance on the recognition tasks of image classification, ob-\nject detection and semantic segmentation. It outperforms\nthe ViT / DeiT [20, 63] and ResNe(X)t models [30, 70] sig-\nnificantly with similar latency on the three tasks. Its 58.7\nbox AP and 51.1 mask AP on the COCO test-dev set sur-\npass the previous state-of-the-art results by +2.7 box AP\n(Copy-paste [26] without external data) and +2.6 mask AP\n(DetectoRS [46]). On ADE20K semantic segmentation, it\nobtains 53.5 mIoU on the val set, an improvement of +3.2\nmIoU over the previous state-of-the-art (SETR [81]). It also\nachieves a top-1 accuracy of 87.3% on ImageNet-1K image\nclassification."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1946
                },
                {
                    "x": 2277,
                    "y": 1946
                },
                {
                    "x": 2277,
                    "y": 2340
                },
                {
                    "x": 1280,
                    "y": 2340
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:18px'>It is our belief that a unified architecture across com-<br>puter vision and natural language processing could benefit<br>both fields, since it would facilitate joint modeling of vi-<br>sual and textual signals and the modeling knowledge from<br>both domains can be more deeply shared. We hope that<br>Swin Transformer's strong performance on various vision<br>problems can drive this belief deeper in the community and<br>encourage unified modeling of vision and language signals.</p>",
            "id": 23,
            "page": 2,
            "text": "It is our belief that a unified architecture across com-\nputer vision and natural language processing could benefit\nboth fields, since it would facilitate joint modeling of vi-\nsual and textual signals and the modeling knowledge from\nboth domains can be more deeply shared. We hope that\nSwin Transformer's strong performance on various vision\nproblems can drive this belief deeper in the community and\nencourage unified modeling of vision and language signals."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2382
                },
                {
                    "x": 1636,
                    "y": 2382
                },
                {
                    "x": 1636,
                    "y": 2433
                },
                {
                    "x": 1281,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:22px'>2. Related Work</p>",
            "id": 24,
            "page": 2,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2466
                },
                {
                    "x": 2277,
                    "y": 2466
                },
                {
                    "x": 2277,
                    "y": 2867
                },
                {
                    "x": 1280,
                    "y": 2867
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:18px'>CNN and variants CNNs serve as the standard network<br>model throughout computer vision. While the CNN has ex-<br>isted for several decades [40], it was not until the introduc-<br>tion of AlexNet [39] that the CNN took off and became<br>mainstream. Since then, deeper and more effective con-<br>volutional neural architectures have been proposed to fur-<br>ther propel the deep learning wave in computer vision, e.g.,<br>VGG [52], GoogleNet [57], ResNet [30], DenseNet [34],</p>",
            "id": 25,
            "page": 2,
            "text": "CNN and variants CNNs serve as the standard network\nmodel throughout computer vision. While the CNN has ex-\nisted for several decades [40], it was not until the introduc-\ntion of AlexNet [39] that the CNN took off and became\nmainstream. Since then, deeper and more effective con-\nvolutional neural architectures have been proposed to fur-\nther propel the deep learning wave in computer vision, e.g.,\nVGG [52], GoogleNet [57], ResNet [30], DenseNet [34],"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2896
                },
                {
                    "x": 2275,
                    "y": 2896
                },
                {
                    "x": 2275,
                    "y": 2971
                },
                {
                    "x": 1283,
                    "y": 2971
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:14px'>weights across a feature map, it is difficult for a sliding-window based<br>self-attention layer to have efficient memory access in practice.</p>",
            "id": 26,
            "page": 2,
            "text": "weights across a feature map, it is difficult for a sliding-window based\nself-attention layer to have efficient memory access in practice."
        },
        {
            "bounding_box": [
                {
                    "x": 1224,
                    "y": 3053
                },
                {
                    "x": 1252,
                    "y": 3053
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1224,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='27' style='font-size:18px'>2</footer>",
            "id": 27,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 304
                },
                {
                    "x": 1198,
                    "y": 304
                },
                {
                    "x": 1198,
                    "y": 854
                },
                {
                    "x": 200,
                    "y": 854
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:16px'>HRNet [65], and EfficientNet [58]. In addition to these<br>architectural advances, there has also been much work on<br>improving individual convolution layers, such as depth-<br>wise convolution [70] and deformable convolution [18, 84].<br>While the CNN and its variants are still the primary back-<br>bone architectures for computer vision applications, we<br>highlight the strong potential of Transformer-like architec-<br>tures for unified modeling between vision and language.<br>Our work achieves strong performance on several basic vi-<br>sual recognition tasks, and we hope it will contribute to a<br>modeling shift.</p>",
            "id": 28,
            "page": 3,
            "text": "HRNet [65], and EfficientNet [58]. In addition to these\narchitectural advances, there has also been much work on\nimproving individual convolution layers, such as depth-\nwise convolution [70] and deformable convolution [18, 84].\nWhile the CNN and its variants are still the primary back-\nbone architectures for computer vision applications, we\nhighlight the strong potential of Transformer-like architec-\ntures for unified modeling between vision and language.\nOur work achieves strong performance on several basic vi-\nsual recognition tasks, and we hope it will contribute to a\nmodeling shift."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 913
                },
                {
                    "x": 1199,
                    "y": 913
                },
                {
                    "x": 1199,
                    "y": 1612
                },
                {
                    "x": 200,
                    "y": 1612
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:18px'>Self-attention based backbone architectures Also in-<br>spired by the success of self-attention layers and Trans-<br>former architectures in the NLP field, some works employ<br>self-attention layers to replace some or all of the spatial con-<br>volution layers in the popular ResNet [33, 50, 80]. In these<br>works, the self-attention is computed within a local window<br>of each pixel to expedite optimization [33], and they achieve<br>slightly better accuracy/FLOPs trade-offs than the counter-<br>part ResNet architecture. However, their costly memory<br>access causes their actual latency to be significantly larger<br>than that of the convolutional networks [33]. Instead of us-<br>ing sliding windows, we propose to shift windows between<br>consecutive layers, which allows for a more efficient imple-<br>mentation in general hardware.</p>",
            "id": 29,
            "page": 3,
            "text": "Self-attention based backbone architectures Also in-\nspired by the success of self-attention layers and Trans-\nformer architectures in the NLP field, some works employ\nself-attention layers to replace some or all of the spatial con-\nvolution layers in the popular ResNet [33, 50, 80]. In these\nworks, the self-attention is computed within a local window\nof each pixel to expedite optimization [33], and they achieve\nslightly better accuracy/FLOPs trade-offs than the counter-\npart ResNet architecture. However, their costly memory\naccess causes their actual latency to be significantly larger\nthan that of the convolutional networks [33]. Instead of us-\ning sliding windows, we propose to shift windows between\nconsecutive layers, which allows for a more efficient imple-\nmentation in general hardware."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1670
                },
                {
                    "x": 1198,
                    "y": 1670
                },
                {
                    "x": 1198,
                    "y": 2218
                },
                {
                    "x": 202,
                    "y": 2218
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>Self-attention/Transformers to complement CNNs An-<br>other line of work is to augment a standard CNN architec-<br>ture with self-attention layers or Transformers. The self-<br>attention layers can complement backbones [67, 7, 3, 71,<br>23, 74, 55] or head networks [32, 27] by providing the ca-<br>pability to encode distant dependencies or heterogeneous<br>interactions. More recently, the encoder-decoder design in<br>Transformer has been applied for the object detection and<br>instance segmentation tasks [8, 13, 85, 56]. Our work ex-<br>plores the adaptation of Transformers for basic visual fea-<br>ture extraction and is complementary to these works.</p>",
            "id": 30,
            "page": 3,
            "text": "Self-attention/Transformers to complement CNNs An-\nother line of work is to augment a standard CNN architec-\nture with self-attention layers or Transformers. The self-\nattention layers can complement backbones [67, 7, 3, 71,\n23, 74, 55] or head networks [32, 27] by providing the ca-\npability to encode distant dependencies or heterogeneous\ninteractions. More recently, the encoder-decoder design in\nTransformer has been applied for the object detection and\ninstance segmentation tasks [8, 13, 85, 56]. Our work ex-\nplores the adaptation of Transformers for basic visual fea-\nture extraction and is complementary to these works."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2276
                },
                {
                    "x": 1198,
                    "y": 2276
                },
                {
                    "x": 1198,
                    "y": 2977
                },
                {
                    "x": 200,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>Transformer based vision backbones Most related to<br>our work is the Vision Transformer (ViT) [20] and its<br>follow-ups [63, 72, 15, 28, 66]. The pioneering work of<br>ViT directly applies a Transformer architecture on non-<br>overlapping medium-sized image patches for image clas-<br>sification. It achieves an impressive speed-accuracy trade-<br>off on image classification compared to convolutional net-<br>works. While ViT requires large-scale training datasets<br>(i.e., JFT-300M) to perform well, DeiT [63] introduces sev-<br>eral training strategies that allow ViT to also be effective<br>using the smaller ImageNet-1K dataset. The results of ViT<br>on image classification are encouraging, but its architec-<br>ture is unsuitable for use as a general-purpose backbone<br>network on dense vision tasks or when the input image</p>",
            "id": 31,
            "page": 3,
            "text": "Transformer based vision backbones Most related to\nour work is the Vision Transformer (ViT) [20] and its\nfollow-ups [63, 72, 15, 28, 66]. The pioneering work of\nViT directly applies a Transformer architecture on non-\noverlapping medium-sized image patches for image clas-\nsification. It achieves an impressive speed-accuracy trade-\noff on image classification compared to convolutional net-\nworks. While ViT requires large-scale training datasets\n(i.e., JFT-300M) to perform well, DeiT [63] introduces sev-\neral training strategies that allow ViT to also be effective\nusing the smaller ImageNet-1K dataset. The results of ViT\non image classification are encouraging, but its architec-\nture is unsuitable for use as a general-purpose backbone\nnetwork on dense vision tasks or when the input image"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 308
                },
                {
                    "x": 2278,
                    "y": 308
                },
                {
                    "x": 2278,
                    "y": 1303
                },
                {
                    "x": 1276,
                    "y": 1303
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='32' style='font-size:16px'>resolution is high, due to its low-resolution feature maps<br>and the quadratic increase in complexity with image size.<br>There are a few works applying ViT models to the dense<br>vision tasks of object detection and semantic segmenta-<br>tion by direct upsampling or deconvolution but with rela-<br>tively lower performance [2, 81]. Concurrent to our work<br>are some that modify the ViT architecture [72, 15, 28]<br>for better image classification. Empirically, we find our<br>Swin Transformer architecture to achieve the best speed-<br>accuracy trade-off among these methods on image classi-<br>fication, even though our work focuses on general-purpose<br>performance rather than specifically on classification. An-<br>other concurrent work [66] explores a similar line of think-<br>ing to build multi-resolution feature maps on Transform-<br>ers. Its complexity is still quadratic to image size, while<br>ours is linear and also operates locally which has proven<br>beneficial in modeling the high correlation in visual sig-<br>nals [36, 25, 41]. Our approach is both efficient and ef-<br>fective, achieving state-of-the-art accuracy on both COCO<br>object detection and ADE20K semantic segmentation.</p>",
            "id": 32,
            "page": 3,
            "text": "resolution is high, due to its low-resolution feature maps\nand the quadratic increase in complexity with image size.\nThere are a few works applying ViT models to the dense\nvision tasks of object detection and semantic segmenta-\ntion by direct upsampling or deconvolution but with rela-\ntively lower performance [2, 81]. Concurrent to our work\nare some that modify the ViT architecture [72, 15, 28]\nfor better image classification. Empirically, we find our\nSwin Transformer architecture to achieve the best speed-\naccuracy trade-off among these methods on image classi-\nfication, even though our work focuses on general-purpose\nperformance rather than specifically on classification. An-\nother concurrent work [66] explores a similar line of think-\ning to build multi-resolution feature maps on Transform-\ners. Its complexity is still quadratic to image size, while\nours is linear and also operates locally which has proven\nbeneficial in modeling the high correlation in visual sig-\nnals [36, 25, 41]. Our approach is both efficient and ef-\nfective, achieving state-of-the-art accuracy on both COCO\nobject detection and ADE20K semantic segmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1358
                },
                {
                    "x": 1505,
                    "y": 1358
                },
                {
                    "x": 1505,
                    "y": 1407
                },
                {
                    "x": 1280,
                    "y": 1407
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:22px'>3. Method</p>",
            "id": 33,
            "page": 3,
            "text": "3. Method"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1444
                },
                {
                    "x": 1776,
                    "y": 1444
                },
                {
                    "x": 1776,
                    "y": 1492
                },
                {
                    "x": 1279,
                    "y": 1492
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:20px'>3.1. Overall Architecture</p>",
            "id": 34,
            "page": 3,
            "text": "3.1. Overall Architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1525
                },
                {
                    "x": 2277,
                    "y": 1525
                },
                {
                    "x": 2277,
                    "y": 2019
                },
                {
                    "x": 1279,
                    "y": 2019
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:16px'>An overview of the Swin Transformer architecture is pre-<br>sented in Figure 3, which illustrates the tiny version (Swin-<br>T). It first splits an input RGB image into non-overlapping<br>patches by a patch splitting module, like ViT. Each patch is<br>treated as a \"token\" and its feature is set as a concatenation<br>of the raw pixel RGB values. In our implementation, we use<br>a patch size of 4 x 4 and thus the feature dimension of each<br>patch is 4 x 4 x 3 = 48. A linear embedding layer is ap-<br>plied on this raw-valued feature to project it to an arbitrary<br>dimension (denoted as C).</p>",
            "id": 35,
            "page": 3,
            "text": "An overview of the Swin Transformer architecture is pre-\nsented in Figure 3, which illustrates the tiny version (Swin-\nT). It first splits an input RGB image into non-overlapping\npatches by a patch splitting module, like ViT. Each patch is\ntreated as a \"token\" and its feature is set as a concatenation\nof the raw pixel RGB values. In our implementation, we use\na patch size of 4 x 4 and thus the feature dimension of each\npatch is 4 x 4 x 3 = 48. A linear embedding layer is ap-\nplied on this raw-valued feature to project it to an arbitrary\ndimension (denoted as C)."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2027
                },
                {
                    "x": 2277,
                    "y": 2027
                },
                {
                    "x": 2277,
                    "y": 2272
                },
                {
                    "x": 1279,
                    "y": 2272
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:18px'>Several Transformer blocks with modified self-attention<br>computation (Swin Transformer blocks) are applied on these<br>patch tokens. The Transformer blocks maintain the number<br>of tokens (부 x 4), and together with the linear embedding<br>are referred to as \"Stage 1\".</p>",
            "id": 36,
            "page": 3,
            "text": "Several Transformer blocks with modified self-attention\ncomputation (Swin Transformer blocks) are applied on these\npatch tokens. The Transformer blocks maintain the number\nof tokens (부 x 4), and together with the linear embedding\nare referred to as \"Stage 1\"."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2280
                },
                {
                    "x": 2276,
                    "y": 2280
                },
                {
                    "x": 2276,
                    "y": 2976
                },
                {
                    "x": 1277,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>To produce a hierarchical representation, the number of<br>tokens is reduced by patch merging layers as the network<br>gets deeper. The first patch merging layer concatenates the<br>features of each group of 2 x 2 neighboring patches, and<br>applies a linear layer on the 4C-dimensional concatenated<br>features. This reduces the number of tokens by a multiple<br>of2 x 2 = 4 (2x downsampling of resolution), and the out-<br>put dimension is set to 2C. Swin Transformer blocks are<br>applied afterwards for feature transformation, with the res-<br>H wo · This first block of patch merging<br>olution kept at x<br>8<br>and feature transformation is denoted as \"Stage 2\" · The pro-<br>cedure is repeated twice, as \"Stage 3\" and \"Stage 4\" , with<br>output resolutions of H W H W<br>x and x respectively.<br>16 16 32 32 ,<br>These stages jointly produce a hierarchical representation,</p>",
            "id": 37,
            "page": 3,
            "text": "To produce a hierarchical representation, the number of\ntokens is reduced by patch merging layers as the network\ngets deeper. The first patch merging layer concatenates the\nfeatures of each group of 2 x 2 neighboring patches, and\napplies a linear layer on the 4C-dimensional concatenated\nfeatures. This reduces the number of tokens by a multiple\nof2 x 2 = 4 (2x downsampling of resolution), and the out-\nput dimension is set to 2C. Swin Transformer blocks are\napplied afterwards for feature transformation, with the res-\nH wo · This first block of patch merging\nolution kept at x\n8\nand feature transformation is denoted as \"Stage 2\" · The pro-\ncedure is repeated twice, as \"Stage 3\" and \"Stage 4\" , with\noutput resolutions of H W H W\nx and x respectively.\n16 16 32 32 ,\nThese stages jointly produce a hierarchical representation,"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3053
                },
                {
                    "x": 1252,
                    "y": 3053
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1225,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='38' style='font-size:14px'>3</footer>",
            "id": 38,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 295
                },
                {
                    "x": 2303,
                    "y": 295
                },
                {
                    "x": 2303,
                    "y": 923
                },
                {
                    "x": 196,
                    "y": 923
                }
            ],
            "category": "figure",
            "html": "<figure><img id='39' style='font-size:14px' alt=\"Z zl+1\nHx⌀ x48 4x⌀xC �x⌀ x2C 품x W16 x4C H2 x W32 x8C MLP MLP\nStage 1 Stage 2 Stage 3 Stage 4\nLN LN\nEmbedding\nMerging\nMerging\nMerging\nHxW x3 2 l zl+1\nSwin Swin Swin Swin\nTransformer Transformer Transformer Transformer W-MSA SW-MSA\nImages Partition\nPatch\nPatch\nPatch\nPatch\nBlock Block Block Block\nLinear\nx2 x2 X6 x2 Z Z\n(a) Architecture (b) Two Successive Swin Transformer Blocks\" data-coord=\"top-left:(196,295); bottom-right:(2303,923)\" /></figure>",
            "id": 39,
            "page": 4,
            "text": "Z zl+1\nHx⌀ x48 4x⌀xC �x⌀ x2C 품x W16 x4C H2 x W32 x8C MLP MLP\nStage 1 Stage 2 Stage 3 Stage 4\nLN LN\nEmbedding\nMerging\nMerging\nMerging\nHxW x3 2 l zl+1\nSwin Swin Swin Swin\nTransformer Transformer Transformer Transformer W-MSA SW-MSA\nImages Partition\nPatch\nPatch\nPatch\nPatch\nBlock Block Block Block\nLinear\nx2 x2 X6 x2 Z Z\n(a) Architecture (b) Two Successive Swin Transformer Blocks"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 928
                },
                {
                    "x": 2276,
                    "y": 928
                },
                {
                    "x": 2276,
                    "y": 1020
                },
                {
                    "x": 199,
                    "y": 1020
                }
            ],
            "category": "caption",
            "html": "<br><caption id='40' style='font-size:16px'>Figure 3. (a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks (notation presented with<br>Eq. (3)). W-MSA and SW-MSA are multi-head self attention modules with regular and shifted windowing configurations, respectively.</caption>",
            "id": 40,
            "page": 4,
            "text": "Figure 3. (a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks (notation presented with\nEq. (3)). W-MSA and SW-MSA are multi-head self attention modules with regular and shifted windowing configurations, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1105
                },
                {
                    "x": 1198,
                    "y": 1105
                },
                {
                    "x": 1198,
                    "y": 1350
                },
                {
                    "x": 201,
                    "y": 1350
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>with the same feature map resolutions as those of typical<br>convolutional networks, e.g., VGG [52] and ResNet [30].<br>As a result, the proposed architecture can conveniently re-<br>place the backbone networks in existing methods for vari-<br>ous vision tasks.</p>",
            "id": 41,
            "page": 4,
            "text": "with the same feature map resolutions as those of typical\nconvolutional networks, e.g., VGG [52] and ResNet [30].\nAs a result, the proposed architecture can conveniently re-\nplace the backbone networks in existing methods for vari-\nous vision tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1485
                },
                {
                    "x": 1199,
                    "y": 1485
                },
                {
                    "x": 1199,
                    "y": 1984
                },
                {
                    "x": 201,
                    "y": 1984
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>Swin Transformer block Swin Transformer is built by<br>replacing the standard multi-head self attention (MSA)<br>module in a Transformer block by a module based on<br>shifted windows (described in Section 3.2), with other lay-<br>ers kept the same. As illustrated in Figure 3(b), a Swin<br>Transformer block consists of a shifted window based MSA<br>module, followed by a 2-layer MLP with GELU non-<br>linearity in between. A LayerNorm (LN) layer is applied<br>before each MSA module and each MLP, and a residual<br>connection is applied after each module.</p>",
            "id": 42,
            "page": 4,
            "text": "Swin Transformer block Swin Transformer is built by\nreplacing the standard multi-head self attention (MSA)\nmodule in a Transformer block by a module based on\nshifted windows (described in Section 3.2), with other lay-\ners kept the same. As illustrated in Figure 3(b), a Swin\nTransformer block consists of a shifted window based MSA\nmodule, followed by a 2-layer MLP with GELU non-\nlinearity in between. A LayerNorm (LN) layer is applied\nbefore each MSA module and each MLP, and a residual\nconnection is applied after each module."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2055
                },
                {
                    "x": 1014,
                    "y": 2055
                },
                {
                    "x": 1014,
                    "y": 2104
                },
                {
                    "x": 201,
                    "y": 2104
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:22px'>3.2. Shifted Window based Self-A ttention</p>",
            "id": 43,
            "page": 4,
            "text": "3.2. Shifted Window based Self-A ttention"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2147
                },
                {
                    "x": 1199,
                    "y": 2147
                },
                {
                    "x": 1199,
                    "y": 2543
                },
                {
                    "x": 201,
                    "y": 2543
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:18px'>The standard Transformer architecture [64] and its adap-<br>tation for image classification [20] both conduct global self-<br>attention, where the relationships between a token and all<br>other tokens are computed. The global computation leads to<br>quadratic complexity with respect to the number of tokens,<br>making it unsuitable for many vision problems requiring an<br>immense set of tokens for dense prediction or to represent a<br>high-resolution image.</p>",
            "id": 44,
            "page": 4,
            "text": "The standard Transformer architecture [64] and its adap-\ntation for image classification [20] both conduct global self-\nattention, where the relationships between a token and all\nother tokens are computed. The global computation leads to\nquadratic complexity with respect to the number of tokens,\nmaking it unsuitable for many vision problems requiring an\nimmense set of tokens for dense prediction or to represent a\nhigh-resolution image."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2677
                },
                {
                    "x": 1199,
                    "y": 2677
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:18px'>Self-attention in non-overlapped windows For efficient<br>modeling, we propose to compute self-attention within lo-<br>cal windows. The windows are arranged to evenly partition<br>the image in a non-overlapping manner. Supposing each<br>window contains M x M patches, the computational com-<br>plexity of a global MSA module and a window based one</p>",
            "id": 45,
            "page": 4,
            "text": "Self-attention in non-overlapped windows For efficient\nmodeling, we propose to compute self-attention within lo-\ncal windows. The windows are arranged to evenly partition\nthe image in a non-overlapping manner. Supposing each\nwindow contains M x M patches, the computational com-\nplexity of a global MSA module and a window based one"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1103
                },
                {
                    "x": 1880,
                    "y": 1103
                },
                {
                    "x": 1880,
                    "y": 1150
                },
                {
                    "x": 1279,
                    "y": 1150
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:16px'>on an image of h x w patches are3:</p>",
            "id": 46,
            "page": 4,
            "text": "on an image of h x w patches are3:"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1353
                },
                {
                    "x": 2276,
                    "y": 1353
                },
                {
                    "x": 2276,
                    "y": 1550
                },
                {
                    "x": 1281,
                    "y": 1550
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:18px'>where the former is quadratic to patch number hw, and the<br>latter is linear when M is fixed (set to 7 by default). Global<br>self-attention computation is generally unaffordable for a<br>large hw, while the window based self-attention is scalable.</p>",
            "id": 47,
            "page": 4,
            "text": "where the former is quadratic to patch number hw, and the\nlatter is linear when M is fixed (set to 7 by default). Global\nself-attention computation is generally unaffordable for a\nlarge hw, while the window based self-attention is scalable."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1612
                },
                {
                    "x": 2277,
                    "y": 1612
                },
                {
                    "x": 2277,
                    "y": 2005
                },
                {
                    "x": 1280,
                    "y": 2005
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>Shifted window partitioning in successive blocks The<br>window-based self-attention module lacks connections<br>across windows, which limits its modeling power. To intro-<br>duce cross-window connections while maintaining the effi-<br>cient computation of non-overlapping windows, we propose<br>a shifted window partitioning approach which alternates be-<br>tween two partitioning configurations in consecutive Swin<br>Transformer blocks.</p>",
            "id": 48,
            "page": 4,
            "text": "Shifted window partitioning in successive blocks The\nwindow-based self-attention module lacks connections\nacross windows, which limits its modeling power. To intro-\nduce cross-window connections while maintaining the effi-\ncient computation of non-overlapping windows, we propose\na shifted window partitioning approach which alternates be-\ntween two partitioning configurations in consecutive Swin\nTransformer blocks."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2012
                },
                {
                    "x": 2276,
                    "y": 2012
                },
                {
                    "x": 2276,
                    "y": 2337
                },
                {
                    "x": 1280,
                    "y": 2337
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:20px'>As illustrated in Figure 2, the first module uses a regular<br>window partitioning strategy which starts from the top-left<br>pixel, and the 8 x 8 feature map is evenly partitioned into<br>2 x 2 windows of size 4 x 4 (M = 4). Then, the next mod-<br>ule adopts a windowing configuration that is shifted from<br>that of the preceding layer, by displacing the windows by<br>(L쓸 」 / 쓸 」) pixels from the regularly partitioned windows.</p>",
            "id": 49,
            "page": 4,
            "text": "As illustrated in Figure 2, the first module uses a regular\nwindow partitioning strategy which starts from the top-left\npixel, and the 8 x 8 feature map is evenly partitioned into\n2 x 2 windows of size 4 x 4 (M = 4). Then, the next mod-\nule adopts a windowing configuration that is shifted from\nthat of the preceding layer, by displacing the windows by\n(L쓸 」 / 쓸 」) pixels from the regularly partitioned windows."
        },
        {
            "bounding_box": [
                {
                    "x": 1383,
                    "y": 2335
                },
                {
                    "x": 1397,
                    "y": 2335
                },
                {
                    "x": 1397,
                    "y": 2356
                },
                {
                    "x": 1383,
                    "y": 2356
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:14px'>,</p>",
            "id": 50,
            "page": 4,
            "text": ","
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2361
                },
                {
                    "x": 2274,
                    "y": 2361
                },
                {
                    "x": 2274,
                    "y": 2456
                },
                {
                    "x": 1281,
                    "y": 2456
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:18px'>With the shifted window partitioning approach, consec-<br>utive Swin Transformer blocks are computed as</p>",
            "id": 51,
            "page": 4,
            "text": "With the shifted window partitioning approach, consec-\nutive Swin Transformer blocks are computed as"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2801
                },
                {
                    "x": 2274,
                    "y": 2801
                },
                {
                    "x": 2274,
                    "y": 2901
                },
                {
                    "x": 1281,
                    "y": 2901
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:18px'>where Z and zl denote the output features of the (S)W-<br>MSA module and the MLP module for block l, respectively;</p>",
            "id": 52,
            "page": 4,
            "text": "where Z and zl denote the output features of the (S)W-\nMSA module and the MLP module for block l, respectively;"
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 2931
                },
                {
                    "x": 2123,
                    "y": 2931
                },
                {
                    "x": 2123,
                    "y": 2974
                },
                {
                    "x": 1332,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:16px'>3We omit SoftMax computation in determining complexity.</p>",
            "id": 53,
            "page": 4,
            "text": "3We omit SoftMax computation in determining complexity."
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3056
                },
                {
                    "x": 1249,
                    "y": 3056
                },
                {
                    "x": 1249,
                    "y": 3088
                },
                {
                    "x": 1226,
                    "y": 3088
                }
            ],
            "category": "footer",
            "html": "<footer id='54' style='font-size:16px'>4</footer>",
            "id": 54,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 291
                },
                {
                    "x": 1195,
                    "y": 291
                },
                {
                    "x": 1195,
                    "y": 540
                },
                {
                    "x": 205,
                    "y": 540
                }
            ],
            "category": "figure",
            "html": "<figure><img id='55' style='font-size:14px' alt=\"C A C\nmasked\nMSA\nB B B B\nmasked\nwindow partition C A C A\nMSA\ncyclic shift reverse cyclic shift\" data-coord=\"top-left:(205,291); bottom-right:(1195,540)\" /></figure>",
            "id": 55,
            "page": 5,
            "text": "C A C\nmasked\nMSA\nB B B B\nmasked\nwindow partition C A C A\nMSA\ncyclic shift reverse cyclic shift"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 561
                },
                {
                    "x": 1195,
                    "y": 561
                },
                {
                    "x": 1195,
                    "y": 649
                },
                {
                    "x": 204,
                    "y": 649
                }
            ],
            "category": "caption",
            "html": "<br><caption id='56' style='font-size:14px'>Figure 4. Illustration of an efficient batch computation approach<br>for self-attention in shifted window partitioning.</caption>",
            "id": 56,
            "page": 5,
            "text": "Figure 4. Illustration of an efficient batch computation approach\nfor self-attention in shifted window partitioning."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 738
                },
                {
                    "x": 1197,
                    "y": 738
                },
                {
                    "x": 1197,
                    "y": 883
                },
                {
                    "x": 202,
                    "y": 883
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:18px'>W-MSA and SW-MSA denote window based multi-head<br>self-attention using regular and shifted window partitioning<br>configurations, respectively.</p>",
            "id": 57,
            "page": 5,
            "text": "W-MSA and SW-MSA denote window based multi-head\nself-attention using regular and shifted window partitioning\nconfigurations, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 891
                },
                {
                    "x": 1197,
                    "y": 891
                },
                {
                    "x": 1197,
                    "y": 1134
                },
                {
                    "x": 202,
                    "y": 1134
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>The shifted window partitioning approach introduces<br>connections between neighboring non-overlapping win-<br>dows in the previous layer and is found to be effective in im-<br>age classification, object detection, and semantic segmenta-<br>tion, as shown in Table 4.</p>",
            "id": 58,
            "page": 5,
            "text": "The shifted window partitioning approach introduces\nconnections between neighboring non-overlapping win-\ndows in the previous layer and is found to be effective in im-\nage classification, object detection, and semantic segmenta-\ntion, as shown in Table 4."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1202
                },
                {
                    "x": 1199,
                    "y": 1202
                },
                {
                    "x": 1199,
                    "y": 2198
                },
                {
                    "x": 200,
                    "y": 2198
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:16px'>Efficient batch computation for shifted configuration<br>An issue with shifted window partitioning is that it will re-<br>sult in more windows, from □ hM 7 x I 巡 / to ([음 1 + 1) x<br>(「 巡 I +1) in the shifted configuration, and some of the win-<br>dows will be smaller than M x M4 · A naive solution is to<br>pad the smaller windows to a size of M x M and mask<br>out the padded values when computing attention. When<br>the number of windows in regular partitioning is small, e.g.<br>2 x 2, the increased computation with this naive solution is<br>considerable (2 x 2 → 3 x 3, which is 2.25 times greater).<br>Here, we propose a more efficient batch computation ap-<br>proach by cyclic-shifting toward the top-left direction, as il-<br>lustrated in Figure 4. After this shift, a batched window may<br>be composed of several sub-windows that are not adjacent<br>in the feature map, SO a masking mechanism is employed to<br>limit self-attention computation to within each sub-window.<br>With the cyclic-shift, the number of batched windows re-<br>mains the same as that of regular window partitioning, and<br>thus is also efficient. The low latency of this approach is<br>shown in Table 5.</p>",
            "id": 59,
            "page": 5,
            "text": "Efficient batch computation for shifted configuration\nAn issue with shifted window partitioning is that it will re-\nsult in more windows, from □ hM 7 x I 巡 / to ([음 1 + 1) x\n(「 巡 I +1) in the shifted configuration, and some of the win-\ndows will be smaller than M x M4 · A naive solution is to\npad the smaller windows to a size of M x M and mask\nout the padded values when computing attention. When\nthe number of windows in regular partitioning is small, e.g.\n2 x 2, the increased computation with this naive solution is\nconsiderable (2 x 2 → 3 x 3, which is 2.25 times greater).\nHere, we propose a more efficient batch computation ap-\nproach by cyclic-shifting toward the top-left direction, as il-\nlustrated in Figure 4. After this shift, a batched window may\nbe composed of several sub-windows that are not adjacent\nin the feature map, SO a masking mechanism is employed to\nlimit self-attention computation to within each sub-window.\nWith the cyclic-shift, the number of batched windows re-\nmains the same as that of regular window partitioning, and\nthus is also efficient. The low latency of this approach is\nshown in Table 5."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2269
                },
                {
                    "x": 1198,
                    "y": 2269
                },
                {
                    "x": 1198,
                    "y": 2421
                },
                {
                    "x": 202,
                    "y": 2421
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:18px'>Relative position bias In computing self-attention, we<br>follow [49, 1, 32, 33] by including a relative position bias<br>B E R M2 x M2<br>to each head in computing similarity:</p>",
            "id": 60,
            "page": 5,
            "text": "Relative position bias In computing self-attention, we\nfollow [49, 1, 32, 33] by including a relative position bias\nB E R M2 x M2\nto each head in computing similarity:"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2560
                },
                {
                    "x": 1199,
                    "y": 2560
                },
                {
                    "x": 1199,
                    "y": 2862
                },
                {
                    "x": 201,
                    "y": 2862
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>where Q, K, V E RM2 xd<br>are the query, key and value ma-<br>trices; d is the querylkey dimension, and M2 is the number<br>of patches in a window. Since the relative position along<br>each axis lies in the range [-M + 1, M - 1], we parameter-<br>ize a smaller-sized bias matrix B E R(2M -1)x(2M -1) and<br>,<br>values in B are taken from B.</p>",
            "id": 61,
            "page": 5,
            "text": "where Q, K, V E RM2 xd\nare the query, key and value ma-\ntrices; d is the querylkey dimension, and M2 is the number\nof patches in a window. Since the relative position along\neach axis lies in the range [-M + 1, M - 1], we parameter-\nize a smaller-sized bias matrix B E R(2M -1)x(2M -1) and\n,\nvalues in B are taken from B."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2892
                },
                {
                    "x": 1197,
                    "y": 2892
                },
                {
                    "x": 1197,
                    "y": 2973
                },
                {
                    "x": 201,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:14px'>4To make the window size (M, M) divisible by the feature map size of<br>(h, w), bottom-right padding is employed on the feature map if needed.</p>",
            "id": 62,
            "page": 5,
            "text": "4To make the window size (M, M) divisible by the feature map size of\n(h, w), bottom-right padding is employed on the feature map if needed."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 308
                },
                {
                    "x": 2277,
                    "y": 308
                },
                {
                    "x": 2277,
                    "y": 552
                },
                {
                    "x": 1278,
                    "y": 552
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:18px'>We observe significant improvements over counterparts<br>without this bias term or that use absolute position embed-<br>ding, as shown in Table 4. Further adding absolute posi-<br>tion embedding to the input as in [20] drops performance<br>slightly, thus it is not adopted in our implementation.</p>",
            "id": 63,
            "page": 5,
            "text": "We observe significant improvements over counterparts\nwithout this bias term or that use absolute position embed-\nding, as shown in Table 4. Further adding absolute posi-\ntion embedding to the input as in [20] drops performance\nslightly, thus it is not adopted in our implementation."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 558
                },
                {
                    "x": 2276,
                    "y": 558
                },
                {
                    "x": 2276,
                    "y": 703
                },
                {
                    "x": 1280,
                    "y": 703
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:16px'>The learnt relative position bias in pre-training can be<br>also used to initialize a model for fine-tuning with a differ-<br>ent window size through bi-cubic interpolation [20, 63].</p>",
            "id": 64,
            "page": 5,
            "text": "The learnt relative position bias in pre-training can be\nalso used to initialize a model for fine-tuning with a differ-\nent window size through bi-cubic interpolation [20, 63]."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 735
                },
                {
                    "x": 1794,
                    "y": 735
                },
                {
                    "x": 1794,
                    "y": 782
                },
                {
                    "x": 1281,
                    "y": 782
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:20px'>3.3. Architecture Variants</p>",
            "id": 65,
            "page": 5,
            "text": "3.3. Architecture Variants"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 810
                },
                {
                    "x": 2277,
                    "y": 810
                },
                {
                    "x": 2277,
                    "y": 1360
                },
                {
                    "x": 1278,
                    "y": 1360
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:16px'>We build our base model, called Swin-B, to have of<br>model size and computation complexity similar to ViT-<br>B/DeiT-B. We also introduce Swin-T, Swin-S and Swin-L,<br>which are versions of about 0.25x, 0.5x and 2x the model<br>size and computational complexity, respectively. Note that<br>the complexity of Swin-T and Swin-S are similar to those<br>of ResNet-50 (DeiT-S) and ResNet-101, respectively. The<br>window size is set to M = 7 by default. The query dimen-<br>sion of each head is d = 32, and the expansion layer of<br>each MLP is a = 4, for all experiments. The architecture<br>hyper-parameters of these model variants are:</p>",
            "id": 66,
            "page": 5,
            "text": "We build our base model, called Swin-B, to have of\nmodel size and computation complexity similar to ViT-\nB/DeiT-B. We also introduce Swin-T, Swin-S and Swin-L,\nwhich are versions of about 0.25x, 0.5x and 2x the model\nsize and computational complexity, respectively. Note that\nthe complexity of Swin-T and Swin-S are similar to those\nof ResNet-50 (DeiT-S) and ResNet-101, respectively. The\nwindow size is set to M = 7 by default. The query dimen-\nsion of each head is d = 32, and the expansion layer of\neach MLP is a = 4, for all experiments. The architecture\nhyper-parameters of these model variants are:"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1717
                },
                {
                    "x": 2276,
                    "y": 1717
                },
                {
                    "x": 2276,
                    "y": 1914
                },
                {
                    "x": 1280,
                    "y": 1914
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>where C is the channel number of the hidden layers in the<br>first stage. The model size, theoretical computational com-<br>plexity (FLOPs), and throughput of the model variants for<br>ImageNet image classification are listed in Table 1.</p>",
            "id": 67,
            "page": 5,
            "text": "where C is the channel number of the hidden layers in the\nfirst stage. The model size, theoretical computational com-\nplexity (FLOPs), and throughput of the model variants for\nImageNet image classification are listed in Table 1."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1959
                },
                {
                    "x": 1612,
                    "y": 1959
                },
                {
                    "x": 1612,
                    "y": 2010
                },
                {
                    "x": 1280,
                    "y": 2010
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:22px'>4. Experiments</p>",
            "id": 68,
            "page": 5,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2043
                },
                {
                    "x": 2276,
                    "y": 2043
                },
                {
                    "x": 2276,
                    "y": 2340
                },
                {
                    "x": 1279,
                    "y": 2340
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>We conduct experiments on ImageNet-1K image classi-<br>fication [19], COCO object detection [43], and ADE20K<br>semantic segmentation [83]. In the following, we first com-<br>pare the proposed Swin Transformer architecture with the<br>previous state-of-the-arts on the three tasks. Then, we ab-<br>late the important design elements of Swin Transformer.</p>",
            "id": 69,
            "page": 5,
            "text": "We conduct experiments on ImageNet-1K image classi-\nfication [19], COCO object detection [43], and ADE20K\nsemantic segmentation [83]. In the following, we first com-\npare the proposed Swin Transformer architecture with the\nprevious state-of-the-arts on the three tasks. Then, we ab-\nlate the important design elements of Swin Transformer."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2372
                },
                {
                    "x": 2098,
                    "y": 2372
                },
                {
                    "x": 2098,
                    "y": 2420
                },
                {
                    "x": 1280,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:20px'>4.1. Image Classification on ImageNet-1K</p>",
            "id": 70,
            "page": 5,
            "text": "4.1. Image Classification on ImageNet-1K"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2448
                },
                {
                    "x": 2276,
                    "y": 2448
                },
                {
                    "x": 2276,
                    "y": 2696
                },
                {
                    "x": 1279,
                    "y": 2696
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:18px'>Settings For image classification, we benchmark the pro-<br>posed Swin Transformer on ImageNet-1K [19], which con-<br>tains 1.28M training images and 50K validation images<br>from 1,000 classes. The top-1 accuracy on a single crop<br>is reported. We consider two training settings:</p>",
            "id": 71,
            "page": 5,
            "text": "Settings For image classification, we benchmark the pro-\nposed Swin Transformer on ImageNet-1K [19], which con-\ntains 1.28M training images and 50K validation images\nfrom 1,000 classes. The top-1 accuracy on a single crop\nis reported. We consider two training settings:"
        },
        {
            "bounding_box": [
                {
                    "x": 1335,
                    "y": 2729
                },
                {
                    "x": 2277,
                    "y": 2729
                },
                {
                    "x": 2277,
                    "y": 2976
                },
                {
                    "x": 1335,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>· Regular ImageNet-1K training. This setting mostly<br>follows [63]. We employ an AdamW [37] optimizer<br>for 300 epochs using a cosine decay learning rate<br>scheduler and 20 epochs of linear warm-up. A batch<br>size of 1024, an initial learning rate of 0.001, and a</p>",
            "id": 72,
            "page": 5,
            "text": "· Regular ImageNet-1K training. This setting mostly\nfollows [63]. We employ an AdamW [37] optimizer\nfor 300 epochs using a cosine decay learning rate\nscheduler and 20 epochs of linear warm-up. A batch\nsize of 1024, an initial learning rate of 0.001, and a"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='73' style='font-size:14px'>5</footer>",
            "id": 73,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 308
                },
                {
                    "x": 1199,
                    "y": 308
                },
                {
                    "x": 1199,
                    "y": 604
                },
                {
                    "x": 285,
                    "y": 604
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:18px'>weight decay of 0.05 are used. We include most of<br>the augmentation and regularization strategies of [63]<br>in training, except for repeated augmentation [31] and<br>EMA [45], which do not enhance performance. Note<br>that this is contrary to [63] where repeated augmenta-<br>tion is crucial to stabilize the training of ViT.</p>",
            "id": 74,
            "page": 6,
            "text": "weight decay of 0.05 are used. We include most of\nthe augmentation and regularization strategies of [63]\nin training, except for repeated augmentation [31] and\nEMA [45], which do not enhance performance. Note\nthat this is contrary to [63] where repeated augmenta-\ntion is crucial to stabilize the training of ViT."
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 664
                },
                {
                    "x": 1199,
                    "y": 664
                },
                {
                    "x": 1199,
                    "y": 1214
                },
                {
                    "x": 239,
                    "y": 1214
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:18px'>● Pre-training on ImageNet-22K and fine-tuning on<br>ImageNet-1K. We also pre-train on the larger<br>ImageNet-22K dataset, which contains 14.2 million<br>images and 22K classes. We employ an AdamW opti-<br>mizer for 90 epochs using a linear decay learning rate<br>scheduler with a 5-epoch linear warm-up. A batch size<br>of 4096, an initial learning rate of 0.001, and a weight<br>decay of 0.01 are used. In ImageNet-1K fine-tuning,<br>we train the models for 30 epochs with a batch size of<br>1024, a constant learning rate of 10-5, and a weight<br>decay of 10-8.</p>",
            "id": 75,
            "page": 6,
            "text": "● Pre-training on ImageNet-22K and fine-tuning on\nImageNet-1K. We also pre-train on the larger\nImageNet-22K dataset, which contains 14.2 million\nimages and 22K classes. We employ an AdamW opti-\nmizer for 90 epochs using a linear decay learning rate\nscheduler with a 5-epoch linear warm-up. A batch size\nof 4096, an initial learning rate of 0.001, and a weight\ndecay of 0.01 are used. In ImageNet-1K fine-tuning,\nwe train the models for 30 epochs with a batch size of\n1024, a constant learning rate of 10-5, and a weight\ndecay of 10-8."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1316
                },
                {
                    "x": 1199,
                    "y": 1316
                },
                {
                    "x": 1199,
                    "y": 1514
                },
                {
                    "x": 202,
                    "y": 1514
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:20px'>Results with regular ImageNet-1K training Table 1(a)<br>presents comparisons to other backbones, including both<br>Transformer-based and ConvNet-based, using regular<br>ImageNet-1K training.</p>",
            "id": 76,
            "page": 6,
            "text": "Results with regular ImageNet-1K training Table 1(a)\npresents comparisons to other backbones, including both\nTransformer-based and ConvNet-based, using regular\nImageNet-1K training."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1525
                },
                {
                    "x": 1199,
                    "y": 1525
                },
                {
                    "x": 1199,
                    "y": 1870
                },
                {
                    "x": 202,
                    "y": 1870
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:18px'>Compared to the previous state-of-the-art Transformer-<br>based architecture, i.e. DeiT [63], Swin Transformers no-<br>ticeably surpass the counterpart DeiT architectures with<br>similar complexities: +1.5% for Swin-T (81.3%) over<br>DeiT-S (79.8%) using 2242 input, and +1.5%/1.4% for<br>Swin-B (83.3%/84.5%) over DeiT-B (81.8%/83.1%) using<br>2242/3842 input, respectively.</p>",
            "id": 77,
            "page": 6,
            "text": "Compared to the previous state-of-the-art Transformer-\nbased architecture, i.e. DeiT [63], Swin Transformers no-\nticeably surpass the counterpart DeiT architectures with\nsimilar complexities: +1.5% for Swin-T (81.3%) over\nDeiT-S (79.8%) using 2242 input, and +1.5%/1.4% for\nSwin-B (83.3%/84.5%) over DeiT-B (81.8%/83.1%) using\n2242/3842 input, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1880
                },
                {
                    "x": 1199,
                    "y": 1880
                },
                {
                    "x": 1199,
                    "y": 2228
                },
                {
                    "x": 201,
                    "y": 2228
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:18px'>Compared with the state-of-the-art ConvNets, i.e. Reg-<br>Net [48] and EfficientNet [58], the Swin Transformer<br>achieves a slightly better speed-accuracy trade-off. Not-<br>ing that while RegNet [48] and EfficientNet [58] are ob-<br>tained via a thorough architecture search, the proposed<br>Swin Transformer is adapted from the standard Transformer<br>and has strong potential for further improvement.</p>",
            "id": 78,
            "page": 6,
            "text": "Compared with the state-of-the-art ConvNets, i.e. Reg-\nNet [48] and EfficientNet [58], the Swin Transformer\nachieves a slightly better speed-accuracy trade-off. Not-\ning that while RegNet [48] and EfficientNet [58] are ob-\ntained via a thorough architecture search, the proposed\nSwin Transformer is adapted from the standard Transformer\nand has strong potential for further improvement."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2327
                },
                {
                    "x": 1199,
                    "y": 2327
                },
                {
                    "x": 1199,
                    "y": 2976
                },
                {
                    "x": 200,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:18px'>Results with ImageNet-22K pre-training We also pre-<br>train the larger-capacity Swin-B and Swin-L on ImageNet-<br>22K. Results fine-tuned on ImageNet-1K image classifica-<br>tion are shown in Table 1(b). For Swin-B, the ImageNet-<br>22K pre-training brings 1.8%~1.9% gains over training<br>on ImageNet-1K from scratch. Compared with the previ-<br>ous best results for ImageNet-22K pre-training, our mod-<br>els achieve significantly better speed-accuracy trade-offs:<br>Swin-B obtains 86.4% top-1 accuracy, which is 2.4% higher<br>than that of ViT with similar inference throughput (84.7<br>VS. 85.9 images/sec) and slightly lower FLOPs (47.0G VS.<br>55.4G). The larger Swin-L model achieves 87.3% top-1 ac-<br>curacy, +0.9% better than that of the Swin-B model.</p>",
            "id": 79,
            "page": 6,
            "text": "Results with ImageNet-22K pre-training We also pre-\ntrain the larger-capacity Swin-B and Swin-L on ImageNet-\n22K. Results fine-tuned on ImageNet-1K image classifica-\ntion are shown in Table 1(b). For Swin-B, the ImageNet-\n22K pre-training brings 1.8%~1.9% gains over training\non ImageNet-1K from scratch. Compared with the previ-\nous best results for ImageNet-22K pre-training, our mod-\nels achieve significantly better speed-accuracy trade-offs:\nSwin-B obtains 86.4% top-1 accuracy, which is 2.4% higher\nthan that of ViT with similar inference throughput (84.7\nVS. 85.9 images/sec) and slightly lower FLOPs (47.0G VS.\n55.4G). The larger Swin-L model achieves 87.3% top-1 ac-\ncuracy, +0.9% better than that of the Swin-B model."
        },
        {
            "bounding_box": [
                {
                    "x": 1448,
                    "y": 306
                },
                {
                    "x": 2108,
                    "y": 306
                },
                {
                    "x": 2108,
                    "y": 345
                },
                {
                    "x": 1448,
                    "y": 345
                }
            ],
            "category": "caption",
            "html": "<br><caption id='80' style='font-size:18px'>(a) Regular ImageNet-1K trained models</caption>",
            "id": 80,
            "page": 6,
            "text": "(a) Regular ImageNet-1K trained models"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 371
                },
                {
                    "x": 2276,
                    "y": 371
                },
                {
                    "x": 2276,
                    "y": 1578
                },
                {
                    "x": 1284,
                    "y": 1578
                }
            ],
            "category": "table",
            "html": "<table id='81' style='font-size:14px'><tr><td>method</td><td>image size</td><td>#param.</td><td>FLOPs</td><td>throughput|ImageNet (image / s)</td><td>top-1 acc.</td></tr><tr><td>RegNetY-4G [48]</td><td>2242</td><td>21M</td><td>4.0G</td><td>1156.7</td><td>80.0</td></tr><tr><td>RegNetY-8G [48]</td><td>2242</td><td>39M</td><td>8.0G</td><td>591.6</td><td>81.7</td></tr><tr><td>RegNetY-16G [48]</td><td>2242</td><td>84M</td><td>16.0G</td><td>334.7</td><td>82.9</td></tr><tr><td>EffNet-B3 [58]</td><td>3002</td><td>12M</td><td>1.8G</td><td>732.1</td><td>81.6</td></tr><tr><td>EffNet-B4 [58]</td><td>3802</td><td>19M</td><td>4.2G</td><td>349.4</td><td>82.9</td></tr><tr><td>EffNet-B5 [58]</td><td>4562</td><td>30M</td><td>9.9G</td><td>169.1</td><td>83.6</td></tr><tr><td>EffNet-B6 [58]</td><td>5282</td><td>43M</td><td>19.0G</td><td>96.9</td><td>84.0</td></tr><tr><td>EffNet-B7 [58]</td><td>6002</td><td>66M</td><td>37.0G</td><td>55.1</td><td>84.3</td></tr><tr><td>ViT-B/16 [20]</td><td>3842</td><td>86M</td><td>55.4G</td><td>85.9</td><td>77.9</td></tr><tr><td>ViT-L/16 [20]</td><td>3842</td><td>307M</td><td>190.7G</td><td>27.3</td><td>76.5</td></tr><tr><td>DeiT-S [63]</td><td>2242</td><td>22M</td><td>4.6G</td><td>940.4</td><td>79.8</td></tr><tr><td>DeiT-B [63]</td><td>2242</td><td>86M</td><td>17.5G</td><td>292.3</td><td>81.8</td></tr><tr><td>DeiT-B [63]</td><td>3842</td><td>86M</td><td>55.4G</td><td>85.9</td><td>83.1</td></tr><tr><td>Swin-T</td><td>2242</td><td>29M</td><td>4.5G</td><td>755.2</td><td>81.3</td></tr><tr><td>Swin-S</td><td>2242</td><td>50M</td><td>8.7G</td><td>436.9</td><td>83.0</td></tr><tr><td>Swin-B</td><td>2242</td><td>88M</td><td>15.4G</td><td>278.1</td><td>83.5</td></tr><tr><td>Swin-B</td><td>3842</td><td>88M</td><td>47.0G</td><td>84.7</td><td>84.5</td></tr><tr><td colspan=\"6\">(b) ImageNet-22K pre-trained models</td></tr><tr><td>method</td><td>image size</td><td>#param.</td><td>FLOPs</td><td>throughput (image /s)</td><td>ImageNet top-1 acc.</td></tr><tr><td>R-101x3 [38]</td><td>3842</td><td>388M</td><td>204.6G</td><td>-</td><td>84.4</td></tr><tr><td>R-152x4 [38]</td><td>4802</td><td>937M</td><td>840.5G</td><td>-</td><td>85.4</td></tr><tr><td>ViT-B/16 [20]</td><td>3842</td><td>86M</td><td>55.4G</td><td>85.9</td><td>84.0</td></tr><tr><td>ViT-L/16 [20]</td><td>3842</td><td>307M</td><td>190.7G</td><td>27.3</td><td>85.2</td></tr><tr><td>Swin-B</td><td>2242</td><td>88M</td><td>15.4G</td><td>278.1</td><td>85.2</td></tr></table>",
            "id": 81,
            "page": 6,
            "text": "method image size #param. FLOPs throughput|ImageNet (image / s) top-1 acc.\n RegNetY-4G [48] 2242 21M 4.0G 1156.7 80.0\n RegNetY-8G [48] 2242 39M 8.0G 591.6 81.7\n RegNetY-16G [48] 2242 84M 16.0G 334.7 82.9\n EffNet-B3 [58] 3002 12M 1.8G 732.1 81.6\n EffNet-B4 [58] 3802 19M 4.2G 349.4 82.9\n EffNet-B5 [58] 4562 30M 9.9G 169.1 83.6\n EffNet-B6 [58] 5282 43M 19.0G 96.9 84.0\n EffNet-B7 [58] 6002 66M 37.0G 55.1 84.3\n ViT-B/16 [20] 3842 86M 55.4G 85.9 77.9\n ViT-L/16 [20] 3842 307M 190.7G 27.3 76.5\n DeiT-S [63] 2242 22M 4.6G 940.4 79.8\n DeiT-B [63] 2242 86M 17.5G 292.3 81.8\n DeiT-B [63] 3842 86M 55.4G 85.9 83.1\n Swin-T 2242 29M 4.5G 755.2 81.3\n Swin-S 2242 50M 8.7G 436.9 83.0\n Swin-B 2242 88M 15.4G 278.1 83.5\n Swin-B 3842 88M 47.0G 84.7 84.5\n (b) ImageNet-22K pre-trained models\n method image size #param. FLOPs throughput (image /s) ImageNet top-1 acc.\n R-101x3 [38] 3842 388M 204.6G - 84.4\n R-152x4 [38] 4802 937M 840.5G - 85.4\n ViT-B/16 [20] 3842 86M 55.4G 85.9 84.0\n ViT-L/16 [20] 3842 307M 190.7G 27.3 85.2\n Swin-B 2242 88M 15.4G 278.1"
        },
        {
            "bounding_box": [
                {
                    "x": 1376,
                    "y": 1593
                },
                {
                    "x": 2227,
                    "y": 1593
                },
                {
                    "x": 2227,
                    "y": 1642
                },
                {
                    "x": 1376,
                    "y": 1642
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:14px'>Swin-B 3842 88M 47.0G 84.7 86.4</p>",
            "id": 82,
            "page": 6,
            "text": "Swin-B 3842 88M 47.0G 84.7 86.4"
        },
        {
            "bounding_box": [
                {
                    "x": 1377,
                    "y": 1639
                },
                {
                    "x": 2226,
                    "y": 1639
                },
                {
                    "x": 2226,
                    "y": 1689
                },
                {
                    "x": 1377,
                    "y": 1689
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:14px'>Swin-L 3842 197M 103.9G 42.1 87.3</p>",
            "id": 83,
            "page": 6,
            "text": "Swin-L 3842 197M 103.9G 42.1 87.3"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 1698
                },
                {
                    "x": 2272,
                    "y": 1698
                },
                {
                    "x": 2272,
                    "y": 1827
                },
                {
                    "x": 1283,
                    "y": 1827
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:16px'>Table 1. Comparison of different backbones on ImageNet-1K clas-<br>sification. Throughput is measured using the GitHub repository<br>of [68] and a V100 GPU, following [63].</p>",
            "id": 84,
            "page": 6,
            "text": "Table 1. Comparison of different backbones on ImageNet-1K clas-\nsification. Throughput is measured using the GitHub repository\nof [68] and a V100 GPU, following [63]."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1931
                },
                {
                    "x": 1907,
                    "y": 1931
                },
                {
                    "x": 1907,
                    "y": 1979
                },
                {
                    "x": 1282,
                    "y": 1979
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:22px'>4.2. Object Detection on COCO</p>",
            "id": 85,
            "page": 6,
            "text": "4.2. Object Detection on COCO"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2019
                },
                {
                    "x": 2277,
                    "y": 2019
                },
                {
                    "x": 2277,
                    "y": 2915
                },
                {
                    "x": 1279,
                    "y": 2915
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>Settings Object detection and instance segmentation ex-<br>periments are conducted on COCO 2017, which contains<br>118K training, 5K validation and 20K test-dev images. An<br>ablation study is performed using the validation set, and a<br>system-level comparison is reported on test-dev. For the<br>ablation study, we consider four typical object detection<br>frameworks: Cascade Mask R-CNN [29, 6], ATSS [79],<br>RepPoints v2 [12], and Sparse RCNN [56] in mmdetec-<br>tion [10]. For these four frameworks, we utilize the same<br>settings: multi-scale training [8, 56] (resizing the input such<br>that the shorter side is between 480 and 800 while the longer<br>side is at most 1333), Adam W [44] optimizer (initial learn-<br>ing rate of 0.0001, weight decay of 0.05, and batch size of<br>16), and 3x schedule (36 epochs). For system-level compar-<br>ison, we adopt an improved HTC [9] (denoted as HTC++)<br>with instaboost [22], stronger multi-scale training [7], 6x<br>schedule (72 epochs), soft-NMS [5], and ImageNet-22K<br>pre-trained model as initialization.</p>",
            "id": 86,
            "page": 6,
            "text": "Settings Object detection and instance segmentation ex-\nperiments are conducted on COCO 2017, which contains\n118K training, 5K validation and 20K test-dev images. An\nablation study is performed using the validation set, and a\nsystem-level comparison is reported on test-dev. For the\nablation study, we consider four typical object detection\nframeworks: Cascade Mask R-CNN [29, 6], ATSS [79],\nRepPoints v2 [12], and Sparse RCNN [56] in mmdetec-\ntion [10]. For these four frameworks, we utilize the same\nsettings: multi-scale training [8, 56] (resizing the input such\nthat the shorter side is between 480 and 800 while the longer\nside is at most 1333), Adam W [44] optimizer (initial learn-\ning rate of 0.0001, weight decay of 0.05, and batch size of\n16), and 3x schedule (36 epochs). For system-level compar-\nison, we adopt an improved HTC [9] (denoted as HTC++)\nwith instaboost [22], stronger multi-scale training [7], 6x\nschedule (72 epochs), soft-NMS [5], and ImageNet-22K\npre-trained model as initialization."
        },
        {
            "bounding_box": [
                {
                    "x": 1331,
                    "y": 2928
                },
                {
                    "x": 2272,
                    "y": 2928
                },
                {
                    "x": 2272,
                    "y": 2973
                },
                {
                    "x": 1331,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:14px'>We compare our Swin Transformer to standard Con-</p>",
            "id": 87,
            "page": 6,
            "text": "We compare our Swin Transformer to standard Con-"
        },
        {
            "bounding_box": [
                {
                    "x": 1227,
                    "y": 3058
                },
                {
                    "x": 1251,
                    "y": 3058
                },
                {
                    "x": 1251,
                    "y": 3090
                },
                {
                    "x": 1227,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='88' style='font-size:14px'>6</footer>",
            "id": 88,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 472,
                    "y": 304
                },
                {
                    "x": 862,
                    "y": 304
                },
                {
                    "x": 862,
                    "y": 346
                },
                {
                    "x": 472,
                    "y": 346
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>(a) Various frameworks</p>",
            "id": 89,
            "page": 7,
            "text": "(a) Various frameworks"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 347
                },
                {
                    "x": 1202,
                    "y": 347
                },
                {
                    "x": 1202,
                    "y": 774
                },
                {
                    "x": 203,
                    "y": 774
                }
            ],
            "category": "table",
            "html": "<br><table id='90' style='font-size:16px'><tr><td>Method</td><td>Backbone</td><td>APbox</td><td>APbox</td><td>APbox</td><td>#param.</td><td>FLOPs</td><td>FPS</td></tr><tr><td rowspan=\"2\">Cascade Mask R-CNN</td><td>R-50</td><td>46.3</td><td>64.3</td><td>50.5</td><td>82M</td><td>739G</td><td>18.0</td></tr><tr><td>Swin-T</td><td>50.5</td><td>69.3</td><td>54.9</td><td>86M</td><td>745G</td><td>15.3</td></tr><tr><td rowspan=\"2\">ATSS</td><td>R-50</td><td>43.5</td><td>61.9</td><td>47.0</td><td>32M</td><td>205G</td><td>28.3</td></tr><tr><td>Swin-T</td><td>47.2</td><td>66.5</td><td>51.3</td><td>36M</td><td>215G</td><td>22.3</td></tr><tr><td rowspan=\"2\">RepPointsV2</td><td>R-50</td><td>46.5</td><td>64.6</td><td>50.3</td><td>42M</td><td>274G</td><td>13.6</td></tr><tr><td>Swin-T</td><td>50.0</td><td>68.5</td><td>54.2</td><td>45M</td><td>283G</td><td>12.0</td></tr><tr><td rowspan=\"2\">Sparse R-CNN</td><td>R-50</td><td>44.5</td><td>63.4</td><td>48.2</td><td>106M</td><td>166G</td><td>21.0</td></tr><tr><td>Swin-T</td><td>47.9</td><td>67.3</td><td>52.3</td><td>110M</td><td>172G</td><td>18.4</td></tr></table>",
            "id": 90,
            "page": 7,
            "text": "Method Backbone APbox APbox APbox #param. FLOPs FPS\n Cascade Mask R-CNN R-50 46.3 64.3 50.5 82M 739G 18.0\n Swin-T 50.5 69.3 54.9 86M 745G 15.3\n ATSS R-50 43.5 61.9 47.0 32M 205G 28.3\n Swin-T 47.2 66.5 51.3 36M 215G 22.3\n RepPointsV2 R-50 46.5 64.6 50.3 42M 274G 13.6\n Swin-T 50.0 68.5 54.2 45M 283G 12.0\n Sparse R-CNN R-50 44.5 63.4 48.2 106M 166G 21.0\n Swin-T 47.9 67.3 52.3 110M 172G"
        },
        {
            "bounding_box": [
                {
                    "x": 282,
                    "y": 778
                },
                {
                    "x": 1055,
                    "y": 778
                },
                {
                    "x": 1055,
                    "y": 816
                },
                {
                    "x": 282,
                    "y": 816
                }
            ],
            "category": "caption",
            "html": "<br><caption id='91' style='font-size:16px'>(b) Various backbones w. Cascade Mask R-CNN</caption>",
            "id": 91,
            "page": 7,
            "text": "(b) Various backbones w. Cascade Mask R-CNN"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 801
                },
                {
                    "x": 1202,
                    "y": 801
                },
                {
                    "x": 1202,
                    "y": 1193
                },
                {
                    "x": 204,
                    "y": 1193
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='92' style='font-size:16px' alt=\"Apbox APbox APbox| APmask APmask APmask paramFLOPsFPS\nDeiT-ST 48.0 67.2 51.7 41.4 64.2 44.3 80M 889G 10.4\nR50 46.3 64.3 50.5 40.1 61.7 43.4 82M 739G 18.0\nSwin-T 50.5 69.3 54.9 43.7 66.6 47.1 86M 745G 15.3\nX101-32 48.1 66.5 52.4 41.6 63.9 45.2 101M 819G 12.8\nSwin-S 51.8 70.4 56.3 44.7 67.9 48.5 107M 838G 12.0\nX101-64 48.3 66.4 52.3 41.7 64.0 45.1 140M 972G 10.4\nSwin-B 51.9 70.9 56.5 45.0 68.4 48.7 145M 982G 11.6\" data-coord=\"top-left:(204,801); bottom-right:(1202,1193)\" /></figure>",
            "id": 92,
            "page": 7,
            "text": "Apbox APbox APbox| APmask APmask APmask paramFLOPsFPS\nDeiT-ST 48.0 67.2 51.7 41.4 64.2 44.3 80M 889G 10.4\nR50 46.3 64.3 50.5 40.1 61.7 43.4 82M 739G 18.0\nSwin-T 50.5 69.3 54.9 43.7 66.6 47.1 86M 745G 15.3\nX101-32 48.1 66.5 52.4 41.6 63.9 45.2 101M 819G 12.8\nSwin-S 51.8 70.4 56.3 44.7 67.9 48.5 107M 838G 12.0\nX101-64 48.3 66.4 52.3 41.7 64.0 45.1 140M 972G 10.4\nSwin-B 51.9 70.9 56.5 45.0 68.4 48.7 145M 982G 11.6"
        },
        {
            "bounding_box": [
                {
                    "x": 471,
                    "y": 1206
                },
                {
                    "x": 935,
                    "y": 1206
                },
                {
                    "x": 935,
                    "y": 1243
                },
                {
                    "x": 471,
                    "y": 1243
                }
            ],
            "category": "caption",
            "html": "<br><caption id='93' style='font-size:20px'>(c) System-level Comparison</caption>",
            "id": 93,
            "page": 7,
            "text": "(c) System-level Comparison"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 1245
                },
                {
                    "x": 1198,
                    "y": 1245
                },
                {
                    "x": 1198,
                    "y": 1950
                },
                {
                    "x": 207,
                    "y": 1950
                }
            ],
            "category": "table",
            "html": "<br><table id='94' style='font-size:14px'><tr><td>Method</td><td colspan=\"2\">mini-val APbox APmask</td><td colspan=\"2\">test-dev Apbox APmask</td><td colspan=\"2\">#param. FLOPs</td></tr><tr><td>RepPointsV2* [12]</td><td>-</td><td>-</td><td>52.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GCNet* [7]</td><td>51.8</td><td>44.7</td><td>52.3</td><td>45.4</td><td>-</td><td>1041G</td></tr><tr><td>RelationNet++* [13]</td><td>-</td><td>-</td><td>52.7</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SpineNet-190 [21]</td><td>52.6</td><td>-</td><td>52.8</td><td>-</td><td>164M</td><td>1885G</td></tr><tr><td>ResNeSt-200* [78]</td><td>52.5</td><td>-</td><td>53.3</td><td>47.1</td><td>-</td><td>-</td></tr><tr><td>EfficientDet-D7 [59]</td><td>54.4</td><td>-</td><td>55.1</td><td>-</td><td>77M</td><td>410G</td></tr><tr><td>DetectoRS* [46]</td><td>-</td><td>-</td><td>55.7</td><td>48.5</td><td>-</td><td>-</td></tr><tr><td>YOLOv4 P7* [4]</td><td>-</td><td>-</td><td>55.8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Copy-paste [26]</td><td>55.9</td><td>47.2</td><td>56.0</td><td>47.4</td><td>185M</td><td>1440G</td></tr><tr><td>X101-64 (HTC++)</td><td>52.3</td><td>46.0</td><td>-</td><td>-</td><td>155M</td><td>1033G</td></tr><tr><td>Swin-B (HTC++)</td><td>56.4</td><td>49.1</td><td>-</td><td>-</td><td>160M</td><td>1043G</td></tr><tr><td>Swin-L (HTC++)</td><td>57.1</td><td>49.5</td><td>57.7</td><td>50.2</td><td>284M</td><td>1470G</td></tr><tr><td>Swin-L (HTC++)*</td><td>58.0</td><td>50.4</td><td>58.7</td><td>51.1</td><td>284M</td><td>-</td></tr></table>",
            "id": 94,
            "page": 7,
            "text": "Method mini-val APbox APmask test-dev Apbox APmask #param. FLOPs\n RepPointsV2* [12] - - 52.1 - - -\n GCNet* [7] 51.8 44.7 52.3 45.4 - 1041G\n RelationNet++* [13] - - 52.7 - - -\n SpineNet-190 [21] 52.6 - 52.8 - 164M 1885G\n ResNeSt-200* [78] 52.5 - 53.3 47.1 - -\n EfficientDet-D7 [59] 54.4 - 55.1 - 77M 410G\n DetectoRS* [46] - - 55.7 48.5 - -\n YOLOv4 P7* [4] - - 55.8 - - -\n Copy-paste [26] 55.9 47.2 56.0 47.4 185M 1440G\n X101-64 (HTC++) 52.3 46.0 - - 155M 1033G\n Swin-B (HTC++) 56.4 49.1 - - 160M 1043G\n Swin-L (HTC++) 57.1 49.5 57.7 50.2 284M 1470G\n Swin-L (HTC++)* 58.0 50.4 58.7 51.1 284M"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1945
                },
                {
                    "x": 1198,
                    "y": 1945
                },
                {
                    "x": 1198,
                    "y": 2076
                },
                {
                    "x": 204,
                    "y": 2076
                }
            ],
            "category": "caption",
            "html": "<br><caption id='95' style='font-size:16px'>Table 2. Results on COCO object detection and instance segmen-<br>tation. T denotes that additional decovolution layers are used to<br>produce hierarchical feature maps. * indicates multi-scale testing.</caption>",
            "id": 95,
            "page": 7,
            "text": "Table 2. Results on COCO object detection and instance segmen-\ntation. T denotes that additional decovolution layers are used to\nproduce hierarchical feature maps. * indicates multi-scale testing."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2166
                },
                {
                    "x": 1199,
                    "y": 2166
                },
                {
                    "x": 1199,
                    "y": 2611
                },
                {
                    "x": 201,
                    "y": 2611
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:20px'>vNets, i.e. ResNe(X)t, and previous Transformer networks,<br>e.g. DeiT. The comparisons are conducted by changing only<br>the backbones with other settings unchanged. Note that<br>while Swin Transformer and ResNe(X)t are directly appli-<br>cable to all the above frameworks because of their hierar-<br>chical feature maps, DeiT only produces a single resolu-<br>tion of feature maps and cannot be directly applied. For fair<br>comparison, we follow [81] to construct hierarchical feature<br>maps for DeiT using deconvolution layers.</p>",
            "id": 96,
            "page": 7,
            "text": "vNets, i.e. ResNe(X)t, and previous Transformer networks,\ne.g. DeiT. The comparisons are conducted by changing only\nthe backbones with other settings unchanged. Note that\nwhile Swin Transformer and ResNe(X)t are directly appli-\ncable to all the above frameworks because of their hierar-\nchical feature maps, DeiT only produces a single resolu-\ntion of feature maps and cannot be directly applied. For fair\ncomparison, we follow [81] to construct hierarchical feature\nmaps for DeiT using deconvolution layers."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2677
                },
                {
                    "x": 1198,
                    "y": 2677
                },
                {
                    "x": 1198,
                    "y": 2922
                },
                {
                    "x": 202,
                    "y": 2922
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>Comparison to ResNe(X)t Table 2(a) lists the results of<br>Swin-T and ResNet-50 on the four object detection frame-<br>works. Our Swin-T architecture brings consistent +3.4~4.2<br>box AP gains over ResNet-50, with slightly larger model<br>size, FLOPs and latency.</p>",
            "id": 97,
            "page": 7,
            "text": "Comparison to ResNe(X)t Table 2(a) lists the results of\nSwin-T and ResNet-50 on the four object detection frame-\nworks. Our Swin-T architecture brings consistent +3.4~4.2\nbox AP gains over ResNet-50, with slightly larger model\nsize, FLOPs and latency."
        },
        {
            "bounding_box": [
                {
                    "x": 255,
                    "y": 2928
                },
                {
                    "x": 1197,
                    "y": 2928
                },
                {
                    "x": 1197,
                    "y": 2972
                },
                {
                    "x": 255,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='98' style='font-size:20px'>Table 2(b) compares Swin Transformer and ResNe(X)t</p>",
            "id": 98,
            "page": 7,
            "text": "Table 2(b) compares Swin Transformer and ResNe(X)t"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 294
                },
                {
                    "x": 2289,
                    "y": 294
                },
                {
                    "x": 2289,
                    "y": 1099
                },
                {
                    "x": 1278,
                    "y": 1099
                }
            ],
            "category": "table",
            "html": "<br><table id='99' style='font-size:14px'><tr><td colspan=\"2\">ADE20K</td><td colspan=\"2\">val test</td><td rowspan=\"2\">#param.</td><td rowspan=\"2\">FLOPs</td><td rowspan=\"2\">FPS</td></tr><tr><td>Method</td><td>Backbone</td><td>mloU score</td><td></td></tr><tr><td>DANet [23]</td><td>ResNet-101</td><td>45.2</td><td>-</td><td>69M</td><td>1119G</td><td>15.2</td></tr><tr><td>DLab.v3+ [11]</td><td>ResNet-101</td><td>44.1</td><td>-</td><td>63M</td><td>1021G</td><td>16.0</td></tr><tr><td>ACNet [24]</td><td>ResNet-101</td><td>45.9</td><td>38.5</td><td>-</td><td></td><td></td></tr><tr><td>DNL [71]</td><td>ResNet-101</td><td>46.0</td><td>56.2</td><td>69M</td><td>1249G</td><td>14.8</td></tr><tr><td>OCRNet [73]</td><td>ResNet-101</td><td>45.3</td><td>56.0</td><td>56M</td><td>923G</td><td>19.3</td></tr><tr><td>UperNet [69]</td><td>ResNet-101</td><td>44.9</td><td>-</td><td>86M</td><td>1029G</td><td>20.1</td></tr><tr><td>OCRNet [73]</td><td>HRNet-w48</td><td>45.7</td><td>-</td><td>71M</td><td>664G</td><td>12.5</td></tr><tr><td>DLab.v3+ [11]</td><td>ResNeSt-101</td><td>46.9</td><td>55.1</td><td>66M</td><td>1051G</td><td>11.9</td></tr><tr><td>DLab.v3+ [11]</td><td>ResNeSt-200</td><td>48.4</td><td>-</td><td>88M</td><td>1381G</td><td>8.1</td></tr><tr><td>SETR [81]</td><td>T-Large‡</td><td>50.3</td><td>61.7</td><td>308M</td><td>-</td><td>-</td></tr><tr><td>UperNet</td><td>DeiT-S†</td><td>44.0</td><td>-</td><td>52M</td><td>1099G</td><td>16.2</td></tr><tr><td>UperNet</td><td>Swin-T</td><td>46.1</td><td>-</td><td>60M</td><td>945G</td><td>18.5</td></tr><tr><td>UperNet</td><td>Swin-S</td><td>49.3</td><td>-</td><td>81M</td><td>1038G</td><td>15.2</td></tr><tr><td>UperNet</td><td>Swin-B‡</td><td>51.6</td><td>-</td><td>121M</td><td>1841G</td><td>8.7</td></tr><tr><td>UperNet</td><td>Swin-L‡</td><td>53.5</td><td>62.8</td><td>234M</td><td>3230G</td><td>6.2</td></tr></table>",
            "id": 99,
            "page": 7,
            "text": "ADE20K val test #param. FLOPs FPS\n Method Backbone mloU score \n DANet [23] ResNet-101 45.2 - 69M 1119G 15.2\n DLab.v3+ [11] ResNet-101 44.1 - 63M 1021G 16.0\n ACNet [24] ResNet-101 45.9 38.5 -  \n DNL [71] ResNet-101 46.0 56.2 69M 1249G 14.8\n OCRNet [73] ResNet-101 45.3 56.0 56M 923G 19.3\n UperNet [69] ResNet-101 44.9 - 86M 1029G 20.1\n OCRNet [73] HRNet-w48 45.7 - 71M 664G 12.5\n DLab.v3+ [11] ResNeSt-101 46.9 55.1 66M 1051G 11.9\n DLab.v3+ [11] ResNeSt-200 48.4 - 88M 1381G 8.1\n SETR [81] T-Large‡ 50.3 61.7 308M - -\n UperNet DeiT-S† 44.0 - 52M 1099G 16.2\n UperNet Swin-T 46.1 - 60M 945G 18.5\n UperNet Swin-S 49.3 - 81M 1038G 15.2\n UperNet Swin-B‡ 51.6 - 121M 1841G 8.7\n UperNet Swin-L‡ 53.5 62.8 234M 3230G"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1099
                },
                {
                    "x": 2277,
                    "y": 1099
                },
                {
                    "x": 2277,
                    "y": 1274
                },
                {
                    "x": 1280,
                    "y": 1274
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:16px'>Table 3. Results of semantic segmentation on the ADE20K val<br>t indicates additional deconvolution layers are used<br>and test set.<br>to produce hierarchical feature maps. 1 indicates that the model is<br>pre-trained on ImageNet-22K.</p>",
            "id": 100,
            "page": 7,
            "text": "Table 3. Results of semantic segmentation on the ADE20K val\nt indicates additional deconvolution layers are used\nand test set.\nto produce hierarchical feature maps. 1 indicates that the model is\npre-trained on ImageNet-22K."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1356
                },
                {
                    "x": 2276,
                    "y": 1356
                },
                {
                    "x": 2276,
                    "y": 2004
                },
                {
                    "x": 1279,
                    "y": 2004
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:20px'>under different model capacity using Cascade Mask R-<br>CNN. Swin Transformer achieves a high detection accuracy<br>of 51.9 box AP and 45.0 mask AP, which are significant<br>gains of +3.6 box AP and +3.3 mask AP over ResNeXt101-<br>64x4d, which has similar model size, FLOPs and latency.<br>On a higher baseline of 52.3 box AP and 46.0 mask AP us-<br>ing an improved HTC framework, the gains by Swin Trans-<br>former are also high, at +4.1 box AP and +3.1 mask AP (see<br>Table 2(c)). Regarding inference speed, while ResNe(X)t is<br>built by highly optimized Cudnn functions, our architecture<br>is implemented with built-in PyTorch functions that are not<br>all well-optimized. A thorough kernel optimization is be-<br>yond the scope of this paper.</p>",
            "id": 101,
            "page": 7,
            "text": "under different model capacity using Cascade Mask R-\nCNN. Swin Transformer achieves a high detection accuracy\nof 51.9 box AP and 45.0 mask AP, which are significant\ngains of +3.6 box AP and +3.3 mask AP over ResNeXt101-\n64x4d, which has similar model size, FLOPs and latency.\nOn a higher baseline of 52.3 box AP and 46.0 mask AP us-\ning an improved HTC framework, the gains by Swin Trans-\nformer are also high, at +4.1 box AP and +3.1 mask AP (see\nTable 2(c)). Regarding inference speed, while ResNe(X)t is\nbuilt by highly optimized Cudnn functions, our architecture\nis implemented with built-in PyTorch functions that are not\nall well-optimized. A thorough kernel optimization is be-\nyond the scope of this paper."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2064
                },
                {
                    "x": 2276,
                    "y": 2064
                },
                {
                    "x": 2276,
                    "y": 2412
                },
                {
                    "x": 1281,
                    "y": 2412
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:20px'>Comparison to DeiT The performance of DeiT-S us-<br>ing the Cascade Mask R-CNN framework is shown in Ta-<br>ble 2(b). The results of Swin-T are +2.5 box AP and +2.3<br>mask AP higher than DeiT-S with similar model size (86M<br>VS. 80M) and significantly higher inference speed (15.3 FPS<br>VS. 10.4 FPS). The lower inference speed of DeiT is mainly<br>due to its quadratic complexity to input image size.</p>",
            "id": 102,
            "page": 7,
            "text": "Comparison to DeiT The performance of DeiT-S us-\ning the Cascade Mask R-CNN framework is shown in Ta-\nble 2(b). The results of Swin-T are +2.5 box AP and +2.3\nmask AP higher than DeiT-S with similar model size (86M\nVS. 80M) and significantly higher inference speed (15.3 FPS\nVS. 10.4 FPS). The lower inference speed of DeiT is mainly\ndue to its quadratic complexity to input image size."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2471
                },
                {
                    "x": 2276,
                    "y": 2471
                },
                {
                    "x": 2276,
                    "y": 2770
                },
                {
                    "x": 1280,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:20px'>Comparison to previous state-of-the-art Table 2(c)<br>compares our best results with those of previous state-of-<br>the-art models. Our best model achieves 58.7 box AP and<br>51.1 mask AP on COCO test-dev, surpassing the previous<br>best results by +2.7 box AP (Copy-paste [26] without exter-<br>nal data) and +2.6 mask AP (DetectoRS [46]).</p>",
            "id": 103,
            "page": 7,
            "text": "Comparison to previous state-of-the-art Table 2(c)\ncompares our best results with those of previous state-of-\nthe-art models. Our best model achieves 58.7 box AP and\n51.1 mask AP on COCO test-dev, surpassing the previous\nbest results by +2.7 box AP (Copy-paste [26] without exter-\nnal data) and +2.6 mask AP (DetectoRS [46])."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2801
                },
                {
                    "x": 2074,
                    "y": 2801
                },
                {
                    "x": 2074,
                    "y": 2849
                },
                {
                    "x": 1281,
                    "y": 2849
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:22px'>4.3. Semantic Segmentation on ADE20K</p>",
            "id": 104,
            "page": 7,
            "text": "4.3. Semantic Segmentation on ADE20K"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2878
                },
                {
                    "x": 2275,
                    "y": 2878
                },
                {
                    "x": 2275,
                    "y": 2976
                },
                {
                    "x": 1282,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:20px'>Settings ADE20K [83] is a widely-used semantic seg-<br>mentation dataset, covering a broad range of 150 semantic</p>",
            "id": 105,
            "page": 7,
            "text": "Settings ADE20K [83] is a widely-used semantic seg-\nmentation dataset, covering a broad range of 150 semantic"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1250,
                    "y": 3057
                },
                {
                    "x": 1250,
                    "y": 3089
                },
                {
                    "x": 1226,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='106' style='font-size:18px'>7</footer>",
            "id": 106,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 292
                },
                {
                    "x": 1168,
                    "y": 292
                },
                {
                    "x": 1168,
                    "y": 726
                },
                {
                    "x": 227,
                    "y": 726
                }
            ],
            "category": "table",
            "html": "<table id='107' style='font-size:14px'><tr><td rowspan=\"2\"></td><td colspan=\"2\">ImageNet</td><td colspan=\"2\">COCO</td><td rowspan=\"2\">ADE20k mIoU</td></tr><tr><td>top-1</td><td>top-5</td><td>APbox</td><td>APmask</td></tr><tr><td>w/o shifting</td><td>80.2</td><td>95.1</td><td>47.7</td><td>41.5</td><td>43.3</td></tr><tr><td>shifted windows</td><td>81.3</td><td>95.6</td><td>50.5</td><td>43.7</td><td>46.1</td></tr><tr><td>no pos.</td><td>80.1</td><td>94.9</td><td>49.2</td><td>42.6</td><td>43.8</td></tr><tr><td>abs. pos.</td><td>80.5</td><td>95.2</td><td>49.0</td><td>42.4</td><td>43.2</td></tr><tr><td>abs.+rel. pos.</td><td>81.3</td><td>95.6</td><td>50.2</td><td>43.4</td><td>44.0</td></tr><tr><td>rel. pos. w/o app.</td><td>79.3</td><td>94.7</td><td>48.2</td><td>41.9</td><td>44.1</td></tr><tr><td>rel. pos.</td><td>81.3</td><td>95.6</td><td>50.5</td><td>43.7</td><td>46.1</td></tr></table>",
            "id": 107,
            "page": 8,
            "text": "ImageNet COCO ADE20k mIoU\n top-1 top-5 APbox APmask\n w/o shifting 80.2 95.1 47.7 41.5 43.3\n shifted windows 81.3 95.6 50.5 43.7 46.1\n no pos. 80.1 94.9 49.2 42.6 43.8\n abs. pos. 80.5 95.2 49.0 42.4 43.2\n abs.+rel. pos. 81.3 95.6 50.2 43.4 44.0\n rel. pos. w/o app. 79.3 94.7 48.2 41.9 44.1\n rel. pos. 81.3 95.6 50.5 43.7"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 726
                },
                {
                    "x": 1199,
                    "y": 726
                },
                {
                    "x": 1199,
                    "y": 1041
                },
                {
                    "x": 202,
                    "y": 1041
                }
            ],
            "category": "caption",
            "html": "<br><caption id='108' style='font-size:16px'>Table 4. Ablation study on the shifted windows approach and dif-<br>ferent position embedding methods on three benchmarks, using<br>the Swin-T architecture. w/o shifting: all self-attention modules<br>adopt regular window partitioning, without shifting; abs. pos.: ab-<br>solute position embedding term of ViT; rel. pos.: the default set-<br>tings with an additional relative position bias term (see Eq. (4));<br>app.: the first scaled dot-product term in Eq. (4).</caption>",
            "id": 108,
            "page": 8,
            "text": "Table 4. Ablation study on the shifted windows approach and dif-\nferent position embedding methods on three benchmarks, using\nthe Swin-T architecture. w/o shifting: all self-attention modules\nadopt regular window partitioning, without shifting; abs. pos.: ab-\nsolute position embedding term of ViT; rel. pos.: the default set-\ntings with an additional relative position bias term (see Eq. (4));\napp.: the first scaled dot-product term in Eq. (4)."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1130
                },
                {
                    "x": 1199,
                    "y": 1130
                },
                {
                    "x": 1199,
                    "y": 1324
                },
                {
                    "x": 202,
                    "y": 1324
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>categories. Ithas 25K images in total, with 20K for training,<br>2K for validation, and another 3K for testing. We utilize<br>UperNet [69] in mmseg [16] as our base framework for its<br>high efficiency. More details are presented in the Appendix.</p>",
            "id": 109,
            "page": 8,
            "text": "categories. Ithas 25K images in total, with 20K for training,\n2K for validation, and another 3K for testing. We utilize\nUperNet [69] in mmseg [16] as our base framework for its\nhigh efficiency. More details are presented in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1391
                },
                {
                    "x": 1199,
                    "y": 1391
                },
                {
                    "x": 1199,
                    "y": 1837
                },
                {
                    "x": 201,
                    "y": 1837
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>Results Table 3 lists the mloU, model size (#param),<br>FLOPs and FPS for different method/backbone pairs. From<br>these results, it can be seen that Swin-S is +5.3 mIoU higher<br>(49.3 VS. 44.0) than DeiT-S with similar computation cost.<br>It is also +4.4 mIoU higher than ResNet-101, and +2.4<br>mIoU higher than ResNeSt-101 [78]. Our Swin-L model<br>with ImageNet-22K pre-training achieves 53.5 mIoU on the<br>val set, surpassing the previous best model by +3.2 mIoU<br>(50.3 mIoU by SETR [81] which has a larger model size).</p>",
            "id": 110,
            "page": 8,
            "text": "Results Table 3 lists the mloU, model size (#param),\nFLOPs and FPS for different method/backbone pairs. From\nthese results, it can be seen that Swin-S is +5.3 mIoU higher\n(49.3 VS. 44.0) than DeiT-S with similar computation cost.\nIt is also +4.4 mIoU higher than ResNet-101, and +2.4\nmIoU higher than ResNeSt-101 [78]. Our Swin-L model\nwith ImageNet-22K pre-training achieves 53.5 mIoU on the\nval set, surpassing the previous best model by +3.2 mIoU\n(50.3 mIoU by SETR [81] which has a larger model size)."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1873
                },
                {
                    "x": 585,
                    "y": 1873
                },
                {
                    "x": 585,
                    "y": 1921
                },
                {
                    "x": 204,
                    "y": 1921
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:20px'>4.4. Ablation Study</p>",
            "id": 111,
            "page": 8,
            "text": "4.4. Ablation Study"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1952
                },
                {
                    "x": 1199,
                    "y": 1952
                },
                {
                    "x": 1199,
                    "y": 2148
                },
                {
                    "x": 202,
                    "y": 2148
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:20px'>In this section, we ablate important design elements in<br>the proposed Swin Transformer, using ImageNet-1K image<br>classification, Cascade Mask R-CNN on COCO object de-<br>tection, and UperNet on ADE20K semantic segmentation.</p>",
            "id": 112,
            "page": 8,
            "text": "In this section, we ablate important design elements in\nthe proposed Swin Transformer, using ImageNet-1K image\nclassification, Cascade Mask R-CNN on COCO object de-\ntection, and UperNet on ADE20K semantic segmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2214
                },
                {
                    "x": 1199,
                    "y": 2214
                },
                {
                    "x": 1199,
                    "y": 2709
                },
                {
                    "x": 201,
                    "y": 2709
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>Shifted windows Ablations of the shifted window ap-<br>proach on the three tasks are reported in Table 4. Swin-T<br>with the shifted window partitioning outperforms the coun-<br>terpart built on a single window partitioning at each stage by<br>+1.1% top-1 accuracy on ImageNet-1K, +2.8 box AP/+2.2<br>mask AP on COCO, and +2.8 mIoU on ADE20K. The re-<br>sults indicate the effectiveness of using shifted windows to<br>build connections among windows in the preceding layers.<br>The latency overhead by shifted window is also small, as<br>shown in Table 5.</p>",
            "id": 113,
            "page": 8,
            "text": "Shifted windows Ablations of the shifted window ap-\nproach on the three tasks are reported in Table 4. Swin-T\nwith the shifted window partitioning outperforms the coun-\nterpart built on a single window partitioning at each stage by\n+1.1% top-1 accuracy on ImageNet-1K, +2.8 box AP/+2.2\nmask AP on COCO, and +2.8 mIoU on ADE20K. The re-\nsults indicate the effectiveness of using shifted windows to\nbuild connections among windows in the preceding layers.\nThe latency overhead by shifted window is also small, as\nshown in Table 5."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2777
                },
                {
                    "x": 1198,
                    "y": 2777
                },
                {
                    "x": 1198,
                    "y": 2975
                },
                {
                    "x": 202,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:18px'>Relative position bias Table 4 shows comparisons of dif-<br>ferent position embedding approaches. Swin-T with rela-<br>tive position bias yields +1.2%/+0.8% top-1 accuracy on<br>ImageNet-1K, +1.3/+1.5 box AP and +1.1/+1.3 mask AP</p>",
            "id": 114,
            "page": 8,
            "text": "Relative position bias Table 4 shows comparisons of dif-\nferent position embedding approaches. Swin-T with rela-\ntive position bias yields +1.2%/+0.8% top-1 accuracy on\nImageNet-1K, +1.3/+1.5 box AP and +1.1/+1.3 mask AP"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 294
                },
                {
                    "x": 2263,
                    "y": 294
                },
                {
                    "x": 2263,
                    "y": 681
                },
                {
                    "x": 1294,
                    "y": 681
                }
            ],
            "category": "table",
            "html": "<br><table id='115' style='font-size:14px'><tr><td rowspan=\"2\">method</td><td colspan=\"4\">MSA in a stage (ms)</td><td colspan=\"2\">Arch. (FPS)</td></tr><tr><td>S1</td><td>S2</td><td>S3</td><td>S4</td><td>T S</td><td>B</td></tr><tr><td>sliding window (naive)</td><td>122.5</td><td>38.3</td><td>12.1</td><td>7.6</td><td>183 109</td><td>77</td></tr><tr><td>sliding window (kernel)</td><td>7.6</td><td>4.7</td><td>2.7</td><td>1.8</td><td>488 283</td><td>187</td></tr><tr><td>Performer [14]</td><td>4.8</td><td>2.8</td><td>1.8</td><td>1.5</td><td>638 370</td><td>241</td></tr><tr><td>window (w/o shifting)</td><td>2.8</td><td>1.7</td><td>1.2</td><td>0.9</td><td>770 444</td><td>280</td></tr><tr><td>shifted window (padding)</td><td>3.3</td><td>2.3</td><td>1.9</td><td>2.2</td><td>670 371</td><td>236</td></tr><tr><td>shifted window (cyclic)</td><td>3.0</td><td>1.9</td><td>1.3</td><td>1.0</td><td>755 437</td><td>278</td></tr></table>",
            "id": 115,
            "page": 8,
            "text": "method MSA in a stage (ms) Arch. (FPS)\n S1 S2 S3 S4 T S B\n sliding window (naive) 122.5 38.3 12.1 7.6 183 109 77\n sliding window (kernel) 7.6 4.7 2.7 1.8 488 283 187\n Performer [14] 4.8 2.8 1.8 1.5 638 370 241\n window (w/o shifting) 2.8 1.7 1.2 0.9 770 444 280\n shifted window (padding) 3.3 2.3 1.9 2.2 670 371 236\n shifted window (cyclic) 3.0 1.9 1.3 1.0 755 437"
        },
        {
            "bounding_box": [
                {
                    "x": 1285,
                    "y": 682
                },
                {
                    "x": 2263,
                    "y": 682
                },
                {
                    "x": 2263,
                    "y": 764
                },
                {
                    "x": 1285,
                    "y": 764
                }
            ],
            "category": "caption",
            "html": "<br><caption id='116' style='font-size:14px'>Table 5. Real speed of different self-attention computation meth-<br>ods and implementations on a V100 GPU.</caption>",
            "id": 116,
            "page": 8,
            "text": "Table 5. Real speed of different self-attention computation meth-\nods and implementations on a V100 GPU."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 849
                },
                {
                    "x": 2277,
                    "y": 849
                },
                {
                    "x": 2277,
                    "y": 1241
                },
                {
                    "x": 1279,
                    "y": 1241
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>on COCO, and +2.3/+2.9 mloU on ADE20K in relation to<br>those without position encoding and with absolute position<br>embedding, respectively, indicating the effectiveness of the<br>relative position bias. Also note that while the inclusion of<br>absolute position embedding improves image classification<br>accuracy (+0.4%), it harms object detection and semantic<br>segmentation (-0.2 box/mask AP on COCO and -0.6 mIoU<br>on ADE20K).</p>",
            "id": 117,
            "page": 8,
            "text": "on COCO, and +2.3/+2.9 mloU on ADE20K in relation to\nthose without position encoding and with absolute position\nembedding, respectively, indicating the effectiveness of the\nrelative position bias. Also note that while the inclusion of\nabsolute position embedding improves image classification\naccuracy (+0.4%), it harms object detection and semantic\nsegmentation (-0.2 box/mask AP on COCO and -0.6 mIoU\non ADE20K)."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1248
                },
                {
                    "x": 2278,
                    "y": 1248
                },
                {
                    "x": 2278,
                    "y": 1593
                },
                {
                    "x": 1280,
                    "y": 1593
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:18px'>While the recent ViT/DeiT models abandon translation<br>invariance in image classification even though it has long<br>been shown to be crucial for visual modeling, we find that<br>inductive bias that encourages certain translation invariance<br>is still preferable for general-purpose visual modeling, par-<br>ticularly for the dense prediction tasks of object detection<br>and semantic segmentation.</p>",
            "id": 118,
            "page": 8,
            "text": "While the recent ViT/DeiT models abandon translation\ninvariance in image classification even though it has long\nbeen shown to be crucial for visual modeling, we find that\ninductive bias that encourages certain translation invariance\nis still preferable for general-purpose visual modeling, par-\nticularly for the dense prediction tasks of object detection\nand semantic segmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1655
                },
                {
                    "x": 2276,
                    "y": 1655
                },
                {
                    "x": 2276,
                    "y": 1950
                },
                {
                    "x": 1281,
                    "y": 1950
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>Different self-attention methods The real speed of dif-<br>ferent self-attention computation methods and implementa-<br>tions are compared in Table 5. Our cyclic implementation<br>is more hardware efficient than naive padding, particularly<br>for deeper stages. Overall, it brings a 13%, 18% and 18%<br>speed-up on Swin-T, Swin-S and Swin-B, respectively.</p>",
            "id": 119,
            "page": 8,
            "text": "Different self-attention methods The real speed of dif-\nferent self-attention computation methods and implementa-\ntions are compared in Table 5. Our cyclic implementation\nis more hardware efficient than naive padding, particularly\nfor deeper stages. Overall, it brings a 13%, 18% and 18%\nspeed-up on Swin-T, Swin-S and Swin-B, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1955
                },
                {
                    "x": 2277,
                    "y": 1955
                },
                {
                    "x": 2277,
                    "y": 2449
                },
                {
                    "x": 1279,
                    "y": 2449
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:18px'>The self-attention modules built on the proposed<br>shifted window approach are 40.8x/2.5x, 20.2x/2.5x,<br>9.3 x/2.1 x, and 7.6x/1.8x more efficient than those of slid-<br>ing windows in naive/kernel implementations on four net-<br>work stages, respectively. Overall, the Swin Transformer<br>architectures built on shifted windows are 4.1/1.5, 4.0/1.5,<br>3.6/1.5 times faster than variants built on sliding windows<br>for Swin-T, Swin-S, and Swin-B, respectively. Table 6 com-<br>pares their accuracy on the three tasks, showing that they are<br>similarly accurate in visual modeling.</p>",
            "id": 120,
            "page": 8,
            "text": "The self-attention modules built on the proposed\nshifted window approach are 40.8x/2.5x, 20.2x/2.5x,\n9.3 x/2.1 x, and 7.6x/1.8x more efficient than those of slid-\ning windows in naive/kernel implementations on four net-\nwork stages, respectively. Overall, the Swin Transformer\narchitectures built on shifted windows are 4.1/1.5, 4.0/1.5,\n3.6/1.5 times faster than variants built on sliding windows\nfor Swin-T, Swin-S, and Swin-B, respectively. Table 6 com-\npares their accuracy on the three tasks, showing that they are\nsimilarly accurate in visual modeling."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2453
                },
                {
                    "x": 2276,
                    "y": 2453
                },
                {
                    "x": 2276,
                    "y": 2749
                },
                {
                    "x": 1281,
                    "y": 2749
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:20px'>Compared to Performer [14], which is one of the fastest<br>Transformer architectures (see [60]), the proposed shifted<br>window based self-attention computation and the overall<br>Swin Transformer architectures are slightly faster (see Ta-<br>ble 5), while achieving +2.3% top-1 accuracy compared to<br>Performer on ImageNet-1K using Swin-T (see Table 6).</p>",
            "id": 121,
            "page": 8,
            "text": "Compared to Performer [14], which is one of the fastest\nTransformer architectures (see [60]), the proposed shifted\nwindow based self-attention computation and the overall\nSwin Transformer architectures are slightly faster (see Ta-\nble 5), while achieving +2.3% top-1 accuracy compared to\nPerformer on ImageNet-1K using Swin-T (see Table 6)."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2793
                },
                {
                    "x": 1578,
                    "y": 2793
                },
                {
                    "x": 1578,
                    "y": 2844
                },
                {
                    "x": 1281,
                    "y": 2844
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:22px'>5. Conclusion</p>",
            "id": 122,
            "page": 8,
            "text": "5. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2879
                },
                {
                    "x": 2274,
                    "y": 2879
                },
                {
                    "x": 2274,
                    "y": 2976
                },
                {
                    "x": 1283,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:16px'>This paper presents Swin Transformer, a new vision<br>Transformer which produces a hierarchical feature repre-</p>",
            "id": 123,
            "page": 8,
            "text": "This paper presents Swin Transformer, a new vision\nTransformer which produces a hierarchical feature repre-"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3090
                },
                {
                    "x": 1226,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='124' style='font-size:16px'>8</footer>",
            "id": 124,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 211,
                    "y": 292
                },
                {
                    "x": 1185,
                    "y": 292
                },
                {
                    "x": 1185,
                    "y": 542
                },
                {
                    "x": 211,
                    "y": 542
                }
            ],
            "category": "table",
            "html": "<table id='125' style='font-size:14px'><tr><td></td><td>Backbone</td><td>ImageNet top-1 top-5</td><td>COCO APbox APmask</td><td>ADE20k mIoU</td></tr><tr><td>sliding window</td><td>Swin-T</td><td>81.4 95.6</td><td>50.2 43.5</td><td>45.8</td></tr><tr><td>Performer [14]</td><td>Swin-T</td><td>79.0 94.2</td><td>- -</td><td>-</td></tr><tr><td>shifted window</td><td>Swin-T</td><td>81.3 95.6</td><td>50.5 43.7</td><td>46.1</td></tr></table>",
            "id": 125,
            "page": 9,
            "text": "Backbone ImageNet top-1 top-5 COCO APbox APmask ADE20k mIoU\n sliding window Swin-T 81.4 95.6 50.2 43.5 45.8\n Performer [14] Swin-T 79.0 94.2 - - -\n shifted window Swin-T 81.3 95.6 50.5 43.7"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 545
                },
                {
                    "x": 1192,
                    "y": 545
                },
                {
                    "x": 1192,
                    "y": 624
                },
                {
                    "x": 207,
                    "y": 624
                }
            ],
            "category": "caption",
            "html": "<br><caption id='126' style='font-size:14px'>Table 6. Accuracy of Swin Transformer using different methods<br>for self-attention computation on three benchmarks.</caption>",
            "id": 126,
            "page": 9,
            "text": "Table 6. Accuracy of Swin Transformer using different methods\nfor self-attention computation on three benchmarks."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 703
                },
                {
                    "x": 1198,
                    "y": 703
                },
                {
                    "x": 1198,
                    "y": 1044
                },
                {
                    "x": 202,
                    "y": 1044
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>sentation and has linear computational complexity with re-<br>spect to input image size. Swin Transformer achieves the<br>state-of-the-art performance on COCO object detection and<br>ADE20K semantic segmentation, significantly surpassing<br>previous best methods. We hope that Swin Transformer's<br>strong performance on various vision problems will encour-<br>age unified modeling of vision and language signals.</p>",
            "id": 127,
            "page": 9,
            "text": "sentation and has linear computational complexity with re-\nspect to input image size. Swin Transformer achieves the\nstate-of-the-art performance on COCO object detection and\nADE20K semantic segmentation, significantly surpassing\nprevious best methods. We hope that Swin Transformer's\nstrong performance on various vision problems will encour-\nage unified modeling of vision and language signals."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1053
                },
                {
                    "x": 1197,
                    "y": 1053
                },
                {
                    "x": 1197,
                    "y": 1250
                },
                {
                    "x": 202,
                    "y": 1250
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='128' style='font-size:14px'>As a key element of Swin Transformer, the shifted win-<br>dow based self-attention is shown to be effective and effi-<br>cient on vision problems, and we look forward to investi-<br>gating its use in natural language processing as well.</p>",
            "id": 128,
            "page": 9,
            "text": "As a key element of Swin Transformer, the shifted win-\ndow based self-attention is shown to be effective and effi-\ncient on vision problems, and we look forward to investi-\ngating its use in natural language processing as well."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1307
                },
                {
                    "x": 602,
                    "y": 1307
                },
                {
                    "x": 602,
                    "y": 1356
                },
                {
                    "x": 205,
                    "y": 1356
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:22px'>Acknowledgement</p>",
            "id": 129,
            "page": 9,
            "text": "Acknowledgement"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1393
                },
                {
                    "x": 1199,
                    "y": 1393
                },
                {
                    "x": 1199,
                    "y": 1540
                },
                {
                    "x": 203,
                    "y": 1540
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:16px'>We thank many colleagues at Microsoft for their help,<br>in particular, Li Dong and Furu Wei for useful discussions;<br>Bin Xiao, Lu Yuan and Lei Zhang for help on datasets.</p>",
            "id": 130,
            "page": 9,
            "text": "We thank many colleagues at Microsoft for their help,\nin particular, Li Dong and Furu Wei for useful discussions;\nBin Xiao, Lu Yuan and Lei Zhang for help on datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1597
                },
                {
                    "x": 775,
                    "y": 1597
                },
                {
                    "x": 775,
                    "y": 1647
                },
                {
                    "x": 204,
                    "y": 1647
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:20px'>A1. Detailed Architectures</p>",
            "id": 131,
            "page": 9,
            "text": "A1. Detailed Architectures"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1683
                },
                {
                    "x": 1199,
                    "y": 1683
                },
                {
                    "x": 1199,
                    "y": 2080
                },
                {
                    "x": 201,
                    "y": 2080
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:14px'>The detailed architecture specifications are shown in Ta-<br>ble 7, where an input image size of 224x224 is assumed for<br>all architectures. \"Concat n x n\" indicates a concatenation<br>of n x n neighboring features in a patch. This operation<br>results in a downsampling of the feature map by a rate of n.<br>\"96-d\" denotes a linear layer with an output dimension of<br>96. \"win. SZ. 7 x 7\" indicates a multi-head self-attention<br>module with window size of 7 x 7.</p>",
            "id": 132,
            "page": 9,
            "text": "The detailed architecture specifications are shown in Ta-\nble 7, where an input image size of 224x224 is assumed for\nall architectures. \"Concat n x n\" indicates a concatenation\nof n x n neighboring features in a patch. This operation\nresults in a downsampling of the feature map by a rate of n.\n\"96-d\" denotes a linear layer with an output dimension of\n96. \"win. SZ. 7 x 7\" indicates a multi-head self-attention\nmodule with window size of 7 x 7."
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 2136
                },
                {
                    "x": 959,
                    "y": 2136
                },
                {
                    "x": 959,
                    "y": 2187
                },
                {
                    "x": 206,
                    "y": 2187
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:22px'>A2. Detailed Experimental Settings</p>",
            "id": 133,
            "page": 9,
            "text": "A2. Detailed Experimental Settings"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2221
                },
                {
                    "x": 1038,
                    "y": 2221
                },
                {
                    "x": 1038,
                    "y": 2269
                },
                {
                    "x": 205,
                    "y": 2269
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:20px'>A2.1. Image classification on ImageNet-1K</p>",
            "id": 134,
            "page": 9,
            "text": "A2.1. Image classification on ImageNet-1K"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2301
                },
                {
                    "x": 1198,
                    "y": 2301
                },
                {
                    "x": 1198,
                    "y": 2599
                },
                {
                    "x": 201,
                    "y": 2599
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>The image classification is performed by applying a<br>global average pooling layer on the output feature map of<br>the last stage, followed by a linear classifier. We find this<br>strategy to be as accurate as using an additional class to-<br>ken as in ViT [20] and DeiT [63]. In evaluation, the top-1<br>accuracy using a single crop is reported.</p>",
            "id": 135,
            "page": 9,
            "text": "The image classification is performed by applying a\nglobal average pooling layer on the output feature map of\nthe last stage, followed by a linear classifier. We find this\nstrategy to be as accurate as using an additional class to-\nken as in ViT [20] and DeiT [63]. In evaluation, the top-1\naccuracy using a single crop is reported."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2677
                },
                {
                    "x": 1198,
                    "y": 2677
                },
                {
                    "x": 1198,
                    "y": 2975
                },
                {
                    "x": 202,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Regular ImageNet-1K training The training settings<br>mostly follow [63]. For all model variants, we adopt a de-<br>fault input image resolution of 2242. For other resolutions<br>such as 3842, we fine-tune the models trained at 2242 reso-<br>lution, instead of training from scratch, to reduce GPU con-<br>sumption.</p>",
            "id": 136,
            "page": 9,
            "text": "Regular ImageNet-1K training The training settings\nmostly follow [63]. For all model variants, we adopt a de-\nfault input image resolution of 2242. For other resolutions\nsuch as 3842, we fine-tune the models trained at 2242 reso-\nlution, instead of training from scratch, to reduce GPU con-\nsumption."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 306
                },
                {
                    "x": 2278,
                    "y": 306
                },
                {
                    "x": 2278,
                    "y": 1101
                },
                {
                    "x": 1277,
                    "y": 1101
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:16px'>When training from scratch with a 2242 input, we em-<br>ploy an AdamW [37] optimizer for 300 epochs using a CO-<br>sine decay learning rate scheduler with 20 epochs of linear<br>warm-up. A batch size of 1024, an initial learning rate of<br>0.001, a weight decay of 0.05, and gradient clipping with<br>a max norm of 1 are used. We include most of the aug-<br>mentation and regularization strategies of [63] in training,<br>including RandAugment [17], Mixup [77], Cutmix [75],<br>random erasing [82] and stochastic depth [35], but not re-<br>peated augmentation [31] and Exponential Moving Average<br>(EMA) [45] which do not enhance performance. Note that<br>this is contrary to [63] where repeated augmentation is cru-<br>cial to stabilize the training of ViT. An increasing degree of<br>stochastic depth augmentation is employed for larger mod-<br>els, i.e. 0.2, 0.3, 0.5 for Swin-T, Swin-S, and Swin-B, re-<br>spectively.</p>",
            "id": 137,
            "page": 9,
            "text": "When training from scratch with a 2242 input, we em-\nploy an AdamW [37] optimizer for 300 epochs using a CO-\nsine decay learning rate scheduler with 20 epochs of linear\nwarm-up. A batch size of 1024, an initial learning rate of\n0.001, a weight decay of 0.05, and gradient clipping with\na max norm of 1 are used. We include most of the aug-\nmentation and regularization strategies of [63] in training,\nincluding RandAugment [17], Mixup [77], Cutmix [75],\nrandom erasing [82] and stochastic depth [35], but not re-\npeated augmentation [31] and Exponential Moving Average\n(EMA) [45] which do not enhance performance. Note that\nthis is contrary to [63] where repeated augmentation is cru-\ncial to stabilize the training of ViT. An increasing degree of\nstochastic depth augmentation is employed for larger mod-\nels, i.e. 0.2, 0.3, 0.5 for Swin-T, Swin-S, and Swin-B, re-\nspectively."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1107
                },
                {
                    "x": 2276,
                    "y": 1107
                },
                {
                    "x": 2276,
                    "y": 1352
                },
                {
                    "x": 1280,
                    "y": 1352
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:16px'>For fine-tuning on input with larger resolution, we em-<br>ploy an adamW [37] optimizer for 30 epochs with a con-<br>stant learning rate of 10-5, weight decay of 10-8 and<br>,<br>the same data augmentation and regularizations as the first<br>stage except for setting the stochastic depth ratio to 0.1.</p>",
            "id": 138,
            "page": 9,
            "text": "For fine-tuning on input with larger resolution, we em-\nploy an adamW [37] optimizer for 30 epochs with a con-\nstant learning rate of 10-5, weight decay of 10-8 and\n,\nthe same data augmentation and regularizations as the first\nstage except for setting the stochastic depth ratio to 0.1."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1418
                },
                {
                    "x": 2276,
                    "y": 1418
                },
                {
                    "x": 2276,
                    "y": 1965
                },
                {
                    "x": 1279,
                    "y": 1965
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:16px'>ImageNet-22K pre-training We also pre-train on the<br>larger ImageNet-22K dataset, which contains 14.2 million<br>images and 22K classes. The training is done in two stages.<br>For the first stage with 2242 input, we employ an AdamW<br>optimizer for 90 epochs using a linear decay learning rate<br>scheduler with a 5-epoch linear warm-up. A batch size of<br>4096, an initial learning rate of 0.001, and a weight decay<br>of 0.01 are used. In the second stage of ImageNet-1K fine-<br>tuning with 2242/3842 input, we train the models for 30<br>epochs with a batch size of 1024, a constant learning rate of<br>10-5, and a weight decay of 10-8</p>",
            "id": 139,
            "page": 9,
            "text": "ImageNet-22K pre-training We also pre-train on the\nlarger ImageNet-22K dataset, which contains 14.2 million\nimages and 22K classes. The training is done in two stages.\nFor the first stage with 2242 input, we employ an AdamW\noptimizer for 90 epochs using a linear decay learning rate\nscheduler with a 5-epoch linear warm-up. A batch size of\n4096, an initial learning rate of 0.001, and a weight decay\nof 0.01 are used. In the second stage of ImageNet-1K fine-\ntuning with 2242/3842 input, we train the models for 30\nepochs with a batch size of 1024, a constant learning rate of\n10-5, and a weight decay of 10-8"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2003
                },
                {
                    "x": 1930,
                    "y": 2003
                },
                {
                    "x": 1930,
                    "y": 2049
                },
                {
                    "x": 1283,
                    "y": 2049
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>A2.2. Object detection on COCO</p>",
            "id": 140,
            "page": 9,
            "text": "A2.2. Object detection on COCO"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2080
                },
                {
                    "x": 2276,
                    "y": 2080
                },
                {
                    "x": 2276,
                    "y": 2573
                },
                {
                    "x": 1280,
                    "y": 2573
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>For an ablation study, we consider four typical ob-<br>ject detection frameworks: Cascade Mask R-CNN [29, 6],<br>ATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in<br>mmdetection [10]. For these four frameworks, we utilize<br>the same settings: multi-scale training [8, 56] (resizing the<br>input such that the shorter side is between 480 and 800<br>while the longer side is at most 1333), AdamW [44] opti-<br>mizer (initial learning rate of 0.0001, weight decay of 0.05,<br>and batch size of 16), and 3x schedule (36 epochs with the<br>learning rate decayed by 10x at epochs 27 and 33).</p>",
            "id": 141,
            "page": 9,
            "text": "For an ablation study, we consider four typical ob-\nject detection frameworks: Cascade Mask R-CNN [29, 6],\nATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in\nmmdetection [10]. For these four frameworks, we utilize\nthe same settings: multi-scale training [8, 56] (resizing the\ninput such that the shorter side is between 480 and 800\nwhile the longer side is at most 1333), AdamW [44] opti-\nmizer (initial learning rate of 0.0001, weight decay of 0.05,\nand batch size of 16), and 3x schedule (36 epochs with the\nlearning rate decayed by 10x at epochs 27 and 33)."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2581
                },
                {
                    "x": 2277,
                    "y": 2581
                },
                {
                    "x": 2277,
                    "y": 2976
                },
                {
                    "x": 1280,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:16px'>For system-level comparison, we adopt an improved<br>HTC [9] (denoted as HTC++) with instaboost [22], stronger<br>multi-scale training [7] (resizing the input such that the<br>shorter side is between 400 and 1400 while the longer side<br>is at most 1600), 6x schedule (72 epochs with the learning<br>rate decayed at epochs 63 and 69 by a factor of 0.1), soft-<br>NMS [5], and an extra global self-attention layer appended<br>at the output of last stage and ImageNet-22K pre-trained</p>",
            "id": 142,
            "page": 9,
            "text": "For system-level comparison, we adopt an improved\nHTC [9] (denoted as HTC++) with instaboost [22], stronger\nmulti-scale training [7] (resizing the input such that the\nshorter side is between 400 and 1400 while the longer side\nis at most 1600), 6x schedule (72 epochs with the learning\nrate decayed at epochs 63 and 69 by a factor of 0.1), soft-\nNMS [5], and an extra global self-attention layer appended\nat the output of last stage and ImageNet-22K pre-trained"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3089
                },
                {
                    "x": 1225,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='143' style='font-size:14px'>9</footer>",
            "id": 143,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 287
                },
                {
                    "x": 2268,
                    "y": 287
                },
                {
                    "x": 2268,
                    "y": 975
                },
                {
                    "x": 207,
                    "y": 975
                }
            ],
            "category": "table",
            "html": "<table id='144' style='font-size:14px'><tr><td></td><td>downsp. rate (output size)</td><td colspan=\"2\">Swin-T</td><td colspan=\"2\">Swin-S</td><td colspan=\"2\">Swin-B</td><td colspan=\"2\">Swin-L</td></tr><tr><td rowspan=\"2\">stage 1</td><td rowspan=\"2\">4x (56x56)</td><td colspan=\"2\">concat 4x4, 96-d, LN</td><td colspan=\"2\">concat 4x4, 96-d, LN</td><td colspan=\"2\">concat 4x4, 128-d, LN</td><td colspan=\"2\">concat 4x4, 192-d, LN</td></tr><tr><td>win. SZ. 7x7, dim 96, head 3</td><td>x2</td><td>win. SZ. 7x7, dim 96, head 3</td><td>x 2</td><td>win. SZ. 7x7, dim 128, head 4</td><td>x2</td><td>win. SZ. 7x7, dim 192, head 6</td><td>x 2</td></tr><tr><td rowspan=\"2\">stage 2</td><td rowspan=\"2\">8x (28x28)</td><td colspan=\"2\">concat 2x2, 192-d, LN</td><td colspan=\"2\">concat 2x2, 192-d, LN</td><td colspan=\"2\">concat 2x2, 256-d, LN</td><td colspan=\"2\">concat 2x2, 384-d, LN</td></tr><tr><td>win. SZ. 7x7, dim 192, head 6</td><td>x 2</td><td>win. SZ. 7x7, dim 192, head 6</td><td>x 2</td><td>win. SZ. 7x7, dim 256, head 8</td><td>x2</td><td>win. SZ. 7x7, dim 384, head 12</td><td>x 2</td></tr><tr><td rowspan=\"2\">stage 3</td><td rowspan=\"2\">16x (14x14)</td><td colspan=\"2\">concat 2x2, 384-d LN</td><td colspan=\"2\">concat 2x2, 384-d , LN</td><td colspan=\"2\">concat 2x2, 512-d, LN</td><td colspan=\"2\">concat 2x2, 768-d, LN</td></tr><tr><td>win. SZ. 7x7, x dim 384, head 12</td><td>6</td><td>win. sz. 7x7, dim 384, head 12</td><td>x 18</td><td>win. SZ. 7x7, dim 512, head 16</td><td>x 18</td><td>win. SZ. 7x7, dim 768, head 24</td><td>x 18</td></tr><tr><td rowspan=\"2\">stage 4</td><td rowspan=\"2\">32x (7x7)</td><td colspan=\"2\">concat 2x2, 768-d , LN</td><td colspan=\"2\">concat 2x2, 768-d , LN</td><td colspan=\"2\">concat 2x2, 1024-d, LN</td><td colspan=\"2\">concat 2x2, 1536-d, LN</td></tr><tr><td>win. SZ. 7x7, dim 768, head 24 Table</td><td>x 2</td><td>win. SZ. 7x7, dim 768, head 24 7. Detailed architecture</td><td>x 2</td><td>win. SZ. 7x7, dim 1024, head 32」</td><td>x 2</td><td>win. sz. 7x7, dim 1536, head 48</td><td>x 2</td></tr></table>",
            "id": 144,
            "page": 10,
            "text": "downsp. rate (output size) Swin-T Swin-S Swin-B Swin-L\n stage 1 4x (56x56) concat 4x4, 96-d, LN concat 4x4, 96-d, LN concat 4x4, 128-d, LN concat 4x4, 192-d, LN\n win. SZ. 7x7, dim 96, head 3 x2 win. SZ. 7x7, dim 96, head 3 x 2 win. SZ. 7x7, dim 128, head 4 x2 win. SZ. 7x7, dim 192, head 6 x 2\n stage 2 8x (28x28) concat 2x2, 192-d, LN concat 2x2, 192-d, LN concat 2x2, 256-d, LN concat 2x2, 384-d, LN\n win. SZ. 7x7, dim 192, head 6 x 2 win. SZ. 7x7, dim 192, head 6 x 2 win. SZ. 7x7, dim 256, head 8 x2 win. SZ. 7x7, dim 384, head 12 x 2\n stage 3 16x (14x14) concat 2x2, 384-d LN concat 2x2, 384-d , LN concat 2x2, 512-d, LN concat 2x2, 768-d, LN\n win. SZ. 7x7, x dim 384, head 12 6 win. sz. 7x7, dim 384, head 12 x 18 win. SZ. 7x7, dim 512, head 16 x 18 win. SZ. 7x7, dim 768, head 24 x 18\n stage 4 32x (7x7) concat 2x2, 768-d , LN concat 2x2, 768-d , LN concat 2x2, 1024-d, LN concat 2x2, 1536-d, LN\n win. SZ. 7x7, dim 768, head 24 Table x 2 win. SZ. 7x7, dim 768, head 24 7. Detailed architecture x 2 win. SZ. 7x7, dim 1024, head 32」 x 2 win. sz. 7x7, dim 1536, head 48"
        },
        {
            "bounding_box": [
                {
                    "x": 901,
                    "y": 957
                },
                {
                    "x": 1564,
                    "y": 957
                },
                {
                    "x": 1564,
                    "y": 994
                },
                {
                    "x": 901,
                    "y": 994
                }
            ],
            "category": "caption",
            "html": "<br><caption id='145' style='font-size:16px'>specifications.</caption>",
            "id": 145,
            "page": 10,
            "text": "specifications."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1086
                },
                {
                    "x": 1196,
                    "y": 1086
                },
                {
                    "x": 1196,
                    "y": 1178
                },
                {
                    "x": 204,
                    "y": 1178
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:16px'>model as initialization. We adopt stochastic depth with ra-<br>tio of 0.2 for all Swin Transformer models.</p>",
            "id": 146,
            "page": 10,
            "text": "model as initialization. We adopt stochastic depth with ra-\ntio of 0.2 for all Swin Transformer models."
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 1224
                },
                {
                    "x": 1020,
                    "y": 1224
                },
                {
                    "x": 1020,
                    "y": 1272
                },
                {
                    "x": 206,
                    "y": 1272
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:20px'>A2.3. Semantic segmentation on ADE20K</p>",
            "id": 147,
            "page": 10,
            "text": "A2.3. Semantic segmentation on ADE20K"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1304
                },
                {
                    "x": 1198,
                    "y": 1304
                },
                {
                    "x": 1198,
                    "y": 1599
                },
                {
                    "x": 203,
                    "y": 1599
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:16px'>ADE20K [83] is a widely-used semantic segmentation<br>dataset, covering a broad range of 150 semantic categories.<br>It has 25K images in total, with 20K for training, 2K for val-<br>idation, and another 3K for testing. We utilize UperNet [69]<br>in mmsegmentation [16] as our base framework for its high<br>efficiency.</p>",
            "id": 148,
            "page": 10,
            "text": "ADE20K [83] is a widely-used semantic segmentation\ndataset, covering a broad range of 150 semantic categories.\nIt has 25K images in total, with 20K for training, 2K for val-\nidation, and another 3K for testing. We utilize UperNet [69]\nin mmsegmentation [16] as our base framework for its high\nefficiency."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1608
                },
                {
                    "x": 1198,
                    "y": 1608
                },
                {
                    "x": 1198,
                    "y": 2303
                },
                {
                    "x": 200,
                    "y": 2303
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='149' style='font-size:16px'>In training, we employ the AdamW [44] optimizer with<br>an initial learning rate of 6 x 10-5. a weight decay of 0.01,<br>a scheduler that uses linear learning rate decay, and a lin-<br>ear warmup of 1,500 iterations. Models are trained on 8<br>GPUs with 2 images per GPU for 160K iterations. For aug-<br>mentations, we adopt the default setting in mmsegmentation<br>of random horizontal flipping, random re-scaling within<br>ratio range [0.5, 2.0] and random photometric distortion.<br>Stochastic depth with ratio of 0.2 is applied for all Swin<br>Transformer models. Swin-T, Swin-S are trained on the<br>standard setting as the previous approaches with an input<br>of 512x512. Swin-B and Swin-L with : indicate that these<br>two models are pre-trained on ImageNet-22K, and trained<br>with the input of 640x640.</p>",
            "id": 149,
            "page": 10,
            "text": "In training, we employ the AdamW [44] optimizer with\nan initial learning rate of 6 x 10-5. a weight decay of 0.01,\na scheduler that uses linear learning rate decay, and a lin-\near warmup of 1,500 iterations. Models are trained on 8\nGPUs with 2 images per GPU for 160K iterations. For aug-\nmentations, we adopt the default setting in mmsegmentation\nof random horizontal flipping, random re-scaling within\nratio range [0.5, 2.0] and random photometric distortion.\nStochastic depth with ratio of 0.2 is applied for all Swin\nTransformer models. Swin-T, Swin-S are trained on the\nstandard setting as the previous approaches with an input\nof 512x512. Swin-B and Swin-L with : indicate that these\ntwo models are pre-trained on ImageNet-22K, and trained\nwith the input of 640x640."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2309
                },
                {
                    "x": 1199,
                    "y": 2309
                },
                {
                    "x": 1199,
                    "y": 2556
                },
                {
                    "x": 201,
                    "y": 2556
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:16px'>In inference, a multi-scale test using resolutions that are<br>[0.5, 0.75, 1.0, 1.25, 1.5, 1.75]x of that in training is em-<br>ployed. When reporting test scores, both the training im-<br>ages and validation images are used for training, following<br>common practice [71].</p>",
            "id": 150,
            "page": 10,
            "text": "In inference, a multi-scale test using resolutions that are\n[0.5, 0.75, 1.0, 1.25, 1.5, 1.75]x of that in training is em-\nployed. When reporting test scores, both the training im-\nages and validation images are used for training, following\ncommon practice [71]."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2613
                },
                {
                    "x": 698,
                    "y": 2613
                },
                {
                    "x": 698,
                    "y": 2666
                },
                {
                    "x": 204,
                    "y": 2666
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:22px'>A3. More Experiments</p>",
            "id": 151,
            "page": 10,
            "text": "A3. More Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 2697
                },
                {
                    "x": 1183,
                    "y": 2697
                },
                {
                    "x": 1183,
                    "y": 2747
                },
                {
                    "x": 206,
                    "y": 2747
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='152' style='font-size:20px'>A3.1. Image classification with different input size</p>",
            "id": 152,
            "page": 10,
            "text": "A3.1. Image classification with different input size"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2778
                },
                {
                    "x": 1198,
                    "y": 2778
                },
                {
                    "x": 1198,
                    "y": 2975
                },
                {
                    "x": 201,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>Table 8 lists the performance of Swin Transformers with<br>different input image sizes from 2242 to 3842. In general,<br>a larger input resolution leads to better top-1 accuracy but<br>with slower inference speed.</p>",
            "id": 153,
            "page": 10,
            "text": "Table 8 lists the performance of Swin Transformers with\ndifferent input image sizes from 2242 to 3842. In general,\na larger input resolution leads to better top-1 accuracy but\nwith slower inference speed."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1072
                },
                {
                    "x": 2274,
                    "y": 1072
                },
                {
                    "x": 2274,
                    "y": 1410
                },
                {
                    "x": 1282,
                    "y": 1410
                }
            ],
            "category": "table",
            "html": "<br><table id='154' style='font-size:14px'><tr><td rowspan=\"2\">input size</td><td colspan=\"2\">Swin-T</td><td colspan=\"2\">Swin-S top-1 throughput / s)</td><td colspan=\"2\">Swin-B throughput</td></tr><tr><td>top-1 acc</td><td>throughput (image / s)</td><td>acc</td><td>(image</td><td>top-1 acc</td><td>(image / s)</td></tr><tr><td>2242</td><td>81.3</td><td>755.2</td><td>83.0</td><td>436.9</td><td>83.3</td><td>278.1</td></tr><tr><td>2562</td><td>81.6</td><td>580.9</td><td>83.4</td><td>336.7</td><td>83.7</td><td>208.1</td></tr><tr><td>3202</td><td>82.1</td><td>342.0</td><td>83.7</td><td>198.2</td><td>84.0</td><td>132.0</td></tr><tr><td>3842</td><td>82.2</td><td>219.5</td><td>83.9</td><td>127.6</td><td>84.5</td><td>84.7</td></tr></table>",
            "id": 154,
            "page": 10,
            "text": "input size Swin-T Swin-S top-1 throughput / s) Swin-B throughput\n top-1 acc throughput (image / s) acc (image top-1 acc (image / s)\n 2242 81.3 755.2 83.0 436.9 83.3 278.1\n 2562 81.6 580.9 83.4 336.7 83.7 208.1\n 3202 82.1 342.0 83.7 198.2 84.0 132.0\n 3842 82.2 219.5 83.9 127.6 84.5"
        },
        {
            "bounding_box": [
                {
                    "x": 1285,
                    "y": 1410
                },
                {
                    "x": 2271,
                    "y": 1410
                },
                {
                    "x": 2271,
                    "y": 1490
                },
                {
                    "x": 1285,
                    "y": 1490
                }
            ],
            "category": "caption",
            "html": "<br><caption id='155' style='font-size:14px'>Table 8. Swin Transformers with different input image size on<br>ImageNet-1K classification.</caption>",
            "id": 155,
            "page": 10,
            "text": "Table 8. Swin Transformers with different input image size on\nImageNet-1K classification."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1539
                },
                {
                    "x": 2276,
                    "y": 1539
                },
                {
                    "x": 2276,
                    "y": 1875
                },
                {
                    "x": 1280,
                    "y": 1875
                }
            ],
            "category": "table",
            "html": "<table id='156' style='font-size:14px'><tr><td>Backbone</td><td>Optimizer</td><td>APbox</td><td>APbox</td><td>APbox</td><td>APmask</td><td>APmask</td><td>APmask</td></tr><tr><td rowspan=\"2\">R50</td><td>SGD</td><td>45.0</td><td>62.9</td><td>48.8</td><td>38.5</td><td>59.9</td><td>41.4</td></tr><tr><td>AdamW</td><td>46.3</td><td>64.3</td><td>50.5</td><td>40.1</td><td>61.7</td><td>43.4</td></tr><tr><td rowspan=\"2\">X101-32x4d</td><td>SGD</td><td>47.8</td><td>65.9</td><td>51.9</td><td>40.4</td><td>62.9</td><td>43.5</td></tr><tr><td>AdamW</td><td>48.1</td><td>66.5</td><td>52.4</td><td>41.6</td><td>63.9</td><td>45.2</td></tr><tr><td rowspan=\"2\">X101-64x4d</td><td>SGD</td><td>48.8</td><td>66.9</td><td>53.0</td><td>41.4</td><td>63.9</td><td>44.7</td></tr><tr><td>AdamW</td><td>48.3</td><td>66.4</td><td>52.3</td><td>41.7</td><td>64.0</td><td>45.1</td></tr></table>",
            "id": 156,
            "page": 10,
            "text": "Backbone Optimizer APbox APbox APbox APmask APmask APmask\n R50 SGD 45.0 62.9 48.8 38.5 59.9 41.4\n AdamW 46.3 64.3 50.5 40.1 61.7 43.4\n X101-32x4d SGD 47.8 65.9 51.9 40.4 62.9 43.5\n AdamW 48.1 66.5 52.4 41.6 63.9 45.2\n X101-64x4d SGD 48.8 66.9 53.0 41.4 63.9 44.7\n AdamW 48.3 66.4 52.3 41.7 64.0"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 1878
                },
                {
                    "x": 2274,
                    "y": 1878
                },
                {
                    "x": 2274,
                    "y": 2007
                },
                {
                    "x": 1283,
                    "y": 2007
                }
            ],
            "category": "caption",
            "html": "<br><caption id='157' style='font-size:14px'>Table 9. Comparison of the SGD and Adam W optimizers for<br>ResNe(X)t backbones on COCO object detection using the Cas-<br>cade Mask R-CNN framework.</caption>",
            "id": 157,
            "page": 10,
            "text": "Table 9. Comparison of the SGD and Adam W optimizers for\nResNe(X)t backbones on COCO object detection using the Cas-\ncade Mask R-CNN framework."
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2088
                },
                {
                    "x": 2273,
                    "y": 2088
                },
                {
                    "x": 2273,
                    "y": 2135
                },
                {
                    "x": 1283,
                    "y": 2135
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:20px'>A3.2. Different Optimizers for ResNe(X)t on COCO</p>",
            "id": 158,
            "page": 10,
            "text": "A3.2. Different Optimizers for ResNe(X)t on COCO"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2166
                },
                {
                    "x": 2276,
                    "y": 2166
                },
                {
                    "x": 2276,
                    "y": 2611
                },
                {
                    "x": 1279,
                    "y": 2611
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:16px'>Table 9 compares the AdamW and SGD optimizers of<br>the ResNe(X)t backbones on COCO object detection. The<br>Cascade Mask R-CNN framework is used in this compar-<br>ison. While SGD is used as a default optimizer for Cas-<br>cade Mask R-CNN framework, we generally observe im-<br>proved accuracy by replacing it with an AdamW optimizer,<br>particularly for smaller backbones. We thus use AdamW<br>for ResNe(X)t backbones when compared to the proposed<br>Swin Transformer architectures.</p>",
            "id": 159,
            "page": 10,
            "text": "Table 9 compares the AdamW and SGD optimizers of\nthe ResNe(X)t backbones on COCO object detection. The\nCascade Mask R-CNN framework is used in this compar-\nison. While SGD is used as a default optimizer for Cas-\ncade Mask R-CNN framework, we generally observe im-\nproved accuracy by replacing it with an AdamW optimizer,\nparticularly for smaller backbones. We thus use AdamW\nfor ResNe(X)t backbones when compared to the proposed\nSwin Transformer architectures."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2650
                },
                {
                    "x": 1746,
                    "y": 2650
                },
                {
                    "x": 1746,
                    "y": 2695
                },
                {
                    "x": 1282,
                    "y": 2695
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:20px'>A3.3. Swin MLP-Mixer</p>",
            "id": 160,
            "page": 10,
            "text": "A3.3. Swin MLP-Mixer"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2727
                },
                {
                    "x": 2277,
                    "y": 2727
                },
                {
                    "x": 2277,
                    "y": 2978
                },
                {
                    "x": 1280,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:16px'>We apply the proposed hierarchical design and the<br>shifted window approach to the MLP-Mixer architec-<br>tures [61], referred to as Swin-Mixer. Table 10 shows the<br>performance of Swin-Mixer compared to the original MLP-<br>Mixer architectures MLP-Mixer [61] and a follow-up ap-</p>",
            "id": 161,
            "page": 10,
            "text": "We apply the proposed hierarchical design and the\nshifted window approach to the MLP-Mixer architec-\ntures [61], referred to as Swin-Mixer. Table 10 shows the\nperformance of Swin-Mixer compared to the original MLP-\nMixer architectures MLP-Mixer [61] and a follow-up ap-"
        },
        {
            "bounding_box": [
                {
                    "x": 1220,
                    "y": 3055
                },
                {
                    "x": 1263,
                    "y": 3055
                },
                {
                    "x": 1263,
                    "y": 3091
                },
                {
                    "x": 1220,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='162' style='font-size:16px'>10</footer>",
            "id": 162,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 292
                },
                {
                    "x": 1230,
                    "y": 292
                },
                {
                    "x": 1230,
                    "y": 909
                },
                {
                    "x": 202,
                    "y": 909
                }
            ],
            "category": "table",
            "html": "<table id='163' style='font-size:14px'><tr><td>method</td><td>image size</td><td>#param.</td><td>FLOPs</td><td>throughput (image / s)</td><td>ImageNet top-1 acc.</td></tr><tr><td>MLP-Mixer-B/16 [61]</td><td>2242</td><td>59M</td><td>12.7G</td><td>-</td><td>76.4</td></tr><tr><td>ResMLP-S24 [62]</td><td>2242</td><td>30M</td><td>6.0G</td><td>715</td><td>79.4</td></tr><tr><td>ResMLP-B24 [62]</td><td>2242</td><td>116M</td><td>23.0G</td><td>231</td><td>81.0</td></tr><tr><td>Swin-T/D24 (Transformer)</td><td>2562</td><td>28M</td><td>5.9G</td><td>563</td><td>81.6</td></tr><tr><td>Swin-Mixer-T/D24</td><td>2562</td><td>20M</td><td>4.0G</td><td>807</td><td>79.4</td></tr><tr><td>Swin-Mixer-T/D12</td><td>2562</td><td>21M</td><td>4.0G</td><td>792</td><td>79.6</td></tr><tr><td>Swin-Mixer-T/D6</td><td>2562</td><td>23M</td><td>4.0G</td><td>766</td><td>79.7</td></tr><tr><td>Swin-Mixer-B/D24 (no shift)</td><td>2242</td><td>61M</td><td>10.4G</td><td>409</td><td>80.3</td></tr><tr><td>Swin-Mixer-B/D24</td><td>2242</td><td>61M</td><td>10.4G</td><td>409</td><td>81.3</td></tr></table>",
            "id": 163,
            "page": 11,
            "text": "method image size #param. FLOPs throughput (image / s) ImageNet top-1 acc.\n MLP-Mixer-B/16 [61] 2242 59M 12.7G - 76.4\n ResMLP-S24 [62] 2242 30M 6.0G 715 79.4\n ResMLP-B24 [62] 2242 116M 23.0G 231 81.0\n Swin-T/D24 (Transformer) 2562 28M 5.9G 563 81.6\n Swin-Mixer-T/D24 2562 20M 4.0G 807 79.4\n Swin-Mixer-T/D12 2562 21M 4.0G 792 79.6\n Swin-Mixer-T/D6 2562 23M 4.0G 766 79.7\n Swin-Mixer-B/D24 (no shift) 2242 61M 10.4G 409 80.3\n Swin-Mixer-B/D24 2242 61M 10.4G 409"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 906
                },
                {
                    "x": 1199,
                    "y": 906
                },
                {
                    "x": 1199,
                    "y": 1086
                },
                {
                    "x": 205,
                    "y": 1086
                }
            ],
            "category": "caption",
            "html": "<br><caption id='164' style='font-size:14px'>Table 10. Performance of Swin MLP-Mixer on ImageNet-1K clas-<br>sification. D indictes the number of channels per head. Through-<br>put is measured using the GitHub repository of [68] and a V100<br>GPU, following [63].</caption>",
            "id": 164,
            "page": 11,
            "text": "Table 10. Performance of Swin MLP-Mixer on ImageNet-1K clas-\nsification. D indictes the number of channels per head. Through-\nput is measured using the GitHub repository of [68] and a V100\nGPU, following [63]."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1167
                },
                {
                    "x": 1197,
                    "y": 1167
                },
                {
                    "x": 1197,
                    "y": 1462
                },
                {
                    "x": 204,
                    "y": 1462
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:20px'>proach, ResMLP [61]. Swin-Mixer performs significantly<br>better than MLP-Mixer (81.3% VS. 76.4%) using slightly<br>smaller computation budget (10.4G VS. 12.7G). It also has<br>better speed accuracy trade-off compared to ResMLP [62].<br>These results indicate the proposed hierarchical design and<br>the shifted window approach are generalizable.</p>",
            "id": 165,
            "page": 11,
            "text": "proach, ResMLP [61]. Swin-Mixer performs significantly\nbetter than MLP-Mixer (81.3% VS. 76.4%) using slightly\nsmaller computation budget (10.4G VS. 12.7G). It also has\nbetter speed accuracy trade-off compared to ResMLP [62].\nThese results indicate the proposed hierarchical design and\nthe shifted window approach are generalizable."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1506
                },
                {
                    "x": 444,
                    "y": 1506
                },
                {
                    "x": 444,
                    "y": 1555
                },
                {
                    "x": 205,
                    "y": 1555
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:22px'>References</p>",
            "id": 166,
            "page": 11,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1578
                },
                {
                    "x": 1201,
                    "y": 1578
                },
                {
                    "x": 1201,
                    "y": 2978
                },
                {
                    "x": 218,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:16px'>[1] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,<br>Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming<br>Zhou, et al. Unilmv2: Pseudo-masked language models for<br>unified language model pre-training. In International Con-<br>ference on Machine Learning, pages 642-652. PMLR, 2020.<br>5<br>[2] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew<br>Zhai, and Dmitry Kislyuk. Toward transformer-based object<br>detection. arXiv preprint arXiv:2012.09958, 2020. 3<br>[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,<br>and Quoc V. Le. Attention augmented convolutional net-<br>works, 2020. 3<br>[4] Alexey Bochkovskiy, Chien- Yao Wang, and Hong-<br>Yuan Mark Liao. Yolov4: Optimal speed and accuracy of<br>object detection. arXiv preprint arXiv:2004. 10934, 2020. 7<br>[5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and<br>Larry S. Davis. Soft-nms - improving object detection with<br>one line of code. In Proceedings of the IEEE International<br>Conference on Computer Vision (ICCV), Oct 2017. 6, 9<br>[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-<br>ing into high quality object detection. In Proceedings of the<br>IEEE Conference on Computer Vision and Pattern Recogni-<br>tion, pages 6154-6162, 2018. 6, 9<br>[7] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han<br>Hu. Gcnet: Non-local networks meet squeeze-excitation net-<br>works and beyond. In Proceedings of the IEEE/CVF Inter-<br>national Conference on Computer Vision (ICCV) Workshops,<br>Oct 2019. 3, 6, 7, 9<br>[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas<br>Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-</p>",
            "id": 167,
            "page": 11,
            "text": "[1] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming\nZhou, et al. Unilmv2: Pseudo-masked language models for\nunified language model pre-training. In International Con-\nference on Machine Learning, pages 642-652. PMLR, 2020.\n5\n[2] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew\nZhai, and Dmitry Kislyuk. Toward transformer-based object\ndetection. arXiv preprint arXiv:2012.09958, 2020. 3\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V. Le. Attention augmented convolutional net-\nworks, 2020. 3\n[4] Alexey Bochkovskiy, Chien- Yao Wang, and Hong-\nYuan Mark Liao. Yolov4: Optimal speed and accuracy of\nobject detection. arXiv preprint arXiv:2004. 10934, 2020. 7\n[5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and\nLarry S. Davis. Soft-nms - improving object detection with\none line of code. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), Oct 2017. 6, 9\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 6154-6162, 2018. 6, 9\n[7] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV) Workshops,\nOct 2019. 3, 6, 7, 9\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 306
                },
                {
                    "x": 2285,
                    "y": 306
                },
                {
                    "x": 2285,
                    "y": 2977
                },
                {
                    "x": 1280,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:16px'>end object detection with transformers. In European Confer-<br>ence on Computer Vision, pages 213-229. Springer, 2020. 3,<br>6, 9<br>[9] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-<br>iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping<br>Shi, Wanli Ouyang, et al. Hybrid task cascade for instance<br>segmentation. In Proceedings of the IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, pages 4974-<br>4983, 2019. 6, 9<br>[10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu<br>Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,<br>Jiarui Xu, et al. Mmdetection: Open mmlab detection tool-<br>box and benchmark. arXiv preprint arXiv:1906.07155, 2019.<br>6, 9<br>[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian<br>Schroff, and Hartwig Adam. Encoder-decoder with atrous<br>separable convolution for semantic image segmentation. In<br>Proceedings of the European conference on computer vision<br>(ECCV), pages 801-818, 2018. 7<br>[12] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen<br>Lin, and Han Hu. Reppoints v2: Verification meets regres-<br>sion for object detection. In NeurIPS, 2020. 6, 7, 9<br>[13] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++:<br>Bridging visual representations for object detection via trans-<br>former decoder. In NeurIPS, 2020. 3, 7<br>[14] Krzysztof Marcin Choromanski, Valerii Likhosherstov,<br>David Dohan, Xingyou Song, Andreea Gane, Tamas Sar-<br>los, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,<br>Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,<br>and Adrian Weller. Rethinking attention with performers.<br>In International Conference on Learning Representations,<br>2021. 8, 9<br>[15] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and<br>Huaxia Xia. Do we really need explicit position encodings<br>for vision transformers? arXiv preprint arXiv:2102.10882,<br>2021. 3<br>[16] MMSegmentation Contributors. MMSegmentation:<br>Openmmlab semantic segmentation toolbox and bench-<br>mark. https : / /github · com/ open-mml ab /<br>mmsegmentation, 2020. 8, 10<br>[17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V<br>Le. Randaugment: Practical automated data augmenta-<br>tion with a reduced search space. In Proceedings of the<br>IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition Workshops, pages 702-703, 2020. 9<br>[18] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong<br>Zhang, Han Hu, and Yichen Wei. Deformable convolutional<br>networks. In Proceedings of the IEEE International Confer-<br>ence on Computer Vision, pages 764-773, 2017. 1, 3<br>[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,<br>and Li Fei-Fei. Imagenet: A large-scale hierarchical image<br>database. In 2009 IEEE conference on computer vision and<br>pattern recognition, pages 248-255. Ieee, 2009. 5<br>[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,<br>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,<br>Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-<br>vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is</p>",
            "id": 168,
            "page": 11,
            "text": "end object detection with transformers. In European Confer-\nence on Computer Vision, pages 213-229. Springer, 2020. 3,\n6, 9\n[9] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-\niao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping\nShi, Wanli Ouyang, et al. Hybrid task cascade for instance\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4974-\n4983, 2019. 6, 9\n[10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n6, 9\n[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProceedings of the European conference on computer vision\n(ECCV), pages 801-818, 2018. 7\n[12] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen\nLin, and Han Hu. Reppoints v2: Verification meets regres-\nsion for object detection. In NeurIPS, 2020. 6, 7, 9\n[13] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++:\nBridging visual representations for object detection via trans-\nformer decoder. In NeurIPS, 2020. 3, 7\n[14] Krzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,\nand Adrian Weller. Rethinking attention with performers.\nIn International Conference on Learning Representations,\n2021. 8, 9\n[15] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv preprint arXiv:2102.10882,\n2021. 3\n[16] MMSegmentation Contributors. MMSegmentation:\nOpenmmlab semantic segmentation toolbox and bench-\nmark. https : / /github · com/ open-mml ab /\nmmsegmentation, 2020. 8, 10\n[17] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmenta-\ntion with a reduced search space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702-703, 2020. 9\n[18] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 764-773, 2017. 1, 3\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248-255. Ieee, 2009. 5\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3052
                },
                {
                    "x": 1260,
                    "y": 3052
                },
                {
                    "x": 1260,
                    "y": 3095
                },
                {
                    "x": 1218,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='169' style='font-size:18px'>11</footer>",
            "id": 169,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 313
                },
                {
                    "x": 1198,
                    "y": 313
                },
                {
                    "x": 1198,
                    "y": 441
                },
                {
                    "x": 286,
                    "y": 441
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:14px'>worth 16x16 words: Transformers for image recognition at<br>scale. In International Conference on Learning Representa-<br>tions, 2021. 1, 2, 3, 4, 5, 6, 9</p>",
            "id": 170,
            "page": 12,
            "text": "worth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 1, 2, 3, 4, 5, 6, 9"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 440
                },
                {
                    "x": 1200,
                    "y": 440
                },
                {
                    "x": 1200,
                    "y": 2975
                },
                {
                    "x": 201,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='171' style='font-size:20px'>[21] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi,<br>Mingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song.<br>Spinenet: Learning scale-permuted backbone for recogni-<br>tion and localization. In Proceedings of the IEEE/CVF Con-<br>ference on Computer Vision and Pattern Recognition, pages<br>11592-11601, 2020. 7<br>[22] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao<br>Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting<br>instance segmentation via probability map guided copy-<br>pasting. In Proceedings of the IEEE/CVF International Con-<br>ference on Computer Vision, pages 682-691, 2019. 6, 9<br>[23] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-<br>wei Fang, and Hanqing Lu. Dual attention network for<br>scene segmentation. In Proceedings of the IEEE Conference<br>on Computer Vision and Pattern Recognition, pages 3146-<br>3154, 2019. 3, 7<br>[24] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin-<br>hui Tang, and Hanqing Lu. Adaptive context network for<br>scene parsing. In Proceedings of the IEEE/CVF Interna-<br>tional Conference on Computer Vision, pages 6748-6757,<br>2019. 7<br>[25] Kunihiko Fukushima. Cognitron: A self-organizing multi-<br>layered neural network. Biological cybernetics, 20(3):121-<br>136, 1975. 3<br>[26] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-<br>Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple<br>copy-paste is a strong data augmentation method for instance<br>segmentation. arXiv preprint arXiv:2012.07177, 2020. 2, 7<br>[27] Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, and Jifeng<br>Dai. Learning region features for object detection. In Pro-<br>ceedings of the European Conference on Computer Vision<br>(ECCV), 2018. 3<br>[28] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,<br>and Yunhe Wang. Transformer in transformer. arXiv preprint<br>arXiv:2103.00112, 2021. 3<br>[29] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-<br>shick. Mask r-cnn. In Proceedings of the IEEE international<br>conference on computer vision, pages 2961-2969, 2017. 6,<br>9<br>[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In Proceed-<br>ings of the IEEE conference on computer vision and pattern<br>recognition, pages 770-778, 2016. 1, 2, 4<br>[31] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten<br>Hoefler, and Daniel Soudry. Augment your batch: Improving<br>generalization through instance repetition. In Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, pages 8129-8138, 2020. 6, 9<br>[32] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen<br>Wei. Relation networks for object detection. In Proceed-<br>ings ofthe IEEE Conference on Computer Vision and Pattern<br>Recognition, pages 3588-3597, 2018. 3, 5<br>[33] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local<br>relation networks for image recognition. In Proceedings of</p>",
            "id": 171,
            "page": 12,
            "text": "[21] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi,\nMingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song.\nSpinenet: Learning scale-permuted backbone for recogni-\ntion and localization. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11592-11601, 2020. 7\n[22] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao\nGou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting\ninstance segmentation via probability map guided copy-\npasting. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 682-691, 2019. 6, 9\n[23] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-\nwei Fang, and Hanqing Lu. Dual attention network for\nscene segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3146-\n3154, 2019. 3, 7\n[24] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin-\nhui Tang, and Hanqing Lu. Adaptive context network for\nscene parsing. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 6748-6757,\n2019. 7\n[25] Kunihiko Fukushima. Cognitron: A self-organizing multi-\nlayered neural network. Biological cybernetics, 20(3):121-\n136, 1975. 3\n[26] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation. arXiv preprint arXiv:2012.07177, 2020. 2, 7\n[27] Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, and Jifeng\nDai. Learning region features for object detection. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 2018. 3\n[28] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021. 3\n[29] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961-2969, 2017. 6,\n9\n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770-778, 2016. 1, 2, 4\n[31] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoefler, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8129-8138, 2020. 6, 9\n[32] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proceed-\nings ofthe IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3588-3597, 2018. 3, 5\n[33] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In Proceedings of"
        },
        {
            "bounding_box": [
                {
                    "x": 1365,
                    "y": 311
                },
                {
                    "x": 2277,
                    "y": 311
                },
                {
                    "x": 2277,
                    "y": 395
                },
                {
                    "x": 1365,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:16px'>the IEEE/CVF International Conference on Computer Vision<br>(ICCV), pages 3464-3473, October 2019. 2, 3, 5</p>",
            "id": 172,
            "page": 12,
            "text": "the IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 3464-3473, October 2019. 2, 3, 5"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 407
                },
                {
                    "x": 2288,
                    "y": 407
                },
                {
                    "x": 2288,
                    "y": 2975
                },
                {
                    "x": 1276,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='173' style='font-size:16px'>[34] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-<br>ian Q Weinberger. Densely connected convolutional net-<br>works. In Proceedings of the IEEE conference on computer<br>vision and pattern recognition, pages 4700-4708, 2017. 1, 2<br>[35] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-<br>ian Q Weinberger. Deep networks with stochastic depth. In<br>European conference on computer vision, pages 646-661.<br>Springer, 2016. 9<br>[36] David H Hubel and Torsten N Wiesel. Receptive fields,<br>binocular interaction and functional architecture in the cat's<br>visual cortex. The Journal of physiology, 160(1):106-154,<br>1962. 3<br>[37] Diederik P Kingma and Jimmy Ba. Adam: A method for<br>stochastic optimization. arXiv preprint arXiv:1412.6980,<br>2014. 5, 9<br>[38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan<br>Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.<br>Big transfer (bit): General visual representation learning.<br>arXiv preprint arXiv:1912.11370, 6(2):8, 2019. 6<br>[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.<br>Imagenet classification with deep convolutional neural net-<br>works. In Advances in neural information processing sys-<br>tems, pages 1097-1105, 2012. 1, 2<br>[40] Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner,<br>et al. Gradient-based learning applied to document recog-<br>nition. Proceedings of the IEEE, 86(11):2278-2324, 1998.<br>2<br>[41] Yann LeCun, Patrick Haffner, Leon Bottou, and Yoshua Ben-<br>gio. Object recognition with gradient-based learning. In<br>Shape, contour and grouping in computer vision, pages 319-<br>345. Springer, 1999. 3<br>[42] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,<br>Bharath Hariharan, and Serge Belongie. Feature pyramid<br>networks for object detection. In The IEEE Conference<br>on Computer Vision and Pattern Recognition (CVPR), July<br>2017. 2<br>[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,<br>Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence<br>Zitnick. Microsoft COCO: Common objects in context. In<br>European conference on computer vision, pages 740-755.<br>Springer, 2014. 5<br>[44] Ilya Loshchilov and Frank Hutter. Decoupled weight de-<br>cay regularization. In International Conference on Learning<br>Representations, 2019. 6, 9, 10<br>[45] Boris T Polyak and Anatoli B Juditsky. Acceleration of<br>stochastic approximation by averaging. SIAM journal on<br>control and optimization, 30(4):838-855, 1992. 6, 9<br>[46] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors:<br>Detecting objects with recursive feature pyramid and switch-<br>able atrous convolution. arXiv preprint arXiv:2006.02334,<br>2020. 2, 7<br>[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya<br>Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,<br>Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen<br>Krueger, and Ilya Sutskever. Learning transferable visual<br>models from natural language supervision, 2021. 1</p>",
            "id": 173,
            "page": 12,
            "text": "[34] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700-4708, 2017. 1, 2\n[35] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nEuropean conference on computer vision, pages 646-661.\nSpringer, 2016. 9\n[36] David H Hubel and Torsten N Wiesel. Receptive fields,\nbinocular interaction and functional architecture in the cat's\nvisual cortex. The Journal of physiology, 160(1):106-154,\n1962. 3\n[37] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014. 5, 9\n[38] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 6\n[39] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097-1105, 2012. 1, 2\n[40] Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner,\net al. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n2\n[41] Yann LeCun, Patrick Haffner, Leon Bottou, and Yoshua Ben-\ngio. Object recognition with gradient-based learning. In\nShape, contour and grouping in computer vision, pages 319-\n345. Springer, 1999. 3\n[42] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July\n2017. 2\n[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEuropean conference on computer vision, pages 740-755.\nSpringer, 2014. 5\n[44] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2019. 6, 9, 10\n[45] Boris T Polyak and Anatoli B Juditsky. Acceleration of\nstochastic approximation by averaging. SIAM journal on\ncontrol and optimization, 30(4):838-855, 1992. 6, 9\n[46] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors:\nDetecting objects with recursive feature pyramid and switch-\nable atrous convolution. arXiv preprint arXiv:2006.02334,\n2020. 2, 7\n[47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision, 2021. 1"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3094
                },
                {
                    "x": 1218,
                    "y": 3094
                }
            ],
            "category": "footer",
            "html": "<footer id='174' style='font-size:20px'>12</footer>",
            "id": 174,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 296
                },
                {
                    "x": 1201,
                    "y": 296
                },
                {
                    "x": 1201,
                    "y": 2969
                },
                {
                    "x": 201,
                    "y": 2969
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:14px'>[48] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,<br>Kaiming He, and Piotr Dollar. Designing network design<br>spaces. In Proceedings of the IEEE/CVF Conference on<br>Computer Vision and Pattern Recognition, pages 10428-<br>10436, 2020. 6<br>[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,<br>Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and<br>Peter J. Liu. Exploring the limits of transfer learning with a<br>unified text-to-text transformer. Journal of Machine Learn-<br>ing Research, 21(140):1-67, 2020. 5<br>[50] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan<br>Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-<br>attention in vision models. In Advances in Neural Informa-<br>tion Processing Systems, volume 32. Curran Associates, Inc.,<br>2019. 2, 3<br>[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-<br>net: Convolutional networks for biomedical image segmen-<br>tation. In International Conference on Medical image com-<br>puting and computer-assisted intervention, pages 234-241.<br>Springer, 2015. 2<br>[52] K. Simonyan and A. Zisserman. Very deep convolutional<br>networks for large-scale image recognition. In International<br>Conference on Learning Representations, May 2015. 2, 4<br>[53] Bharat Singh and Larry S Davis. An analysis of scale in-<br>variance in object detection snip. In Proceedings of the<br>IEEE conference on computer vision and pattern recogni-<br>tion, pages 3578-3587, 2018. 2<br>[54] Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper:<br>Efficient multi-scale training. In Advances in Neural Infor-<br>mation Processing Systems, volume 31. Curran Associates,<br>Inc., 2018. 2<br>[55] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon<br>Shlens, Pieter Abbeel, and Ashish Vaswani. Bottle-<br>neck transformers for visual recognition. arXiv preprint<br>arXiv:2101.11605, 2021. 3<br>[56] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-<br>feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan<br>Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end<br>object detection with learnable proposals. arXiv preprint<br>arXiv:2011.12450, 2020. 3, 6, 9<br>[57] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,<br>Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent<br>Vanhoucke, and Andrew Rabinovich. Going deeper with<br>convolutions. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pages 1-9, 2015.<br>2<br>[58] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model<br>scaling for convolutional neural networks. In International<br>Conference on Machine Learning, pages 6105-6114. PMLR,<br>2019. 3, 6<br>[59] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet:<br>Scalable and efficient object detection. In Proceedings of<br>the IEEE/CVF conference on computer vision and pattern<br>recognition, pages 10781-10790, 2020. 7<br>[60] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,<br>Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian<br>Ruder, and Donald Metzler. Long range arena : A bench-</p>",
            "id": 175,
            "page": 13,
            "text": "[48] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Dollar. Designing network design\nspaces. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10428-\n10436, 2020. 6\n[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learn-\ning Research, 21(140):1-67, 2020. 5\n[50] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In Advances in Neural Informa-\ntion Processing Systems, volume 32. Curran Associates, Inc.,\n2019. 2, 3\n[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, pages 234-241.\nSpringer, 2015. 2\n[52] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In International\nConference on Learning Representations, May 2015. 2, 4\n[53] Bharat Singh and Larry S Davis. An analysis of scale in-\nvariance in object detection snip. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 3578-3587, 2018. 2\n[54] Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper:\nEfficient multi-scale training. In Advances in Neural Infor-\nmation Processing Systems, volume 31. Curran Associates,\nInc., 2018. 2\n[55] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021. 3\n[56] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-\nfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan\nYuan, Changhu Wang, et al. Sparse r-cnn: End-to-end\nobject detection with learnable proposals. arXiv preprint\narXiv:2011.12450, 2020. 3, 6, 9\n[57] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1-9, 2015.\n2\n[58] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105-6114. PMLR,\n2019. 3, 6\n[59] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet:\nScalable and efficient object detection. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10781-10790, 2020. 7\n[60] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\nRuder, and Donald Metzler. Long range arena : A bench-"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 319
                },
                {
                    "x": 2289,
                    "y": 319
                },
                {
                    "x": 2289,
                    "y": 2973
                },
                {
                    "x": 1272,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:14px'>mark for efficient transformers. In International Conference<br>on Learning Representations, 2021. 8<br>[61] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-<br>cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,<br>Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario<br>Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar-<br>chitecture for vision, 2021. 2, 10, 11<br>[62] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu<br>Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac-<br>ard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and<br>Herve Jegou. Resmlp: Feedforward networks for image clas-<br>sification with data-efficient training, 2021. 11<br>[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco<br>Massa, Alexandre Sablayrolles, and Herve Jegou. Training<br>data-efficient image transformers & distillation through at-<br>tention. arXiv preprint arXiv:2012.12877, 2020. 2, 3, 5, 6,<br>9, 11<br>[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia<br>Polosukhin. Attention is all you need. In Advances in Neural<br>Information Processing Systems, pages 5998-6008, 2017. 1,<br>2, 4<br>[65] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,<br>Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui<br>Tan, Xinggang Wang, et al. Deep high-resolution represen-<br>tation learning for visual recognition. IEEE transactions on<br>pattern analysis and machine intelligence, 2020. 3<br>[66] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao<br>Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.<br>Pyramid vision transformer: A versatile backbone for<br>dense prediction without convolutions. arXiv preprint<br>arXiv:2102.12122, 2021. 3<br>[67] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-<br>ing He. Non-local neural networks. In IEEE Conference<br>on Computer Vision and Pattern Recognition, CVPR 2018,<br>2018. 3<br>[68] Ross Wightman. Pytorch image mod-<br>els. https : / / github · com/ rwi ghtman/<br>pytorch- image-models, 2019. 6, 11<br>[69] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and<br>Jian Sun. Unified perceptual parsing for scene understand-<br>ing. In Proceedings of the European Conference on Com-<br>puter Vision (ECCV), pages 418-434, 2018. 7, 8, 10<br>[70] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and<br>Kaiming He. Aggregated residual transformations for deep<br>neural networks. In Proceedings of the IEEE Conference<br>on Computer Vision and Pattern Recognition, pages 1492-<br>1500, 2017. 1, 2, 3<br>[71] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang,<br>Stephen Lin, and Han Hu. Disentangled non-local neural<br>networks. In Proceedings of the European conference on<br>computer vision (ECCV), 2020. 3, 7, 10<br>[72] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,<br>Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-<br>to-token vit: Training vision transformers from scratch on<br>imagenet. arXiv preprint arXiv:2101.11986, 2021. 3<br>[73] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-<br>contextual representations for semantic segmentation. In</p>",
            "id": 176,
            "page": 13,
            "text": "mark for efficient transformers. In International Conference\non Learning Representations, 2021. 8\n[61] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario\nLucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar-\nchitecture for vision, 2021. 2, 10, 11\n[62] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu\nCord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac-\nard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and\nHerve Jegou. Resmlp: Feedforward networks for image clas-\nsification with data-efficient training, 2021. 11\n[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 2, 3, 5, 6,\n9, 11\n[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998-6008, 2017. 1,\n2, 4\n[65] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, et al. Deep high-resolution represen-\ntation learning for visual recognition. IEEE transactions on\npattern analysis and machine intelligence, 2020. 3\n[66] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021. 3\n[67] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2018,\n2018. 3\n[68] Ross Wightman. Pytorch image mod-\nels. https : / / github · com/ rwi ghtman/\npytorch- image-models, 2019. 6, 11\n[69] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 418-434, 2018. 7, 8, 10\n[70] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 1492-\n1500, 2017. 1, 2, 3\n[71] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang,\nStephen Lin, and Han Hu. Disentangled non-local neural\nnetworks. In Proceedings of the European conference on\ncomputer vision (ECCV), 2020. 3, 7, 10\n[72] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 3\n[73] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-\ncontextual representations for semantic segmentation. In"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3094
                },
                {
                    "x": 1218,
                    "y": 3094
                }
            ],
            "category": "footer",
            "html": "<footer id='177' style='font-size:18px'>13</footer>",
            "id": 177,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 310
                },
                {
                    "x": 1200,
                    "y": 310
                },
                {
                    "x": 1200,
                    "y": 395
                },
                {
                    "x": 286,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:18px'>16th European Conference Computer Vision (ECCV 2020),<br>August 2020. 7</p>",
            "id": 178,
            "page": 14,
            "text": "16th European Conference Computer Vision (ECCV 2020),\nAugust 2020. 7"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 364
                },
                {
                    "x": 1201,
                    "y": 364
                },
                {
                    "x": 1201,
                    "y": 2690
                },
                {
                    "x": 205,
                    "y": 2690
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='179' style='font-size:14px'>[74] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-<br>work for scene parsing. arXiv preprint arXiv:1809.00916,<br>2018. 3<br>[75] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk<br>Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-<br>ization strategy to train strong classifiers with localizable fea-<br>tures. In Proceedings of the IEEE/CVF International Con-<br>ference on Computer Vision, pages 6023-6032, 2019. 9<br>[76] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-<br>works. In BMVC, 2016. 1<br>[77] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and<br>David Lopez-Paz. mixup: Beyond empirical risk minimiza-<br>tion. arXiv preprint arXiv:1710.09412, 2017. 9<br>[78] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi<br>Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R<br>Manmatha, et al. Resnest: Split-attention networks. arXiv<br>preprint arXiv:2004.08955, 2020. 7, 8<br>[79] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and<br>Stan Z Li. Bridging the gap between anchor-based and<br>anchor-free detection via adaptive training sample selection.<br>In Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, pages 9759-9768, 2020. 6,<br>9<br>[80] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-<br>ing self-attention for image recognition. In Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, pages 10076-10085, 2020. 3<br>[81] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,<br>Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao<br>Xiang, Philip HS Torr, et al. Rethinking semantic segmen-<br>tation from a sequence-to-sequence perspective with trans-<br>formers. arXiv preprint arXiv:2012.15840, 2020. 2, 3, 7,<br>8<br>[82] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and<br>Yi Yang. Random erasing data augmentation. In Proceedings<br>ofthe AAAI Conference on Artificial Intelligence, volume 34,<br>pages 13001-13008, 2020. 9<br>[83] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-<br>dler, Adela Barriuso, and Antonio Torralba. Semantic under-<br>standing of scenes through the ade20k dataset. International<br>Journal on Computer Vision, 2018. 5, 7, 10<br>[84] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-<br>formable convnets v2: More deformable, better results. In<br>Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition, pages 9308-9316, 2019. 1, 3<br>[85] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,<br>and Jifeng Dai. Deformable {detr}: Deformable transform-<br>ers for end-to-end object detection. In International Confer-<br>ence on Learning Representations, 2021. 3</p>",
            "id": 179,
            "page": 14,
            "text": "[74] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-\nwork for scene parsing. arXiv preprint arXiv:1809.00916,\n2018. 3\n[75] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classifiers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023-6032, 2019. 9\n[76] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. In BMVC, 2016. 1\n[77] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 9\n[78] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R\nManmatha, et al. Resnest: Split-attention networks. arXiv\npreprint arXiv:2004.08955, 2020. 7, 8\n[79] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\nStan Z Li. Bridging the gap between anchor-based and\nanchor-free detection via adaptive training sample selection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9759-9768, 2020. 6,\n9\n[80] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-\ning self-attention for image recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10076-10085, 2020. 3\n[81] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. arXiv preprint arXiv:2012.15840, 2020. 2, 3, 7,\n8\n[82] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In Proceedings\nofthe AAAI Conference on Artificial Intelligence, volume 34,\npages 13001-13008, 2020. 9\n[83] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal on Computer Vision, 2018. 5, 7, 10\n[84] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-\nformable convnets v2: More deformable, better results. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 9308-9316, 2019. 1, 3\n[85] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable {detr}: Deformable transform-\ners for end-to-end object detection. In International Confer-\nence on Learning Representations, 2021. 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3056
                },
                {
                    "x": 1262,
                    "y": 3056
                },
                {
                    "x": 1262,
                    "y": 3091
                },
                {
                    "x": 1218,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='180' style='font-size:14px'>14</footer>",
            "id": 180,
            "page": 14,
            "text": "14"
        }
    ]
}