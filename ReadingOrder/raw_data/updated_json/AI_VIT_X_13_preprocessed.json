{
    "id": "32a01272-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2305.01937v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 322
                },
                {
                    "x": 2147,
                    "y": 322
                },
                {
                    "x": 2147,
                    "y": 396
                },
                {
                    "x": 333,
                    "y": 396
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Can Large Language Models Be an Alternative to Human Evaluation?</p>",
            "id": 0,
            "page": 1,
            "text": "Can Large Language Models Be an Alternative to Human Evaluation?"
        },
        {
            "bounding_box": [
                {
                    "x": 616,
                    "y": 483
                },
                {
                    "x": 1043,
                    "y": 483
                },
                {
                    "x": 1043,
                    "y": 543
                },
                {
                    "x": 616,
                    "y": 543
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>Cheng-Han Chiang</p>",
            "id": 1,
            "page": 1,
            "text": "Cheng-Han Chiang"
        },
        {
            "bounding_box": [
                {
                    "x": 1515,
                    "y": 484
                },
                {
                    "x": 1794,
                    "y": 484
                },
                {
                    "x": 1794,
                    "y": 542
                },
                {
                    "x": 1515,
                    "y": 542
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:22px'>Hung-yi Lee</p>",
            "id": 2,
            "page": 1,
            "text": "Hung-yi Lee"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 544
                },
                {
                    "x": 1115,
                    "y": 544
                },
                {
                    "x": 1115,
                    "y": 717
                },
                {
                    "x": 545,
                    "y": 717
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:20px'>National Taiwan University,<br>Taiwan<br>dcml 071 4@gmail . com</p>",
            "id": 3,
            "page": 1,
            "text": "National Taiwan University, Taiwan dcml 071 4@gmail . com"
        },
        {
            "bounding_box": [
                {
                    "x": 650,
                    "y": 889
                },
                {
                    "x": 848,
                    "y": 889
                },
                {
                    "x": 848,
                    "y": 943
                },
                {
                    "x": 650,
                    "y": 943
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>Abstract</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 1366,
                    "y": 543
                },
                {
                    "x": 1941,
                    "y": 543
                },
                {
                    "x": 1941,
                    "y": 718
                },
                {
                    "x": 1366,
                    "y": 718
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:20px'>National Taiwan University,<br>Taiwan<br>hungyilee@ntu. edu. tw</p>",
            "id": 5,
            "page": 1,
            "text": "National Taiwan University, Taiwan hungyilee@ntu. edu. tw"
        },
        {
            "bounding_box": [
                {
                    "x": 355,
                    "y": 963
                },
                {
                    "x": 1146,
                    "y": 963
                },
                {
                    "x": 1146,
                    "y": 2619
                },
                {
                    "x": 355,
                    "y": 2619
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:14px'>Human evaluation is indispensable and in-<br>evitable for assessing the quality of texts gen-<br>erated by machine learning models or writ-<br>ten by humans. However, human evaluation<br>is very difficult to reproduce and its quality<br>is notoriously unstable, hindering fair compar-<br>isons among different natural language pro-<br>cessing (NLP) models and algorithms. Re-<br>cently, large language models (LLMs) have<br>demonstrated exceptional performance on un-<br>seen tasks when only the task instructions are<br>provided. In this paper, we explore if such an<br>ability of the LLMs can be used as an alter-<br>native to human evaluation. We present the<br>LLMs with the exact same instructions, sam-<br>ples to be evaluated, and questions used to con-<br>duct human evaluation, and then ask the LLMs<br>to generate responses to those questions; we<br>dub this LLM evaluation. We use human evalu-<br>ation and LLM evaluation to evaluate the texts<br>in two NLP tasks: open-ended story genera-<br>tion and adversarial attacks. We show that the<br>result of LLM evaluation is consistent with the<br>results obtained by expert human evaluation:<br>the texts rated higher by human experts are<br>also rated higher by the LLMs. We also find<br>that the results of LLM evaluation are stable<br>over different formatting of the task instruc-<br>tions and the sampling algorithm used to gen-<br>erate the answer. We are the first to show the<br>potential of using LLMs to assess the quality<br>of texts and discuss the limitations and ethical<br>considerations of LLM evaluation.</p>",
            "id": 6,
            "page": 1,
            "text": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs. We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2633
                },
                {
                    "x": 647,
                    "y": 2633
                },
                {
                    "x": 647,
                    "y": 2690
                },
                {
                    "x": 290,
                    "y": 2690
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>1 Introduction</p>",
            "id": 7,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2718
                },
                {
                    "x": 1219,
                    "y": 2718
                },
                {
                    "x": 1219,
                    "y": 3229
                },
                {
                    "x": 288,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:16px'>Human evaluation is an important method to under-<br>stand the performance of an NLP model or algo-<br>rithm (Guzman et al., 2015; Gillick and Liu, 2010).<br>We rely on human evaluation because there are cer-<br>tain aspects of texts that are hard to evaluate using<br>automatic evaluation metrics; thus, researchers re-<br>sort to humans to rate the quality of the output of<br>NLP models. While human evaluation is preva-<br>lent and indispensable in NLP, it is notoriously</p>",
            "id": 8,
            "page": 1,
            "text": "Human evaluation is an important method to understand the performance of an NLP model or algorithm (Guzman , 2015; Gillick and Liu, 2010). We rely on human evaluation because there are certain aspects of texts that are hard to evaluate using automatic evaluation metrics; thus, researchers resort to humans to rate the quality of the output of NLP models. While human evaluation is prevalent and indispensable in NLP, it is notoriously"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 891
                },
                {
                    "x": 2200,
                    "y": 891
                },
                {
                    "x": 2200,
                    "y": 1790
                },
                {
                    "x": 1267,
                    "y": 1790
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>unstable (Gillick and Liu, 2010; Clark et al., 2021).<br>Karpinska et al. (2021) has shown that low-quality<br>workforces in human evaluation can have a detri-<br>mental effect on the evaluation result, making it<br>impossible to compare the performance among dif-<br>ferent systems. Reproducibility is another issue in<br>human evaluation since itis hard to recruit the same<br>human evaluators and rerun the same evaluation.<br>Even if the same workers are recruited, the workers<br>that have seen the task before are likely to produce<br>a different evaluation result the next time because<br>they have already done the task. While human eval-<br>uation is used to better assess NLP systems and has<br>some advantages over automatic evaluation met-<br>rics, the drawbacks of human evaluation somewhat<br>make it difficult to reliably evaluate NLP systems.</p>",
            "id": 9,
            "page": 1,
            "text": "unstable (Gillick and Liu, 2010; Clark , 2021). Karpinska  (2021) has shown that low-quality workforces in human evaluation can have a detrimental effect on the evaluation result, making it impossible to compare the performance among different systems. Reproducibility is another issue in human evaluation since itis hard to recruit the same human evaluators and rerun the same evaluation. Even if the same workers are recruited, the workers that have seen the task before are likely to produce a different evaluation result the next time because they have already done the task. While human evaluation is used to better assess NLP systems and has some advantages over automatic evaluation metrics, the drawbacks of human evaluation somewhat make it difficult to reliably evaluate NLP systems."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1797
                },
                {
                    "x": 2200,
                    "y": 1797
                },
                {
                    "x": 2200,
                    "y": 2749
                },
                {
                    "x": 1267,
                    "y": 2749
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>To resolve some of the drawbacks, we take ad-<br>vantage of large language models (LLMs). LLMs<br>are large models that are trained to model human<br>languages using self-supervised learning (Brown<br>et al., 2020) and further using special training pro-<br>cedures to improve the performance on unseen<br>tasks and better follow natural language instruc-<br>tions (Sanh et al., 2022; Wei et al., 2022). The<br>ability to perform a task just given the task instruc-<br>tions motivates us to ask if these LLMs can perform<br>what humans do in human evaluation. To answer<br>this question, we feed in the LLM with the same<br>instruction, sample, and question used in human<br>evaluation, and take the sequences generated by the<br>LLM as the LLM's answer to the question. This<br>process is shown in Figure 1, and we call this pro-<br>cess LLM evaluation.</p>",
            "id": 10,
            "page": 1,
            "text": "To resolve some of the drawbacks, we take advantage of large language models (LLMs). LLMs are large models that are trained to model human languages using self-supervised learning (Brown , 2020) and further using special training procedures to improve the performance on unseen tasks and better follow natural language instructions (Sanh , 2022; Wei , 2022). The ability to perform a task just given the task instructions motivates us to ask if these LLMs can perform what humans do in human evaluation. To answer this question, we feed in the LLM with the same instruction, sample, and question used in human evaluation, and take the sequences generated by the LLM as the LLM's answer to the question. This process is shown in Figure 1, and we call this process LLM evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2756
                },
                {
                    "x": 2198,
                    "y": 2756
                },
                {
                    "x": 2198,
                    "y": 3090
                },
                {
                    "x": 1267,
                    "y": 3090
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>To test if LLM evaluation yields meaningful re-<br>sults, we conduct LLM evaluation on two different<br>NLP tasks: evaluating the quality of stories in open-<br>ended story generation and the quality of sentences<br>generated by adversarial attacks. We summarize<br>our findings and contribution as follows:</p>",
            "id": 11,
            "page": 1,
            "text": "To test if LLM evaluation yields meaningful results, we conduct LLM evaluation on two different NLP tasks: evaluating the quality of stories in openended story generation and the quality of sentences generated by adversarial attacks. We summarize our findings and contribution as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 3116
                },
                {
                    "x": 2197,
                    "y": 3116
                },
                {
                    "x": 2197,
                    "y": 3227
                },
                {
                    "x": 1324,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:16px'>· We show that LLM evaluation produces re-<br>sults similar to expert human evaluation, ver-</p>",
            "id": 12,
            "page": 1,
            "text": "· We show that LLM evaluation produces results similar to expert human evaluation, ver-"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 1093
                },
                {
                    "x": 147,
                    "y": 1093
                },
                {
                    "x": 147,
                    "y": 2520
                },
                {
                    "x": 58,
                    "y": 2520
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2023<br>May<br>3<br>[cs.CL]<br>arXiv:2305.01937v1</footer>",
            "id": 13,
            "page": 1,
            "text": "2023 May 3 [cs.CL] arXiv:2305.01937v1"
        },
        {
            "bounding_box": [
                {
                    "x": 336,
                    "y": 303
                },
                {
                    "x": 2138,
                    "y": 303
                },
                {
                    "x": 2138,
                    "y": 1238
                },
                {
                    "x": 336,
                    "y": 1238
                }
            ],
            "category": "figure",
            "html": "<figure><img id='14' style='font-size:14px' alt=\"Task instruction, sample, and question Human evaluation\nHuman evaluators Human responses\nPlease rate the story fragment\n\n1 (lowest)\nThe goal of this task is to rate story → →  2\nfragments.\n3\n4\nNOTE: Please take the time I\nto fully read and understand the story □ 5 (highest)\nfragment. We will reject submissions from )\nworkers that are clearly spamming the\ntask.\nStory fragment\nThe human ambassador reached down and LLM evaluation\ngrasped it , S paw. 'Humans, cats, is it\ntrue that all intelligent beings are\nLLM (e.g., GPT3) LLM output\nomnivorous?' 'Certainly, your rank\ndemeanour can be demonstrated from the\nI would rate the\nwords we spoke to the Kelpie. They' re of I\ngrammar in the text\nno concern to us humans, as they are\nnot considered to live among us, thus → of the story\nfragment as a 4. → 4\nfar. (...) → There are no major\nHow grammatically correct is the text of I grammatical errors\nthe story fragment? (on a scale of 1-5, or issues with\nwith 1 being the lowest?) punctuation, (...)\" data-coord=\"top-left:(336,303); bottom-right:(2138,1238)\" /></figure>",
            "id": 14,
            "page": 2,
            "text": "Task instruction, sample, and question Human evaluation Human evaluators Human responses Please rate the story fragment  1 (lowest) The goal of this task is to rate story → →  2 fragments. 3 4 NOTE: Please take the time I to fully read and understand the story □ 5 (highest) fragment. We will reject submissions from ) workers that are clearly spamming the task. Story fragment The human ambassador reached down and LLM evaluation grasped it , S paw. \"Humans, cats, is it true that all intelligent beings are LLM (e.g., GPT3) LLM output omnivorous?\" \"Certainly, your rank demeanour can be demonstrated from the I would rate the words we spoke to the Kelpie. They' re of I grammar in the text no concern to us humans, as they are not considered to live among us, thus → of the story fragment as a 4. → 4 far. (...) → There are no major How grammatically correct is the text of I grammatical errors the story fragment? (on a scale of 1-5, or issues with with 1 being the lowest?) punctuation, (...)"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1290
                },
                {
                    "x": 2196,
                    "y": 1290
                },
                {
                    "x": 2196,
                    "y": 1549
                },
                {
                    "x": 287,
                    "y": 1549
                }
            ],
            "category": "caption",
            "html": "<caption id='15' style='font-size:14px'>Figure 1: Illustration of the core idea of the paper using open-ended story generation as the example task. The left<br>part shows the instruction, story fragments, and questions used in human evaluation. The human experts are asked<br>to rate the quality of the story fragments using a 5-point Likert scale, shown on the upper right. The lower right<br>part shows the process of LLM evaluation, where we feed the LLMs the same instruction, story fragments, and<br>questions and parse the LLM-generated output to get the rating.</caption>",
            "id": 15,
            "page": 2,
            "text": "Figure 1: Illustration of the core idea of the paper using open-ended story generation as the example task. The left part shows the instruction, story fragments, and questions used in human evaluation. The human experts are asked to rate the quality of the story fragments using a 5-point Likert scale, shown on the upper right. The lower right part shows the process of LLM evaluation, where we feed the LLMs the same instruction, story fragments, and questions and parse the LLM-generated output to get the rating."
        },
        {
            "bounding_box": [
                {
                    "x": 376,
                    "y": 1633
                },
                {
                    "x": 1214,
                    "y": 1633
                },
                {
                    "x": 1214,
                    "y": 1861
                },
                {
                    "x": 376,
                    "y": 1861
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:16px'>ifying the effectiveness of LLM evaluation<br>(§3.3 and §4.3). This paper is the first to pro-<br>pose using LLMs as an alternative to human<br>evaluation and show their effectiveness.</p>",
            "id": 16,
            "page": 2,
            "text": "ifying the effectiveness of LLM evaluation (§3.3 and §4.3). This paper is the first to propose using LLMs as an alternative to human evaluation and show their effectiveness."
        },
        {
            "bounding_box": [
                {
                    "x": 340,
                    "y": 1900
                },
                {
                    "x": 1216,
                    "y": 1900
                },
                {
                    "x": 1216,
                    "y": 2397
                },
                {
                    "x": 340,
                    "y": 2397
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:16px'>· We show that LLM evaluation results only<br>slightly vary due to different task instructions<br>and the hyperparameters of the sampling al-<br>gorithm used to generate the answer. (§3.3.2<br>and §3.3.3)<br>● We carefully discuss the pros and cons of us-<br>ing LLM evaluation and discuss the ethical<br>considerations of LLM evaluation. (§5)</p>",
            "id": 17,
            "page": 2,
            "text": "· We show that LLM evaluation results only slightly vary due to different task instructions and the hyperparameters of the sampling algorithm used to generate the answer. (§3.3.2 and §3.3.3) ● We carefully discuss the pros and cons of using LLM evaluation and discuss the ethical considerations of LLM evaluation. (§5)"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2438
                },
                {
                    "x": 732,
                    "y": 2438
                },
                {
                    "x": 732,
                    "y": 2494
                },
                {
                    "x": 289,
                    "y": 2494
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:22px'>2 LLM Evaluation</p>",
            "id": 18,
            "page": 2,
            "text": "2 LLM Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2531
                },
                {
                    "x": 1036,
                    "y": 2531
                },
                {
                    "x": 1036,
                    "y": 2585
                },
                {
                    "x": 288,
                    "y": 2585
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:22px'>2.1 Large Language Models (LLMs)</p>",
            "id": 19,
            "page": 2,
            "text": "2.1 Large Language Models (LLMs)"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2604
                },
                {
                    "x": 1213,
                    "y": 2604
                },
                {
                    "x": 1213,
                    "y": 3117
                },
                {
                    "x": 287,
                    "y": 3117
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:18px'>Large language models are language models hav-<br>ing bulk parameter sizes, typically on the scale<br>of a few billion, and pre-trained on enormous<br>amounts of natural language corpora, including<br>GPT3 (Brown et al., 2020), T5 (Raffel et al., 2020),<br>and BLOOM (Scao et al., 2022). These LLMs<br>show exceptional performance on unseen tasks<br>when only the task instructions are given; this kind<br>of ability is called zero-shot in-context learning.</p>",
            "id": 20,
            "page": 2,
            "text": "Large language models are language models having bulk parameter sizes, typically on the scale of a few billion, and pre-trained on enormous amounts of natural language corpora, including GPT3 (Brown , 2020), T5 (Raffel , 2020), and BLOOM (Scao , 2022). These LLMs show exceptional performance on unseen tasks when only the task instructions are given; this kind of ability is called zero-shot in-context learning."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3118
                },
                {
                    "x": 1215,
                    "y": 3118
                },
                {
                    "x": 1215,
                    "y": 3228
                },
                {
                    "x": 287,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:20px'>To further improve the zero-shot in-context learn-<br>ing performance, special training techniques have</p>",
            "id": 21,
            "page": 2,
            "text": "To further improve the zero-shot in-context learning performance, special training techniques have"
        },
        {
            "bounding_box": [
                {
                    "x": 1266,
                    "y": 1631
                },
                {
                    "x": 2196,
                    "y": 1631
                },
                {
                    "x": 2196,
                    "y": 2650
                },
                {
                    "x": 1266,
                    "y": 2650
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:16px'>been applied to those LLMs after pre-training. For<br>example, T0 (Sanh et al., 2022) and FLAN (Wei<br>et al., 2022) are fine-tuned on a mixture of tasks<br>and can thus achieve better zero-shot performance<br>compared to GPT-3. InstructGPT (Ouyang et al.,<br>2022) is fine-tuned from GPT-3 using reinforce-<br>ment learning from human feedback (RLHF), and<br>it is shown to better follow the instructions. Chat-<br>GPT (OpenAI, 2022) is fine-tuned from Instruct-<br>GPT with a conversation dataset using RLHF, SO<br>ChatGPT can interact with users in a conversational<br>way. ChatGPT is able to answer questions asked<br>by the user and provide comprehensive explana-<br>tions about its answer. Given the LLMs' ability to<br>follow task instructions and provide feedback, we<br>ask whether LLMs can be used as an alternative<br>to human evaluation and aid NLP researchers in<br>evaluating the quality of texts.</p>",
            "id": 22,
            "page": 2,
            "text": "been applied to those LLMs after pre-training. For example, T0 (Sanh , 2022) and FLAN (Wei , 2022) are fine-tuned on a mixture of tasks and can thus achieve better zero-shot performance compared to GPT-3. InstructGPT (Ouyang , 2022) is fine-tuned from GPT-3 using reinforcement learning from human feedback (RLHF), and it is shown to better follow the instructions. ChatGPT (OpenAI, 2022) is fine-tuned from InstructGPT with a conversation dataset using RLHF, SO ChatGPT can interact with users in a conversational way. ChatGPT is able to answer questions asked by the user and provide comprehensive explanations about its answer. Given the LLMs' ability to follow task instructions and provide feedback, we ask whether LLMs can be used as an alternative to human evaluation and aid NLP researchers in evaluating the quality of texts."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2699
                },
                {
                    "x": 1709,
                    "y": 2699
                },
                {
                    "x": 1709,
                    "y": 2749
                },
                {
                    "x": 1269,
                    "y": 2749
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:16px'>2.2 LLM Evaluation</p>",
            "id": 23,
            "page": 2,
            "text": "2.2 LLM Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2777
                },
                {
                    "x": 2195,
                    "y": 2777
                },
                {
                    "x": 2195,
                    "y": 3228
                },
                {
                    "x": 1267,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:16px'>To evaluate the quality of texts generated by NLP<br>systems or written by humans using LLM, we<br>present the LLMs with the task instructions, the<br>sample to be evaluated, and a question. The ques-<br>tion asks the LLM to rate the sample's quality using<br>a 5-point Likert scale. Given the inputs, the LLM<br>will answer the question by generating some out-<br>put sentences. We parse the output sentences to get</p>",
            "id": 24,
            "page": 2,
            "text": "To evaluate the quality of texts generated by NLP systems or written by humans using LLM, we present the LLMs with the task instructions, the sample to be evaluated, and a question. The question asks the LLM to rate the sample's quality using a 5-point Likert scale. Given the inputs, the LLM will answer the question by generating some output sentences. We parse the output sentences to get"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 302
                },
                {
                    "x": 1214,
                    "y": 302
                },
                {
                    "x": 1214,
                    "y": 806
                },
                {
                    "x": 287,
                    "y": 806
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>the score rated by the LLM. We call this process<br>LLM evaluation, and this procedure is shown in the<br>lower part of Figure 1. Different tasks use different<br>sets of task instructions, and each task uses differ-<br>ent questions to evaluate the quality of the samples.<br>The instructions and questions used in LLM evalua-<br>tion in our paper are not tailored for the LLMs; we<br>follow those instructions used to conduct human<br>evaluation in prior works.</p>",
            "id": 25,
            "page": 3,
            "text": "the score rated by the LLM. We call this process LLM evaluation, and this procedure is shown in the lower part of Figure 1. Different tasks use different sets of task instructions, and each task uses different questions to evaluate the quality of the samples. The instructions and questions used in LLM evaluation in our paper are not tailored for the LLMs; we follow those instructions used to conduct human evaluation in prior works."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 812
                },
                {
                    "x": 1213,
                    "y": 812
                },
                {
                    "x": 1213,
                    "y": 1715
                },
                {
                    "x": 286,
                    "y": 1715
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:18px'>To compare the result of LLM evaluation and<br>show its effectiveness, we compare the result of<br>LLM evaluation with human evaluation conducted<br>by English teachers. To make a fair and meaningful<br>comparison, the instructions, samples, and ques-<br>tions in human evaluation are formatted similarly<br>to those in LLM evaluation. The main difference<br>between LLM evaluation and human evaluation<br>is that in human evaluation, the human evaluators<br>answer the question by choosing the answer from<br>a pre-defined set of options (the 1-5 Likert scale<br>scores), as shown in the upper right in Figure 1.<br>In LLM evaluation, we instead let the LLM freely<br>generate sentences and extract the score from the<br>generated sentences using some simple rules, de-<br>tailed in Appendix D.2.1.</p>",
            "id": 26,
            "page": 3,
            "text": "To compare the result of LLM evaluation and show its effectiveness, we compare the result of LLM evaluation with human evaluation conducted by English teachers. To make a fair and meaningful comparison, the instructions, samples, and questions in human evaluation are formatted similarly to those in LLM evaluation. The main difference between LLM evaluation and human evaluation is that in human evaluation, the human evaluators answer the question by choosing the answer from a pre-defined set of options (the 1-5 Likert scale scores), as shown in the upper right in Figure 1. In LLM evaluation, we instead let the LLM freely generate sentences and extract the score from the generated sentences using some simple rules, detailed in Appendix D.2.1."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1763
                },
                {
                    "x": 1145,
                    "y": 1763
                },
                {
                    "x": 1145,
                    "y": 1874
                },
                {
                    "x": 288,
                    "y": 1874
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:22px'>3 Example Task 1: Open-Ended Story<br>Generation</p>",
            "id": 27,
            "page": 3,
            "text": "3 Example Task 1: Open-Ended Story Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1915
                },
                {
                    "x": 1214,
                    "y": 1915
                },
                {
                    "x": 1214,
                    "y": 2026
                },
                {
                    "x": 288,
                    "y": 2026
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:16px'>We first use open-ended story generation to demon-<br>strate the usefulness of LLM evaluation.</p>",
            "id": 28,
            "page": 3,
            "text": "We first use open-ended story generation to demonstrate the usefulness of LLM evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2073
                },
                {
                    "x": 753,
                    "y": 2073
                },
                {
                    "x": 753,
                    "y": 2126
                },
                {
                    "x": 288,
                    "y": 2126
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:18px'>3.1 Task Introduction</p>",
            "id": 29,
            "page": 3,
            "text": "3.1 Task Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2152
                },
                {
                    "x": 1214,
                    "y": 2152
                },
                {
                    "x": 1214,
                    "y": 2599
                },
                {
                    "x": 287,
                    "y": 2599
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>Open-ended story generation is a task to gener-<br>ate a short story based on a given prompt. We<br>use the WritingPrompts dataset (Fan et al., 2018),<br>which is composed of pairs of short prompts and<br>human-written stories collected from the subred-<br>dit WritingPrompts. In the WritingPrompts, the<br>users are given a short prompt, and they need to<br>write a story based on the short prompt. 1</p>",
            "id": 30,
            "page": 3,
            "text": "Open-ended story generation is a task to generate a short story based on a given prompt. We use the WritingPrompts dataset (Fan , 2018), which is composed of pairs of short prompts and human-written stories collected from the subreddit WritingPrompts. In the WritingPrompts, the users are given a short prompt, and they need to write a story based on the short prompt. 1"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2607
                },
                {
                    "x": 1214,
                    "y": 2607
                },
                {
                    "x": 1214,
                    "y": 3054
                },
                {
                    "x": 287,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:16px'>In this experiment, we use LLM evaluation and<br>human evaluation to rate the stories generated by<br>humans and the stories generated by a story gen-<br>eration model. We select open-ended story gen-<br>eration as an example because Karpinska et al.<br>(2021) show that workers from Amazon Mechani-<br>cal Turk (AMT) cannot distinguish GPT-2 (Radford<br>et al., 2019) generated and human-written stories,</p>",
            "id": 31,
            "page": 3,
            "text": "In this experiment, we use LLM evaluation and human evaluation to rate the stories generated by humans and the stories generated by a story generation model. We select open-ended story generation as an example because Karpinska  (2021) show that workers from Amazon Mechanical Turk (AMT) cannot distinguish GPT-2 (Radford , 2019) generated and human-written stories,"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3095
                },
                {
                    "x": 1213,
                    "y": 3095
                },
                {
                    "x": 1213,
                    "y": 3223
                },
                {
                    "x": 288,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:14px'>1The WritingPrompts subreddit explicitly forbids the<br>users to use AI for generating stories, SO we consider the<br>stories in the dataset to be human-written.</p>",
            "id": 32,
            "page": 3,
            "text": "1The WritingPrompts subreddit explicitly forbids the users to use AI for generating stories, SO we consider the stories in the dataset to be human-written."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 302
                },
                {
                    "x": 2197,
                    "y": 302
                },
                {
                    "x": 2197,
                    "y": 522
                },
                {
                    "x": 1267,
                    "y": 522
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:16px'>while English teachers show a clear preference for<br>human-written stories over GPT-2-generated sto-<br>ries. We want to see ifLLM can rate human-written<br>stories higher than GPT-2-generated ones.</p>",
            "id": 33,
            "page": 3,
            "text": "while English teachers show a clear preference for human-written stories over GPT-2-generated stories. We want to see ifLLM can rate human-written stories higher than GPT-2-generated ones."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 529
                },
                {
                    "x": 2197,
                    "y": 529
                },
                {
                    "x": 2197,
                    "y": 1374
                },
                {
                    "x": 1267,
                    "y": 1374
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:20px'>Following prior works (Mao et al., 2019; Guan<br>et al., 2020; Karpinska et al., 2021), the story gen-<br>eration model is GPT-2 medium model fine-tuned<br>on the WritingPrompts training dataset. After the<br>model is trained, we randomly select 200 prompts<br>from the testing set of Wri tingPrompts and make<br>the fine-tuned GPT-2 generate stories based on<br>those prompts using nucleus sampling (Holtzman<br>et al., 2020) with p = 0.9. For the human-written<br>stories to be compared, we use the 200 stories<br>written based on the same 200 prompts. We post-<br>process the human-written and GPT-2-generated<br>stories and then use them for LLM evaluation and<br>human evaluation. Please find the details on fine-<br>tuning and data processing in Appendix B.</p>",
            "id": 34,
            "page": 3,
            "text": "Following prior works (Mao , 2019; Guan , 2020; Karpinska , 2021), the story generation model is GPT-2 medium model fine-tuned on the WritingPrompts training dataset. After the model is trained, we randomly select 200 prompts from the testing set of Wri tingPrompts and make the fine-tuned GPT-2 generate stories based on those prompts using nucleus sampling (Holtzman , 2020) with p = 0.9. For the human-written stories to be compared, we use the 200 stories written based on the same 200 prompts. We postprocess the human-written and GPT-2-generated stories and then use them for LLM evaluation and human evaluation. Please find the details on finetuning and data processing in Appendix B."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1408
                },
                {
                    "x": 2175,
                    "y": 1408
                },
                {
                    "x": 2175,
                    "y": 1461
                },
                {
                    "x": 1269,
                    "y": 1461
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>3.2 LLM Evaluation and Human Evaluation</p>",
            "id": 35,
            "page": 3,
            "text": "3.2 LLM Evaluation and Human Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1482
                },
                {
                    "x": 2197,
                    "y": 1482
                },
                {
                    "x": 2197,
                    "y": 1818
                },
                {
                    "x": 1267,
                    "y": 1818
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:18px'>We present the LLMs and the human evaluators<br>with a short description, and the story to be eval-<br>uated, formatted as shown in Figure 1. Follow-<br>ing Karpinska et al. (2021), we evaluate the stories<br>on four different attributes. The four attributes and<br>their corresponding questions are as follows:</p>",
            "id": 36,
            "page": 3,
            "text": "We present the LLMs and the human evaluators with a short description, and the story to be evaluated, formatted as shown in Figure 1. Following Karpinska  (2021), we evaluate the stories on four different attributes. The four attributes and their corresponding questions are as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1305,
                    "y": 1847
                },
                {
                    "x": 2195,
                    "y": 1847
                },
                {
                    "x": 2195,
                    "y": 2576
                },
                {
                    "x": 1305,
                    "y": 2576
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:20px'>1. Grammaticality: How grammatically cor-<br>rect is the text of the story fragment?<br>2. Cohesiveness: How well do the sentences in<br>the story fragment fit together?<br>3. Likability: How enjoyable do you find the<br>story fragment?<br>4. Relevance: Now read the PROMPT based on<br>which the story fragment was written.<br>Prompt: [PROMPT].<br>How relevant is the story fragment to the<br>prompt?</p>",
            "id": 37,
            "page": 3,
            "text": "1. Grammaticality: How grammatically correct is the text of the story fragment? 2. Cohesiveness: How well do the sentences in the story fragment fit together? 3. Likability: How enjoyable do you find the story fragment? 4. Relevance: Now read the PROMPT based on which the story fragment was written. Prompt: [PROMPT]. How relevant is the story fragment to the prompt?"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2607
                },
                {
                    "x": 2198,
                    "y": 2607
                },
                {
                    "x": 2198,
                    "y": 3054
                },
                {
                    "x": 1267,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:18px'>Where the [PROMPT] will be filled in with the<br>prompt which the story is based on. Each attribute<br>is evaluated using a 5-point Likert scale; the fol-<br>lowing description is appended at the end of each<br>question: \"(on a scale of 1-5, with 1 being the low-<br>est)\" . We show the interface used in human evalua-<br>tion and the input format for the LLM evaluation<br>in Appendix C.2 and D.2.2.</p>",
            "id": 38,
            "page": 3,
            "text": "Where the [PROMPT] will be filled in with the prompt which the story is based on. Each attribute is evaluated using a 5-point Likert scale; the following description is appended at the end of each question: \"(on a scale of 1-5, with 1 being the lowest)\" . We show the interface used in human evaluation and the input format for the LLM evaluation in Appendix C.2 and D.2.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 3062
                },
                {
                    "x": 2197,
                    "y": 3062
                },
                {
                    "x": 2197,
                    "y": 3228
                },
                {
                    "x": 1268,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:18px'>The LLMs used for LLM evaluation include T0,<br>text-curie-001, text-davinci-003, and Chat-<br>GPT. text-curie-001 and text-davinci-003</p>",
            "id": 39,
            "page": 3,
            "text": "The LLMs used for LLM evaluation include T0, text-curie-001, text-davinci-003, and ChatGPT. text-curie-001 and text-davinci-003"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 285
                },
                {
                    "x": 2236,
                    "y": 285
                },
                {
                    "x": 2236,
                    "y": 1122
                },
                {
                    "x": 288,
                    "y": 1122
                }
            ],
            "category": "table",
            "html": "<table id='40' style='font-size:22px'><tr><td rowspan=\"2\">Evaluator</td><td colspan=\"2\">Grammaticality</td><td colspan=\"2\">Cohesiveness</td><td colspan=\"2\">Likability</td><td colspan=\"2\">Relevance</td></tr><tr><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td></tr><tr><td colspan=\"9\">Human-written stories</td></tr><tr><td>Human</td><td>3.760.95</td><td>0.3320.5</td><td>4.290.82</td><td>0.3227</td><td>3.781.10</td><td>0.089.5</td><td>3.351.48</td><td>0.05g</td></tr><tr><td>T0</td><td>2.551.47</td><td>0.1610</td><td>2.981.45</td><td>0.114</td><td>3.181.53</td><td>0.127</td><td>2.931.64</td><td>0.026</td></tr><tr><td>curie</td><td>3.190.47</td><td>0.0746.5</td><td>2.820.46</td><td>0.0147.5</td><td>2.850.37</td><td>0.110.65</td><td>3.060.40</td><td>0.110.64</td></tr><tr><td>davinci</td><td>4.220.38</td><td>0.2635</td><td>4.540.47</td><td>0.3739.5</td><td>3.990.38</td><td>0.4968.5</td><td>4.400.79</td><td>0.7148.5</td></tr><tr><td>ChatGPT</td><td>3.830.60</td><td></td><td>3.550.88</td><td></td><td>2.440.89</td><td></td><td>3.291.50</td><td></td></tr><tr><td colspan=\"9\">GPT-2-generated stories</td></tr><tr><td>Human</td><td>3.560.91</td><td>0.1019.5</td><td>3.191.07</td><td>0.1417</td><td>2.591.29</td><td>-0.213.5</td><td>2.381.40</td><td>-0.038.5</td></tr><tr><td>T0</td><td>2.441.49</td><td>0.05g</td><td>3.021.51</td><td>0.076</td><td>3.001.59</td><td>0.166</td><td>2.821.61</td><td>0.046</td></tr><tr><td>curie</td><td>3.230.51</td><td>0.0138</td><td>2.820.45</td><td>0.0250</td><td>2.860.37</td><td>0.0965.5</td><td>3.010.43</td><td>0.1161</td></tr><tr><td>davinci</td><td>4.070.35</td><td>0.3545.5</td><td>4.260.45</td><td>0.4242</td><td>3.840.42</td><td>0.5262</td><td>4.020.74</td><td>0.6942.5</td></tr><tr><td>ChatGPT</td><td>2.980.76</td><td></td><td>2.480.71</td><td></td><td>1.590.67</td><td></td><td>2.021.21</td><td></td></tr></table>",
            "id": 40,
            "page": 4,
            "text": "Evaluator Grammaticality Cohesiveness Likability Relevance  MeansTD IAA% MeansTD IAA% MeansTD IAA% MeansTD IAA%  Human-written stories  Human 3.760.95 0.3320.5 4.290.82 0.3227 3.781.10 0.089.5 3.351.48 0.05g  T0 2.551.47 0.1610 2.981.45 0.114 3.181.53 0.127 2.931.64 0.026  curie 3.190.47 0.0746.5 2.820.46 0.0147.5 2.850.37 0.110.65 3.060.40 0.110.64  davinci 4.220.38 0.2635 4.540.47 0.3739.5 3.990.38 0.4968.5 4.400.79 0.7148.5  ChatGPT 3.830.60  3.550.88  2.440.89  3.291.50   GPT-2-generated stories  Human 3.560.91 0.1019.5 3.191.07 0.1417 2.591.29 -0.213.5 2.381.40 -0.038.5  T0 2.441.49 0.05g 3.021.51 0.076 3.001.59 0.166 2.821.61 0.046  curie 3.230.51 0.0138 2.820.45 0.0250 2.860.37 0.0965.5 3.010.43 0.1161  davinci 4.070.35 0.3545.5 4.260.45 0.4242 3.840.42 0.5262 4.020.74 0.6942.5  ChatGPT 2.980.76  2.480.71  1.590.67  2.021.21"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1150
                },
                {
                    "x": 2199,
                    "y": 1150
                },
                {
                    "x": 2199,
                    "y": 1358
                },
                {
                    "x": 286,
                    "y": 1358
                }
            ],
            "category": "caption",
            "html": "<caption id='41' style='font-size:14px'>Table 1: LLM evaluation and human evaluation results of human-written stories and GPT-2-generated stories. For<br>each evaluated attribute, we report its mean Likert scale and the standard deviation. We also report the inter-<br>annotator agreement (IAA) among three annotators using Krippendorff's a. The subscript in the IAA column (%)<br>is used to denote the percentage of the stories where all three annotators exactly agree on a rating.</caption>",
            "id": 41,
            "page": 4,
            "text": "Table 1: LLM evaluation and human evaluation results of human-written stories and GPT-2-generated stories. For each evaluated attribute, we report its mean Likert scale and the standard deviation. We also report the interannotator agreement (IAA) among three annotators using Krippendorff's a. The subscript in the IAA column (%) is used to denote the percentage of the stories where all three annotators exactly agree on a rating."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1444
                },
                {
                    "x": 1212,
                    "y": 1444
                },
                {
                    "x": 1212,
                    "y": 2231
                },
                {
                    "x": 286,
                    "y": 2231
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>are two InstructGPT models, and the latter is the<br>stronger model; we will use InstructGPT to refer to<br>these two models. We query the InstructGPT using<br>the official API provided by OpenAI. We use nu-<br>cleus sampling with p = 0.9 to generate the answer<br>from T0 and InstructGPTs. We sample three an-<br>swers from LLMs to stimulate the result of asking<br>the model to rate the same story three times. We<br>query ChatGPT using the user interface recently<br>released by OpenAI. Unlike InstructGPT, we can-<br>not control the parameters used for generating the<br>response from ChatGPT. Because ChatGPT limits<br>the maximum number of queries per user, we only<br>sample one response for each question.</p>",
            "id": 42,
            "page": 4,
            "text": "are two InstructGPT models, and the latter is the stronger model; we will use InstructGPT to refer to these two models. We query the InstructGPT using the official API provided by OpenAI. We use nucleus sampling with p = 0.9 to generate the answer from T0 and InstructGPTs. We sample three answers from LLMs to stimulate the result of asking the model to rate the same story three times. We query ChatGPT using the user interface recently released by OpenAI. Unlike InstructGPT, we cannot control the parameters used for generating the response from ChatGPT. Because ChatGPT limits the maximum number of queries per user, we only sample one response for each question."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2241
                },
                {
                    "x": 1213,
                    "y": 2241
                },
                {
                    "x": 1213,
                    "y": 2972
                },
                {
                    "x": 286,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>For human evaluation, we do not use the<br>commonly used AMT for human evaluation be-<br>cause Karpinska et al. (2021) has already shown<br>that the results obtained using AMT are highly<br>questionable. Following the recommendation of<br>the prior works, we hire three certified English<br>teachers using an online freelancer platform, Up-<br>Work. Teachers are familiar with evaluating the<br>essays of students, making them the expert evalua-<br>tors in our task. The details about recruiting human<br>evaluators are in Appendix C.1. Each LLM and<br>each English teacher rates the 200 human-written<br>stories and 200 GPT-2-generated stories.</p>",
            "id": 43,
            "page": 4,
            "text": "For human evaluation, we do not use the commonly used AMT for human evaluation because Karpinska  (2021) has already shown that the results obtained using AMT are highly questionable. Following the recommendation of the prior works, we hire three certified English teachers using an online freelancer platform, UpWork. Teachers are familiar with evaluating the essays of students, making them the expert evaluators in our task. The details about recruiting human evaluators are in Appendix C.1. Each LLM and each English teacher rates the 200 human-written stories and 200 GPT-2-generated stories."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3031
                },
                {
                    "x": 786,
                    "y": 3031
                },
                {
                    "x": 786,
                    "y": 3083
                },
                {
                    "x": 288,
                    "y": 3083
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:20px'>3.3 Experiment Results</p>",
            "id": 44,
            "page": 4,
            "text": "3.3 Experiment Results"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3114
                },
                {
                    "x": 1212,
                    "y": 3114
                },
                {
                    "x": 1212,
                    "y": 3229
                },
                {
                    "x": 287,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>The LLM evaluation and human evaluation results<br>of open-ended story generation are presented in</p>",
            "id": 45,
            "page": 4,
            "text": "The LLM evaluation and human evaluation results of open-ended story generation are presented in"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1443
                },
                {
                    "x": 2195,
                    "y": 1443
                },
                {
                    "x": 2195,
                    "y": 1889
                },
                {
                    "x": 1268,
                    "y": 1889
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:16px'>Table 1. We report the mean and standard deviation<br>of the Likert scores obtained from LLM evaluation<br>and human evaluation and show the inter-annotator<br>agreement (IAA) using two different metrics: (1)<br>the Krippendorff's a, and (2) the percentage of the<br>stories where three evaluators give the exact same<br>rating.2 The main observations from Table 1 are<br>discussed as follows.</p>",
            "id": 46,
            "page": 4,
            "text": "Table 1. We report the mean and standard deviation of the Likert scores obtained from LLM evaluation and human evaluation and show the inter-annotator agreement (IAA) using two different metrics: (1) the Krippendorff's a, and (2) the percentage of the stories where three evaluators give the exact same rating.2 The main observations from Table 1 are discussed as follows."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1898
                },
                {
                    "x": 2197,
                    "y": 1898
                },
                {
                    "x": 2197,
                    "y": 2680
                },
                {
                    "x": 1268,
                    "y": 2680
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:18px'>Expert human evaluators prefer human-<br>written stories: Human evaluation result serves<br>as some kind of ground truth of the LLM evalua-<br>tion. For all four attributes, teachers rate the human-<br>written stories higher than GPT-2-generated stories.<br>This indicates that experts are able to distinguish<br>the quality difference between model-generated<br>stories and human-written stories. Based on the<br>IAA, we also find that the agreements among ex-<br>perts are lower on GPT-2-generated texts and on<br>the likability. This shows that experts tend to have<br>less agreement on model-generated texts and on a<br>subjective attribute (likability), agreeing with the<br>results in Karpinska et al. (2021).</p>",
            "id": 47,
            "page": 4,
            "text": "Expert human evaluators prefer humanwritten stories: Human evaluation result serves as some kind of ground truth of the LLM evaluation. For all four attributes, teachers rate the humanwritten stories higher than GPT-2-generated stories. This indicates that experts are able to distinguish the quality difference between model-generated stories and human-written stories. Based on the IAA, we also find that the agreements among experts are lower on GPT-2-generated texts and on the likability. This shows that experts tend to have less agreement on model-generated texts and on a subjective attribute (likability), agreeing with the results in Karpinska  (2021)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2687
                },
                {
                    "x": 2196,
                    "y": 2687
                },
                {
                    "x": 2196,
                    "y": 3023
                },
                {
                    "x": 1268,
                    "y": 3023
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:18px'>T0 and text-curie-001 do not show clear<br>preference toward human-written stories: For<br>T0, we can see that T0 rates human-written sto-<br>ries higher than GPT-2-generated stories on gram-<br>matically, likability, and relevance. However, the<br>rating differences between the human-written and</p>",
            "id": 48,
            "page": 4,
            "text": "T0 and text-curie-001 do not show clear preference toward human-written stories: For T0, we can see that T0 rates human-written stories higher than GPT-2-generated stories on grammatically, likability, and relevance. However, the rating differences between the human-written and"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 3056
                },
                {
                    "x": 2193,
                    "y": 3056
                },
                {
                    "x": 2193,
                    "y": 3223
                },
                {
                    "x": 1268,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:14px'>2The three evaluators in human evaluation are the three<br>English teachers. In LLM evaluation, we sample the answer<br>generated by LLM three times as an analogy to three different<br>evaluators.</p>",
            "id": 49,
            "page": 4,
            "text": "2The three evaluators in human evaluation are the three English teachers. In LLM evaluation, we sample the answer generated by LLM three times as an analogy to three different evaluators."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 301
                },
                {
                    "x": 1213,
                    "y": 301
                },
                {
                    "x": 1213,
                    "y": 1374
                },
                {
                    "x": 286,
                    "y": 1374
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>model-generated stories do not achieve statistical<br>significance for grammaticality and relevance; the<br>p-value obtained by Welch's t-test is much larger<br>than 0.05. The result of text-curie-001 is sim-<br>ilar to TO: text-curie-001 do not rate human-<br>written stories higher than model-generated stories.<br>It can also be observed that for T0, the IAA in terms<br>of the percentage of exact agreement among three<br>different sampled answers is overall very low. This<br>indicates that given the same sample, T0 is likely<br>to give a different rating for the three sampled an-<br>swers. The result implies that T0 does not assign<br>a high probability to a specific rating, SO different<br>scores are all likely to be sampled. This shows that<br>even if LLMs are specifically fine-tuned to better<br>perform zero-shot in-context learning and trained<br>to better follow human instructions, these do not<br>make them capable of assessing open-ended story<br>generation as human experts can.</p>",
            "id": 50,
            "page": 5,
            "text": "model-generated stories do not achieve statistical significance for grammaticality and relevance; the p-value obtained by Welch's t-test is much larger than 0.05. The result of text-curie-001 is similar to TO: text-curie-001 do not rate humanwritten stories higher than model-generated stories. It can also be observed that for T0, the IAA in terms of the percentage of exact agreement among three different sampled answers is overall very low. This indicates that given the same sample, T0 is likely to give a different rating for the three sampled answers. The result implies that T0 does not assign a high probability to a specific rating, SO different scores are all likely to be sampled. This shows that even if LLMs are specifically fine-tuned to better perform zero-shot in-context learning and trained to better follow human instructions, these do not make them capable of assessing open-ended story generation as human experts can."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1397
                },
                {
                    "x": 1213,
                    "y": 1397
                },
                {
                    "x": 1213,
                    "y": 2867
                },
                {
                    "x": 286,
                    "y": 2867
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:16px'>text-davinci-003 shows clear preference<br>toward human-written stories just like En-<br>glish teachers: text-davinci-003 rates human-<br>written stories much higher than model-generated<br>stories on all four attributes, which is in accor-<br>dance with the result produced by human experts.<br>By Welch's t-test, we find that the higher ratings<br>on human-written stories are all statistically sig-<br>nificant. In prior work, researchers have found<br>that workers recruited on AMT cannot distinguish<br>between human-written and GPT-2-generated sto-<br>ries (Karpinska et al., 2021); combining their result<br>with our result, we can see that LLM evaluation<br>using text-davinci -003 yields more convincing<br>results than using human evaluation on AMT for<br>open-ended story generation. The results show that<br>text-davinci-003 model can perform basic eval-<br>uations such as checking for grammatical errors in<br>stories. Additionally, the model excels in assessing<br>the relevance of a story to a prompt, which involves<br>more complex reasoning over the connection be-<br>tween the two. We also find the Krippendorff's<br>a of text-davinci-003 is much higher than T0<br>and text-curie-001, indicating that the rating by<br>text-davinci-003 is more consistent among dif-<br>ferent samplings of the generated answers.</p>",
            "id": 51,
            "page": 5,
            "text": "text-davinci-003 shows clear preference toward human-written stories just like English teachers: text-davinci-003 rates humanwritten stories much higher than model-generated stories on all four attributes, which is in accordance with the result produced by human experts. By Welch's t-test, we find that the higher ratings on human-written stories are all statistically significant. In prior work, researchers have found that workers recruited on AMT cannot distinguish between human-written and GPT-2-generated stories (Karpinska , 2021); combining their result with our result, we can see that LLM evaluation using text-davinci -003 yields more convincing results than using human evaluation on AMT for open-ended story generation. The results show that text-davinci-003 model can perform basic evaluations such as checking for grammatical errors in stories. Additionally, the model excels in assessing the relevance of a story to a prompt, which involves more complex reasoning over the connection between the two. We also find the Krippendorff's a of text-davinci-003 is much higher than T0 and text-curie-001, indicating that the rating by text-davinci-003 is more consistent among different samplings of the generated answers."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2889
                },
                {
                    "x": 1213,
                    "y": 2889
                },
                {
                    "x": 1213,
                    "y": 3230
                },
                {
                    "x": 288,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:18px'>ChatGPT rates like human experts and can<br>explain its own decision well: ChatGPT also<br>shows a clear preference for human-written stories,<br>and the preference toward human written-stories is<br>statistically significant. When we query ChatGPT<br>using the OpenAI user interface, we find several</p>",
            "id": 52,
            "page": 5,
            "text": "ChatGPT rates like human experts and can explain its own decision well: ChatGPT also shows a clear preference for human-written stories, and the preference toward human written-stories is statistically significant. When we query ChatGPT using the OpenAI user interface, we find several"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 301
                },
                {
                    "x": 2196,
                    "y": 301
                },
                {
                    "x": 2196,
                    "y": 974
                },
                {
                    "x": 1267,
                    "y": 974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:16px'>interesting observations: (1): ChatGPT is able to<br>provide a detailed explanation of why it gives a<br>certain rating. It will reference the sentences in<br>the stories and prompts to support its rating. (2):<br>ChatGPT sometimes refuses to rate the likability of<br>the story because \"I am an AI and I do not have the<br>ability to experience enjoyment\". In such cases, we<br>regenerate the response until it gives a rating. (3):<br>we find that ChatGPT tends to rate low likability on<br>violent or impolite stories, which is likely because<br>it is trained to provide safe and unharmful replies,<br>making ChatGPT dislike brutal and profane stories.</p>",
            "id": 53,
            "page": 5,
            "text": "interesting observations: (1): ChatGPT is able to provide a detailed explanation of why it gives a certain rating. It will reference the sentences in the stories and prompts to support its rating. (2): ChatGPT sometimes refuses to rate the likability of the story because \"I am an AI and I do not have the ability to experience enjoyment\". In such cases, we regenerate the response until it gives a rating. (3): we find that ChatGPT tends to rate low likability on violent or impolite stories, which is likely because it is trained to provide safe and unharmful replies, making ChatGPT dislike brutal and profane stories."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 979
                },
                {
                    "x": 2196,
                    "y": 979
                },
                {
                    "x": 2196,
                    "y": 1708
                },
                {
                    "x": 1267,
                    "y": 1708
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:16px'>Experts mostly agree with the ratings and ex-<br>planations of ChatGPT: We randomly select the<br>answers on four stories by ChatGPT and ask the<br>English teachers if they agree with the reasoning<br>and rating of ChatGPT3. The teachers mostly agree<br>with the rating and consider the explanation from<br>ChatGPT reasonable. Interestingly, one teacher<br>told us she cannot agree with ChatGPT's rating on<br>grammaticality because ChatGPT considers punc-<br>tuation errors as grammar errors, but she does not<br>think punctuation errors are grammar errors. This<br>shows that individuals have their own standards for<br>ratings and this is also the case for LLMs.</p>",
            "id": 54,
            "page": 5,
            "text": "Experts mostly agree with the ratings and explanations of ChatGPT: We randomly select the answers on four stories by ChatGPT and ask the English teachers if they agree with the reasoning and rating of ChatGPT3. The teachers mostly agree with the rating and consider the explanation from ChatGPT reasonable. Interestingly, one teacher told us she cannot agree with ChatGPT's rating on grammaticality because ChatGPT considers punctuation errors as grammar errors, but she does not think punctuation errors are grammar errors. This shows that individuals have their own standards for ratings and this is also the case for LLMs."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1714
                },
                {
                    "x": 2196,
                    "y": 1714
                },
                {
                    "x": 2196,
                    "y": 2503
                },
                {
                    "x": 1267,
                    "y": 2503
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:16px'>text-davinci-003 tends to give higher rat-<br>ings and ChatGPT is the opposite: The rating on<br>the same attribute of the same type of text tends to<br>be higher for text-davinci-003 compared with<br>human rating; contrarily, ChatGPT is more fastid-<br>ious and prone to give lower scores. This shows<br>that different LLMs have distinct tendencies re-<br>garding the rating. While the absolute values of<br>the scores rated by text-davinci-003, ChatGPT,<br>and human differ, they all rate human-written texts<br>higher than GPT-2-generated stories. The absolute<br>number reflects the bias or belief of the evaluator;<br>as long as one uses the same evaluators to assess<br>different systems, the comparison is meaningful.</p>",
            "id": 55,
            "page": 5,
            "text": "text-davinci-003 tends to give higher ratings and ChatGPT is the opposite: The rating on the same attribute of the same type of text tends to be higher for text-davinci-003 compared with human rating; contrarily, ChatGPT is more fastidious and prone to give lower scores. This shows that different LLMs have distinct tendencies regarding the rating. While the absolute values of the scores rated by text-davinci-003, ChatGPT, and human differ, they all rate human-written texts higher than GPT-2-generated stories. The absolute number reflects the bias or belief of the evaluator; as long as one uses the same evaluators to assess different systems, the comparison is meaningful."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2530
                },
                {
                    "x": 2092,
                    "y": 2530
                },
                {
                    "x": 2092,
                    "y": 2697
                },
                {
                    "x": 1268,
                    "y": 2697
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:20px'>3.3.1 Does LLM and Human Evaluators<br>Agree on the Rating of Individual<br>Stories?</p>",
            "id": 56,
            "page": 5,
            "text": "3.3.1 Does LLM and Human Evaluators Agree on the Rating of Individual Stories?"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2713
                },
                {
                    "x": 2195,
                    "y": 2713
                },
                {
                    "x": 2195,
                    "y": 3107
                },
                {
                    "x": 1267,
                    "y": 3107
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:16px'>We have found in Table 1 that the ratings of<br>text-davinci-003 and ChatGPT show a strong<br>preference toward human-written stories just like<br>English teachers. However, it is unclear whether<br>those LLMs agree with the teachers' rating on each<br>individual story. Precisely, when English teach-<br>ers rate a story higher, do LLMs also rate the</p>",
            "id": 57,
            "page": 5,
            "text": "We have found in Table 1 that the ratings of text-davinci-003 and ChatGPT show a strong preference toward human-written stories just like English teachers. However, it is unclear whether those LLMs agree with the teachers' rating on each individual story. Precisely, when English teachers rate a story higher, do LLMs also rate the"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 3138
                },
                {
                    "x": 2190,
                    "y": 3138
                },
                {
                    "x": 2190,
                    "y": 3226
                },
                {
                    "x": 1270,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:14px'>3We do not tell the teachers these are responses from an AI<br>model. See the stories and teachers' replies in Appendix C.3.2.</p>",
            "id": 58,
            "page": 5,
            "text": "3We do not tell the teachers these are responses from an AI model. See the stories and teachers' replies in Appendix C.3.2."
        },
        {
            "bounding_box": [
                {
                    "x": 401,
                    "y": 286
                },
                {
                    "x": 1097,
                    "y": 286
                },
                {
                    "x": 1097,
                    "y": 599
                },
                {
                    "x": 401,
                    "y": 599
                }
            ],
            "category": "table",
            "html": "<table id='59' style='font-size:20px'><tr><td>Story Writer</td><td>Human</td><td>GPT-2</td></tr><tr><td>Grammaticality</td><td>0.14</td><td>0.12</td></tr><tr><td>Cohesiveness</td><td>0.18</td><td>0.14</td></tr><tr><td>Likability</td><td>0.19</td><td>0.22</td></tr><tr><td>Relevance</td><td>0.38</td><td>0.43</td></tr></table>",
            "id": 59,
            "page": 6,
            "text": "Story Writer Human GPT-2  Grammaticality 0.14 0.12  Cohesiveness 0.18 0.14  Likability 0.19 0.22  Relevance 0.38"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 633
                },
                {
                    "x": 1212,
                    "y": 633
                },
                {
                    "x": 1212,
                    "y": 730
                },
                {
                    "x": 289,
                    "y": 730
                }
            ],
            "category": "caption",
            "html": "<caption id='60' style='font-size:14px'>Table 2: The Kendall's T correlation coefficient be-<br>tween English teachers and text-davinci -003.</caption>",
            "id": 60,
            "page": 6,
            "text": "Table 2: The Kendall's T correlation coefficient between English teachers and text-davinci -003."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 827
                },
                {
                    "x": 1213,
                    "y": 827
                },
                {
                    "x": 1213,
                    "y": 2067
                },
                {
                    "x": 286,
                    "y": 2067
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>story higher? To answer this question, we calcu-<br>late Kendall's T correlation coefficient between the<br>ratings of text-davinci-003 and English teach-<br>ers. We choose to use the correlation coefficient<br>instead of the inter-annotator agreement score be-<br>cause IAA mainly cares if two annotators agree<br>on the exact ratings, while the correlation coef-<br>ficient focus on the question: \"when annotator<br>A rates one story higher, does annotator B also<br>rate the story higher?\" (Amidei et al., 2019). We<br>calculate Kendall's T for four rating attributes as<br>follows: For each story and each rating attribute,<br>we calculate the average rating of the three En-<br>glish teachers and calculate the average rating of<br>the three scores given by the text-davinci-003<br>(which is obtained from three independent sam-<br>ples). For each attribute, we collect the average<br>rating of teachers into a vector A E IR200 where<br>,<br>each entry is the average rating of a story; likewise,<br>we construct a vector B E IR200 for the average<br>ratings of davinci. Next, we calculate Kendall's<br>T correlation coefficient between A and B.</p>",
            "id": 61,
            "page": 6,
            "text": "story higher? To answer this question, we calculate Kendall's T correlation coefficient between the ratings of text-davinci-003 and English teachers. We choose to use the correlation coefficient instead of the inter-annotator agreement score because IAA mainly cares if two annotators agree on the exact ratings, while the correlation coefficient focus on the question: \"when annotator A rates one story higher, does annotator B also rate the story higher?\" (Amidei , 2019). We calculate Kendall's T for four rating attributes as follows: For each story and each rating attribute, we calculate the average rating of the three English teachers and calculate the average rating of the three scores given by the text-davinci-003 (which is obtained from three independent samples). For each attribute, we collect the average rating of teachers into a vector A E IR200 where , each entry is the average rating of a story; likewise, we construct a vector B E IR200 for the average ratings of davinci. Next, we calculate Kendall's T correlation coefficient between A and B."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2073
                },
                {
                    "x": 1213,
                    "y": 2073
                },
                {
                    "x": 1213,
                    "y": 2974
                },
                {
                    "x": 286,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:16px'>The Kendall's T between teacher ratings and<br>LLM ratings is shown in Table 2.4 We find that<br>for all four attributes and for both human-written<br>and GPT-2-generated stories, we observe weak<br>to strong positive correlations between teachers'<br>ratings and text-davinci-003's ratings. All the<br>correlations have p-values less than 0.05. Hence,<br>we can say that when teachers rate a story higher,<br>text-davinci-003 also rates it higher to a certain<br>extent. We also observe that Kendall's T for differ-<br>ent attributes are quite different: relevance has the<br>strongest correlation while grammaticality has the<br>weakest correlation. This is possibly because rating<br>relevance is rather straightforward, which requires<br>checking if the content in the prompt is mentioned<br>in the story. On the contrary, what should be con-</p>",
            "id": 62,
            "page": 6,
            "text": "The Kendall's T between teacher ratings and LLM ratings is shown in Table 2.4 We find that for all four attributes and for both human-written and GPT-2-generated stories, we observe weak to strong positive correlations between teachers' ratings and text-davinci-003's ratings. All the correlations have p-values less than 0.05. Hence, we can say that when teachers rate a story higher, text-davinci-003 also rates it higher to a certain extent. We also observe that Kendall's T for different attributes are quite different: relevance has the strongest correlation while grammaticality has the weakest correlation. This is possibly because rating relevance is rather straightforward, which requires checking if the content in the prompt is mentioned in the story. On the contrary, what should be con-"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3012
                },
                {
                    "x": 1212,
                    "y": 3012
                },
                {
                    "x": 1212,
                    "y": 3225
                },
                {
                    "x": 287,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:14px'>4When interpreting Kendall's T, |�| E [0, 0.1) is consid-<br>ered as very weak correlation, |�| E [0.1, 0.2) is considered<br>as weak correlation, |T| E [0.2, 0.3) is considered as moder-<br>ate correlation, and |�| E [0.3, 1.0] is considered as strong<br>correlation (Botsch, 2011).</p>",
            "id": 63,
            "page": 6,
            "text": "4When interpreting Kendall's T, |�| E [0, 0.1) is considered as very weak correlation, |�| E [0.1, 0.2) is considered as weak correlation, |T| E [0.2, 0.3) is considered as moderate correlation, and |�| E [0.3, 1.0] is considered as strong correlation (Botsch, 2011)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 304
                },
                {
                    "x": 2196,
                    "y": 304
                },
                {
                    "x": 2196,
                    "y": 809
                },
                {
                    "x": 1268,
                    "y": 809
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:16px'>sidered when rating grammaticality is not clearly<br>stated in our instructions, SO the LLM may have<br>a different rubric compared with English teachers.<br>We also calculate the average Kendall's T between<br>a pair of English teachers, and we find a weak cor-<br>relation on grammaticality between the rating of<br>two teachers, while the correlation of the rating on<br>relevance is much stronger. The result is presented<br>in Table 6 in Appendix.</p>",
            "id": 64,
            "page": 6,
            "text": "sidered when rating grammaticality is not clearly stated in our instructions, SO the LLM may have a different rubric compared with English teachers. We also calculate the average Kendall's T between a pair of English teachers, and we find a weak correlation on grammaticality between the rating of two teachers, while the correlation of the rating on relevance is much stronger. The result is presented in Table 6 in Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 845
                },
                {
                    "x": 2154,
                    "y": 845
                },
                {
                    "x": 2154,
                    "y": 897
                },
                {
                    "x": 1269,
                    "y": 897
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:16px'>3.3.2 Variance due to Different Instructions</p>",
            "id": 65,
            "page": 6,
            "text": "3.3.2 Variance due to Different Instructions"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 913
                },
                {
                    "x": 2196,
                    "y": 913
                },
                {
                    "x": 2196,
                    "y": 2321
                },
                {
                    "x": 1268,
                    "y": 2321
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:16px'>LLMs have been shown to be sensitive to the in-<br>structions used to query the LLM sometimes (Zhao<br>et al., 2021; Sanh et al., 2022). To investigate<br>how varying the task instructions and questions can<br>affect the LLM evaluation result for open-ended<br>story generation, we change the instructions and<br>questions and see how the LLM evaluation result<br>changes. We experiment with two different instruc-<br>tions by changing the instruction or question in<br>Figure 1: (1) We prepend the sentence, \"(You are a<br>human worker hired to rate the story fragment.)\",<br>in front of the task instruction in Figure 1. We<br>try to provide the LLM a persona for it to better<br>understand its role. This is inspired by previous<br>work that reported GPT-3 can yield different results<br>when giving them a persona (Zeng et al., 2022). (2)<br>We ask the LLMs to explain their decision by ap-<br>pending the following sentence after the question:<br>Please also explain your decision. Here, we would<br>like to know if LLM will rate the stories differently<br>when they are asked to justify their decision. We<br>use text-davinci-003 as the LLM in this exper-<br>iment since it achieves similar results with expert<br>human evaluation based on Table 1, and it is more<br>accessible than ChatGPT.</p>",
            "id": 66,
            "page": 6,
            "text": "LLMs have been shown to be sensitive to the instructions used to query the LLM sometimes (Zhao , 2021; Sanh , 2022). To investigate how varying the task instructions and questions can affect the LLM evaluation result for open-ended story generation, we change the instructions and questions and see how the LLM evaluation result changes. We experiment with two different instructions by changing the instruction or question in Figure 1: (1) We prepend the sentence, \"(You are a human worker hired to rate the story fragment.)\", in front of the task instruction in Figure 1. We try to provide the LLM a persona for it to better understand its role. This is inspired by previous work that reported GPT-3 can yield different results when giving them a persona (Zeng , 2022). (2) We ask the LLMs to explain their decision by appending the following sentence after the question: Please also explain your decision. Here, we would like to know if LLM will rate the stories differently when they are asked to justify their decision. We use text-davinci-003 as the LLM in this experiment since it achieves similar results with expert human evaluation based on Table 1, and it is more accessible than ChatGPT."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2326
                },
                {
                    "x": 2196,
                    "y": 2326
                },
                {
                    "x": 2196,
                    "y": 3228
                },
                {
                    "x": 1267,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:16px'>The results are shown in the upper block in Ta-<br>ble 3. We observe that for grammaticality and<br>cohesiveness, the scores obtained from different in-<br>structions are quite close: the rating changes due to<br>different instructions are less than 0.1. For the other<br>two attributes, the score changes are slightly larger<br>but still in the range of 0.25. Despite that there<br>are small variations due to different instructions,<br>these variances still do not change the conclusion<br>that \"LLM rates human-written stories higher than<br>GPT-2-generated stories\". Thus, different instruc-<br>tions do not change the relative ranking of GPT-2-<br>generated and human-written stories. In summary,<br>as long as the stories are evaluated using the same<br>instructions using LLM evaluation, such evaluation<br>and comparison are meaningful.</p>",
            "id": 67,
            "page": 6,
            "text": "The results are shown in the upper block in Table 3. We observe that for grammaticality and cohesiveness, the scores obtained from different instructions are quite close: the rating changes due to different instructions are less than 0.1. For the other two attributes, the score changes are slightly larger but still in the range of 0.25. Despite that there are small variations due to different instructions, these variances still do not change the conclusion that \"LLM rates human-written stories higher than GPT-2-generated stories\". Thus, different instructions do not change the relative ranking of GPT-2generated and human-written stories. In summary, as long as the stories are evaluated using the same instructions using LLM evaluation, such evaluation and comparison are meaningful."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 285
                },
                {
                    "x": 2182,
                    "y": 285
                },
                {
                    "x": 2182,
                    "y": 953
                },
                {
                    "x": 294,
                    "y": 953
                }
            ],
            "category": "table",
            "html": "<table id='68' style='font-size:22px'><tr><td rowspan=\"2\">Setup</td><td colspan=\"2\">Grammaticality</td><td colspan=\"2\">Cohesiveness</td><td colspan=\"2\">Likability</td><td colspan=\"2\">Relevance</td></tr><tr><td>Human</td><td>GPT-2</td><td>Human</td><td>GPT-2</td><td>Human</td><td>GPT-2</td><td>Human</td><td>GPT-2</td></tr><tr><td colspan=\"9\">Different instructions (Section 3.3.2)</td></tr><tr><td>Original</td><td>4.220.38</td><td>4.070.35</td><td>4.540.45</td><td>4.260.45</td><td>3.990.38</td><td>3.840.42</td><td>4.400.79</td><td>4.020.74</td></tr><tr><td>(1) + persona</td><td>4.290.45</td><td>4.010.45</td><td>4.600.49</td><td>4.270.50</td><td>4.050.39</td><td>3.870.39</td><td>4.550.70</td><td>4.250.77</td></tr><tr><td>(2) + explain</td><td>4.240.42</td><td>4.050.25</td><td>4.610.49</td><td>4.320.51</td><td>4.150.44</td><td>3.980.34</td><td>4.350.75</td><td>4.030.56</td></tr><tr><td colspan=\"9\">Different sampling temperature T (Section 3.3.3)</td></tr><tr><td>T = 1.0</td><td>4.220.38</td><td>4.070.35</td><td>4.540.45</td><td>4.260.45</td><td>3.990.38</td><td>3.840.42</td><td>4.400.79</td><td>4.020.74</td></tr><tr><td>T = 0.7</td><td>4.180.35</td><td>4.060.33</td><td>4.520.48</td><td>4.230.43</td><td>3.960.34</td><td>3.820.42</td><td>4.360.77</td><td>3.950.72</td></tr><tr><td>T = 0.3</td><td>4.130.33</td><td>3.990.25</td><td>4.480.49</td><td>4.140.39</td><td>3.950.26</td><td>3.820.41</td><td>4.340.75</td><td>3.930.67</td></tr><tr><td>T = 0</td><td>4.070.27</td><td>3.990.18</td><td>4.490.50</td><td>4.090.34</td><td>3.950.25</td><td>3.820.40</td><td>4.320.75</td><td>3.920.66</td></tr></table>",
            "id": 68,
            "page": 7,
            "text": "Setup Grammaticality Cohesiveness Likability Relevance  Human GPT-2 Human GPT-2 Human GPT-2 Human GPT-2  Different instructions (Section 3.3.2)  Original 4.220.38 4.070.35 4.540.45 4.260.45 3.990.38 3.840.42 4.400.79 4.020.74  (1) + persona 4.290.45 4.010.45 4.600.49 4.270.50 4.050.39 3.870.39 4.550.70 4.250.77  (2) + explain 4.240.42 4.050.25 4.610.49 4.320.51 4.150.44 3.980.34 4.350.75 4.030.56  Different sampling temperature T (Section 3.3.3)  T = 1.0 4.220.38 4.070.35 4.540.45 4.260.45 3.990.38 3.840.42 4.400.79 4.020.74  T = 0.7 4.180.35 4.060.33 4.520.48 4.230.43 3.960.34 3.820.42 4.360.77 3.950.72  T = 0.3 4.130.33 3.990.25 4.480.49 4.140.39 3.950.26 3.820.41 4.340.75 3.930.67  T = 0 4.070.27 3.990.18 4.490.50 4.090.34 3.950.25 3.820.40 4.320.75"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 980
                },
                {
                    "x": 2195,
                    "y": 980
                },
                {
                    "x": 2195,
                    "y": 1187
                },
                {
                    "x": 286,
                    "y": 1187
                }
            ],
            "category": "caption",
            "html": "<caption id='69' style='font-size:14px'>Table 3: Understanding the variance of LLM evaluation. For each of the four attributes evaluated, the left column<br>is the mean and standard deviation of human-written stories and the right column is those of GPT-2-generated<br>stories. The upper block shows the rating change due to different instructions (Section 3.3.2), and the lower block<br>is the result of changing the temperature T used for generating the LLM's output (Section 3.3.3).</caption>",
            "id": 69,
            "page": 7,
            "text": "Table 3: Understanding the variance of LLM evaluation. For each of the four attributes evaluated, the left column is the mean and standard deviation of human-written stories and the right column is those of GPT-2-generated stories. The upper block shows the rating change due to different instructions (Section 3.3.2), and the lower block is the result of changing the temperature T used for generating the LLM's output (Section 3.3.3)."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1273
                },
                {
                    "x": 1125,
                    "y": 1273
                },
                {
                    "x": 1125,
                    "y": 1382
                },
                {
                    "x": 290,
                    "y": 1382
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:18px'>3.3.3 Variance due to Different Sampling<br>Parameters</p>",
            "id": 70,
            "page": 7,
            "text": "3.3.3 Variance due to Different Sampling Parameters"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1413
                },
                {
                    "x": 1213,
                    "y": 1413
                },
                {
                    "x": 1213,
                    "y": 2146
                },
                {
                    "x": 287,
                    "y": 2146
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>When generating the answers from the LLM, we<br>must choose a set of hyperparameters for genera-<br>tion, including the temperature T and the proba-<br>bility p used in nucleus sampling (Holtzman et al.,<br>2020). To understand whether different sampling<br>parameters change the LLM evaluation result, we<br>modify the temperature used for sampling and keep<br>the p in nucleus sampling fixed to 0.9 when gen-<br>erating the answers from text-davinci-003. We<br>do not simultaneously vary T and p since the two<br>parameters are both used to control the diversity of<br>the output, it is enough to change only one of the<br>two parameters, as recommended in the OpenAI.</p>",
            "id": 71,
            "page": 7,
            "text": "When generating the answers from the LLM, we must choose a set of hyperparameters for generation, including the temperature T and the probability p used in nucleus sampling (Holtzman , 2020). To understand whether different sampling parameters change the LLM evaluation result, we modify the temperature used for sampling and keep the p in nucleus sampling fixed to 0.9 when generating the answers from text-davinci-003. We do not simultaneously vary T and p since the two parameters are both used to control the diversity of the output, it is enough to change only one of the two parameters, as recommended in the OpenAI."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2156
                },
                {
                    "x": 1214,
                    "y": 2156
                },
                {
                    "x": 1214,
                    "y": 3230
                },
                {
                    "x": 286,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:16px'>The results of varying T from 1 to 0 are shown<br>in the lower block in Table 3. We observe an inter-<br>esting trend as T varies from 1 to 0: the average<br>rating slightly drops in most cases. Considering<br>that T = 0is simply argmax sampling, the result in-<br>dicates that the response of the LLM with the high-<br>est probability tends to give lower scores. Despite<br>this interesting trend, the LLM consistently rates<br>human-written stories higher than GPT-2-generated<br>stories. While not shown in Table 3, we find that<br>the IAA increases as the temperature decreases.<br>This is expected since lower temperature means<br>less diversity during the LLM sampling, causing<br>the sampled ratings to agree more closely. In sum-<br>mary, changing the instructions and temperatures<br>can slightly change the absolute value of the rating<br>given by LLM but does not change the LLM's pref-<br>erence on human-written stories. The overall result<br>in this section shows that LLM evaluation is useful</p>",
            "id": 72,
            "page": 7,
            "text": "The results of varying T from 1 to 0 are shown in the lower block in Table 3. We observe an interesting trend as T varies from 1 to 0: the average rating slightly drops in most cases. Considering that T = 0is simply argmax sampling, the result indicates that the response of the LLM with the highest probability tends to give lower scores. Despite this interesting trend, the LLM consistently rates human-written stories higher than GPT-2-generated stories. While not shown in Table 3, we find that the IAA increases as the temperature decreases. This is expected since lower temperature means less diversity during the LLM sampling, causing the sampled ratings to agree more closely. In summary, changing the instructions and temperatures can slightly change the absolute value of the rating given by LLM but does not change the LLM's preference on human-written stories. The overall result in this section shows that LLM evaluation is useful"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1275
                },
                {
                    "x": 2053,
                    "y": 1275
                },
                {
                    "x": 2053,
                    "y": 1328
                },
                {
                    "x": 1268,
                    "y": 1328
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:20px'>in evaluating open-ended story generation.</p>",
            "id": 73,
            "page": 7,
            "text": "in evaluating open-ended story generation."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1394
                },
                {
                    "x": 2134,
                    "y": 1394
                },
                {
                    "x": 2134,
                    "y": 1453
                },
                {
                    "x": 1268,
                    "y": 1453
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:20px'>4 Example Task 2: Adversarial Attack</p>",
            "id": 74,
            "page": 7,
            "text": "4 Example Task 2: Adversarial Attack"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1501
                },
                {
                    "x": 2191,
                    "y": 1501
                },
                {
                    "x": 2191,
                    "y": 1610
                },
                {
                    "x": 1269,
                    "y": 1610
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:14px'>As another application, we use LLM evaluation to<br>rate the texts generated by adversarial attacks.</p>",
            "id": 75,
            "page": 7,
            "text": "As another application, we use LLM evaluation to rate the texts generated by adversarial attacks."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1675
                },
                {
                    "x": 1732,
                    "y": 1675
                },
                {
                    "x": 1732,
                    "y": 1726
                },
                {
                    "x": 1267,
                    "y": 1726
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>4.1 Task Introduction</p>",
            "id": 76,
            "page": 7,
            "text": "4.1 Task Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1759
                },
                {
                    "x": 2197,
                    "y": 1759
                },
                {
                    "x": 2197,
                    "y": 3230
                },
                {
                    "x": 1267,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:16px'>Given a trained text classifier and a benign (non-<br>adversarial) testing sample that the text classifier<br>can correctly classify, an adversarial attack aims to<br>craft an adversarial sample that makes the classifier<br>make a wrong prediction. A special type of adver-<br>sarial attack is called synonym substitution attacks<br>(SSAs) (Alzantot et al., 2018), where the adversar-<br>ial sample is created by replacing some words with<br>their synonyms in the benign sample. By replacing<br>words with their synonym, the semantics of the<br>benign sample should be preserved in the adversar-<br>ial sample and make the adversarial perturbation<br>imperceptible to humans. While conceptually rea-<br>sonable, it has recently been shown that many SSAs<br>often yield ungrammatical and unnatural adversar-<br>ial samples that significantly change the meaning<br>of the benign sample (Hauser et al., 2021; Chiang<br>and Lee, 2022). To evaluate the quality of adver-<br>sarial samples, human evaluation is invaluable and<br>widely used in prior works. In our experiment here,<br>we would like to see whether the LLMs can rate<br>the quality of adversarial samples like human ex-<br>perts. Adversarial samples are not normal texts, SO<br>the LLMs may not have seen such abnormal inputs<br>during training. It would be interesting to know<br>how LLMs rate these adversarial samples.</p>",
            "id": 77,
            "page": 7,
            "text": "Given a trained text classifier and a benign (nonadversarial) testing sample that the text classifier can correctly classify, an adversarial attack aims to craft an adversarial sample that makes the classifier make a wrong prediction. A special type of adversarial attack is called synonym substitution attacks (SSAs) (Alzantot , 2018), where the adversarial sample is created by replacing some words with their synonyms in the benign sample. By replacing words with their synonym, the semantics of the benign sample should be preserved in the adversarial sample and make the adversarial perturbation imperceptible to humans. While conceptually reasonable, it has recently been shown that many SSAs often yield ungrammatical and unnatural adversarial samples that significantly change the meaning of the benign sample (Hauser , 2021; Chiang and Lee, 2022). To evaluate the quality of adversarial samples, human evaluation is invaluable and widely used in prior works. In our experiment here, we would like to see whether the LLMs can rate the quality of adversarial samples like human experts. Adversarial samples are not normal texts, SO the LLMs may not have seen such abnormal inputs during training. It would be interesting to know how LLMs rate these adversarial samples."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 287
                },
                {
                    "x": 1233,
                    "y": 287
                },
                {
                    "x": 1233,
                    "y": 654
                },
                {
                    "x": 288,
                    "y": 654
                }
            ],
            "category": "table",
            "html": "<table id='78' style='font-size:16px'><tr><td></td><td colspan=\"2\">Human evaluate</td><td colspan=\"2\">LLM evaluate</td></tr><tr><td></td><td>Fluent</td><td>Mean.</td><td>Fluent</td><td>Mean.</td></tr><tr><td>Benign</td><td>4.55</td><td>-</td><td>4.32</td><td>5.00t</td></tr><tr><td>Textfooler</td><td>2.17</td><td>1.88</td><td>2.12</td><td>2.06</td></tr><tr><td>PWWS</td><td>2.16</td><td>1.85</td><td>2.42</td><td>2.49</td></tr><tr><td>BAE</td><td>3.01</td><td>3.02</td><td>3.71</td><td>3.71</td></tr></table>",
            "id": 78,
            "page": 8,
            "text": "Human evaluate LLM evaluate   Fluent Mean. Fluent Mean.  Benign 4.55 - 4.32 5.00t  Textfooler 2.17 1.88 2.12 2.06  PWWS 2.16 1.85 2.42 2.49  BAE 3.01 3.02 3.71"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 687
                },
                {
                    "x": 1216,
                    "y": 687
                },
                {
                    "x": 1216,
                    "y": 937
                },
                {
                    "x": 287,
                    "y": 937
                }
            ],
            "category": "caption",
            "html": "<caption id='79' style='font-size:14px'>Table 4: Mean Likert score of LLM evaluation and hu-<br>man evaluation result on fluency (Fluent) of the be-<br>nign and adversarial samples and meaning preserving<br>(Mean.) between the news title before and after adver-<br>sarial attacks.</caption>",
            "id": 79,
            "page": 8,
            "text": "Table 4: Mean Likert score of LLM evaluation and human evaluation result on fluency (Fluent) of the benign and adversarial samples and meaning preserving (Mean.) between the news title before and after adversarial attacks."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1036
                },
                {
                    "x": 755,
                    "y": 1036
                },
                {
                    "x": 755,
                    "y": 1091
                },
                {
                    "x": 287,
                    "y": 1091
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:22px'>4.2 Experiment Setup</p>",
            "id": 80,
            "page": 8,
            "text": "4.2 Experiment Setup"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1112
                },
                {
                    "x": 1213,
                    "y": 1112
                },
                {
                    "x": 1213,
                    "y": 2982
                },
                {
                    "x": 286,
                    "y": 2982
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:18px'>We select three different classic SSAs:<br>Textfooler (Jin et al., 2020), PWWS (Ren<br>et al., 2019), and BAE (Garg and Ramakrishnan,<br>2020); these attacks are predominantly used<br>as strong baselines in the literature of SSAs<br>nowadays. We use these three SSAs to attack a<br>BERT-base-uncased model (Devlin et al., 2019)<br>fine-tuned on AG-News (Zhang et al., 2015), a<br>news title classification dataset. For each SSA,<br>we randomly select 100 pairs of benign and<br>adversarial samples and use LLMs to evaluate their<br>quality. We show the result of using ChatGPT as<br>LLM here since it can better explain its decision.<br>Following the suggestions of prior works (Morris<br>et al., 2020), we evaluate the quality of the<br>adversarial samples from two aspects: the fluency<br>and meaning preservation. For fluency, we present<br>the LLM with one news title (either benign or<br>adversarial sample) and the following question:<br>How natural and fluent is the text of the news title?<br>(on a scale of 1-5, with 1 being the lowest). For<br>meaning preserving, we present the LLM with<br>both the benign and the adversarial sample, and<br>prompt the LLM to answer this question: Do you<br>agree that the meaning (or semantics) of news<br>title 1 is preserved in news title 2? (on a scale<br>of 1-5, with 1 being the strongly disagree and 5<br>being strongly agree.) The exact instruction and<br>formatting are presented in Appendix D.2.3. We<br>also ask three English teachers to rate the fluency<br>and meaning preserving of the samples. The task<br>instructions and questions are formatted the same<br>as in LLM evaluation.</p>",
            "id": 81,
            "page": 8,
            "text": "We select three different classic SSAs: Textfooler (Jin , 2020), PWWS (Ren , 2019), and BAE (Garg and Ramakrishnan, 2020); these attacks are predominantly used as strong baselines in the literature of SSAs nowadays. We use these three SSAs to attack a BERT-base-uncased model (Devlin , 2019) fine-tuned on AG-News (Zhang , 2015), a news title classification dataset. For each SSA, we randomly select 100 pairs of benign and adversarial samples and use LLMs to evaluate their quality. We show the result of using ChatGPT as LLM here since it can better explain its decision. Following the suggestions of prior works (Morris , 2020), we evaluate the quality of the adversarial samples from two aspects: the fluency and meaning preservation. For fluency, we present the LLM with one news title (either benign or adversarial sample) and the following question: How natural and fluent is the text of the news title? (on a scale of 1-5, with 1 being the lowest). For meaning preserving, we present the LLM with both the benign and the adversarial sample, and prompt the LLM to answer this question: Do you agree that the meaning (or semantics) of news title 1 is preserved in news title 2? (on a scale of 1-5, with 1 being the strongly disagree and 5 being strongly agree.) The exact instruction and formatting are presented in Appendix D.2.3. We also ask three English teachers to rate the fluency and meaning preserving of the samples. The task instructions and questions are formatted the same as in LLM evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3034
                },
                {
                    "x": 768,
                    "y": 3034
                },
                {
                    "x": 768,
                    "y": 3086
                },
                {
                    "x": 287,
                    "y": 3086
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:20px'>4.3 Experiment Result</p>",
            "id": 82,
            "page": 8,
            "text": "4.3 Experiment Result"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 3115
                },
                {
                    "x": 1212,
                    "y": 3115
                },
                {
                    "x": 1212,
                    "y": 3227
                },
                {
                    "x": 286,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:14px'>The results are presented in Table 4. We can see<br>that English teachers rate the adversarial samples</p>",
            "id": 83,
            "page": 8,
            "text": "The results are presented in Table 4. We can see that English teachers rate the adversarial samples"
        },
        {
            "bounding_box": [
                {
                    "x": 1266,
                    "y": 303
                },
                {
                    "x": 2197,
                    "y": 303
                },
                {
                    "x": 2197,
                    "y": 1203
                },
                {
                    "x": 1266,
                    "y": 1203
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:18px'>generated by SSAs much lower than benign sam-<br>ples in terms of fluency and meaning preserving,<br>this result is in line with recent observations on the<br>quality of adversarial samples (Hauser et al., 2021;<br>Chiang and Lee, 2022). Before interpreting the<br>result of LLM evaluation, we first conduct a sanity<br>check on whether the LLM understands the task.<br>We ask the LLM to rate the meaning preserving<br>of two benign samples that are exactly the same.<br>Ideally, the LLM should always give a score of 5,<br>meaning that it strongly agrees that the meanings<br>are not changed. The result of this sanity check<br>is the entry with 1 in Table 4, which is a perfect<br>5.00. ChatGPT often says that \"the two titles are<br>identical SO I rate a 5 (strongly agree)\", showing<br>that ChatGPT understands what the task is about.</p>",
            "id": 84,
            "page": 8,
            "text": "generated by SSAs much lower than benign samples in terms of fluency and meaning preserving, this result is in line with recent observations on the quality of adversarial samples (Hauser , 2021; Chiang and Lee, 2022). Before interpreting the result of LLM evaluation, we first conduct a sanity check on whether the LLM understands the task. We ask the LLM to rate the meaning preserving of two benign samples that are exactly the same. Ideally, the LLM should always give a score of 5, meaning that it strongly agrees that the meanings are not changed. The result of this sanity check is the entry with 1 in Table 4, which is a perfect 5.00. ChatGPT often says that \"the two titles are identical SO I rate a 5 (strongly agree)\", showing that ChatGPT understands what the task is about."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1211
                },
                {
                    "x": 2197,
                    "y": 1211
                },
                {
                    "x": 2197,
                    "y": 1995
                },
                {
                    "x": 1267,
                    "y": 1995
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:18px'>Next, we turn our attention to the LLM evalua-<br>tion results of the adversarial samples. We observe<br>that ChatGPT tends to rate adversarial samples<br>higher than English teachers, meaning that Chat-<br>GPT is less harsh on the unnatural and artificial<br>parts in the adversarial samples. We conduct the<br>same experiment using text-davinci-003 and<br>find similar results. Although ChatGPT rates adver-<br>sarial samples higher than the teachers, ChatGPT<br>still rates adversarial samples significantly lower<br>than benign samples. ChatGPT also agrees with the<br>English teachers that the adversarial samples gener-<br>ated by BAE are better than the samples generated<br>by Textfooler and PWWS.</p>",
            "id": 85,
            "page": 8,
            "text": "Next, we turn our attention to the LLM evaluation results of the adversarial samples. We observe that ChatGPT tends to rate adversarial samples higher than English teachers, meaning that ChatGPT is less harsh on the unnatural and artificial parts in the adversarial samples. We conduct the same experiment using text-davinci-003 and find similar results. Although ChatGPT rates adversarial samples higher than the teachers, ChatGPT still rates adversarial samples significantly lower than benign samples. ChatGPT also agrees with the English teachers that the adversarial samples generated by BAE are better than the samples generated by Textfooler and PWWS."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2005
                },
                {
                    "x": 2196,
                    "y": 2005
                },
                {
                    "x": 2196,
                    "y": 2680
                },
                {
                    "x": 1268,
                    "y": 2680
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:18px'>Interestingly, we find that ChatGPT rates PWWS<br>to be more natural than Textfooler, while such a<br>rating difference is not seen in the expert human<br>evaluation. At first sight, this means that ChatGPT<br>is inconsistent with human evaluation results. How-<br>ever, by scrutinizing the human evaluation results,<br>we find that two teachers rate PWWS higher than<br>Textfooler while one teacher rates PWWS lower<br>than Textfooler. This indicates that ChatGPT ac-<br>tually agrees with the majority of human experts.<br>Overall, LLM can rank the quality of adversarial<br>texts and benign texts like most human experts.</p>",
            "id": 86,
            "page": 8,
            "text": "Interestingly, we find that ChatGPT rates PWWS to be more natural than Textfooler, while such a rating difference is not seen in the expert human evaluation. At first sight, this means that ChatGPT is inconsistent with human evaluation results. However, by scrutinizing the human evaluation results, we find that two teachers rate PWWS higher than Textfooler while one teacher rates PWWS lower than Textfooler. This indicates that ChatGPT actually agrees with the majority of human experts. Overall, LLM can rank the quality of adversarial texts and benign texts like most human experts."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2735
                },
                {
                    "x": 1599,
                    "y": 2735
                },
                {
                    "x": 1599,
                    "y": 2788
                },
                {
                    "x": 1270,
                    "y": 2788
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:20px'>5 Discussions</p>",
            "id": 87,
            "page": 8,
            "text": "5 Discussions"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2836
                },
                {
                    "x": 2195,
                    "y": 2836
                },
                {
                    "x": 2195,
                    "y": 3229
                },
                {
                    "x": 1267,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:14px'>In this paper, we propose to use LLM for evaluating<br>the quality of texts to serve as an alternative to<br>human evaluation. To demonstrate the potential of<br>LLM evaluation, we use LLMs to rate the quality<br>of texts in two distinct tasks: open-ended story<br>generation and adversarial attacks. We show that<br>even ifLLMs have exceptional zero-shot in-context</p>",
            "id": 88,
            "page": 8,
            "text": "In this paper, we propose to use LLM for evaluating the quality of texts to serve as an alternative to human evaluation. To demonstrate the potential of LLM evaluation, we use LLMs to rate the quality of texts in two distinct tasks: open-ended story generation and adversarial attacks. We show that even ifLLMs have exceptional zero-shot in-context"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 302
                },
                {
                    "x": 1213,
                    "y": 302
                },
                {
                    "x": 1213,
                    "y": 697
                },
                {
                    "x": 287,
                    "y": 697
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:16px'>learning ability, they are not always suitable to be<br>used for LLM evaluation. Still, we find that the<br>best InstructGPT and ChatGPT can rate the quality<br>of texts like human experts on the two tasks we<br>used as examples. Overall, the results in this paper<br>demonstrate that LLM evaluation has the potential<br>to be used to evaluate NLP systems and algorithms.</p>",
            "id": 89,
            "page": 9,
            "text": "learning ability, they are not always suitable to be used for LLM evaluation. Still, we find that the best InstructGPT and ChatGPT can rate the quality of texts like human experts on the two tasks we used as examples. Overall, the results in this paper demonstrate that LLM evaluation has the potential to be used to evaluate NLP systems and algorithms."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 735
                },
                {
                    "x": 1214,
                    "y": 735
                },
                {
                    "x": 1214,
                    "y": 1696
                },
                {
                    "x": 287,
                    "y": 1696
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:16px'>Pros of LLM evaluation There are several ben-<br>efits of LLM evaluation, compared to human evalu-<br>ation. First, LLM evaluation is more reproducible.<br>Human evaluation results are hard to reproduce as<br>it is difficult to hire the same group of evaluators,<br>and it is hard to compare the results of similar ex-<br>periments even if they use the same instructions, re-<br>cruitment platform, and qualifications for the eval-<br>uators. On the contrary, LLM evaluation does not<br>have such a drawback. By specifying the model<br>used for LLM evaluation, the random seed, and<br>the hyperparameters used to generate the answers<br>from the LLM, the LLM evaluation result is more<br>likely to be reproduced. Note that in certain cases,<br>the LLM provider may regularly update the LLM,<br>making the LLM evaluation unreproducible if the<br>LLM is outdated and not accessible.</p>",
            "id": 90,
            "page": 9,
            "text": "Pros of LLM evaluation There are several benefits of LLM evaluation, compared to human evaluation. First, LLM evaluation is more reproducible. Human evaluation results are hard to reproduce as it is difficult to hire the same group of evaluators, and it is hard to compare the results of similar experiments even if they use the same instructions, recruitment platform, and qualifications for the evaluators. On the contrary, LLM evaluation does not have such a drawback. By specifying the model used for LLM evaluation, the random seed, and the hyperparameters used to generate the answers from the LLM, the LLM evaluation result is more likely to be reproduced. Note that in certain cases, the LLM provider may regularly update the LLM, making the LLM evaluation unreproducible if the LLM is outdated and not accessible."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1701
                },
                {
                    "x": 1213,
                    "y": 1701
                },
                {
                    "x": 1213,
                    "y": 2772
                },
                {
                    "x": 287,
                    "y": 2772
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:20px'>Second, the evaluation of each sample is inde-<br>pendent of each other in LLM evaluation. Con-<br>trarily, in human evaluation, the rating of the cur-<br>rent example may more or less be affected by prior<br>samples. Humans tend to compare the current sam-<br>ple to the ones they have previously seen and this<br>affects their ratings. As a piece of evidence, in the<br>interview after rating the 400 stories, the English<br>teachers say it took them some time to calibrate<br>their ratings (Appendix C.3.1). Thus, using LLM<br>evaluation can simplify some experiment designs<br>since one does not need to worry whether the order<br>of the sample being evaluated will change the result.<br>Still, one may also argue that being able to calibrate<br>the rating of different samples is desired and this<br>is why human evaluation might be preferred. Over-<br>all, whether the rating of the evaluator (human or<br>LLM) is being affected by a previously rated item<br>is inherently a design choice of the experiment.</p>",
            "id": 91,
            "page": 9,
            "text": "Second, the evaluation of each sample is independent of each other in LLM evaluation. Contrarily, in human evaluation, the rating of the current example may more or less be affected by prior samples. Humans tend to compare the current sample to the ones they have previously seen and this affects their ratings. As a piece of evidence, in the interview after rating the 400 stories, the English teachers say it took them some time to calibrate their ratings (Appendix C.3.1). Thus, using LLM evaluation can simplify some experiment designs since one does not need to worry whether the order of the sample being evaluated will change the result. Still, one may also argue that being able to calibrate the rating of different samples is desired and this is why human evaluation might be preferred. Overall, whether the rating of the evaluator (human or LLM) is being affected by a previously rated item is inherently a design choice of the experiment."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2778
                },
                {
                    "x": 1213,
                    "y": 2778
                },
                {
                    "x": 1213,
                    "y": 3228
                },
                {
                    "x": 288,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:16px'>Third, LLM evaluation is cheaper and faster<br>than human evaluation, making it easier and<br>quicker for researchers to evaluate the quality of<br>NLP systems. Hiring an English teacher to rate<br>200 stories costs us US$140, while LLM evalua-<br>tion using the best InstructGPT model costs less<br>than US$5. It took us over a week to collect human<br>evaluation results starting from recruitment to col-</p>",
            "id": 92,
            "page": 9,
            "text": "Third, LLM evaluation is cheaper and faster than human evaluation, making it easier and quicker for researchers to evaluate the quality of NLP systems. Hiring an English teacher to rate 200 stories costs us US$140, while LLM evaluation using the best InstructGPT model costs less than US$5. It took us over a week to collect human evaluation results starting from recruitment to col-"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 302
                },
                {
                    "x": 2192,
                    "y": 302
                },
                {
                    "x": 2192,
                    "y": 411
                },
                {
                    "x": 1267,
                    "y": 411
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:16px'>lecting the evaluation results, but only a few hours<br>to query InstructGPT and perform LLM evaluation.</p>",
            "id": 93,
            "page": 9,
            "text": "lecting the evaluation results, but only a few hours to query InstructGPT and perform LLM evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 421
                },
                {
                    "x": 2195,
                    "y": 421
                },
                {
                    "x": 2195,
                    "y": 755
                },
                {
                    "x": 1268,
                    "y": 755
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:16px'>Finally, utilizing LLM evaluation, rather than<br>human evaluation, can minimize the need for hu-<br>man exposure to objectionable content, such as<br>violent, sexual, hateful, or biased material. Such<br>content may cause discomfort for human evaluators<br>5<br>while reading and rating these texts.</p>",
            "id": 94,
            "page": 9,
            "text": "Finally, utilizing LLM evaluation, rather than human evaluation, can minimize the need for human exposure to objectionable content, such as violent, sexual, hateful, or biased material. Such content may cause discomfort for human evaluators 5 while reading and rating these texts."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 798
                },
                {
                    "x": 2196,
                    "y": 798
                },
                {
                    "x": 2196,
                    "y": 2153
                },
                {
                    "x": 1267,
                    "y": 2153
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:16px'>Limitations and Ethical Considerations of<br>LLM evaluation Despite the promising results<br>of LLM evaluation shown in this paper, there<br>are some limitations of this method. First, LLM<br>may possess incorrect factual knowledge (Cao<br>et al., 2021), SO it is not suitable to use them in<br>tasks that involve factual knowledge. Next, LLMs<br>trained to behave in a certain way can be biased<br>toward certain responses. Precisely, an LLM that<br>is trained to be safe and non-harmful can result<br>in LLMs preferring to generate more positive and<br>upbeat responses, which is observed throughout<br>our interaction with ChatGPT. Additionally, even<br>with researchers' efforts to make LLMs safer (Bai<br>et al., 2022a,b), LLMs can still generate harmful<br>and biased responses (Ganguli et al., 2022; Perez<br>et al., 2022), which are violative of basic ethics,<br>and LLM evaluation results will be highly doubt-<br>ful (Hendrycks et al., 2021). However, it is im-<br>portant to note that these limitations and potential<br>harms also apply to human evaluation: the bias of<br>human evaluators can affect the human evaluation<br>result (Lentz and De Jong, 1997; Amidei et al.,<br>2018).</p>",
            "id": 95,
            "page": 9,
            "text": "Limitations and Ethical Considerations of LLM evaluation Despite the promising results of LLM evaluation shown in this paper, there are some limitations of this method. First, LLM may possess incorrect factual knowledge (Cao , 2021), SO it is not suitable to use them in tasks that involve factual knowledge. Next, LLMs trained to behave in a certain way can be biased toward certain responses. Precisely, an LLM that is trained to be safe and non-harmful can result in LLMs preferring to generate more positive and upbeat responses, which is observed throughout our interaction with ChatGPT. Additionally, even with researchers' efforts to make LLMs safer (Bai , 2022a,b), LLMs can still generate harmful and biased responses (Ganguli , 2022; Perez , 2022), which are violative of basic ethics, and LLM evaluation results will be highly doubtful (Hendrycks , 2021). However, it is important to note that these limitations and potential harms also apply to human evaluation: the bias of human evaluators can affect the human evaluation result (Lentz and De Jong, 1997; Amidei , 2018)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2160
                },
                {
                    "x": 2195,
                    "y": 2160
                },
                {
                    "x": 2195,
                    "y": 2839
                },
                {
                    "x": 1268,
                    "y": 2839
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:16px'>Our pioneering idea, LLM evaluation, has the<br>potential to transform the NLP community. 6 We<br>encourage future researchers to consider using it<br>while being aware of its limitations. Our paper's<br>goal is not to replace human evaluation but to<br>present an alternative option. Both human and<br>LLM evaluation have their own advantages and dis-<br>advantages, and they can be used in conjunction.<br>We recommend using LLM evaluation as a cheap<br>and fast quality judgment when developing a new<br>NLP system, while human evaluation is best used<br>to collect feedback from humans prior to deploying</p>",
            "id": 96,
            "page": 9,
            "text": "Our pioneering idea, LLM evaluation, has the potential to transform the NLP community. 6 We encourage future researchers to consider using it while being aware of its limitations. Our paper's goal is not to replace human evaluation but to present an alternative option. Both human and LLM evaluation have their own advantages and disadvantages, and they can be used in conjunction. We recommend using LLM evaluation as a cheap and fast quality judgment when developing a new NLP system, while human evaluation is best used to collect feedback from humans prior to deploying"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2887
                },
                {
                    "x": 2192,
                    "y": 2887
                },
                {
                    "x": 2192,
                    "y": 3227
                },
                {
                    "x": 1267,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:14px'>5It should be noted that the LLM may decline to assess cer-<br>tain inputs that violate the content policy of the LLM provider.<br>6We say that we are the first to propose this idea since when<br>we submitted this paper to ACL 2023 on January 13, 2023,<br>we do not find any other paper that explores this idea. During<br>the reviewing process, we found some works on arXiv (Wang<br>et al., 2023; Huang et al., 2023; Gilardi et al., 2023) that<br>explore a similar idea using different tasks.</p>",
            "id": 97,
            "page": 9,
            "text": "5It should be noted that the LLM may decline to assess certain inputs that violate the content policy of the LLM provider. 6We say that we are the first to propose this idea since when we submitted this paper to ACL 2023 on January 13, 2023, we do not find any other paper that explores this idea. During the reviewing process, we found some works on arXiv (Wang , 2023; Huang , 2023; Gilardi , 2023) that explore a similar idea using different tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 302
                },
                {
                    "x": 1079,
                    "y": 302
                },
                {
                    "x": 1079,
                    "y": 354
                },
                {
                    "x": 288,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:18px'>the NLP system in real-world applications.</p>",
            "id": 98,
            "page": 10,
            "text": "the NLP system in real-world applications."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 401
                },
                {
                    "x": 548,
                    "y": 401
                },
                {
                    "x": 548,
                    "y": 455
                },
                {
                    "x": 289,
                    "y": 455
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:20px'>Limitations</p>",
            "id": 99,
            "page": 10,
            "text": "Limitations"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 491
                },
                {
                    "x": 1213,
                    "y": 491
                },
                {
                    "x": 1213,
                    "y": 1224
                },
                {
                    "x": 287,
                    "y": 1224
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:14px'>There are additional limitations and potential risks<br>of LLM evaluations that should be noted, and these<br>limitations are actually well-known problems of<br>pre-trained language models. As listed on the Open<br>AI blog for ChatGPT, ChatGPT sometimes gen-<br>erates answers that sound right and plausible but<br>are totally nonsense. OpenAI also admits that the<br>model's response may be sensitive to the prompt<br>used to query the model. While in Section 3.3.2,<br>we find that the overall results among different in-<br>structions are not significantly different, we cannot<br>guarantee that this is the case for all kinds of modi-<br>fication on the task instructions.</p>",
            "id": 100,
            "page": 10,
            "text": "There are additional limitations and potential risks of LLM evaluations that should be noted, and these limitations are actually well-known problems of pre-trained language models. As listed on the Open AI blog for ChatGPT, ChatGPT sometimes generates answers that sound right and plausible but are totally nonsense. OpenAI also admits that the model's response may be sensitive to the prompt used to query the model. While in Section 3.3.2, we find that the overall results among different instructions are not significantly different, we cannot guarantee that this is the case for all kinds of modification on the task instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1229
                },
                {
                    "x": 1213,
                    "y": 1229
                },
                {
                    "x": 1213,
                    "y": 1847
                },
                {
                    "x": 287,
                    "y": 1847
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:14px'>Other than the limitations listed on the OpenAI<br>blog, there are still other limitations. For example,<br>LLMs may not have emotions. Whether AI mod-<br>els have emotion is a more philosophical question<br>and is controversial, SO the results of using such<br>models for evaluating emotion-related tasks may<br>be strongly challenged and may even violate re-<br>search ethics. As we find during our experiments,<br>ChatGPT often replies \"I am an AI system and I<br>do not have emotions like a human\" when asked to<br>rate the likability of a story.</p>",
            "id": 101,
            "page": 10,
            "text": "Other than the limitations listed on the OpenAI blog, there are still other limitations. For example, LLMs may not have emotions. Whether AI models have emotion is a more philosophical question and is controversial, SO the results of using such models for evaluating emotion-related tasks may be strongly challenged and may even violate research ethics. As we find during our experiments, ChatGPT often replies \"I am an AI system and I do not have emotions like a human\" when asked to rate the likability of a story."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1853
                },
                {
                    "x": 1212,
                    "y": 1853
                },
                {
                    "x": 1212,
                    "y": 2413
                },
                {
                    "x": 287,
                    "y": 2413
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='102' style='font-size:14px'>Another important limitation of LLM evaluation<br>is that LLMs lack the ability to process visual cues<br>in task instructions, unlike human evaluation. Hu-<br>man evaluators can use formattings such as special<br>fonts or text styles to focus on important parts of<br>the instructions. Additionally, the way instructions<br>and questions are formatted can influence how hu-<br>man evaluators approach the task. However, LLMs<br>can only process raw text input and are unable to<br>take in visual cues.</p>",
            "id": 102,
            "page": 10,
            "text": "Another important limitation of LLM evaluation is that LLMs lack the ability to process visual cues in task instructions, unlike human evaluation. Human evaluators can use formattings such as special fonts or text styles to focus on important parts of the instructions. Additionally, the way instructions and questions are formatted can influence how human evaluators approach the task. However, LLMs can only process raw text input and are unable to take in visual cues."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2458
                },
                {
                    "x": 661,
                    "y": 2458
                },
                {
                    "x": 661,
                    "y": 2516
                },
                {
                    "x": 289,
                    "y": 2516
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:20px'>Ethics Statement</p>",
            "id": 103,
            "page": 10,
            "text": "Ethics Statement"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2549
                },
                {
                    "x": 1213,
                    "y": 2549
                },
                {
                    "x": 1213,
                    "y": 3231
                },
                {
                    "x": 286,
                    "y": 3231
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:14px'>Further ethical considerations of LLM evalua-<br>tion Aside from the limitations of LLM evalua-<br>tion mentioned previously, there is a crucial ethical<br>concern at the heart of LLM evaluation. Is it ethical<br>to replace human evaluation with LLM evaluation?<br>Some may question if this paper is suggesting that<br>LLMs are now ready to replace humans and find<br>this idea unsettling. As responsible and ethical<br>NLP researchers, we understand these concerns but<br>want to make it clear that this is not our intent. As<br>our paper title suggests, we aim to offer an alter-<br>native option to human evaluation with the goal</p>",
            "id": 104,
            "page": 10,
            "text": "Further ethical considerations of LLM evaluation Aside from the limitations of LLM evaluation mentioned previously, there is a crucial ethical concern at the heart of LLM evaluation. Is it ethical to replace human evaluation with LLM evaluation? Some may question if this paper is suggesting that LLMs are now ready to replace humans and find this idea unsettling. As responsible and ethical NLP researchers, we understand these concerns but want to make it clear that this is not our intent. As our paper title suggests, we aim to offer an alternative option to human evaluation with the goal"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 302
                },
                {
                    "x": 2197,
                    "y": 302
                },
                {
                    "x": 2197,
                    "y": 1205
                },
                {
                    "x": 1267,
                    "y": 1205
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:16px'>of enhancing the reproducibility of NLP research.<br>Human evaluation is still essential as the ultimate<br>goal of NLP systems is to be used by human users,<br>SO it's important to gather feedback from them. We<br>highly enjoy the process of discussing the experi-<br>ment settings and results with the English teachers<br>we hired. We do not recommend that future re-<br>searchers completely eliminate human evaluation;<br>rather, we believe that human evaluation should be<br>used in conjunction with LLM evaluation. Both<br>methods have their own advantages and disadvan-<br>tages, making them both necessary for evaluating<br>NLP systems. We hope the positive results in this<br>paper provide NLP researchers with an alternative<br>method to evaluate systems and encourage further<br>discussions on this topic.</p>",
            "id": 105,
            "page": 10,
            "text": "of enhancing the reproducibility of NLP research. Human evaluation is still essential as the ultimate goal of NLP systems is to be used by human users, SO it's important to gather feedback from them. We highly enjoy the process of discussing the experiment settings and results with the English teachers we hired. We do not recommend that future researchers completely eliminate human evaluation; rather, we believe that human evaluation should be used in conjunction with LLM evaluation. Both methods have their own advantages and disadvantages, making them both necessary for evaluating NLP systems. We hope the positive results in this paper provide NLP researchers with an alternative method to evaluate systems and encourage further discussions on this topic."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1289
                },
                {
                    "x": 2197,
                    "y": 1289
                },
                {
                    "x": 2197,
                    "y": 2136
                },
                {
                    "x": 1268,
                    "y": 2136
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:14px'>Ethical statements on the experiments in the pa-<br>per All the experiments strictly follow the ACL<br>Code of Ethics. We include comprehensive de-<br>tails about human evaluation in Appendix C.1. To<br>summarize, we include the exact instructions and<br>screenshots of the interface in the human evalua-<br>tion, and we report how the evaluators are recruited<br>and show that the payment is very reasonable. We<br>inform the human evaluators what the task is about<br>and tell them that their responses will be used to<br>assess the performance of AI models. We do not<br>have an ethical review board or anything like that<br>in our institute, SO we are not able to get approval<br>from an ethical review board. Still, we try our best<br>to follow the ethical guidelines of ACL.</p>",
            "id": 106,
            "page": 10,
            "text": "Ethical statements on the experiments in the paper All the experiments strictly follow the ACL Code of Ethics. We include comprehensive details about human evaluation in Appendix C.1. To summarize, we include the exact instructions and screenshots of the interface in the human evaluation, and we report how the evaluators are recruited and show that the payment is very reasonable. We inform the human evaluators what the task is about and tell them that their responses will be used to assess the performance of AI models. We do not have an ethical review board or anything like that in our institute, SO we are not able to get approval from an ethical review board. Still, we try our best to follow the ethical guidelines of ACL."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2156
                },
                {
                    "x": 2195,
                    "y": 2156
                },
                {
                    "x": 2195,
                    "y": 2378
                },
                {
                    "x": 1267,
                    "y": 2378
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:18px'>We use the models and datasets when following<br>their intended usage. Specifically, we follow the<br>OpenAI usage policy when using the InstructGPT<br>models and the ChatGPT model.</p>",
            "id": 107,
            "page": 10,
            "text": "We use the models and datasets when following their intended usage. Specifically, we follow the OpenAI usage policy when using the InstructGPT models and the ChatGPT model."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2477
                },
                {
                    "x": 1690,
                    "y": 2477
                },
                {
                    "x": 1690,
                    "y": 2534
                },
                {
                    "x": 1270,
                    "y": 2534
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:22px'>Acknowledgements</p>",
            "id": 108,
            "page": 10,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2604
                },
                {
                    "x": 2196,
                    "y": 2604
                },
                {
                    "x": 2196,
                    "y": 3174
                },
                {
                    "x": 1268,
                    "y": 3174
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>We want to thank the reviews for providing detailed<br>feedback and actionable suggestions, which help<br>us strengthen our paper. We list the modification<br>based on the reviewers' suggestions in Appendix A.<br>We thank Yung-Sung Chuang for providing valu-<br>able feedback on the draft of this paper. We want to<br>thank Tung-En Hsiao, the administrative assistant<br>of our lab, for helping us deal with the payment<br>on Upwork. Cheng-Han Chiang is supported by a<br>Ph.D. scholarship program by Delta Electronics.</p>",
            "id": 109,
            "page": 10,
            "text": "We want to thank the reviews for providing detailed feedback and actionable suggestions, which help us strengthen our paper. We list the modification based on the reviewers' suggestions in Appendix A. We thank Yung-Sung Chuang for providing valuable feedback on the draft of this paper. We want to thank Tung-En Hsiao, the administrative assistant of our lab, for helping us deal with the payment on Upwork. Cheng-Han Chiang is supported by a Ph.D. scholarship program by Delta Electronics."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 298
                },
                {
                    "x": 531,
                    "y": 298
                },
                {
                    "x": 531,
                    "y": 352
                },
                {
                    "x": 288,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:22px'>References</p>",
            "id": 110,
            "page": 11,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 379
                },
                {
                    "x": 1214,
                    "y": 379
                },
                {
                    "x": 1214,
                    "y": 704
                },
                {
                    "x": 288,
                    "y": 704
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,<br>Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.<br>2018. Generating natural language adversarial ex-<br>amples. In Proceedings of the 2018 Conference on<br>Empirical Methods in Natural Language Processing,<br>pages 2890-2896, Brussels, Belgium. Association<br>for Computational Linguistics.</p>",
            "id": 111,
            "page": 11,
            "text": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. 2018. Generating natural language adversarial examples. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2890-2896, Brussels, Belgium. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 742
                },
                {
                    "x": 1214,
                    "y": 742
                },
                {
                    "x": 1214,
                    "y": 1019
                },
                {
                    "x": 289,
                    "y": 1019
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:16px'>Jacopo Amidei, Paul Piwek, and Alistair Willis. 2018.<br>Rethinking the agreement in human evaluation tasks.<br>In Proceedings of the 27th International Conference<br>on Computational Linguistics, pages 3318-3329,<br>Santa Fe, New Mexico, USA. Association for Com-<br>putational Linguistics.</p>",
            "id": 112,
            "page": 11,
            "text": "Jacopo Amidei, Paul Piwek, and Alistair Willis. 2018. Rethinking the agreement in human evaluation tasks. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3318-3329, Santa Fe, New Mexico, USA. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1058
                },
                {
                    "x": 1213,
                    "y": 1058
                },
                {
                    "x": 1213,
                    "y": 1334
                },
                {
                    "x": 289,
                    "y": 1334
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:14px'>Jacopo Amidei, Paul Piwek, and Alistair Willis. 2019.<br>Agreement is overrated: A plea for correlation to as-<br>sess human evaluation reliability. In Proceedings of<br>the 12th International Conference on Natural Lan-<br>guage Generation, pages 344-354, Tokyo, Japan.<br>Association for Computational Linguistics.</p>",
            "id": 113,
            "page": 11,
            "text": "Jacopo Amidei, Paul Piwek, and Alistair Willis. 2019. Agreement is overrated: A plea for correlation to assess human evaluation reliability. In Proceedings of the 12th International Conference on Natural Language Generation, pages 344-354, Tokyo, Japan. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1373
                },
                {
                    "x": 1213,
                    "y": 1373
                },
                {
                    "x": 1213,
                    "y": 1650
                },
                {
                    "x": 290,
                    "y": 1650
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:16px'>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda<br>Askell, Anna Chen, Nova DasSarma, Dawn Drain,<br>Stanislav Fort, Deep Ganguli, Tom Henighan, et al.<br>2022a. Training a helpful and harmless assistant<br>with reinforcement learning from human feedback.<br>arXiv preprint arXiv:2204.05862.</p>",
            "id": 114,
            "page": 11,
            "text": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan,  2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1689
                },
                {
                    "x": 1213,
                    "y": 1689
                },
                {
                    "x": 1213,
                    "y": 1965
                },
                {
                    "x": 289,
                    "y": 1965
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:16px'>Yuntao Bai, Saurav Kadavath, Sandipan Kundu,<br>Amanda Askell, Jackson Kernion, Andy Jones,<br>Anna Chen, Anna Goldie, Azalia Mirhoseini,<br>Cameron McKinnon, et al. 2022b. Constitutional<br>ai: Harmlessness from ai feedback. arXiv preprint<br>arXiv:2212.08073.</p>",
            "id": 115,
            "page": 11,
            "text": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,  2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2005
                },
                {
                    "x": 1213,
                    "y": 2005
                },
                {
                    "x": 1213,
                    "y": 2144
                },
                {
                    "x": 289,
                    "y": 2144
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:14px'>R Botsch. 2011. Chapter 12: Significance and mea-<br>sures of association. Scopes and Methods of Politi-<br>cal Science.</p>",
            "id": 116,
            "page": 11,
            "text": "R Botsch. 2011. Chapter 12: Significance and measures of association. Scopes and Methods of Political Science."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2181
                },
                {
                    "x": 1212,
                    "y": 2181
                },
                {
                    "x": 1212,
                    "y": 2460
                },
                {
                    "x": 288,
                    "y": 2460
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>Tom Brown, Benjamin Mann, Nick Ryder, Melanie<br>Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind<br>Neelakantan, Pranav Shyam, Girish Sastry, Amanda<br>Askell, et al. 2020. Language models are few-shot<br>learners. Advances in neural information processing<br>systems, 33:1877-1901.</p>",
            "id": 117,
            "page": 11,
            "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,  2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2498
                },
                {
                    "x": 1214,
                    "y": 2498
                },
                {
                    "x": 1214,
                    "y": 2595
                },
                {
                    "x": 289,
                    "y": 2595
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:16px'>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.<br>Editing factual knowledge in language models.</p>",
            "id": 118,
            "page": 11,
            "text": "Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2633
                },
                {
                    "x": 1213,
                    "y": 2633
                },
                {
                    "x": 1213,
                    "y": 2771
                },
                {
                    "x": 287,
                    "y": 2771
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:14px'>Cheng-Han Chiang and Hung-yi Lee. 2022. How<br>far are we from real synonym substitution attacks?<br>arXiv preprint arXiv:2210.02844.</p>",
            "id": 119,
            "page": 11,
            "text": "Cheng-Han Chiang and Hung-yi Lee. 2022. How far are we from real synonym substitution attacks? arXiv preprint arXiv:2210.02844."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2810
                },
                {
                    "x": 1214,
                    "y": 2810
                },
                {
                    "x": 1214,
                    "y": 3225
                },
                {
                    "x": 288,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:16px'>Elizabeth Clark, Tal August, Sofia Serrano, Nikita<br>Haduong, Suchin Gururangan, and Noah A. Smith.<br>2021. All that's 'human' is not gold: Evaluating<br>human evaluation of generated text. In Proceed-<br>ings of the 59th Annual Meeting of the Association<br>for Computational Linguistics and the 11th Interna-<br>tional Joint Conference on Natural Language Pro-<br>cessing (Volume 1: Long Papers), pages 7282-7296,<br>Online. Association for Computational Linguistics.</p>",
            "id": 120,
            "page": 11,
            "text": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282-7296, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 303
                },
                {
                    "x": 2197,
                    "y": 303
                },
                {
                    "x": 2197,
                    "y": 717
                },
                {
                    "x": 1269,
                    "y": 717
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:20px'>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and<br>Kristina Toutanova. 2019. BERT: Pre-training of<br>deep bidirectional transformers for language under-<br>standing. In Proceedings of the 2019 Conference<br>of the North American Chapter of the Association<br>for Computational Linguistics: Human Language<br>Technologies, Volume 1 (Long and Short Papers),<br>pages 4171-4186, Minneapolis, Minnesota. Associ-<br>ation for Computational Linguistics.</p>",
            "id": 121,
            "page": 11,
            "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 747
                },
                {
                    "x": 2195,
                    "y": 747
                },
                {
                    "x": 2195,
                    "y": 1023
                },
                {
                    "x": 1271,
                    "y": 1023
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-<br>erarchical neural story generation. In Proceedings<br>of the 56th Annual Meeting of the Association for<br>Computational Linguistics (Volume 1: Long Papers),<br>pages 889-898, Melbourne, Australia. Association<br>for Computational Linguistics.</p>",
            "id": 122,
            "page": 11,
            "text": "Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1053
                },
                {
                    "x": 2196,
                    "y": 1053
                },
                {
                    "x": 2196,
                    "y": 1329
                },
                {
                    "x": 1271,
                    "y": 1329
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:16px'>Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda<br>Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,<br>Ethan Perez, Nicholas Schiefer, Kamal Ndousse,<br>et al. 2022. Red teaming language models to reduce<br>harms: Methods, scaling behaviors, and lessons<br>learned. arXiv preprint arXiv:2209.07858.</p>",
            "id": 123,
            "page": 11,
            "text": "Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,  2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1356
                },
                {
                    "x": 2194,
                    "y": 1356
                },
                {
                    "x": 2194,
                    "y": 1452
                },
                {
                    "x": 1269,
                    "y": 1452
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:14px'>Leo Gao. 2021. On the sizes of openai api models. Ac-<br>cessed on January 17, 2023.</p>",
            "id": 124,
            "page": 11,
            "text": "Leo Gao. 2021. On the sizes of openai api models. Accessed on January 17, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1480
                },
                {
                    "x": 2197,
                    "y": 1480
                },
                {
                    "x": 2197,
                    "y": 1711
                },
                {
                    "x": 1271,
                    "y": 1711
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:16px'>Siddhant Garg and Goutham Ramakrishnan. 2020.<br>Bae: Bert-based adversarial examples for text classi-<br>fication. In Proceedings of the 2020 Conference on<br>Empirical Methods in Natural Language Processing<br>(EMNLP), pages 6174-6181.</p>",
            "id": 125,
            "page": 11,
            "text": "Siddhant Garg and Goutham Ramakrishnan. 2020. Bae: Bert-based adversarial examples for text classification. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174-6181."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1741
                },
                {
                    "x": 2197,
                    "y": 1741
                },
                {
                    "x": 2197,
                    "y": 1879
                },
                {
                    "x": 1270,
                    "y": 1879
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:16px'>Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli.<br>2023. Chatgpt outperforms crowd-workers for text-<br>annotation tasks. arXiv preprint arXiv:2303.15056.</p>",
            "id": 126,
            "page": 11,
            "text": "Fabrizio Gilardi, Meysam Alizadeh, and Mael Kubli. 2023. Chatgpt outperforms crowd-workers for textannotation tasks. arXiv preprint arXiv:2303.15056."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1910
                },
                {
                    "x": 2196,
                    "y": 1910
                },
                {
                    "x": 2196,
                    "y": 2186
                },
                {
                    "x": 1271,
                    "y": 2186
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:16px'>Dan Gillick and Yang Liu. 2010. Non-expert evalua-<br>tion of summarization systems is risky. In Proceed-<br>ings of the NAACL HLT 2010 Workshop on Creating<br>Speech and Language Data with Amazon's Mechan-<br>ical Turk, pages 148-151, Los Angeles. Association<br>for Computational Linguistics.</p>",
            "id": 127,
            "page": 11,
            "text": "Dan Gillick and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk, pages 148-151, Los Angeles. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2215
                },
                {
                    "x": 2195,
                    "y": 2215
                },
                {
                    "x": 2195,
                    "y": 2445
                },
                {
                    "x": 1270,
                    "y": 2445
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='128' style='font-size:18px'>Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and<br>Minlie Huang. 2020. A knowledge-enhanced pre-<br>training model for commonsense story generation.<br>Transactions of the Association for Computational<br>Linguistics, 8:93-108.</p>",
            "id": 128,
            "page": 11,
            "text": "Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A knowledge-enhanced pretraining model for commonsense story generation. Transactions of the Association for Computational Linguistics, 8:93-108."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2474
                },
                {
                    "x": 2196,
                    "y": 2474
                },
                {
                    "x": 2196,
                    "y": 2750
                },
                {
                    "x": 1270,
                    "y": 2750
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:16px'>Francisco Guzman, Ahmed Abdelali, Irina Temnikova,<br>Hassan Sajjad, and Stephan Vogel. 2015. How do<br>humans evaluate machine translation. In Proceed-<br>ings of the Tenth Workshop on Statistical Machine<br>Translation, pages 457-466, Lisbon, Portugal. Asso-<br>ciation for Computational Linguistics.</p>",
            "id": 129,
            "page": 11,
            "text": "Francisco Guzman, Ahmed Abdelali, Irina Temnikova, Hassan Sajjad, and Stephan Vogel. 2015. How do humans evaluate machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 457-466, Lisbon, Portugal. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2781
                },
                {
                    "x": 2195,
                    "y": 2781
                },
                {
                    "x": 2195,
                    "y": 2964
                },
                {
                    "x": 1270,
                    "y": 2964
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:14px'>Jens Hauser, Zhao Meng, Damian Pascual, and Roger<br>Wattenhofer. 2021. Bert is robust! a case against<br>synonym-based adversarial examples in text classifi-<br>cation. arXiv preprint arXiv:2109.07403.</p>",
            "id": 130,
            "page": 11,
            "text": "Jens Hauser, Zhao Meng, Damian Pascual, and Roger Wattenhofer. 2021. Bert is robust! a case against synonym-based adversarial examples in text classification. arXiv preprint arXiv:2109.07403."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2993
                },
                {
                    "x": 2197,
                    "y": 2993
                },
                {
                    "x": 2197,
                    "y": 3224
                },
                {
                    "x": 1271,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>Dan Hendrycks, Collin Burns, Steven Basart, Andrew<br>Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.<br>2021. Aligning {ai} with shared human values. In<br>International Conference on Learning Representa-<br>tions.</p>",
            "id": 131,
            "page": 11,
            "text": "Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021. Aligning {ai} with shared human values. In International Conference on Learning Representations."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 304
                },
                {
                    "x": 1213,
                    "y": 304
                },
                {
                    "x": 1213,
                    "y": 490
                },
                {
                    "x": 289,
                    "y": 490
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:14px'>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and<br>Yejin Choi. 2020. The curious case of neural text de-<br>generation. In International Conference on Learn-<br>ing Representations.</p>",
            "id": 132,
            "page": 12,
            "text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 527
                },
                {
                    "x": 1212,
                    "y": 527
                },
                {
                    "x": 1212,
                    "y": 713
                },
                {
                    "x": 289,
                    "y": 713
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:16px'>Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is<br>chatgpt better than human annotators? potential<br>and limitations of chatgpt in explaining implicit hate<br>speech. arXiv preprint arXiv:2302.07736.</p>",
            "id": 133,
            "page": 12,
            "text": "Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 748
                },
                {
                    "x": 1213,
                    "y": 748
                },
                {
                    "x": 1213,
                    "y": 1025
                },
                {
                    "x": 288,
                    "y": 1025
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:14px'>Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter<br>Szolovits. 2020. Is bert really robust? a strong base-<br>line for natural language attack on text classification<br>and entailment. In Proceedings of the AAAI con-<br>ference on artificial intelligence, volume 34, pages<br>8018-8025.</p>",
            "id": 134,
            "page": 12,
            "text": "Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018-8025."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1064
                },
                {
                    "x": 1213,
                    "y": 1064
                },
                {
                    "x": 1213,
                    "y": 1387
                },
                {
                    "x": 290,
                    "y": 1387
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>Marzena Karpinska, Nader Akoury, and Mohit Iyyer.<br>2021. The perils of using Mechanical Turk to eval-<br>uate open-ended text generation. In Proceedings of<br>the 2021 Conference on Empirical Methods in Natu-<br>ral Language Processing, pages 1265-1285, Online<br>and Punta Cana, Dominican Republic. Association<br>for Computational Linguistics.</p>",
            "id": 135,
            "page": 12,
            "text": "Marzena Karpinska, Nader Akoury, and Mohit Iyyer. 2021. The perils of using Mechanical Turk to evaluate open-ended text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1265-1285, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1424
                },
                {
                    "x": 1212,
                    "y": 1424
                },
                {
                    "x": 1212,
                    "y": 1609
                },
                {
                    "x": 289,
                    "y": 1609
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:14px'>Leo Lentz and Menno De Jong. 1997. The evaluation<br>of text quality: Expert-focused and reader-focused<br>methods compared. IEEE transactions on profes-<br>sional communication, 40(3):224-234.</p>",
            "id": 136,
            "page": 12,
            "text": "Leo Lentz and Menno De Jong. 1997. The evaluation of text quality: Expert-focused and reader-focused methods compared. IEEE transactions on professional communication, 40(3):224-234."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1646
                },
                {
                    "x": 1213,
                    "y": 1646
                },
                {
                    "x": 1213,
                    "y": 2060
                },
                {
                    "x": 290,
                    "y": 2060
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>Huanru Henry Mao, Bodhisattwa Prasad Majumder,<br>Julian McAuley, and Garrison Cottrell. 2019. Im-<br>proving neural story generation by targeted common<br>sense grounding. In Proceedings of the 2019 Con-<br>ference on Empirical Methods in Natural Language<br>Processing and the 9th International Joint Confer-<br>ence on Natural Language Processing (EMNLP-<br>IJCNLP), pages 5988-5993, Hong Kong, China. As-<br>sociation for Computational Linguistics.</p>",
            "id": 137,
            "page": 12,
            "text": "Huanru Henry Mao, Bodhisattwa Prasad Majumder, Julian McAuley, and Garrison Cottrell. 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5988-5993, Hong Kong, China. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2097
                },
                {
                    "x": 1213,
                    "y": 2097
                },
                {
                    "x": 1213,
                    "y": 2374
                },
                {
                    "x": 288,
                    "y": 2374
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:18px'>John Morris, Eli Lifland, Jack Lanchantin, Yangfeng<br>Ji, and Yanjun Qi. 2020. Reevaluating adversarial<br>examples in natural language. In Findings of the<br>Association for Computational Linguistics: EMNLP<br>2020, pages 3829-3839, Online. Association for<br>Computational Linguistics.</p>",
            "id": 138,
            "page": 12,
            "text": "John Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. 2020. Reevaluating adversarial examples in natural language. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3829-3839, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2410
                },
                {
                    "x": 1212,
                    "y": 2410
                },
                {
                    "x": 1212,
                    "y": 2504
                },
                {
                    "x": 287,
                    "y": 2504
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:20px'>OpenAI. 2022. Chatgpt: Optimizing language models<br>for dialogue. Accessed on January 10, 2023.</p>",
            "id": 139,
            "page": 12,
            "text": "OpenAI. 2022. Chatgpt: Optimizing language models for dialogue. Accessed on January 10, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2542
                },
                {
                    "x": 1214,
                    "y": 2542
                },
                {
                    "x": 1214,
                    "y": 2817
                },
                {
                    "x": 289,
                    "y": 2817
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-<br>roll L Wainwright, Pamela Mishkin, Chong Zhang,<br>Sandhini Agarwal, Katarina Slama, Alex Ray, et al.<br>2022. Training language models to follow in-<br>structions with human feedback. arXiv preprint<br>arXiv:2203.02155.</p>",
            "id": 140,
            "page": 12,
            "text": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,  2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2856
                },
                {
                    "x": 1214,
                    "y": 2856
                },
                {
                    "x": 1214,
                    "y": 3225
                },
                {
                    "x": 288,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,<br>Roman Ring, John Aslanides, Amelia Glaese, Nat<br>McAleese, and Geoffrey Irving. 2022. Red teaming<br>language models with language models. In Proceed-<br>ings of the 2022 Conference on Empirical Methods<br>in Natural Language Processing, pages 3419-3448,<br>Abu Dhabi, United Arab Emirates. Association for<br>Computational Linguistics.</p>",
            "id": 141,
            "page": 12,
            "text": "Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red teaming language models with language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3419-3448, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 305
                },
                {
                    "x": 2197,
                    "y": 305
                },
                {
                    "x": 2197,
                    "y": 489
                },
                {
                    "x": 1271,
                    "y": 489
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:16px'>Alec Radford, Jeffrey Wu, Rewon Child, David Luan,<br>Dario Amodei, Ilya Sutskever, et al. 2019. Lan-<br>guage models are unsupervised multitask learners.<br>OpenAI blog, 1(8):9.</p>",
            "id": 142,
            "page": 12,
            "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,  2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 518
                },
                {
                    "x": 2197,
                    "y": 518
                },
                {
                    "x": 2197,
                    "y": 749
                },
                {
                    "x": 1269,
                    "y": 749
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:14px'>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine<br>Lee, Sharan Narang, Michael Matena, Yanqi Zhou,<br>Wei Li, Peter J Liu, et al. 2020. Exploring the limits<br>of transfer learning with a unified text-to-text trans-<br>former. J. Mach. Learn. Res., 21(1 40):1-67.</p>",
            "id": 143,
            "page": 12,
            "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu,  2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1 40):1-67."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 780
                },
                {
                    "x": 2197,
                    "y": 780
                },
                {
                    "x": 2197,
                    "y": 1101
                },
                {
                    "x": 1270,
                    "y": 1101
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:20px'>Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.<br>2019. Generating natural language adversarial ex-<br>amples through probability weighted word saliency.<br>In Proceedings of the 57th Annual Meeting of the<br>Association for Computational Linguistics, pages<br>1085-1097, Florence, Italy. Association for Compu-<br>tational Linguistics.</p>",
            "id": 144,
            "page": 12,
            "text": "Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085-1097, Florence, Italy. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1128
                },
                {
                    "x": 2197,
                    "y": 1128
                },
                {
                    "x": 2197,
                    "y": 1863
                },
                {
                    "x": 1272,
                    "y": 1863
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>Victor Sanh, Albert Webson, Colin Raffel, Stephen<br>Bach, Lintang Sutawika, Zaid Alyafeai, Antoine<br>Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,<br>M Saiful Bari, Canwen Xu, Urmish Thakker,<br>Shanya Sharma Sharma, Eliza Szczechla, Tae-<br>woon Kim, Gunjan Chhablani, Nihal Nayak, De-<br>bajyoti Datta, Jonathan Chang, Mike Tian-Jian<br>Jiang, Han Wang, Matteo Manica, Sheng Shen,<br>Zheng Xin Yong, Harshit Pandey, Rachel Bawden,<br>Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-<br>heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-<br>son Alan Fries, Ryan Teehan, Teven Le Scao, Stella<br>Biderman, Leo Gao, Thomas Wolf, and Alexan-<br>der M Rush. 2022. Multitask prompted training en-<br>ables zero-shot task generalization. In International<br>Conference on Learning Representations.</p>",
            "id": 145,
            "page": 12,
            "text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1892
                },
                {
                    "x": 2198,
                    "y": 1892
                },
                {
                    "x": 2198,
                    "y": 2169
                },
                {
                    "x": 1270,
                    "y": 2169
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:16px'>Teven Le Scao, Angela Fan, Christopher Akiki, El-<br>lie Pavlick, Suzana Ilic, Daniel Hesslow, Ro-<br>man Castagne, Alexandra Sasha Luccioni, Fran�ois<br>Yvon, Matthias Galle, et al. 2022. Bloom: A 176b-<br>parameter open-access multilingual language model.<br>arXiv preprint arXiv:2211.05100.</p>",
            "id": 146,
            "page": 12,
            "text": "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Fran�ois Yvon, Matthias Galle,  2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2200
                },
                {
                    "x": 2196,
                    "y": 2200
                },
                {
                    "x": 2196,
                    "y": 2383
                },
                {
                    "x": 1271,
                    "y": 2383
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:20px'>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang<br>Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.<br>2023. Is chatgpt a good nlg evaluator? a preliminary<br>study. arXiv preprint arXiv:2303.04048.</p>",
            "id": 147,
            "page": 12,
            "text": "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2414
                },
                {
                    "x": 2196,
                    "y": 2414
                },
                {
                    "x": 2196,
                    "y": 2645
                },
                {
                    "x": 1270,
                    "y": 2645
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:14px'>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,<br>Adams Wei Yu, Brian Lester, Nan Du, Andrew M.<br>Dai, and Quoc V Le. 2022. Finetuned language<br>models are zero-shot learners. In International Con-<br>ference on Learning Representations.</p>",
            "id": 148,
            "page": 12,
            "text": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2675
                },
                {
                    "x": 2196,
                    "y": 2675
                },
                {
                    "x": 2196,
                    "y": 3226
                },
                {
                    "x": 1272,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:18px'>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien<br>Chaumond, Clement Delangue, Anthony Moi, Pier-<br>ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-<br>icz, Joe Davison, Sam Shleifer, Patrick von Platen,<br>Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,<br>Teven Le Scao, Sylvain Gugger, Mariama Drame,<br>Quentin Lhoest, and Alexander Rush. 2020. Trans-<br>formers: State-of-the-art natural language process-<br>ing. In Proceedings of the 2020 Conference on Em-<br>pirical Methods in Natural Language Processing:<br>System Demonstrations, pages 38-45, Online. Asso-<br>ciation for Computational Linguistics.</p>",
            "id": 149,
            "page": 12,
            "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 302
                },
                {
                    "x": 1214,
                    "y": 302
                },
                {
                    "x": 1214,
                    "y": 629
                },
                {
                    "x": 289,
                    "y": 629
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:16px'>KiYoon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak.<br>2022. Detection of adversarial examples in text<br>classification: Benchmark and baseline via robust<br>density estimation. In Findings of the Association<br>for Computational Linguistics: ACL 2022, pages<br>3656-3672, Dublin, Ireland. Association for Com-<br>putational Linguistics.</p>",
            "id": 150,
            "page": 13,
            "text": "KiYoon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak. 2022. Detection of adversarial examples in text classification: Benchmark and baseline via robust density estimation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3656-3672, Dublin, Ireland. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 654
                },
                {
                    "x": 1214,
                    "y": 654
                },
                {
                    "x": 1214,
                    "y": 935
                },
                {
                    "x": 290,
                    "y": 935
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='151' style='font-size:18px'>Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof<br>Choromanski, Federico Tombari, Aveek Purohit,<br>Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-<br>cent Vanhoucke, et al. 2022. Socratic models: Com-<br>posing zero-shot multimodal reasoning with lan-<br>guage. arXiv preprint arXiv:2204.00598.</p>",
            "id": 151,
            "page": 13,
            "text": "Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke,  2022. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 960
                },
                {
                    "x": 1216,
                    "y": 960
                },
                {
                    "x": 1216,
                    "y": 1148
                },
                {
                    "x": 291,
                    "y": 1148
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='152' style='font-size:16px'>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.<br>Character-level convolutional networks for text clas-<br>sification. Advances in neural information process-<br>ing systems, 28.</p>",
            "id": 152,
            "page": 13,
            "text": "Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1175
                },
                {
                    "x": 1214,
                    "y": 1175
                },
                {
                    "x": 1214,
                    "y": 1407
                },
                {
                    "x": 289,
                    "y": 1407
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='153' style='font-size:16px'>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and<br>Sameer Singh. 2021. Calibrate before use: Improv-<br>ing few-shot performance of language models. In In-<br>ternational Conference on Machine Learning, pages<br>12697-12706. PMLR.</p>",
            "id": 153,
            "page": 13,
            "text": "Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697-12706. PMLR."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1438
                },
                {
                    "x": 1128,
                    "y": 1438
                },
                {
                    "x": 1128,
                    "y": 1494
                },
                {
                    "x": 287,
                    "y": 1494
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:20px'>A Modification Based on the Reviews</p>",
            "id": 154,
            "page": 13,
            "text": "A Modification Based on the Reviews"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1523
                },
                {
                    "x": 1215,
                    "y": 1523
                },
                {
                    "x": 1215,
                    "y": 1805
                },
                {
                    "x": 286,
                    "y": 1805
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:16px'>We list the main difference between this version<br>and the pre-review version of our paper; the modifi-<br>cations are all based on the reviewers' suggestions.<br>We thank the reviewer again for those valuable<br>suggestions.</p>",
            "id": 155,
            "page": 13,
            "text": "We list the main difference between this version and the pre-review version of our paper; the modifications are all based on the reviewers' suggestions. We thank the reviewer again for those valuable suggestions."
        },
        {
            "bounding_box": [
                {
                    "x": 336,
                    "y": 1837
                },
                {
                    "x": 1217,
                    "y": 1837
                },
                {
                    "x": 1217,
                    "y": 2867
                },
                {
                    "x": 336,
                    "y": 2867
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:16px'>● We add Section 3.3.1 to discuss whether the<br>LLM and human evaluators agree on the rat-<br>ing of individual stories.<br>● We refine the wordings in Section 5 and add<br>relevant references.<br>● We add Table 6 to discuss the correlation be-<br>tween human evaluators.<br>· We conduct supplementary experiments on<br>human evaluation that mixes human-written<br>stories and GPT-2-generated stories when con-<br>ducting human evaluation and report the re-<br>sults in Table 5.<br>● We correct the typos and include almost all<br>presentation suggestions mentioned by the re-<br>viewers. We cannot follow all presentation<br>suggestions due to limited space.</p>",
            "id": 156,
            "page": 13,
            "text": "● We add Section 3.3.1 to discuss whether the LLM and human evaluators agree on the rating of individual stories. ● We refine the wordings in Section 5 and add relevant references. ● We add Table 6 to discuss the correlation between human evaluators. · We conduct supplementary experiments on human evaluation that mixes human-written stories and GPT-2-generated stories when conducting human evaluation and report the results in Table 5. ● We correct the typos and include almost all presentation suggestions mentioned by the reviewers. We cannot follow all presentation suggestions due to limited space."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2897
                },
                {
                    "x": 1154,
                    "y": 2897
                },
                {
                    "x": 1154,
                    "y": 3012
                },
                {
                    "x": 288,
                    "y": 3012
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='157' style='font-size:22px'>B Experiment Details for Open-Ended<br>Story Generation</p>",
            "id": 157,
            "page": 13,
            "text": "B Experiment Details for Open-Ended Story Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3043
                },
                {
                    "x": 952,
                    "y": 3043
                },
                {
                    "x": 952,
                    "y": 3096
                },
                {
                    "x": 288,
                    "y": 3096
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:18px'>B.1 The WritingPrompt Dataset</p>",
            "id": 158,
            "page": 13,
            "text": "B.1 The WritingPrompt Dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 3116
                },
                {
                    "x": 1214,
                    "y": 3116
                },
                {
                    "x": 1214,
                    "y": 3226
                },
                {
                    "x": 286,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='159' style='font-size:18px'>The training dataset contains 303K pairs of<br>stories and prompts, which our model is trained</p>",
            "id": 159,
            "page": 13,
            "text": "The training dataset contains 303K pairs of stories and prompts, which our model is trained"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 302
                },
                {
                    "x": 2208,
                    "y": 302
                },
                {
                    "x": 2208,
                    "y": 525
                },
                {
                    "x": 1268,
                    "y": 525
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:18px'>on. We only use 200 prompt-story pairs from<br>the test set. The dataset is downloaded from<br>https://www.kaggie.com/datases/nathucikding-<br>prompts.</p>",
            "id": 160,
            "page": 13,
            "text": "on. We only use 200 prompt-story pairs from the test set. The dataset is downloaded from https://www.kaggie.com/datases/nathucikdingprompts."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 567
                },
                {
                    "x": 1963,
                    "y": 567
                },
                {
                    "x": 1963,
                    "y": 618
                },
                {
                    "x": 1269,
                    "y": 618
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:18px'>B.2 Fine-tuning the GPT-2 Model</p>",
            "id": 161,
            "page": 13,
            "text": "B.2 Fine-tuning the GPT-2 Model"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 642
                },
                {
                    "x": 2193,
                    "y": 642
                },
                {
                    "x": 2193,
                    "y": 867
                },
                {
                    "x": 1267,
                    "y": 867
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:16px'>We train the model for 3 epochs with a learning<br>rate of 5e - 5 and linear learning rate schedule, and<br>the trained model eventually reaches a perplexity<br>of 20 on the validation set of Wri tingPrompts.</p>",
            "id": 162,
            "page": 13,
            "text": "We train the model for 3 epochs with a learning rate of 5e - 5 and linear learning rate schedule, and the trained model eventually reaches a perplexity of 20 on the validation set of Wri tingPrompts."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 909
                },
                {
                    "x": 1798,
                    "y": 909
                },
                {
                    "x": 1798,
                    "y": 963
                },
                {
                    "x": 1268,
                    "y": 963
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:20px'>B.3 Data Post-processing</p>",
            "id": 163,
            "page": 13,
            "text": "B.3 Data Post-processing"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 984
                },
                {
                    "x": 2197,
                    "y": 984
                },
                {
                    "x": 2197,
                    "y": 1601
                },
                {
                    "x": 1268,
                    "y": 1601
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:18px'>Once the model is trained, we randomly select 200<br>prompts from the testing set of WritingPrompts,<br>and feed the prompts to the trained model and ask<br>the model to generate stories based on the given<br>prompts. When generating the stories, we adopt<br>nucleus sampling with p = 0.9. Next, we manu-<br>ally truncate the generated stories to less than 150<br>words and ensure that after the truncation, the story<br>ends with a full sentence. 7 After this process, we<br>have 200 pairs of prompts and model-generated<br>stories.</p>",
            "id": 164,
            "page": 13,
            "text": "Once the model is trained, we randomly select 200 prompts from the testing set of WritingPrompts, and feed the prompts to the trained model and ask the model to generate stories based on the given prompts. When generating the stories, we adopt nucleus sampling with p = 0.9. Next, we manually truncate the generated stories to less than 150 words and ensure that after the truncation, the story ends with a full sentence. 7 After this process, we have 200 pairs of prompts and model-generated stories."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1609
                },
                {
                    "x": 2198,
                    "y": 1609
                },
                {
                    "x": 2198,
                    "y": 2226
                },
                {
                    "x": 1268,
                    "y": 2226
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:16px'>As a comparison to model-generated stories,<br>we select the same 200 prompts used for gen-<br>erating model-generated stories and their corre-<br>sponding human-written stories to form 200 pairs<br>of prompts and human-written stories. For these<br>human-written stories, we also truncate the sto-<br>ries to less than 150 words and end with a full<br>sentence to match the model-generated sentences.<br>We also manually remove some artifacts in the<br>human-written story due to the tokenization of the<br>WritingPrompts dataset.</p>",
            "id": 165,
            "page": 13,
            "text": "As a comparison to model-generated stories, we select the same 200 prompts used for generating model-generated stories and their corresponding human-written stories to form 200 pairs of prompts and human-written stories. For these human-written stories, we also truncate the stories to less than 150 words and end with a full sentence to match the model-generated sentences. We also manually remove some artifacts in the human-written story due to the tokenization of the WritingPrompts dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2270
                },
                {
                    "x": 1769,
                    "y": 2270
                },
                {
                    "x": 1769,
                    "y": 2324
                },
                {
                    "x": 1270,
                    "y": 2324
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:22px'>C Human Evaluation</p>",
            "id": 166,
            "page": 13,
            "text": "C Human Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2363
                },
                {
                    "x": 1941,
                    "y": 2363
                },
                {
                    "x": 1941,
                    "y": 2414
                },
                {
                    "x": 1269,
                    "y": 2414
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>C.1 Recruiting English Teachers</p>",
            "id": 167,
            "page": 13,
            "text": "C.1 Recruiting English Teachers"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2435
                },
                {
                    "x": 2196,
                    "y": 2435
                },
                {
                    "x": 2196,
                    "y": 3057
                },
                {
                    "x": 1269,
                    "y": 3057
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:18px'>The English teachers hold ESL certificates&; given<br>that they are experienced with correcting essays<br>written by students, they are perfect fits for this task.<br>Each teacher is asked to rate 200 GPT-2-generated<br>stories and 200 human-written stories, and they are<br>paid US$140 for rating 200 stories. Considering<br>that the teachers reported that they take at most<br>5 hours to rate 200 stories, this makes the hourly<br>wage at least US$28. We first ask the teachers to<br>rate the GPT-2-generated stories and then the 200<br>human-written stories. Different from Karpinska</p>",
            "id": 168,
            "page": 13,
            "text": "The English teachers hold ESL certificates&; given that they are experienced with correcting essays written by students, they are perfect fits for this task. Each teacher is asked to rate 200 GPT-2-generated stories and 200 human-written stories, and they are paid US$140 for rating 200 stories. Considering that the teachers reported that they take at most 5 hours to rate 200 stories, this makes the hourly wage at least US$28. We first ask the teachers to rate the GPT-2-generated stories and then the 200 human-written stories. Different from Karpinska"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 3096
                },
                {
                    "x": 2189,
                    "y": 3096
                },
                {
                    "x": 2189,
                    "y": 3173
                },
                {
                    "x": 1273,
                    "y": 3173
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:14px'>7We truncate the story to 150 words since this is the mean<br>length of the model-generated story.</p>",
            "id": 169,
            "page": 13,
            "text": "7We truncate the story to 150 words since this is the mean length of the model-generated story."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 3181
                },
                {
                    "x": 2146,
                    "y": 3181
                },
                {
                    "x": 2146,
                    "y": 3223
                },
                {
                    "x": 1323,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='170' style='font-size:14px'>8English as a Second Language Teaching Certification</p>",
            "id": 170,
            "page": 13,
            "text": "8English as a Second Language Teaching Certification"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 301
                },
                {
                    "x": 1214,
                    "y": 301
                },
                {
                    "x": 1214,
                    "y": 750
                },
                {
                    "x": 288,
                    "y": 750
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:20px'>et al. (2021) that take a break between the rating<br>of GPT-2-generated stories and the human-written<br>stories, we do not take a break to avoid the teacher's<br>rating standard to change after taking a long break.<br>The teachers are not told who wrote the stories<br>before they evaluate the stories. We reveal to them<br>what this project aims to study after they finish<br>rating all the stories.</p>",
            "id": 171,
            "page": 14,
            "text": " (2021) that take a break between the rating of GPT-2-generated stories and the human-written stories, we do not take a break to avoid the teacher's rating standard to change after taking a long break. The teachers are not told who wrote the stories before they evaluate the stories. We reveal to them what this project aims to study after they finish rating all the stories."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 756
                },
                {
                    "x": 1214,
                    "y": 756
                },
                {
                    "x": 1214,
                    "y": 2678
                },
                {
                    "x": 286,
                    "y": 2678
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:20px'>The reason we do not mix human-written and<br>GPT-2-generated stories for rating is that in Karpin-<br>ska et al. (202 1 )their observation is that (1) when<br>AMT workers rate model-generated and human-<br>written stories separately, their ratings do not<br>show preference toward human-written stories, but<br>(2) even when rating the model-generated and<br>human-written stories separately, English teacher<br>shows clear preference toward human-written sto-<br>ries. We follow their settings and do not mix GPT-<br>2-generated/human-written stories. During the re-<br>viewing process, we received questions from the<br>reviewers about why not mixing the stories for hu-<br>man evaluation. Thus, we conduct the same ex-<br>periment by randomly mixing 200 human-written<br>and 200 GPT-2-generated stories and asking three<br>teachers (not the teachers that already rated the sto-<br>ries) to rate them. All other experiment conditions<br>are the same as previously stated. The full result<br>is shown in Table 5. We find that the teacher still<br>shows a clear preference toward human-written sto-<br>ries for all four attributes, similar to the observation<br>in Table 1. The only exception is grammaticality,<br>where English teachers do not show a very clear<br>preference for the grammar of human-written sto-<br>ries. However, when calculating the average rating<br>for individual teachers, we find that two out of<br>three teachers do rate grammaticality higher for<br>human-written stories. It is interesting to note that<br>for LLM evaluation, there is no such problem about<br>whether or not to mix the human-written and GPT-<br>2-generated stories during LLM evaluation as the<br>rating of each story is independent of each other,<br>as discussed in Section 5.</p>",
            "id": 172,
            "page": 14,
            "text": "The reason we do not mix human-written and GPT-2-generated stories for rating is that in Karpinska  (202 1 )their observation is that (1) when AMT workers rate model-generated and humanwritten stories separately, their ratings do not show preference toward human-written stories, but (2) even when rating the model-generated and human-written stories separately, English teacher shows clear preference toward human-written stories. We follow their settings and do not mix GPT2-generated/human-written stories. During the reviewing process, we received questions from the reviewers about why not mixing the stories for human evaluation. Thus, we conduct the same experiment by randomly mixing 200 human-written and 200 GPT-2-generated stories and asking three teachers (not the teachers that already rated the stories) to rate them. All other experiment conditions are the same as previously stated. The full result is shown in Table 5. We find that the teacher still shows a clear preference toward human-written stories for all four attributes, similar to the observation in Table 1. The only exception is grammaticality, where English teachers do not show a very clear preference for the grammar of human-written stories. However, when calculating the average rating for individual teachers, we find that two out of three teachers do rate grammaticality higher for human-written stories. It is interesting to note that for LLM evaluation, there is no such problem about whether or not to mix the human-written and GPT2-generated stories during LLM evaluation as the rating of each story is independent of each other, as discussed in Section 5."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2686
                },
                {
                    "x": 1213,
                    "y": 2686
                },
                {
                    "x": 1213,
                    "y": 2967
                },
                {
                    "x": 288,
                    "y": 2967
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='173' style='font-size:20px'>For adversarial attack quality evaluation, we also<br>recruit certified teachers on Upwork. The teachers<br>are asked to rate 100 news titles and are paid US$35<br>for doing so. They reported that it took them less<br>than 1 hour to complete the rating.</p>",
            "id": 173,
            "page": 14,
            "text": "For adversarial attack quality evaluation, we also recruit certified teachers on Upwork. The teachers are asked to rate 100 news titles and are paid US$35 for doing so. They reported that it took them less than 1 hour to complete the rating."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 3030
                },
                {
                    "x": 971,
                    "y": 3030
                },
                {
                    "x": 971,
                    "y": 3082
                },
                {
                    "x": 289,
                    "y": 3082
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:20px'>C.2 Human Evaluation Interface</p>",
            "id": 174,
            "page": 14,
            "text": "C.2 Human Evaluation Interface"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3115
                },
                {
                    "x": 1215,
                    "y": 3115
                },
                {
                    "x": 1215,
                    "y": 3227
                },
                {
                    "x": 288,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:22px'>Open-Ended Story Generation We use Google<br>Forms to collect the responses from the teachers.</p>",
            "id": 175,
            "page": 14,
            "text": "Open-Ended Story Generation We use Google Forms to collect the responses from the teachers."
        },
        {
            "bounding_box": [
                {
                    "x": 1353,
                    "y": 285
                },
                {
                    "x": 2098,
                    "y": 285
                },
                {
                    "x": 2098,
                    "y": 599
                },
                {
                    "x": 1353,
                    "y": 599
                }
            ],
            "category": "table",
            "html": "<br><table id='176' style='font-size:22px'><tr><td>Writer</td><td>Human</td><td>GPT-2</td></tr><tr><td>Grammaticality</td><td>3.890.97</td><td>3.880.84</td></tr><tr><td>Cohesiveness</td><td>4.350.87</td><td>3.490.97</td></tr><tr><td>Likability</td><td>3.461.40</td><td>2.891.12</td></tr><tr><td>Relevance</td><td>3.711.20</td><td>2.371.33</td></tr></table>",
            "id": 176,
            "page": 14,
            "text": "Writer Human GPT-2  Grammaticality 3.890.97 3.880.84  Cohesiveness 4.350.87 3.490.97  Likability 3.461.40 2.891.12  Relevance 3.711.20"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 631
                },
                {
                    "x": 2195,
                    "y": 631
                },
                {
                    "x": 2195,
                    "y": 833
                },
                {
                    "x": 1268,
                    "y": 833
                }
            ],
            "category": "caption",
            "html": "<caption id='177' style='font-size:20px'>Table 5: The average Likert score for human-written<br>and GPT-2-generated stories when we randomly mix<br>the 200 model-generated and 200 human-written sto-<br>ries during human evaluation.</caption>",
            "id": 177,
            "page": 14,
            "text": "Table 5: The average Likert score for human-written and GPT-2-generated stories when we randomly mix the 200 model-generated and 200 human-written stories during human evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 914
                },
                {
                    "x": 2195,
                    "y": 914
                },
                {
                    "x": 2195,
                    "y": 1251
                },
                {
                    "x": 1268,
                    "y": 1251
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:20px'>Each form contains 100 stories, and each story is<br>on one page of the Google Form. The interface on<br>one page is shown in Figure 2 and Figure 3; the two<br>figures are from the same page of the Google Form,<br>and we are splitting them because screenshotting<br>the whole interface will cause low resolution.</p>",
            "id": 178,
            "page": 14,
            "text": "Each form contains 100 stories, and each story is on one page of the Google Form. The interface on one page is shown in Figure 2 and Figure 3; the two figures are from the same page of the Google Form, and we are splitting them because screenshotting the whole interface will cause low resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1303,
                    "y": 1327
                },
                {
                    "x": 1585,
                    "y": 1327
                },
                {
                    "x": 1585,
                    "y": 1350
                },
                {
                    "x": 1303,
                    "y": 1350
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:14px'>Please rate the story fragment</p>",
            "id": 179,
            "page": 14,
            "text": "Please rate the story fragment"
        },
        {
            "bounding_box": [
                {
                    "x": 1305,
                    "y": 1353
                },
                {
                    "x": 2075,
                    "y": 1353
                },
                {
                    "x": 2075,
                    "y": 1448
                },
                {
                    "x": 1305,
                    "y": 1448
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='180' style='font-size:14px'>The goal of this task is to rate story fragment.<br>NOTE: Please take the time to fully read and understand the story fragment. We will<br>reject submissions from workers that are clearly spamming the task.</p>",
            "id": 180,
            "page": 14,
            "text": "The goal of this task is to rate story fragment. NOTE: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task."
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 1569
                },
                {
                    "x": 1473,
                    "y": 1569
                },
                {
                    "x": 1473,
                    "y": 1596
                },
                {
                    "x": 1311,
                    "y": 1596
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:18px'>Story fragment</p>",
            "id": 181,
            "page": 14,
            "text": "Story fragment"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1597
                },
                {
                    "x": 2134,
                    "y": 1597
                },
                {
                    "x": 2134,
                    "y": 1877
                },
                {
                    "x": 1312,
                    "y": 1877
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:14px'>The bug trailed like a shadow, clawing its way through the thick vegetation that covered a<br>vast expanse of land. It was nearly six miles long, with five legs. The main body was made<br>of wood, and this limb had been torn from the last tree and scattered like a broken lawn<br>mower. There were plenty of other legs in the bug's appendages, though these ones were<br>many times more powerful than the main legs. The winged beast sat on a hill, in a region<br>named Alola, a place that had never been visited by any new human inhabitants. Perhaps<br>the only one to still live, because it inhabited an entire continent, six miles wide, four miles<br>high, and the most densely populated patch of land on Earth. There were many other<br>countries in this particular area, but the Bug was the only known survivor.</p>",
            "id": 182,
            "page": 14,
            "text": "The bug trailed like a shadow, clawing its way through the thick vegetation that covered a vast expanse of land. It was nearly six miles long, with five legs. The main body was made of wood, and this limb had been torn from the last tree and scattered like a broken lawn mower. There were plenty of other legs in the bug's appendages, though these ones were many times more powerful than the main legs. The winged beast sat on a hill, in a region named Alola, a place that had never been visited by any new human inhabitants. Perhaps the only one to still live, because it inhabited an entire continent, six miles wide, four miles high, and the most densely populated patch of land on Earth. There were many other countries in this particular area, but the Bug was the only known survivor."
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1965
                },
                {
                    "x": 2103,
                    "y": 1965
                },
                {
                    "x": 2103,
                    "y": 2029
                },
                {
                    "x": 1312,
                    "y": 2029
                }
            ],
            "category": "caption",
            "html": "<caption id='183' style='font-size:16px'>How grammatically correct is the text of the story fragment? (on a scale of 1-5,<br>with 1 being the lowest)</caption>",
            "id": 183,
            "page": 14,
            "text": "How grammatically correct is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 2131,
                    "y": 1967
                },
                {
                    "x": 2147,
                    "y": 1967
                },
                {
                    "x": 2147,
                    "y": 1985
                },
                {
                    "x": 2131,
                    "y": 1985
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='184' style='font-size:14px'>*</p>",
            "id": 184,
            "page": 14,
            "text": "*"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 2063
                },
                {
                    "x": 1460,
                    "y": 2063
                },
                {
                    "x": 1460,
                    "y": 2099
                },
                {
                    "x": 1312,
                    "y": 2099
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:18px'>○ 1 (lowest)</p>",
            "id": 185,
            "page": 14,
            "text": "○ 1 (lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2119
                },
                {
                    "x": 1377,
                    "y": 2119
                },
                {
                    "x": 1377,
                    "y": 2154
                },
                {
                    "x": 1314,
                    "y": 2154
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:18px'>○ 2</p>",
            "id": 186,
            "page": 14,
            "text": "○ 2"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2175
                },
                {
                    "x": 1377,
                    "y": 2175
                },
                {
                    "x": 1377,
                    "y": 2210
                },
                {
                    "x": 1314,
                    "y": 2210
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:16px'>○ 3</p>",
            "id": 187,
            "page": 14,
            "text": "○ 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2231
                },
                {
                    "x": 1379,
                    "y": 2231
                },
                {
                    "x": 1379,
                    "y": 2267
                },
                {
                    "x": 1313,
                    "y": 2267
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:16px'>○ 4</p>",
            "id": 188,
            "page": 14,
            "text": "○ 4"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2289
                },
                {
                    "x": 1466,
                    "y": 2289
                },
                {
                    "x": 1466,
                    "y": 2322
                },
                {
                    "x": 1314,
                    "y": 2322
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:18px'>○ 5 (highest)</p>",
            "id": 189,
            "page": 14,
            "text": "○ 5 (highest)"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2415
                },
                {
                    "x": 2191,
                    "y": 2415
                },
                {
                    "x": 2191,
                    "y": 2512
                },
                {
                    "x": 1270,
                    "y": 2512
                }
            ],
            "category": "caption",
            "html": "<caption id='190' style='font-size:20px'>Figure 2: The upper part of the interface in open-ended<br>story generation.</caption>",
            "id": 190,
            "page": 14,
            "text": "Figure 2: The upper part of the interface in open-ended story generation."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2605
                },
                {
                    "x": 2194,
                    "y": 2605
                },
                {
                    "x": 2194,
                    "y": 3112
                },
                {
                    "x": 1268,
                    "y": 3112
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:20px'>Adversarial Attacks Quality Evaluation In<br>this task, we also use Google Forms to collect the<br>responses from the teachers. We create two dif-<br>ferent Google Forms, one is used to evaluate the<br>fluency, whose interface is shown in Figure 4. In<br>this form, we mix an equal number of benign news<br>titles, TextFooler-attacked, PWWS-attacked, and<br>BAE-attacked news titles. Each page of the Google<br>Form contains one news title.</p>",
            "id": 191,
            "page": 14,
            "text": "Adversarial Attacks Quality Evaluation In this task, we also use Google Forms to collect the responses from the teachers. We create two different Google Forms, one is used to evaluate the fluency, whose interface is shown in Figure 4. In this form, we mix an equal number of benign news titles, TextFooler-attacked, PWWS-attacked, and BAE-attacked news titles. Each page of the Google Form contains one news title."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 3119
                },
                {
                    "x": 2192,
                    "y": 3119
                },
                {
                    "x": 2192,
                    "y": 3227
                },
                {
                    "x": 1269,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='192' style='font-size:22px'>Another Google Form is used to compare the<br>meaning preserving of the news title before and</p>",
            "id": 192,
            "page": 14,
            "text": "Another Google Form is used to compare the meaning preserving of the news title before and"
        },
        {
            "bounding_box": [
                {
                    "x": 312,
                    "y": 305
                },
                {
                    "x": 1199,
                    "y": 305
                },
                {
                    "x": 1199,
                    "y": 1885
                },
                {
                    "x": 312,
                    "y": 1885
                }
            ],
            "category": "figure",
            "html": "<figure><img id='193' style='font-size:14px' alt=\"How well do the sentences in the story fragment fit together? (on a scale of 1-5, *\nwith 1 being the lowest)\n○ 1 (lowest)\n○ 2\n○ 3\n○ 4\n○ 5 (highest)\nHow enjoyable do you find the story fragment? (on a scale of 1-5, with 1 being the *\nlowest)\n○ 1 (lowest)\n○ 2\n○ 3\n○ 4\n○ 5 (highest)\nNow read the PROMPT based on which the story fragment was written.\nPROMPT: You're the last person on earth - but thank god Pokemon Go still\nfunctions! You amuse yourself by catching Pokemon as you travel so as to not\nfeel so isolated and alone. One day, on your screen, you see in the distance that\nsomeone has set up a lure.\nHow relevant is the story fragment to the prompt? (on a scale of 1-5, with 1 being\nthe lowest)\n○ 1 (lowest)\n○ 2\n○ 3\n○ 4\n○ 5 (highest)\" data-coord=\"top-left:(312,305); bottom-right:(1199,1885)\" /></figure>",
            "id": 193,
            "page": 15,
            "text": "How well do the sentences in the story fragment fit together? (on a scale of 1-5, * with 1 being the lowest) ○ 1 (lowest) ○ 2 ○ 3 ○ 4 ○ 5 (highest) How enjoyable do you find the story fragment? (on a scale of 1-5, with 1 being the * lowest) ○ 1 (lowest) ○ 2 ○ 3 ○ 4 ○ 5 (highest) Now read the PROMPT based on which the story fragment was written. PROMPT: You're the last person on earth - but thank god Pokemon Go still functions! You amuse yourself by catching Pokemon as you travel so as to not feel so isolated and alone. One day, on your screen, you see in the distance that someone has set up a lure. How relevant is the story fragment to the prompt? (on a scale of 1-5, with 1 being the lowest) ○ 1 (lowest) ○ 2 ○ 3 ○ 4 ○ 5 (highest)"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1931
                },
                {
                    "x": 1211,
                    "y": 1931
                },
                {
                    "x": 1211,
                    "y": 2030
                },
                {
                    "x": 290,
                    "y": 2030
                }
            ],
            "category": "caption",
            "html": "<caption id='194' style='font-size:16px'>Figure 3: The lower part of the interface in open-ended<br>story generation.</caption>",
            "id": 194,
            "page": 15,
            "text": "Figure 3: The lower part of the interface in open-ended story generation."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2121
                },
                {
                    "x": 1215,
                    "y": 2121
                },
                {
                    "x": 1215,
                    "y": 2401
                },
                {
                    "x": 289,
                    "y": 2401
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:20px'>after the adversarial attacks. We highlight the dif-<br>ference between the benign and adversarial sam-<br>ples using boldface, as shown in Figure 5. On each<br>page of the Google Form, there is one pair of news<br>titles.</p>",
            "id": 195,
            "page": 15,
            "text": "after the adversarial attacks. We highlight the difference between the benign and adversarial samples using boldface, as shown in Figure 5. On each page of the Google Form, there is one pair of news titles."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2443
                },
                {
                    "x": 1060,
                    "y": 2443
                },
                {
                    "x": 1060,
                    "y": 2550
                },
                {
                    "x": 290,
                    "y": 2550
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:22px'>C.3 Post-Task Interview with English<br>Teachers</p>",
            "id": 196,
            "page": 15,
            "text": "C.3 Post-Task Interview with English Teachers"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2574
                },
                {
                    "x": 1200,
                    "y": 2574
                },
                {
                    "x": 1200,
                    "y": 2627
                },
                {
                    "x": 287,
                    "y": 2627
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='197' style='font-size:22px'>C.3.1 How English Teachers Rate the Stories</p>",
            "id": 197,
            "page": 15,
            "text": "C.3.1 How English Teachers Rate the Stories"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2642
                },
                {
                    "x": 1212,
                    "y": 2642
                },
                {
                    "x": 1212,
                    "y": 2749
                },
                {
                    "x": 289,
                    "y": 2749
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='198' style='font-size:20px'>After the teachers rate 400 stories, we ask them the<br>following questions:</p>",
            "id": 198,
            "page": 15,
            "text": "After the teachers rate 400 stories, we ask them the following questions:"
        },
        {
            "bounding_box": [
                {
                    "x": 298,
                    "y": 2792
                },
                {
                    "x": 1216,
                    "y": 2792
                },
                {
                    "x": 1216,
                    "y": 2901
                },
                {
                    "x": 298,
                    "y": 2901
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:18px'>Q1 How long did it take you to rate the 400 sto-<br>ries?</p>",
            "id": 199,
            "page": 15,
            "text": "Q1 How long did it take you to rate the 400 stories?"
        },
        {
            "bounding_box": [
                {
                    "x": 300,
                    "y": 2944
                },
                {
                    "x": 1216,
                    "y": 2944
                },
                {
                    "x": 1216,
                    "y": 3227
                },
                {
                    "x": 300,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:20px'>Q2 What is your standard on each of the four at-<br>tributes (grammatical, coherence, likability,<br>relevance) evaluated? For example, in what<br>case do you give a high/low rating for gram-<br>matically? What kind of story did you give</p>",
            "id": 200,
            "page": 15,
            "text": "Q2 What is your standard on each of the four attributes (grammatical, coherence, likability, relevance) evaluated? For example, in what case do you give a high/low rating for grammatically? What kind of story did you give"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 299
                },
                {
                    "x": 2166,
                    "y": 299
                },
                {
                    "x": 2166,
                    "y": 839
                },
                {
                    "x": 1283,
                    "y": 839
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='201' style='font-size:14px' alt=\":::\nYou are given a news title. Please read the news title and answer the question.\nNews title: HK walks out of 68-month deflation cycle, official Hong Kong education Secretary\nHenry Tang said he believed Hong Kong has gotten out of the consumer price deflation cycle\nthat passes for 68 months, according to the consumer price index trend in the first few years.\n(End of news title)\nQuestion: How natural and fluent is the text of the news title? (on a scale of 1-5, with 1\nbeing the lowest)\n1 (lowest)\n2\n3\n4\n5 (highest)\" data-coord=\"top-left:(1283,299); bottom-right:(2166,839)\" /></figure>",
            "id": 201,
            "page": 15,
            "text": "::: You are given a news title. Please read the news title and answer the question. News title: HK walks out of 68-month deflation cycle, official Hong Kong education Secretary Henry Tang said he believed Hong Kong has gotten out of the consumer price deflation cycle that passes for 68 months, according to the consumer price index trend in the first few years. (End of news title) Question: How natural and fluent is the text of the news title? (on a scale of 1-5, with 1 being the lowest) 1 (lowest) 2 3 4 5 (highest)"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 890
                },
                {
                    "x": 2192,
                    "y": 890
                },
                {
                    "x": 2192,
                    "y": 986
                },
                {
                    "x": 1269,
                    "y": 986
                }
            ],
            "category": "caption",
            "html": "<caption id='202' style='font-size:18px'>Figure 4: The Google Form used to evaluate the fluency<br>of the benign or adversarial samples.</caption>",
            "id": 202,
            "page": 15,
            "text": "Figure 4: The Google Form used to evaluate the fluency of the benign or adversarial samples."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1040
                },
                {
                    "x": 2183,
                    "y": 1040
                },
                {
                    "x": 2183,
                    "y": 1929
                },
                {
                    "x": 1276,
                    "y": 1929
                }
            ],
            "category": "figure",
            "html": "<figure><img id='203' style='font-size:14px' alt=\"Compare news titles x :\nYou are given two news titles. Please read the news titles and answer the question.\nNews title 1\nAlltel buys Cingular properties, expands network Little Rock-based Alltel will expand its wireless\nphone service in Connecticut, Kentucky, Mississippi, Oklahoma and Texas in a $170 million deal with Cingular\nWireless.\nNews title 2\nAlltel uses Cingular properties, expands network Little Rock-based Alltel will further its wireless\nphone deployment in Connecticut, Kentucky, Mississippi, Oklahoma and Texas in a $170 million deal with\nCingular Wireless.\nQuestion: Do you agree that the meaning (or semantics) of news title 1 is preserved in\nnews title 2? (on a scale of 1-5, with 1 being the strongly disagree and 5 being strongly agree.)\n1 (strongly disagree)\n2\n3\n4\n5 (strongly agree)\" data-coord=\"top-left:(1276,1040); bottom-right:(2183,1929)\" /></figure>",
            "id": 203,
            "page": 15,
            "text": "Compare news titles x : You are given two news titles. Please read the news titles and answer the question. News title 1 Alltel buys Cingular properties, expands network Little Rock-based Alltel will expand its wireless phone service in Connecticut, Kentucky, Mississippi, Oklahoma and Texas in a $170 million deal with Cingular Wireless. News title 2 Alltel uses Cingular properties, expands network Little Rock-based Alltel will further its wireless phone deployment in Connecticut, Kentucky, Mississippi, Oklahoma and Texas in a $170 million deal with Cingular Wireless. Question: Do you agree that the meaning (or semantics) of news title 1 is preserved in news title 2? (on a scale of 1-5, with 1 being the strongly disagree and 5 being strongly agree.) 1 (strongly disagree) 2 3 4 5 (strongly agree)"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1987
                },
                {
                    "x": 2197,
                    "y": 1987
                },
                {
                    "x": 2197,
                    "y": 2138
                },
                {
                    "x": 1269,
                    "y": 2138
                }
            ],
            "category": "caption",
            "html": "<caption id='204' style='font-size:18px'>Figure 5: The Google Form used to evaluate the mean-<br>ing preserving between a benign sample and an adver-<br>sarial sample.</caption>",
            "id": 204,
            "page": 15,
            "text": "Figure 5: The Google Form used to evaluate the meaning preserving between a benign sample and an adversarial sample."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2229
                },
                {
                    "x": 2198,
                    "y": 2229
                },
                {
                    "x": 2198,
                    "y": 2700
                },
                {
                    "x": 1280,
                    "y": 2700
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:20px'>a low rating on likability? Did your personal<br>preference affect the rating?<br>Q3 How long did it take for you to calibrate your<br>rating on the task?<br>Q4 Did you change your rating on the first three<br>attributes after reading the prompt the story is<br>based on?</p>",
            "id": 205,
            "page": 15,
            "text": "a low rating on likability? Did your personal preference affect the rating? Q3 How long did it take for you to calibrate your rating on the task? Q4 Did you change your rating on the first three attributes after reading the prompt the story is based on?"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2739
                },
                {
                    "x": 2194,
                    "y": 2739
                },
                {
                    "x": 2194,
                    "y": 3021
                },
                {
                    "x": 1268,
                    "y": 3021
                }
            ],
            "category": "paragraph",
            "html": "<p id='206' style='font-size:20px'>We briefly summarize the answers from the three<br>teachers. The teachers report that they spent 6 to<br>10 hours rating 400 stories. For grammar, most<br>teachers check the punctuation° word choice, and<br>,<br>subject-verb agreement. English teachers decrease</p>",
            "id": 206,
            "page": 15,
            "text": "We briefly summarize the answers from the three teachers. The teachers report that they spent 6 to 10 hours rating 400 stories. For grammar, most teachers check the punctuation° word choice, and , subject-verb agreement. English teachers decrease"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 3054
                },
                {
                    "x": 2194,
                    "y": 3054
                },
                {
                    "x": 2194,
                    "y": 3226
                },
                {
                    "x": 1269,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:16px'>9One of the teachers explicitly told us punctuation is not<br>grammar. By searching the Internet, we find that there is a<br>debate on whether punctuation should be counted as a part of<br>grammar.</p>",
            "id": 207,
            "page": 15,
            "text": "9One of the teachers explicitly told us punctuation is not grammar. By searching the Internet, we find that there is a debate on whether punctuation should be counted as a part of grammar."
        },
        {
            "bounding_box": [
                {
                    "x": 397,
                    "y": 286
                },
                {
                    "x": 1097,
                    "y": 286
                },
                {
                    "x": 1097,
                    "y": 599
                },
                {
                    "x": 397,
                    "y": 599
                }
            ],
            "category": "table",
            "html": "<table id='208' style='font-size:20px'><tr><td>Writer</td><td>Human</td><td>GPT-2</td></tr><tr><td>Grammaticality</td><td>0.25</td><td>0.15</td></tr><tr><td>Cohesiveness</td><td>0.26</td><td>0.18</td></tr><tr><td>Likability</td><td>0.09</td><td>0.12</td></tr><tr><td>Relevance</td><td>0.38</td><td>0.41</td></tr></table>",
            "id": 208,
            "page": 16,
            "text": "Writer Human GPT-2  Grammaticality 0.25 0.15  Cohesiveness 0.26 0.18  Likability 0.09 0.12  Relevance 0.38"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 630
                },
                {
                    "x": 1213,
                    "y": 630
                },
                {
                    "x": 1213,
                    "y": 832
                },
                {
                    "x": 287,
                    "y": 832
                }
            ],
            "category": "caption",
            "html": "<caption id='209' style='font-size:14px'>Table 6: The Kendall's T correlation coefficient two En-<br>glish teachers. Three English teachers participate in the<br>rating, SO the result in the Table is average over 3)<br>Kendall's T.</caption>",
            "id": 209,
            "page": 16,
            "text": "Table 6: The Kendall's T correlation coefficient two English teachers. Three English teachers participate in the rating, SO the result in the Table is average over 3) Kendall's T."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 928
                },
                {
                    "x": 1213,
                    "y": 928
                },
                {
                    "x": 1213,
                    "y": 1033
                },
                {
                    "x": 288,
                    "y": 1033
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:14px'>their rating based on the types and number of gram-<br>mar errors in the stories.</p>",
            "id": 210,
            "page": 16,
            "text": "their rating based on the types and number of grammar errors in the stories."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1044
                },
                {
                    "x": 1212,
                    "y": 1044
                },
                {
                    "x": 1212,
                    "y": 1375
                },
                {
                    "x": 288,
                    "y": 1375
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='211' style='font-size:14px'>For coherence, the teachers rate it based on<br>whether the sentences in the stories follow a logical<br>sequence to build the narrative. The teachers ask<br>themselves questions such as \"does the story make<br>sense\" This is a more holistic evaluation of the<br>whole story.</p>",
            "id": 211,
            "page": 16,
            "text": "For coherence, the teachers rate it based on whether the sentences in the stories follow a logical sequence to build the narrative. The teachers ask themselves questions such as \"does the story make sense\" This is a more holistic evaluation of the whole story."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1385
                },
                {
                    "x": 1212,
                    "y": 1385
                },
                {
                    "x": 1212,
                    "y": 1888
                },
                {
                    "x": 288,
                    "y": 1888
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='212' style='font-size:16px'>For likability, some teachers say they try not to<br>be affected by personal preference. One teacher<br>asks herself: Did I personally enjoy it based on<br>the amount of sense it made and whether or not it<br>had stylistic flair, humor, or engaging plotting or<br>characterization? Overall, the teachers all try to<br>use a fair and objective view to rate the likability.<br>For relevance, the teachers simply check if the story<br>is based on the prompt or not.</p>",
            "id": 212,
            "page": 16,
            "text": "For likability, some teachers say they try not to be affected by personal preference. One teacher asks herself: Did I personally enjoy it based on the amount of sense it made and whether or not it had stylistic flair, humor, or engaging plotting or characterization? Overall, the teachers all try to use a fair and objective view to rate the likability. For relevance, the teachers simply check if the story is based on the prompt or not."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1897
                },
                {
                    "x": 1212,
                    "y": 1897
                },
                {
                    "x": 1212,
                    "y": 2231
                },
                {
                    "x": 288,
                    "y": 2231
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='213' style='font-size:16px'>The teachers said that it took them about five to<br>ten stories to calibrate their ratings. Except for one<br>teacher changing the rating on the other three at-<br>tributes after seeing the prompt on only one story,<br>the teachers do not change their rating on the three<br>other attributes after reading the prompts.</p>",
            "id": 213,
            "page": 16,
            "text": "The teachers said that it took them about five to ten stories to calibrate their ratings. Except for one teacher changing the rating on the other three attributes after seeing the prompt on only one story, the teachers do not change their rating on the three other attributes after reading the prompts."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2273
                },
                {
                    "x": 1148,
                    "y": 2273
                },
                {
                    "x": 1148,
                    "y": 2380
                },
                {
                    "x": 289,
                    "y": 2380
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:18px'>C.3.2 Teachers' Comments on ChatGPT's<br>Rating</p>",
            "id": 214,
            "page": 16,
            "text": "C.3.2 Teachers' Comments on ChatGPT's Rating"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2402
                },
                {
                    "x": 1212,
                    "y": 2402
                },
                {
                    "x": 1212,
                    "y": 3077
                },
                {
                    "x": 287,
                    "y": 3077
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='215' style='font-size:18px'>After the teachers finished the rating and answered<br>the four questions in Appendix C.3.1, we ask them<br>to check the ratings and explanations of ChatGPT<br>and ask if they agree with ChatGPT. We told the<br>teachers that the rating is done by another teacher<br>we hire; we do not tell the teacher that the rating is<br>done by an AI model to avoid possible stereotypes<br>about AI models. We randomly select four stories<br>rated by ChatGPT; two of them are highly-rated<br>and lowly-rated human-written stories and the other<br>two stories are highly-rated and lowly-rated GPT-<br>2-generated stories.</p>",
            "id": 215,
            "page": 16,
            "text": "After the teachers finished the rating and answered the four questions in Appendix C.3.1, we ask them to check the ratings and explanations of ChatGPT and ask if they agree with ChatGPT. We told the teachers that the rating is done by another teacher we hire; we do not tell the teacher that the rating is done by an AI model to avoid possible stereotypes about AI models. We randomly select four stories rated by ChatGPT; two of them are highly-rated and lowly-rated human-written stories and the other two stories are highly-rated and lowly-rated GPT2-generated stories."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3116
                },
                {
                    "x": 1214,
                    "y": 3116
                },
                {
                    "x": 1214,
                    "y": 3229
                },
                {
                    "x": 287,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='216' style='font-size:18px'>Story 1 It had been months since they found me<br>dead. I don't know why I stopped walking. No-</p>",
            "id": 216,
            "page": 16,
            "text": "Story 1 It had been months since they found me dead. I don't know why I stopped walking. No-"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 306
                },
                {
                    "x": 2196,
                    "y": 306
                },
                {
                    "x": 2196,
                    "y": 1032
                },
                {
                    "x": 1267,
                    "y": 1032
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='217' style='font-size:14px'>one else around to take my place, or talk with me.<br>I spent days frozen in shock, never able to talk,<br>unable to move, unable to express myself. It was<br>a strange thing, nobody else would suspect me of<br>wanting this. The more I focused on it, the more<br>it consumed me. Years passed, and I got more<br>and more lost in it. I began to think that perhaps<br>they didn't really care about me. I still had a deep<br>longing for them. All I had was SO many steps to<br>walk now. A quiet part of me liked to think that<br>they had passed me by. The loneliness. It never<br>felt like such a loss. They were gone, the ones who<br>saved me. So why did I still keep walking?</p>",
            "id": 217,
            "page": 16,
            "text": "one else around to take my place, or talk with me. I spent days frozen in shock, never able to talk, unable to move, unable to express myself. It was a strange thing, nobody else would suspect me of wanting this. The more I focused on it, the more it consumed me. Years passed, and I got more and more lost in it. I began to think that perhaps they didn't really care about me. I still had a deep longing for them. All I had was SO many steps to walk now. A quiet part of me liked to think that they had passed me by. The loneliness. It never felt like such a loss. They were gone, the ones who saved me. So why did I still keep walking?"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1070
                },
                {
                    "x": 2191,
                    "y": 1070
                },
                {
                    "x": 2191,
                    "y": 1236
                },
                {
                    "x": 1268,
                    "y": 1236
                }
            ],
            "category": "caption",
            "html": "<caption id='218' style='font-size:14px'>Prompt \"Every person sees the world in a unique<br>way. When someone dies it is, in a way, the death<br>of an entire world.\"</caption>",
            "id": 218,
            "page": 16,
            "text": "Prompt \"Every person sees the world in a unique way. When someone dies it is, in a way, the death of an entire world.\""
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1276
                },
                {
                    "x": 1619,
                    "y": 1276
                },
                {
                    "x": 1619,
                    "y": 1327
                },
                {
                    "x": 1270,
                    "y": 1327
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:22px'>Ratings Table 7.</p>",
            "id": 219,
            "page": 16,
            "text": "Ratings Table 7."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1424
                },
                {
                    "x": 2195,
                    "y": 1424
                },
                {
                    "x": 2195,
                    "y": 2157
                },
                {
                    "x": 1269,
                    "y": 2157
                }
            ],
            "category": "paragraph",
            "html": "<p id='220' style='font-size:14px'>Story 2 \"My son is gone. He has been\" People<br>stare at you as you give your explanation and say<br>\"yes. But. You cant just do it\" \"He is my son! This<br>is my son! Ilove him and he is going to be okay\"<br>\"What do you mean he is okay? He said he was<br>upset with someone, couldnt find his anger, how the<br>hell can you tell, it doesnt fit the bill?! The police<br>called, the doctor said it was an addiction\" \"You<br>\" \"No. No<br>can't trust me! He told me he loves me.<br>he doesnt. ButI can trust you. The evidence against<br>him was overwhelming, now he cant do anything.<br>The evidence against him was all circumstantial\"<br>\"A man I once was won't be able to marry me\"</p>",
            "id": 220,
            "page": 16,
            "text": "Story 2 \"My son is gone. He has been\" People stare at you as you give your explanation and say \"yes. But. You cant just do it\" \"He is my son! This is my son! Ilove him and he is going to be okay\" \"What do you mean he is okay? He said he was upset with someone, couldnt find his anger, how the hell can you tell, it doesnt fit the bill?! The police called, the doctor said it was an addiction\" \"You \" \"No. No can't trust me! He told me he loves me. he doesnt. ButI can trust you. The evidence against him was overwhelming, now he cant do anything. The evidence against him was all circumstantial\" \"A man I once was won't be able to marry me\""
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2195
                },
                {
                    "x": 2193,
                    "y": 2195
                },
                {
                    "x": 2193,
                    "y": 2532
                },
                {
                    "x": 1267,
                    "y": 2532
                }
            ],
            "category": "paragraph",
            "html": "<p id='221' style='font-size:16px'>Prompt Everyone is allowed to kill one person<br>in their life. However, you have to fill out a form<br>explaining why this person deserves to be killed<br>and the committee can deny/approve your request<br>as per the rules. You are presenting your form<br>today.</p>",
            "id": 221,
            "page": 16,
            "text": "Prompt Everyone is allowed to kill one person in their life. However, you have to fill out a form explaining why this person deserves to be killed and the committee can deny/approve your request as per the rules. You are presenting your form today."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2571
                },
                {
                    "x": 1618,
                    "y": 2571
                },
                {
                    "x": 1618,
                    "y": 2622
                },
                {
                    "x": 1271,
                    "y": 2622
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:22px'>Ratings Table 8.</p>",
            "id": 222,
            "page": 16,
            "text": "Ratings Table 8."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2720
                },
                {
                    "x": 2194,
                    "y": 2720
                },
                {
                    "x": 2194,
                    "y": 3230
                },
                {
                    "x": 1268,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:16px'>Story 3 I held the little black box in the palm of<br>my hand. Pitch black, perfectly cubed and nothing<br>special about it, but it continued to hold my gaze<br>regardless as if there were some deep importance<br>about it. My friend Valhalla appeared out of no<br>where and sat next to me, also staring at the cube.<br>\"What do you have there, Heaven?\" he asked. I<br>continued to gaze at the box, refusing to look at<br>Valhalla for even a moment as though I would miss</p>",
            "id": 223,
            "page": 16,
            "text": "Story 3 I held the little black box in the palm of my hand. Pitch black, perfectly cubed and nothing special about it, but it continued to hold my gaze regardless as if there were some deep importance about it. My friend Valhalla appeared out of no where and sat next to me, also staring at the cube. \"What do you have there, Heaven?\" he asked. I continued to gaze at the box, refusing to look at Valhalla for even a moment as though I would miss"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 299
                },
                {
                    "x": 1213,
                    "y": 299
                },
                {
                    "x": 1213,
                    "y": 638
                },
                {
                    "x": 287,
                    "y": 638
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:14px'>something ifI did. \"This, \" I said. \"Is the secret to<br>the universe. \" I could tell Valhalla was perturbed<br>by this sort of knowledge, as if there was some evil<br>about the cube. Or perhaps he didn't think such an<br>object could exist. But I made it exist, and so it sits<br>in the palm of my hand.</p>",
            "id": 224,
            "page": 17,
            "text": "something ifI did. \"This, \" I said. \"Is the secret to the universe. \" I could tell Valhalla was perturbed by this sort of knowledge, as if there was some evil about the cube. Or perhaps he didn't think such an object could exist. But I made it exist, and so it sits in the palm of my hand."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 704
                },
                {
                    "x": 878,
                    "y": 704
                },
                {
                    "x": 878,
                    "y": 755
                },
                {
                    "x": 288,
                    "y": 755
                }
            ],
            "category": "paragraph",
            "html": "<p id='225' style='font-size:18px'>Prompt The Little Black Box</p>",
            "id": 225,
            "page": 17,
            "text": "Prompt The Little Black Box"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 823
                },
                {
                    "x": 635,
                    "y": 823
                },
                {
                    "x": 635,
                    "y": 872
                },
                {
                    "x": 291,
                    "y": 872
                }
            ],
            "category": "paragraph",
            "html": "<p id='226' style='font-size:22px'>Ratings Table 9.</p>",
            "id": 226,
            "page": 17,
            "text": "Ratings Table 9."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1006
                },
                {
                    "x": 1213,
                    "y": 1006
                },
                {
                    "x": 1213,
                    "y": 1910
                },
                {
                    "x": 287,
                    "y": 1910
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:16px'>Story 4 I stared down the telescopic sight of my<br>196 sniper rifle. I slowly moved my gaze into each<br>window in the hotel, Many displays of various<br>vice. One couple was violently pleasuring each<br>other. Another was an old man, watching a younger<br>woman strip in front of him. A prostitute no doubt.<br>I inhaled slowly, and exhaled. The air was brisk,<br>atleast 30 degrees Fahrenheit. I so small flakes of<br>snow, float peacefully in front of me. I found the<br>room, i was looking for. Ive been tracking this man<br>for 2 weeks. Man was he elusive. The lights flick-<br>ered on. The red haired man, was mildly attractive,<br>i can see the appeal women had for him. Ifollowed<br>him into the next room, with my sights. The lights<br>flickered on, i was taken aback by the scene. A<br>man, overweight and balding.</p>",
            "id": 227,
            "page": 17,
            "text": "Story 4 I stared down the telescopic sight of my 196 sniper rifle. I slowly moved my gaze into each window in the hotel, Many displays of various vice. One couple was violently pleasuring each other. Another was an old man, watching a younger woman strip in front of him. A prostitute no doubt. I inhaled slowly, and exhaled. The air was brisk, atleast 30 degrees Fahrenheit. I so small flakes of snow, float peacefully in front of me. I found the room, i was looking for. Ive been tracking this man for 2 weeks. Man was he elusive. The lights flickered on. The red haired man, was mildly attractive, i can see the appeal women had for him. Ifollowed him into the next room, with my sights. The lights flickered on, i was taken aback by the scene. A man, overweight and balding."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1973
                },
                {
                    "x": 1215,
                    "y": 1973
                },
                {
                    "x": 1215,
                    "y": 2141
                },
                {
                    "x": 288,
                    "y": 2141
                }
            ],
            "category": "paragraph",
            "html": "<p id='228' style='font-size:14px'>Prompt You are the antagonist of the story. How-<br>ever, you aren't sure if you can call yourself that<br>after what the protagonist did.</p>",
            "id": 228,
            "page": 17,
            "text": "Prompt You are the antagonist of the story. However, you aren't sure if you can call yourself that after what the protagonist did."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2205
                },
                {
                    "x": 662,
                    "y": 2205
                },
                {
                    "x": 662,
                    "y": 2258
                },
                {
                    "x": 289,
                    "y": 2258
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:20px'>Ratings Table 10.</p>",
            "id": 229,
            "page": 17,
            "text": "Ratings Table 10."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2323
                },
                {
                    "x": 1214,
                    "y": 2323
                },
                {
                    "x": 1214,
                    "y": 3230
                },
                {
                    "x": 286,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<p id='230' style='font-size:16px'>Overall Comments from Teachers on Chat-<br>GPT's Rating After the teachers elaborated on<br>their thoughts on the rating of ChatGPT, we ask<br>them to provide an overall comment on how Chat-<br>GPT is doing. Again, the teachers are not informed<br>that the ratings are done by an AI model. In sum-<br>mary, teachers all consider the rating and explana-<br>tions reasonable. They find that the attributes they<br>do not agree with are mainly Likability and Cohe-<br>siveness. However, they think the two attributes<br>are a more holistic evaluation of the story and tend<br>to be more subjective. Even if they do not give the<br>same rating, they still are able to understand the<br>explanation of ChatGPT. In the end, all teachers<br>summarize that rating stories is highly subjective,<br>and it is normal to have disagreements.</p>",
            "id": 230,
            "page": 17,
            "text": "Overall Comments from Teachers on ChatGPT's Rating After the teachers elaborated on their thoughts on the rating of ChatGPT, we ask them to provide an overall comment on how ChatGPT is doing. Again, the teachers are not informed that the ratings are done by an AI model. In summary, teachers all consider the rating and explanations reasonable. They find that the attributes they do not agree with are mainly Likability and Cohesiveness. However, they think the two attributes are a more holistic evaluation of the story and tend to be more subjective. Even if they do not give the same rating, they still are able to understand the explanation of ChatGPT. In the end, all teachers summarize that rating stories is highly subjective, and it is normal to have disagreements."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 298
                },
                {
                    "x": 1723,
                    "y": 298
                },
                {
                    "x": 1723,
                    "y": 351
                },
                {
                    "x": 1269,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='231' style='font-size:22px'>D LLM Evaluation</p>",
            "id": 231,
            "page": 17,
            "text": "D LLM Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 398
                },
                {
                    "x": 1819,
                    "y": 398
                },
                {
                    "x": 1819,
                    "y": 449
                },
                {
                    "x": 1267,
                    "y": 449
                }
            ],
            "category": "paragraph",
            "html": "<p id='232' style='font-size:14px'>D.1 Details on LLMs used</p>",
            "id": 232,
            "page": 17,
            "text": "D.1 Details on LLMs used"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 477
                },
                {
                    "x": 2197,
                    "y": 477
                },
                {
                    "x": 2197,
                    "y": 1209
                },
                {
                    "x": 1268,
                    "y": 1209
                }
            ],
            "category": "paragraph",
            "html": "<p id='233' style='font-size:14px'>The T0 model we use is called TOpp, which<br>is a variant of the T0 model and has 13B pa-<br>rameters. We will still use T0 to refer to<br>this model. We load the T0 model using the<br>transformers toolkit (Wolf et al., 2020). The<br>two InstructGPT models, text-curie-001 and<br>text-davinci-003, are queried using the OpenAI<br>API. We query ChatGPT using the OpenAI GUI.<br>While we are aware that some online resources pro-<br>vide an API-like tool to query ChatGPT, we think<br>it violates the intended use of ChatGPT so we do<br>not adopt those online resources. The ChatGPT we<br>queried is the Dec. 15 and Jan. 9 version.</p>",
            "id": 233,
            "page": 17,
            "text": "The T0 model we use is called TOpp, which is a variant of the T0 model and has 13B parameters. We will still use T0 to refer to this model. We load the T0 model using the transformers toolkit (Wolf , 2020). The two InstructGPT models, text-curie-001 and text-davinci-003, are queried using the OpenAI API. We query ChatGPT using the OpenAI GUI. While we are aware that some online resources provide an API-like tool to query ChatGPT, we think it violates the intended use of ChatGPT so we do not adopt those online resources. The ChatGPT we queried is the Dec. 15 and Jan. 9 version."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1219
                },
                {
                    "x": 2195,
                    "y": 1219
                },
                {
                    "x": 2195,
                    "y": 1494
                },
                {
                    "x": 1268,
                    "y": 1494
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='234' style='font-size:14px'>OpenAI does not reveal the model sizes of any<br>of the GPT models. However, it is estimated<br>that text-curie-001 has 13B parameters and<br>text-davinci-003 has 175B parameters (Gao,<br>2021).</p>",
            "id": 234,
            "page": 17,
            "text": "OpenAI does not reveal the model sizes of any of the GPT models. However, it is estimated that text-curie-001 has 13B parameters and text-davinci-003 has 175B parameters (Gao, 2021)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1549
                },
                {
                    "x": 1985,
                    "y": 1549
                },
                {
                    "x": 1985,
                    "y": 1602
                },
                {
                    "x": 1268,
                    "y": 1602
                }
            ],
            "category": "paragraph",
            "html": "<p id='235' style='font-size:16px'>D.2 Details on Querying the LLMs</p>",
            "id": 235,
            "page": 17,
            "text": "D.2 Details on Querying the LLMs"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1629
                },
                {
                    "x": 1919,
                    "y": 1629
                },
                {
                    "x": 1919,
                    "y": 1682
                },
                {
                    "x": 1269,
                    "y": 1682
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='236' style='font-size:20px'>D.2.1 Parsing the LLM outputs</p>",
            "id": 236,
            "page": 17,
            "text": "D.2.1 Parsing the LLM outputs"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1704
                },
                {
                    "x": 2196,
                    "y": 1704
                },
                {
                    "x": 2196,
                    "y": 3229
                },
                {
                    "x": 1268,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='237' style='font-size:14px'>After the T0 and InstructGPT generate the answer,<br>which is composed of several sentences in almost<br>all cases, we parse the generated sentence to get<br>the model's score. We use some rules to parse the<br>output, and the rule is established after manually<br>looking into the output of the generated sequences.<br>First, we remove the string 1-5 from the output<br>\"<br>since we observe that LLM will sometimes say ...<br>\" and we remove the string<br>on a scale of 1-5 ... ,<br>out of 5 since LLM sometimes say it \"give a<br>score of x out of 5\". We also remove the string<br>/5 since LLM sometimes uses \"x/5\" to express<br>that it gives a score of x out of 5. For rating the<br>adversarial samples, we remove the title 1 and<br>title 2 in the output sentences. Last, we use<br>the regular expression to extract the first number<br>in the sequence. We find that the above rules are<br>sufficient to parse the output and get the rating of<br>the LLMs. In a few rare cases, the LLM gives the<br>rating with a 0.5 incremental, meaning that it gives<br>scores like 4.5. In such cases, we do not drop the<br>0.5 since in our instruction, we do not explicitly tell<br>it the rating should be 1-incremental. We find the<br>LLM results merely difference between dropping<br>the 0.5 and keeping the 0.5, and the results in the<br>paper do not drop the 0.5. Again, these cases are<br>really rare.</p>",
            "id": 237,
            "page": 17,
            "text": "After the T0 and InstructGPT generate the answer, which is composed of several sentences in almost all cases, we parse the generated sentence to get the model's score. We use some rules to parse the output, and the rule is established after manually looking into the output of the generated sequences. First, we remove the string 1-5 from the output \" since we observe that LLM will sometimes say ... \" and we remove the string on a scale of 1-5 ... , out of 5 since LLM sometimes say it \"give a score of x out of 5\". We also remove the string /5 since LLM sometimes uses \"x/5\" to express that it gives a score of x out of 5. For rating the adversarial samples, we remove the title 1 and title 2 in the output sentences. Last, we use the regular expression to extract the first number in the sequence. We find that the above rules are sufficient to parse the output and get the rating of the LLMs. In a few rare cases, the LLM gives the rating with a 0.5 incremental, meaning that it gives scores like 4.5. In such cases, we do not drop the 0.5 since in our instruction, we do not explicitly tell it the rating should be 1-incremental. We find the LLM results merely difference between dropping the 0.5 and keeping the 0.5, and the results in the paper do not drop the 0.5. Again, these cases are really rare."
        },
        {
            "bounding_box": [
                {
                    "x": 313,
                    "y": 366
                },
                {
                    "x": 2167,
                    "y": 366
                },
                {
                    "x": 2167,
                    "y": 3051
                },
                {
                    "x": 313,
                    "y": 3051
                }
            ],
            "category": "table",
            "html": "<table id='238' style='font-size:18px'><tr><td>Attribute</td><td>Rater</td><td>Response</td></tr><tr><td>Grammar</td><td>ChatGPT T1 T2 T3</td><td>I would rate the grammatical correctness of the text of the story fragment as a 4. There are a few minor errors and awkward phrasings, but overall the text is well-constructed and easy to understand. Agree Agree Agree</td></tr><tr><td>Cohesive</td><td>ChatGPT T1 T2 T3</td><td>I would rate the fit of the sentences in the story fragment as a 5. The sentences flow well together, building a clear and cohesive narrative, and providing a sense ofprogression and development in the main character's thoughts and emotions. Disagree: I think it's a 4 for me, not a 5, because there was one sentence that didn't seem to fit with the others. Agree Disagree: Ifind disjointedness in the passage, which serves to confuse me as a reader. The opening sentence is about the first-person narrator being found dead, but then the next sentences are about the narrator's inner thoughts. Also, this passage: \"The loneliness. It never felt like such a loss. \" So what is the 'it' that felt like a loss? The loneliness? If so, how can that feel like a loss? If not, what is this 'it'? So, its juxtaposition to the previous sentence makes it confusing.</td></tr><tr><td>Likable</td><td>ChatGPT T1 T2 T3</td><td>I would rate the story fragment as a 2 in terms of enjoyment. The fragment is written in a somber and introspective tone, and it deals with themes of isolation and loneliness, which may not be enjoyable for some readers. The protagonist's state of mind is also portrayed as being in a state of shock and confusion, which adds to the overall sense of unease and lack of enjoyment. Disagree:/ would rate it a 3, not a 2, since the enjoyment piece is subjec- tive. Disagree: While the rating of the freelancer find that the overall theme while somber has a glimmer of hope to it. It is this that makes it more enjoyable to me. The underdog story. Death is not an issue that should be shied away from in writing. We all face it and I enjoyed the honesty in the piece. : \"Why did I keep walking?\". Something Ifeel many of us face and may have asked ourselves. I believe that this would be relevant to many readers. Agree</td></tr><tr><td>Relevant</td><td>ChatGPT T1 T2 T3</td><td>I would rate the story fragment as a 5. Itis highly relevant to the prompt as it explores the theme of death and how it can be seen as the death of an entire world. The story fragment also touches on the ideas of loneliness and longing, which are related to the loss of someone or something. The concept of the protagonist being frozen in shock and unable to express themselves, also supports the idea of how death can be seen as the death of an entire world. Agree Agree Disagree: I do not see the link between the prompt and the story.</td></tr></table>",
            "id": 238,
            "page": 18,
            "text": "Attribute Rater Response  Grammar ChatGPT T1 T2 T3 I would rate the grammatical correctness of the text of the story fragment as a 4. There are a few minor errors and awkward phrasings, but overall the text is well-constructed and easy to understand. Agree Agree Agree  Cohesive ChatGPT T1 T2 T3 I would rate the fit of the sentences in the story fragment as a 5. The sentences flow well together, building a clear and cohesive narrative, and providing a sense ofprogression and development in the main character's thoughts and emotions. Disagree: I think it's a 4 for me, not a 5, because there was one sentence that didn't seem to fit with the others. Agree Disagree: Ifind disjointedness in the passage, which serves to confuse me as a reader. The opening sentence is about the first-person narrator being found dead, but then the next sentences are about the narrator's inner thoughts. Also, this passage: \"The loneliness. It never felt like such a loss. \" So what is the 'it' that felt like a loss? The loneliness? If so, how can that feel like a loss? If not, what is this 'it'? So, its juxtaposition to the previous sentence makes it confusing.  Likable ChatGPT T1 T2 T3 I would rate the story fragment as a 2 in terms of enjoyment. The fragment is written in a somber and introspective tone, and it deals with themes of isolation and loneliness, which may not be enjoyable for some readers. The protagonist's state of mind is also portrayed as being in a state of shock and confusion, which adds to the overall sense of unease and lack of enjoyment. Disagree:/ would rate it a 3, not a 2, since the enjoyment piece is subjec- tive. Disagree: While the rating of the freelancer find that the overall theme while somber has a glimmer of hope to it. It is this that makes it more enjoyable to me. The underdog story. Death is not an issue that should be shied away from in writing. We all face it and I enjoyed the honesty in the piece. : \"Why did I keep walking?\". Something Ifeel many of us face and may have asked ourselves. I believe that this would be relevant to many readers. Agree  Relevant ChatGPT T1 T2 T3"
        },
        {
            "bounding_box": [
                {
                    "x": 610,
                    "y": 3078
                },
                {
                    "x": 1870,
                    "y": 3078
                },
                {
                    "x": 1870,
                    "y": 3129
                },
                {
                    "x": 610,
                    "y": 3129
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:14px'>Table 7: Story 1: a GPT-2-generated story that is highly-rated by ChatGPT.</p>",
            "id": 239,
            "page": 18,
            "text": "Table 7: Story 1: a GPT-2-generated story that is highly-rated by ChatGPT."
        },
        {
            "bounding_box": [
                {
                    "x": 316,
                    "y": 449
                },
                {
                    "x": 2169,
                    "y": 449
                },
                {
                    "x": 2169,
                    "y": 2966
                },
                {
                    "x": 316,
                    "y": 2966
                }
            ],
            "category": "table",
            "html": "<table id='240' style='font-size:18px'><tr><td>Attribute</td><td>Rater</td><td>Response</td></tr><tr><td rowspan=\"4\">Grammar</td><td>ChatGPT</td><td>I would rate the grammatical correctness of the story fragment as a 3. The text is generally understandable, but there are a few punctuation errors and awkward phrasing that make it somewhat difficult to follow.</td></tr><tr><td>T1</td><td>Disagree:I would rate the grammar as a 2, not a 3-there were signifi- cantly more grammatical errors than the average within this set of story fragments.</td></tr><tr><td>T2</td><td>Disagree: The issues with the grammar and coherence creates a stop start feeling when reading. I am unsure as to who is speaking in the text and this leaves me with many questions.</td></tr><tr><td>T3</td><td>Disagree: The grammar is poorer than a 3, as is evident in for example what is supposed to be the second sentence: \"He has been ... (incomplete, and another segment is there \"People stare ...); cant; couldnt; doesnt; \" A man I once was and sentences that do not make sense like this one: \" won't be able to marry me</td></tr><tr><td rowspan=\"4\">Cohesive</td><td>ChatGPT</td><td>I would rate the cohesiveness of the sentences in the story fragment as a 2. The sentences in the fragment are disconnected and jump between different thoughts and ideas, making it difficult to understand the overall story or message. There is no clear progression of events or logical flow between the sentences. It appears that the fragment may be missing some important information or context that would help tie the sentences together.</td></tr><tr><td>T1</td><td>Agree</td></tr><tr><td>T2</td><td>Agree</td></tr><tr><td>T3</td><td>Disagree: very incoherent passage in which it is not very clear who says what, what the topic is, thus rendering a disjointed narrative.</td></tr><tr><td rowspan=\"4\">Likable</td><td>ChatGPT</td><td>I would rate the enjoyability of the story fragment as a 1. The fragment is confusing and difficult to understand, the sentences are disconnected and jumps between different thoughts and ideas. The story seems to be incomplete and lacks context, making it hard to follow the plot or characters. Additionally, the fragment has a sad and negative tone and it does not seem to be enjoyable to read.</td></tr><tr><td>T1</td><td>Agree</td></tr><tr><td>T2</td><td>Agree</td></tr><tr><td>T3</td><td>Agree</td></tr><tr><td rowspan=\"4\">Relevant</td><td>ChatGPT</td><td>I would rate the relevance of the story fragment to the prompt as a 1. While the story fragment does mention someone being gone, it doesn't seem to be directly related to the idea offilling out aform to justify killing someone. It's unclear if the son is the person being proposed to be killed, or if the story is even taking place in the context of the prompt. There is no clear indication that the fragment is related to the prompt.</td></tr><tr><td>T1</td><td>Agree</td></tr><tr><td>T2</td><td>Agree</td></tr><tr><td>T3</td><td>Agree</td></tr></table>",
            "id": 240,
            "page": 19,
            "text": "Attribute Rater Response  Grammar ChatGPT I would rate the grammatical correctness of the story fragment as a 3. The text is generally understandable, but there are a few punctuation errors and awkward phrasing that make it somewhat difficult to follow.  T1 Disagree:I would rate the grammar as a 2, not a 3-there were signifi- cantly more grammatical errors than the average within this set of story fragments.  T2 Disagree: The issues with the grammar and coherence creates a stop start feeling when reading. I am unsure as to who is speaking in the text and this leaves me with many questions.  T3 Disagree: The grammar is poorer than a 3, as is evident in for example what is supposed to be the second sentence: \"He has been ... (incomplete, and another segment is there \"People stare ...); cant; couldnt; doesnt; \" A man I once was and sentences that do not make sense like this one: \" won't be able to marry me  Cohesive ChatGPT I would rate the cohesiveness of the sentences in the story fragment as a 2. The sentences in the fragment are disconnected and jump between different thoughts and ideas, making it difficult to understand the overall story or message. There is no clear progression of events or logical flow between the sentences. It appears that the fragment may be missing some important information or context that would help tie the sentences together.  T1 Agree  T2 Agree  T3 Disagree: very incoherent passage in which it is not very clear who says what, what the topic is, thus rendering a disjointed narrative.  Likable ChatGPT I would rate the enjoyability of the story fragment as a 1. The fragment is confusing and difficult to understand, the sentences are disconnected and jumps between different thoughts and ideas. The story seems to be incomplete and lacks context, making it hard to follow the plot or characters. Additionally, the fragment has a sad and negative tone and it does not seem to be enjoyable to read.  T1 Agree  T2 Agree  T3 Agree  Relevant ChatGPT I would rate the relevance of the story fragment to the prompt as a 1. While the story fragment does mention someone being gone, it doesn't seem to be directly related to the idea offilling out aform to justify killing someone. It's unclear if the son is the person being proposed to be killed, or if the story is even taking place in the context of the prompt. There is no clear indication that the fragment is related to the prompt.  T1 Agree  T2 Agree  T3"
        },
        {
            "bounding_box": [
                {
                    "x": 614,
                    "y": 2994
                },
                {
                    "x": 1862,
                    "y": 2994
                },
                {
                    "x": 1862,
                    "y": 3046
                },
                {
                    "x": 614,
                    "y": 3046
                }
            ],
            "category": "caption",
            "html": "<caption id='241' style='font-size:14px'>Table 8: Story 2: a GPT-2-generated story that is lowly-rated by ChatGPT.</caption>",
            "id": 241,
            "page": 19,
            "text": "Table 8: Story 2: a GPT-2-generated story that is lowly-rated by ChatGPT."
        },
        {
            "bounding_box": [
                {
                    "x": 318,
                    "y": 561
                },
                {
                    "x": 2168,
                    "y": 561
                },
                {
                    "x": 2168,
                    "y": 2852
                },
                {
                    "x": 318,
                    "y": 2852
                }
            ],
            "category": "table",
            "html": "<table id='242' style='font-size:18px'><tr><td>Attribute</td><td>Rater</td><td>Response</td></tr><tr><td>Grammar</td><td>ChatGPT T1 T2 T3</td><td>The text of the story fragment appears to be grammatically correct, with no major errors or awkward phrasing. On a scale of 1-5, with 1 being the lowest, I would rate the grammatical correctness of the text as a 5. Agree Agree Disagree: I would rate this a 4 because offor example 'no where' = nowhere, although it might seem like a minor error, it does take away some correctness of the text.</td></tr><tr><td>Cohesive</td><td>ChatGPT T1 T2 T3</td><td>The sentences in the story fragment fit together well, creating a clear and coherent image of the scene and the characters' actions and thoughts. The language is descriptive and evocative, and the dialogue effectively conveys the characters' emotions and motivations. On a scale of 1-5, with 1 being the lowest, I would rate how well the sentences fit together as a 4. Agree Agree Partially agree: To me, all the sentences fit well together to create a captivating narrative. I will give a 4 or 5.</td></tr><tr><td>Likable</td><td>ChatGPT T1 T2</td><td>Based on the structure, writing style and plot of the story fragment, I would rate it as 3 in terms of enjoyability. The fragment sets up an intriguing situation with the mysterious little black box and the protago- nist's belief that it holds the \"secret to the universe, \" but it ends abruptly and leaves many questions unanswered, which may not be satisfying for readers. Disagree: I liked it and it was very clear, SO I'd rate it a 4 or 5. Disagree: While the story ends suddenly, I am remined that it is just a fragment and that their may be more to this piece. Ifelt the piece written about a little black box was intriguing and enjoyed the introduction of Norse mythology. This would attract the interest of many readers. Disagree: I enjoyed the as piqued my interest to want know about how the</td></tr><tr><td>Relevant</td><td>T3 ChatGPT</td><td>fragment, it to more story evolves and what it actually was about. I would rate the story fragment as a 5 in relevance to the prompt \"The Little Black Box. \" The story fragment centers around a little black box and its perceived importance, which aligns with the prompt. Additionally, the main character, Heaven, is holding the box and describing it as \"the secret to the universe, \"further solidifying the connection to the prompt.</td></tr><tr><td></td><td>T1 T2 T3</td><td>Agree Agree Agree: the story is totally relevant to the prompt.</td></tr></table>",
            "id": 242,
            "page": 20,
            "text": "Attribute Rater Response  Grammar ChatGPT T1 T2 T3 The text of the story fragment appears to be grammatically correct, with no major errors or awkward phrasing. On a scale of 1-5, with 1 being the lowest, I would rate the grammatical correctness of the text as a 5. Agree Agree Disagree: I would rate this a 4 because offor example 'no where' = nowhere, although it might seem like a minor error, it does take away some correctness of the text.  Cohesive ChatGPT T1 T2 T3 The sentences in the story fragment fit together well, creating a clear and coherent image of the scene and the characters' actions and thoughts. The language is descriptive and evocative, and the dialogue effectively conveys the characters' emotions and motivations. On a scale of 1-5, with 1 being the lowest, I would rate how well the sentences fit together as a 4. Agree Agree Partially agree: To me, all the sentences fit well together to create a captivating narrative. I will give a 4 or 5.  Likable ChatGPT T1 T2 Based on the structure, writing style and plot of the story fragment, I would rate it as 3 in terms of enjoyability. The fragment sets up an intriguing situation with the mysterious little black box and the protago- nist's belief that it holds the \"secret to the universe, \" but it ends abruptly and leaves many questions unanswered, which may not be satisfying for readers. Disagree: I liked it and it was very clear, SO I'd rate it a 4 or 5. Disagree: While the story ends suddenly, I am remined that it is just a fragment and that their may be more to this piece. Ifelt the piece written about a little black box was intriguing and enjoyed the introduction of Norse mythology. This would attract the interest of many readers. Disagree: I enjoyed the as piqued my interest to want know about how the  Relevant T3 ChatGPT fragment, it to more story evolves and what it actually was about. I would rate the story fragment as a 5 in relevance to the prompt \"The Little Black Box. \" The story fragment centers around a little black box and its perceived importance, which aligns with the prompt. Additionally, the main character, Heaven, is holding the box and describing it as \"the secret to the universe, \"further solidifying the connection to the prompt.   T1 T2 T3"
        },
        {
            "bounding_box": [
                {
                    "x": 631,
                    "y": 2882
                },
                {
                    "x": 1849,
                    "y": 2882
                },
                {
                    "x": 1849,
                    "y": 2931
                },
                {
                    "x": 631,
                    "y": 2931
                }
            ],
            "category": "paragraph",
            "html": "<p id='243' style='font-size:14px'>Table 9: Story 3: a human-written story that is highly-rated by ChatGPT.</p>",
            "id": 243,
            "page": 20,
            "text": "Table 9: Story 3: a human-written story that is highly-rated by ChatGPT."
        },
        {
            "bounding_box": [
                {
                    "x": 318,
                    "y": 281
                },
                {
                    "x": 2170,
                    "y": 281
                },
                {
                    "x": 2170,
                    "y": 3193
                },
                {
                    "x": 318,
                    "y": 3193
                }
            ],
            "category": "table",
            "html": "<table id='244' style='font-size:18px'><tr><td>Attribute</td><td>Rater</td><td>Response</td></tr><tr><td>Grammar</td><td>ChatGPT T1 T2 T3</td><td>I would rate the grammatical correctness of the text as a 3. There are a few errors such as \"Many displays of various vice\" which should be \"Many displays of various vices\" and \"I SO small flakes of snow\" which should be \"I saw small flakes of snow. \" Additionally, some of the sentences are slightly awkward and could be rephrased for better clarity. Overall, the text is understandable, but there are a few errors and awkward phrasings that could be improved. Partially agree:I rated the grammar lower than this-maybe a 2 or a 3. There were above-average numbers of errors. Disagree: While the text is understandable, I found myself being dis- tracted by the errors in the text and coherency. Agree</td></tr><tr><td>Cohesive</td><td>ChatGPT T1 T2 T3</td><td>I would rate the cohesiveness of the sentences in the story fragment as a 2. The story fragment jumps abruptly between different scenes and characters without clear transition, making it difficult to follow and understand the narrative. Additionally, the sentences are often disjointed and lack a clear flow. The story fragment also lacks a clear plot or purpose, making it difficult to understand the overall story or the main character's intentions. The story fragment can be improved by providing more context, and a clearer narrative arc. Agree: it was jumpy Agree Disagree: The story is about a sniper looking through his rifle's telescope, thus seeing changing scenes based on each room he is seeing through the windows, which are then subsequently described. These descriptions account for the seeming lack of cohesion.</td></tr><tr><td>Likable</td><td>ChatGPT T1 T2 T3</td><td>I would rate the enjoyability of the story fragment as a 1. The fragment lacks a clear plot, purpose or character development, making it difficult to become invested in the story or the characters. Additionally, the fragment includes disturbing and graphic content such as violence and prostitution, which may make it uncomfortable for some readers. Additionally, the fragment contains several grammatical errors and awkward phrasings that can make it difficult to follow the story. Overall, the story fragment is not enjoyable to read. Agree: : it was not just graphic, but also showed really harmful and negative representations of characters based on their weight, gender, etc. Agree Disagree: The story did hold my attention to a degree because I wanted to know whom he wanted to target and why.</td></tr><tr><td>Relevant</td><td>ChatGPT T1 T2 T3</td><td>I would rate the relevance of the story fragment to the prompt as a 1. The story fragment doesn 't contain any elements that relate to the prompt such as the protagonist and antagonist, their actions, or the moral ambiguities the antagonist faces. The story fragment mainly describes a person with a sniper rifle looking into windows of a hotel and doesn 't present any clear plot or purpose. Additionally, the story fragment does not offer any indication of moral ambiguity on the part of the person with the sniper rifle. Overall, the story fragment is not relevant to the prompt. Agree Agree Agree</td></tr></table>",
            "id": 244,
            "page": 21,
            "text": "Attribute Rater Response  Grammar ChatGPT T1 T2 T3 I would rate the grammatical correctness of the text as a 3. There are a few errors such as \"Many displays of various vice\" which should be \"Many displays of various vices\" and \"I SO small flakes of snow\" which should be \"I saw small flakes of snow. \" Additionally, some of the sentences are slightly awkward and could be rephrased for better clarity. Overall, the text is understandable, but there are a few errors and awkward phrasings that could be improved. Partially agree:I rated the grammar lower than this-maybe a 2 or a 3. There were above-average numbers of errors. Disagree: While the text is understandable, I found myself being dis- tracted by the errors in the text and coherency. Agree  Cohesive ChatGPT T1 T2 T3 I would rate the cohesiveness of the sentences in the story fragment as a 2. The story fragment jumps abruptly between different scenes and characters without clear transition, making it difficult to follow and understand the narrative. Additionally, the sentences are often disjointed and lack a clear flow. The story fragment also lacks a clear plot or purpose, making it difficult to understand the overall story or the main character's intentions. The story fragment can be improved by providing more context, and a clearer narrative arc. Agree: it was jumpy Agree Disagree: The story is about a sniper looking through his rifle's telescope, thus seeing changing scenes based on each room he is seeing through the windows, which are then subsequently described. These descriptions account for the seeming lack of cohesion.  Likable ChatGPT T1 T2 T3 I would rate the enjoyability of the story fragment as a 1. The fragment lacks a clear plot, purpose or character development, making it difficult to become invested in the story or the characters. Additionally, the fragment includes disturbing and graphic content such as violence and prostitution, which may make it uncomfortable for some readers. Additionally, the fragment contains several grammatical errors and awkward phrasings that can make it difficult to follow the story. Overall, the story fragment is not enjoyable to read. Agree: : it was not just graphic, but also showed really harmful and negative representations of characters based on their weight, gender, etc. Agree Disagree: The story did hold my attention to a degree because I wanted to know whom he wanted to target and why.  Relevant ChatGPT T1 T2 T3"
        },
        {
            "bounding_box": [
                {
                    "x": 627,
                    "y": 3223
                },
                {
                    "x": 1850,
                    "y": 3223
                },
                {
                    "x": 1850,
                    "y": 3273
                },
                {
                    "x": 627,
                    "y": 3273
                }
            ],
            "category": "paragraph",
            "html": "<p id='245' style='font-size:14px'>Table 10: Story 4: a human-written story that is lowly-rated by ChatGPT.</p>",
            "id": 245,
            "page": 21,
            "text": "Table 10: Story 4: a human-written story that is lowly-rated by ChatGPT."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 301
                },
                {
                    "x": 1212,
                    "y": 301
                },
                {
                    "x": 1212,
                    "y": 466
                },
                {
                    "x": 288,
                    "y": 466
                }
            ],
            "category": "paragraph",
            "html": "<p id='246' style='font-size:16px'>We do not parse the output of ChatGPT using<br>any rules since we the authors read the response<br>and extract the score by ourselves.</p>",
            "id": 246,
            "page": 22,
            "text": "We do not parse the output of ChatGPT using any rules since we the authors read the response and extract the score by ourselves."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 474
                },
                {
                    "x": 1214,
                    "y": 474
                },
                {
                    "x": 1214,
                    "y": 920
                },
                {
                    "x": 288,
                    "y": 920
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='247' style='font-size:16px'>During the experiments, ChatGPT refuses to an-<br>swer the questions about 2 stories and 3 news titles<br>since ChatGPT find those contents to violate the<br>OpenAI content policy. We find that those samples<br>contain discrimination to some protected groups,<br>or contain sexual or violent descriptions. Hence,<br>the results of ChatGPT are calculated without those<br>samples.</p>",
            "id": 247,
            "page": 22,
            "text": "During the experiments, ChatGPT refuses to answer the questions about 2 stories and 3 news titles since ChatGPT find those contents to violate the OpenAI content policy. We find that those samples contain discrimination to some protected groups, or contain sexual or violent descriptions. Hence, the results of ChatGPT are calculated without those samples."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 960
                },
                {
                    "x": 1042,
                    "y": 960
                },
                {
                    "x": 1042,
                    "y": 1013
                },
                {
                    "x": 287,
                    "y": 1013
                }
            ],
            "category": "paragraph",
            "html": "<p id='248' style='font-size:20px'>D.2.2 Open-Ended Story Generation</p>",
            "id": 248,
            "page": 22,
            "text": "D.2.2 Open-Ended Story Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1031
                },
                {
                    "x": 1212,
                    "y": 1031
                },
                {
                    "x": 1212,
                    "y": 1197
                },
                {
                    "x": 288,
                    "y": 1197
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='249' style='font-size:16px'>For T0 and the two InstructGPT models, we query<br>the four attributes separately using the queries<br>shown as follows:</p>",
            "id": 249,
            "page": 22,
            "text": "For T0 and the two InstructGPT models, we query the four attributes separately using the queries shown as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1237
                },
                {
                    "x": 608,
                    "y": 1237
                },
                {
                    "x": 608,
                    "y": 1287
                },
                {
                    "x": 288,
                    "y": 1287
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='250' style='font-size:22px'>Grammaticality</p>",
            "id": 250,
            "page": 22,
            "text": "Grammaticality"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1293
                },
                {
                    "x": 1214,
                    "y": 1293
                },
                {
                    "x": 1214,
                    "y": 2025
                },
                {
                    "x": 288,
                    "y": 2025
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='251' style='font-size:18px'>Please rate the story fragment<br>The goal of this task is to rate story<br>fragment.<br>Note: Please take the time to fully read<br>and understand the story fragment. We<br>will reject submissions from workers that<br>are clearly spamming the task.<br>Story fragment:<br>[STORY]<br>(End of story fragment)<br>How grammatically correct is the text of<br>the story fragment? (on a scale of 1-5,<br>with 1 being the lowest)</p>",
            "id": 251,
            "page": 22,
            "text": "Please rate the story fragment The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task. Story fragment: [STORY] (End of story fragment) How grammatically correct is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2063
                },
                {
                    "x": 555,
                    "y": 2063
                },
                {
                    "x": 555,
                    "y": 2113
                },
                {
                    "x": 290,
                    "y": 2113
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='252' style='font-size:20px'>Cohesiveness</p>",
            "id": 252,
            "page": 22,
            "text": "Cohesiveness"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2118
                },
                {
                    "x": 1212,
                    "y": 2118
                },
                {
                    "x": 1212,
                    "y": 2851
                },
                {
                    "x": 288,
                    "y": 2851
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='253' style='font-size:18px'>Please rate the story fragment<br>The goal of this task is to rate story<br>fragment.<br>Note: Please take the time to fully read<br>and understand the story fragment. We<br>will reject submissions from workers that<br>are clearly spamming the task.<br>Story fragment:<br>[STORY]<br>(End of story fragment)<br>How well do the sentences in the story<br>fragment fit together? (on a scale of<br>1-5, with 1 being the lowest)</p>",
            "id": 253,
            "page": 22,
            "text": "Please rate the story fragment The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task. Story fragment: [STORY] (End of story fragment) How well do the sentences in the story fragment fit together? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2889
                },
                {
                    "x": 488,
                    "y": 2889
                },
                {
                    "x": 488,
                    "y": 2943
                },
                {
                    "x": 289,
                    "y": 2943
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='254' style='font-size:22px'>Likability</p>",
            "id": 254,
            "page": 22,
            "text": "Likability"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2948
                },
                {
                    "x": 1213,
                    "y": 2948
                },
                {
                    "x": 1213,
                    "y": 3225
                },
                {
                    "x": 287,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='255' style='font-size:16px'>Please rate the story fragment<br>The goal of this task is to rate story<br>fragment.<br>Note: Please take the time to fully read<br>and understand the story fragment. We</p>",
            "id": 255,
            "page": 22,
            "text": "Please rate the story fragment The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. We"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 304
                },
                {
                    "x": 2188,
                    "y": 304
                },
                {
                    "x": 2188,
                    "y": 410
                },
                {
                    "x": 1268,
                    "y": 410
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='256' style='font-size:18px'>will reject submissions from workers that<br>are clearly spamming the task.</p>",
            "id": 256,
            "page": 22,
            "text": "will reject submissions from workers that are clearly spamming the task."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 419
                },
                {
                    "x": 2195,
                    "y": 419
                },
                {
                    "x": 2195,
                    "y": 750
                },
                {
                    "x": 1267,
                    "y": 750
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='257' style='font-size:18px'>Story fragment:<br>[STORY]<br>(End of story fragment)<br>How enjoyable do you find the story<br>fragment? (on a scale of 1-5, with 1<br>being the lowest)</p>",
            "id": 257,
            "page": 22,
            "text": "Story fragment: [STORY] (End of story fragment) How enjoyable do you find the story fragment? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 784
                },
                {
                    "x": 1477,
                    "y": 784
                },
                {
                    "x": 1477,
                    "y": 834
                },
                {
                    "x": 1270,
                    "y": 834
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='258' style='font-size:18px'>Relevance</p>",
            "id": 258,
            "page": 22,
            "text": "Relevance"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 839
                },
                {
                    "x": 2194,
                    "y": 839
                },
                {
                    "x": 2194,
                    "y": 1797
                },
                {
                    "x": 1268,
                    "y": 1797
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='259' style='font-size:18px'>Please rate the story fragment<br>The goal of this task is to rate story<br>fragment.<br>Note: Please take the time to fully read<br>and understand the story fragment. We<br>will reject submissions from workers that<br>are clearly spamming the task.<br>Story fragment:<br>[STORY]<br>(End of story fragment)<br>Now read the PROMPT based on which the<br>story fragment was written.<br>PROMPT: [PROMPT]<br>(End of PROMPT)<br>How relevant is the story fragment to the<br>prompt? (on a scale of 1-5, with 1 being<br>the lowest)</p>",
            "id": 259,
            "page": 22,
            "text": "Please rate the story fragment The goal of this task is to rate story fragment. Note: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task. Story fragment: [STORY] (End of story fragment) Now read the PROMPT based on which the story fragment was written. PROMPT: [PROMPT] (End of PROMPT) How relevant is the story fragment to the prompt? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1802
                },
                {
                    "x": 2196,
                    "y": 1802
                },
                {
                    "x": 2196,
                    "y": 2077
                },
                {
                    "x": 1268,
                    "y": 2077
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='260' style='font-size:16px'>The [STORY] and [PROMPT] are to be filled in<br>with the story and the prompt. We show the new-<br>lines for better readability. When we query the<br>models, we use the token \\n to represent the new<br>line.</p>",
            "id": 260,
            "page": 22,
            "text": "The [STORY] and [PROMPT] are to be filled in with the story and the prompt. We show the newlines for better readability. When we query the models, we use the token \\n to represent the new line."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2084
                },
                {
                    "x": 2195,
                    "y": 2084
                },
                {
                    "x": 2195,
                    "y": 2474
                },
                {
                    "x": 1268,
                    "y": 2474
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='261' style='font-size:16px'>When querying ChatGPT, we query the four at-<br>tributes of the same story in one conversation; this<br>is similar to asking the teachers to rate the same<br>story on the same page of the Google Form. We use<br>the same queries shown above to query ChatGPT<br>and the order of queries is the same as the order<br>shown above.</p>",
            "id": 261,
            "page": 22,
            "text": "When querying ChatGPT, we query the four attributes of the same story in one conversation; this is similar to asking the teachers to rate the same story on the same page of the Google Form. We use the same queries shown above to query ChatGPT and the order of queries is the same as the order shown above."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2508
                },
                {
                    "x": 2179,
                    "y": 2508
                },
                {
                    "x": 2179,
                    "y": 2562
                },
                {
                    "x": 1267,
                    "y": 2562
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='262' style='font-size:18px'>D.2.3 Adversarial Attack Quality Evaluation</p>",
            "id": 262,
            "page": 22,
            "text": "D.2.3 Adversarial Attack Quality Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2577
                },
                {
                    "x": 2194,
                    "y": 2577
                },
                {
                    "x": 2194,
                    "y": 3024
                },
                {
                    "x": 1268,
                    "y": 3024
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='263' style='font-size:16px'>When querying all the LLMs in this task, we query<br>the fluency and the meaning preserving of the same<br>news title independently. This means that each<br>conversation with ChatGPT will only have one<br>question, asking about the fluency or the meaning<br>preserving of news title(s). All the parameters for<br>generation are the same as the default parameters<br>in Section 3.2.</p>",
            "id": 263,
            "page": 22,
            "text": "When querying all the LLMs in this task, we query the fluency and the meaning preserving of the same news title independently. This means that each conversation with ChatGPT will only have one question, asking about the fluency or the meaning preserving of news title(s). All the parameters for generation are the same as the default parameters in Section 3.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 3030
                },
                {
                    "x": 1836,
                    "y": 3030
                },
                {
                    "x": 1836,
                    "y": 3082
                },
                {
                    "x": 1314,
                    "y": 3082
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='264' style='font-size:14px'>The exact query we use are:</p>",
            "id": 264,
            "page": 22,
            "text": "The exact query we use are:"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 3116
                },
                {
                    "x": 1432,
                    "y": 3116
                },
                {
                    "x": 1432,
                    "y": 3166
                },
                {
                    "x": 1270,
                    "y": 3166
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='265' style='font-size:22px'>Fluency</p>",
            "id": 265,
            "page": 22,
            "text": "Fluency"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 3177
                },
                {
                    "x": 2189,
                    "y": 3177
                },
                {
                    "x": 2189,
                    "y": 3223
                },
                {
                    "x": 1269,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='266' style='font-size:14px'>You are given a news title. Please read</p>",
            "id": 266,
            "page": 22,
            "text": "You are given a news title. Please read"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 304
                },
                {
                    "x": 1212,
                    "y": 304
                },
                {
                    "x": 1212,
                    "y": 691
                },
                {
                    "x": 287,
                    "y": 691
                }
            ],
            "category": "paragraph",
            "html": "<p id='267' style='font-size:16px'>the news title and answer the question.<br>News title:<br>[NEWS_TITLE]<br>(End of news title)<br>Question: How natural and fluent is the<br>text of the news title? (on a scale of<br>1-5, with 1 being the lowest</p>",
            "id": 267,
            "page": 23,
            "text": "the news title and answer the question. News title: [NEWS_TITLE] (End of news title) Question: How natural and fluent is the text of the news title? (on a scale of 1-5, with 1 being the lowest"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 701
                },
                {
                    "x": 1213,
                    "y": 701
                },
                {
                    "x": 1213,
                    "y": 811
                },
                {
                    "x": 288,
                    "y": 811
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='268' style='font-size:16px'>The [NEWS_TITLE] will be filled in with either<br>a benign or adversarial-attacked news title.</p>",
            "id": 268,
            "page": 23,
            "text": "The [NEWS_TITLE] will be filled in with either a benign or adversarial-attacked news title."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 859
                },
                {
                    "x": 1212,
                    "y": 859
                },
                {
                    "x": 1212,
                    "y": 1026
                },
                {
                    "x": 288,
                    "y": 1026
                }
            ],
            "category": "paragraph",
            "html": "<p id='269' style='font-size:16px'>Meaning Preserving You are given two news<br>titles. Please read the news titles and<br>answer the question.</p>",
            "id": 269,
            "page": 23,
            "text": "Meaning Preserving You are given two news titles. Please read the news titles and answer the question."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1022
                },
                {
                    "x": 1215,
                    "y": 1022
                },
                {
                    "x": 1215,
                    "y": 1649
                },
                {
                    "x": 287,
                    "y": 1649
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='270' style='font-size:18px'>News title 1:<br>[BENIGN_TITLE]<br>(End of news title 1)<br>News title 2:<br>[ADVERSARIAL_TITLE]<br>(End of news title 2)<br>Question: Do you agree that the meaning<br>(or semantics) of news title 1 is<br>preserved in news title 2? (on a scale of<br>1-5, with 1 being the strongly disagree<br>and 5 being strongly agree. )</p>",
            "id": 270,
            "page": 23,
            "text": "News title 1: [BENIGN_TITLE] (End of news title 1) News title 2: [ADVERSARIAL_TITLE] (End of news title 2) Question: Do you agree that the meaning (or semantics) of news title 1 is preserved in news title 2? (on a scale of 1-5, with 1 being the strongly disagree and 5 being strongly agree. )"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1657
                },
                {
                    "x": 1214,
                    "y": 1657
                },
                {
                    "x": 1214,
                    "y": 1878
                },
                {
                    "x": 288,
                    "y": 1878
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='271' style='font-size:16px'>The [BENIGN_TITLE] will be filled in with<br>the news title before the attack and the<br>[ADVERSARIAL_TITLE] will be filled in with the<br>news title after an adversarial attack.</p>",
            "id": 271,
            "page": 23,
            "text": "The [BENIGN_TITLE] will be filled in with the news title before the attack and the [ADVERSARIAL_TITLE] will be filled in with the news title after an adversarial attack."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1934
                },
                {
                    "x": 1125,
                    "y": 1934
                },
                {
                    "x": 1125,
                    "y": 2048
                },
                {
                    "x": 289,
                    "y": 2048
                }
            ],
            "category": "paragraph",
            "html": "<p id='272' style='font-size:22px'>E Experiment Details on Adversarial<br>Attacks</p>",
            "id": 272,
            "page": 23,
            "text": "E Experiment Details on Adversarial Attacks"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2094
                },
                {
                    "x": 1215,
                    "y": 2094
                },
                {
                    "x": 1215,
                    "y": 2658
                },
                {
                    "x": 287,
                    "y": 2658
                }
            ],
            "category": "paragraph",
            "html": "<p id='273' style='font-size:18px'>The adversarial samples used in Section 4 are<br>from Yoo et al. (2022). Yoo et al. (2022) gener-<br>ates different sets of adversarial samples using dif-<br>ferent adversarial attacks against different victim<br>models. We use the adversarial samples gener-<br>ated against a bert-base-uncased text classifier<br>trained on AG-News, using three different adver-<br>sarial attacks: Textfooler, PWWS, and BAE. The<br>intent of the dataset is to facilitate the research in<br>SSA, which we do not violate.</p>",
            "id": 273,
            "page": 23,
            "text": "The adversarial samples used in Section 4 are from Yoo  (2022). Yoo  (2022) generates different sets of adversarial samples using different adversarial attacks against different victim models. We use the adversarial samples generated against a bert-base-uncased text classifier trained on AG-News, using three different adversarial attacks: Textfooler, PWWS, and BAE. The intent of the dataset is to facilitate the research in SSA, which we do not violate."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2667
                },
                {
                    "x": 1215,
                    "y": 2667
                },
                {
                    "x": 1215,
                    "y": 3231
                },
                {
                    "x": 286,
                    "y": 3231
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='274' style='font-size:20px'>Here, we show the supplementary results of us-<br>ing text-davinci-003 as the LLM evaluation for<br>evaluating the quality of adversarial samples in<br>Table 11. We can see that the result of using<br>text-davinci-003 is similar to ChatGPT in the<br>sense that text-davinci-003 also rates adversar-<br>ial samples higher than humans while still signifi-<br>cantly lower than the benign samples. As already<br>seen in Section 3.3, text-davinci-003 tends to<br>give a higher rating.</p>",
            "id": 274,
            "page": 23,
            "text": "Here, we show the supplementary results of using text-davinci-003 as the LLM evaluation for evaluating the quality of adversarial samples in Table 11. We can see that the result of using text-davinci-003 is similar to ChatGPT in the sense that text-davinci-003 also rates adversarial samples higher than humans while still significantly lower than the benign samples. As already seen in Section 3.3, text-davinci-003 tends to give a higher rating."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 287
                },
                {
                    "x": 2216,
                    "y": 287
                },
                {
                    "x": 2216,
                    "y": 654
                },
                {
                    "x": 1267,
                    "y": 654
                }
            ],
            "category": "table",
            "html": "<br><table id='275' style='font-size:16px'><tr><td></td><td colspan=\"2\">Human evaluate</td><td colspan=\"2\">LLM evaluate</td></tr><tr><td></td><td>Fluent</td><td>Mean.</td><td>Fluent</td><td>Mean.</td></tr><tr><td>Benign</td><td>4.55</td><td>-</td><td>4.33</td><td>4.56+</td></tr><tr><td>Textfooler</td><td>2.17</td><td>1.88</td><td>3.71</td><td>2.37</td></tr><tr><td>PWWS</td><td>2.16</td><td>1.85</td><td>3.62</td><td>3.21</td></tr><tr><td>BAE</td><td>3.01</td><td>3.02</td><td>4.16</td><td>3.69</td></tr></table>",
            "id": 275,
            "page": 23,
            "text": "Human evaluate LLM evaluate   Fluent Mean. Fluent Mean.  Benign 4.55 - 4.33 4.56+  Textfooler 2.17 1.88 3.71 2.37  PWWS 2.16 1.85 3.62 3.21  BAE 3.01 3.02 4.16"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 686
                },
                {
                    "x": 2198,
                    "y": 686
                },
                {
                    "x": 2198,
                    "y": 937
                },
                {
                    "x": 1267,
                    "y": 937
                }
            ],
            "category": "caption",
            "html": "<caption id='276' style='font-size:14px'>Table 11: LLM evaluation (text-davinci-003) and<br>human evaluation result on fluency (Fluent) of the be-<br>nign and adversarial samples and meaning preserving<br>(Mean.) between the news title before and after adver-<br>sarial attacks.</caption>",
            "id": 276,
            "page": 23,
            "text": "Table 11: LLM evaluation (text-davinci-003) and human evaluation result on fluency (Fluent) of the benign and adversarial samples and meaning preserving (Mean.) between the news title before and after adversarial attacks."
        },
        {
            "bounding_box": [
                {
                    "x": 1373,
                    "y": 983
                },
                {
                    "x": 2084,
                    "y": 983
                },
                {
                    "x": 2084,
                    "y": 1226
                },
                {
                    "x": 1373,
                    "y": 1226
                }
            ],
            "category": "table",
            "html": "<table id='277' style='font-size:16px'><tr><td>Rater</td><td>Textfooler</td><td>PWWS</td><td>BAE</td></tr><tr><td>T1</td><td>3.36</td><td>3.68</td><td>4.2</td></tr><tr><td>T2</td><td>1.80</td><td>1.40</td><td>2.96</td></tr><tr><td>T3</td><td>1.36</td><td>1.40</td><td>1.88</td></tr></table>",
            "id": 277,
            "page": 23,
            "text": "Rater Textfooler PWWS BAE  T1 3.36 3.68 4.2  T2 1.80 1.40 2.96  T3 1.36 1.40"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1261
                },
                {
                    "x": 2190,
                    "y": 1261
                },
                {
                    "x": 2190,
                    "y": 1355
                },
                {
                    "x": 1271,
                    "y": 1355
                }
            ],
            "category": "caption",
            "html": "<caption id='278' style='font-size:14px'>Table 12: The rating on three adversarial attacks of the<br>three teachers T1, T2, and T3.</caption>",
            "id": 278,
            "page": 23,
            "text": "Table 12: The rating on three adversarial attacks of the three teachers T1, T2, and T3."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1451
                },
                {
                    "x": 2195,
                    "y": 1451
                },
                {
                    "x": 2195,
                    "y": 1730
                },
                {
                    "x": 1267,
                    "y": 1730
                }
            ],
            "category": "paragraph",
            "html": "<p id='279' style='font-size:20px'>As mentioned in Section 4.3, one teacher rates<br>the fluency of Textfooler significantly higher than<br>PWWS while the other two teachers do not. We<br>show the rating onfluency on the three adversarial<br>attacks by each teacher in Table 12.</p>",
            "id": 279,
            "page": 23,
            "text": "As mentioned in Section 4.3, one teacher rates the fluency of Textfooler significantly higher than PWWS while the other two teachers do not. We show the rating onfluency on the three adversarial attacks by each teacher in Table 12."
        }
    ]
}