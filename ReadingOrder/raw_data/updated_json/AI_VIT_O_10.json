{
    "id": "62a75860-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2104.12533v5.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 2326,
                    "y": 105
                },
                {
                    "x": 2349,
                    "y": 105
                },
                {
                    "x": 2349,
                    "y": 135
                },
                {
                    "x": 2326,
                    "y": 135
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:14px'>1</header>",
            "id": 0,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 369,
                    "y": 240
                },
                {
                    "x": 2180,
                    "y": 240
                },
                {
                    "x": 2180,
                    "y": 351
                },
                {
                    "x": 369,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>Visformer: The Vision-friendly Transformer</p>",
            "id": 1,
            "page": 1,
            "text": "Visformer: The Vision-friendly Transformer"
        },
        {
            "bounding_box": [
                {
                    "x": 663,
                    "y": 375
                },
                {
                    "x": 1883,
                    "y": 375
                },
                {
                    "x": 1883,
                    "y": 489
                },
                {
                    "x": 663,
                    "y": 489
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:22px'>Zhengsu Chen, Lingxi Xie, Jianwei Niu, Senior Member, IEEE,<br>Xuefeng Liu, Longhui Wei, Qi Tian, Fellow, IEEE</p>",
            "id": 2,
            "page": 1,
            "text": "Zhengsu Chen, Lingxi Xie, Jianwei Niu, Senior Member, IEEE,\nXuefeng Liu, Longhui Wei, Qi Tian, Fellow, IEEE"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 633
                },
                {
                    "x": 1255,
                    "y": 633
                },
                {
                    "x": 1255,
                    "y": 1428
                },
                {
                    "x": 195,
                    "y": 1428
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:18px'>Abstract-The past few years have witnessed the rapid devel-<br>opment of applying the Transformer module to vision problems.<br>While some researchers have demonstrated that Transformer-<br>based models enjoy a favorable ability of fitting data, there are<br>still growing number of evidences showing that these models<br>suffer over-fitting especially when the training data is limited.<br>This paper offers an empirical study by performing step-by-<br>step operations to gradually transit a Transformer-based model<br>to a convolution-based model. The results we obtain during<br>the transition process deliver useful messages for improving<br>visual recognition. Based on these observations, we propose a<br>new architecture named Visformer, which is abbreviated from<br>the 'Vision-friendly Transformer'. With the same computational<br>complexity, Visformer outperforms both the Transformer-based<br>and convolution-based models in terms of ImageNet classification<br>and object detection performance, and the advantage becomes<br>more significant when the model complexity is lower or the<br>training set is smaller. The code is available at https://github.<br>com/danczs/Visformer.</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract-The past few years have witnessed the rapid devel-\nopment of applying the Transformer module to vision problems.\nWhile some researchers have demonstrated that Transformer-\nbased models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models\nsuffer over-fitting especially when the training data is limited.\nThis paper offers an empirical study by performing step-by-\nstep operations to gradually transit a Transformer-based model\nto a convolution-based model. The results we obtain during\nthe transition process deliver useful messages for improving\nvisual recognition. Based on these observations, we propose a\nnew architecture named Visformer, which is abbreviated from\nthe 'Vision-friendly Transformer'. With the same computational\ncomplexity, Visformer outperforms both the Transformer-based\nand convolution-based models in terms of ImageNet classification\nand object detection performance, and the advantage becomes\nmore significant when the model complexity is lower or the\ntraining set is smaller. The code is available at https://github.\ncom/danczs/Visformer."
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1447
                },
                {
                    "x": 1257,
                    "y": 1447
                },
                {
                    "x": 1257,
                    "y": 1537
                },
                {
                    "x": 198,
                    "y": 1537
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:20px'>Index Terms-vision-friendly Transformer, vision Transformer,<br>convolutional neural network, image recognition.</p>",
            "id": 4,
            "page": 1,
            "text": "Index Terms-vision-friendly Transformer, vision Transformer,\nconvolutional neural network, image recognition."
        },
        {
            "bounding_box": [
                {
                    "x": 558,
                    "y": 1626
                },
                {
                    "x": 893,
                    "y": 1626
                },
                {
                    "x": 893,
                    "y": 1671
                },
                {
                    "x": 558,
                    "y": 1671
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>I. INTRODUCTION</p>",
            "id": 5,
            "page": 1,
            "text": "I. INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1699
                },
                {
                    "x": 1258,
                    "y": 1699
                },
                {
                    "x": 1258,
                    "y": 2449
                },
                {
                    "x": 194,
                    "y": 2449
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>IN the past decade, convolution used to play a central role<br>the deep learning models [1]-[4] for visual recognition.<br>This situation starts to change when the Transformer [5], a<br>module that originates from natural language processing [5]-<br>[7], is transplanted to the vision scenarios. It was shown in<br>the ViT model [8] that an image can be partitioned into a<br>grid of patches and the Transformer is directly applied upon<br>the grid as if each patch is a visual word. ViT requires a<br>large amount of training data (e.g., the ImageNet-21K [9]<br>or the JFT-300M dataset), arguably because the Transformer<br>is equipped with long-range attention and interaction, and is<br>prone to over-fitting. The follow-up efforts [10] improved ViT<br>to some extent, but these models still perform badly especially<br>under limited training data or moderate data augmentation<br>compared with convolution-based models.</p>",
            "id": 6,
            "page": 1,
            "text": "IN the past decade, convolution used to play a central role\nthe deep learning models [1]-[4] for visual recognition.\nThis situation starts to change when the Transformer [5], a\nmodule that originates from natural language processing [5]-\n[7], is transplanted to the vision scenarios. It was shown in\nthe ViT model [8] that an image can be partitioned into a\ngrid of patches and the Transformer is directly applied upon\nthe grid as if each patch is a visual word. ViT requires a\nlarge amount of training data (e.g., the ImageNet-21K [9]\nor the JFT-300M dataset), arguably because the Transformer\nis equipped with long-range attention and interaction, and is\nprone to over-fitting. The follow-up efforts [10] improved ViT\nto some extent, but these models still perform badly especially\nunder limited training data or moderate data augmentation\ncompared with convolution-based models."
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2453
                },
                {
                    "x": 1257,
                    "y": 2453
                },
                {
                    "x": 1257,
                    "y": 2553
                },
                {
                    "x": 198,
                    "y": 2553
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>On the other hand, vision Transformers can achieve much<br>better performance than convolution-based models when</p>",
            "id": 7,
            "page": 1,
            "text": "On the other hand, vision Transformers can achieve much\nbetter performance than convolution-based models when"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 2600
                },
                {
                    "x": 1254,
                    "y": 2600
                },
                {
                    "x": 1254,
                    "y": 2714
                },
                {
                    "x": 196,
                    "y": 2714
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:16px'>Zhengsu Chen and Xuefeng Liu are with the School of Computer Sci-<br>ence and Engineering, Beihang University, 100191, Beijing, China (e-mail:<br>danczs@buaa.edu.cn; liu_xuefeng@buaa.edu.cn)</p>",
            "id": 8,
            "page": 1,
            "text": "Zhengsu Chen and Xuefeng Liu are with the School of Computer Sci-\nence and Engineering, Beihang University, 100191, Beijing, China (e-mail:\ndanczs@buaa.edu.cn; liu_xuefeng@buaa.edu.cn)"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2714
                },
                {
                    "x": 1253,
                    "y": 2714
                },
                {
                    "x": 1253,
                    "y": 2788
                },
                {
                    "x": 198,
                    "y": 2788
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>Lingxi Xie is with Johns Hopkins University, Baltimore, USA (e-mail:<br>198808xc@gmail.com)</p>",
            "id": 9,
            "page": 1,
            "text": "Lingxi Xie is with Johns Hopkins University, Baltimore, USA (e-mail:\n198808xc@gmail.com)"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2790
                },
                {
                    "x": 1255,
                    "y": 2790
                },
                {
                    "x": 1255,
                    "y": 2901
                },
                {
                    "x": 198,
                    "y": 2901
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>Jianwei Niu is with Hangzhou Innovation Institute of Beihang University,<br>Zhengzhou University, and Beihang University, 100191, Beijing, China (e-<br>mail: @niujianwei@buaa.edu.cn)</p>",
            "id": 10,
            "page": 1,
            "text": "Jianwei Niu is with Hangzhou Innovation Institute of Beihang University,\nZhengzhou University, and Beihang University, 100191, Beijing, China (e-\nmail: @niujianwei@buaa.edu.cn)"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2903
                },
                {
                    "x": 1252,
                    "y": 2903
                },
                {
                    "x": 1252,
                    "y": 2976
                },
                {
                    "x": 198,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>Longhui wei is with the University of Science and Technology of China,<br>230026, Hefei, China (e-mail:longhuiwei@pku.edu.cn)</p>",
            "id": 11,
            "page": 1,
            "text": "Longhui wei is with the University of Science and Technology of China,\n230026, Hefei, China (e-mail:longhuiwei@pku.edu.cn)"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2977
                },
                {
                    "x": 1252,
                    "y": 2977
                },
                {
                    "x": 1252,
                    "y": 3054
                },
                {
                    "x": 195,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:16px'>Qi Tian is with Xidian University, 710126, Xi' an, China (e-mail: wywq-<br>tian@gmail.com)</p>",
            "id": 12,
            "page": 1,
            "text": "Qi Tian is with Xidian University, 710126, Xi' an, China (e-mail: wywq-\ntian@gmail.com)"
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 633
                },
                {
                    "x": 2317,
                    "y": 633
                },
                {
                    "x": 2317,
                    "y": 751
                },
                {
                    "x": 1332,
                    "y": 751
                }
            ],
            "category": "caption",
            "html": "<br><caption id='13' style='font-size:14px'>TABLE I<br>THE COMPARISON AMONG RESNET-50, DEIT-S, AND THE PROPOSED<br>VISFORMER-S MODEL ON IMAGENET.</caption>",
            "id": 13,
            "page": 1,
            "text": "TABLE I\nTHE COMPARISON AMONG RESNET-50, DEIT-S, AND THE PROPOSED\nVISFORMER-S MODEL ON IMAGENET."
        },
        {
            "bounding_box": [
                {
                    "x": 1414,
                    "y": 778
                },
                {
                    "x": 2234,
                    "y": 778
                },
                {
                    "x": 2234,
                    "y": 1080
                },
                {
                    "x": 1414,
                    "y": 1080
                }
            ],
            "category": "table",
            "html": "<table id='14' style='font-size:16px'><tr><td colspan=\"2\">Network</td><td>ResNet-50</td><td>DeiT-S</td><td>Visformer-S</td></tr><tr><td colspan=\"2\">FLOPs (G)</td><td>4.1</td><td>4.6</td><td>4.9</td></tr><tr><td colspan=\"2\">Parameters (M)</td><td>25.6</td><td>21.8</td><td>40.2</td></tr><tr><td rowspan=\"2\">Full data</td><td>base setting</td><td>77.43</td><td>63.12</td><td>77.20</td></tr><tr><td>elite setting</td><td>78.73</td><td>80.07</td><td>82.19</td></tr><tr><td rowspan=\"2\">Part of data</td><td>10% labels</td><td>58.37</td><td>40.41</td><td>58.74</td></tr><tr><td>10% classes</td><td>89.90</td><td>80.06</td><td>90.06</td></tr></table>",
            "id": 14,
            "page": 1,
            "text": "Network ResNet-50 DeiT-S Visformer-S\n FLOPs (G) 4.1 4.6 4.9\n Parameters (M) 25.6 21.8 40.2\n Full data base setting 77.43 63.12 77.20\n elite setting 78.73 80.07 82.19\n Part of data 10% labels 58.37 40.41 58.74\n 10% classes 89.90 80.06"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1204
                },
                {
                    "x": 2352,
                    "y": 1204
                },
                {
                    "x": 2352,
                    "y": 1552
                },
                {
                    "x": 1291,
                    "y": 1552
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:20px'>trained with large amount of data. Namely, vision Transform-<br>ers have higher 'upper-bound' while convolution-based models<br>are better in 'lower-bound'. Both upper-bound and lower-<br>bound are important properties for neural networks. Upper-<br>bound is the potential to achieve higher performance and<br>lower-bound enables networks to perform better when trained<br>with limited data or scaled to different complexity.</p>",
            "id": 15,
            "page": 1,
            "text": "trained with large amount of data. Namely, vision Transform-\ners have higher 'upper-bound' while convolution-based models\nare better in 'lower-bound'. Both upper-bound and lower-\nbound are important properties for neural networks. Upper-\nbound is the potential to achieve higher performance and\nlower-bound enables networks to perform better when trained\nwith limited data or scaled to different complexity."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1556
                },
                {
                    "x": 2355,
                    "y": 1556
                },
                {
                    "x": 2355,
                    "y": 2403
                },
                {
                    "x": 1294,
                    "y": 2403
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:20px'>Based on the observation of lower-bound and upper-bound<br>on Transformer-based and convolution-based networks, the<br>main goal of this paper is to identify the reasons behind<br>the difference, by which we can design networks with higher<br>lower-bound and upper-bound. The gap between Transformer-<br>based and convolution-based networks can be revealed with<br>two different training settings on ImageNet. The first one is<br>the base setting. It is the standard setting for convolution-<br>based models, i.e., the training schedule is shorter and the data<br>augmentation only contains basic operators such as random-<br>size cropping [11] and flipping. The performance under this<br>setting is called base performance in this paper. The other<br>one is the training setting used in [10]. It is carefully tuned<br>for Transformer-based models, i.e., the training schedule is<br>longer and the data augmentation is stronger (e.g., RandAug-<br>ment [12], CutMix [13], etc., have been added). We use the<br>elite performance to refer to the accuracy produced by it.</p>",
            "id": 16,
            "page": 1,
            "text": "Based on the observation of lower-bound and upper-bound\non Transformer-based and convolution-based networks, the\nmain goal of this paper is to identify the reasons behind\nthe difference, by which we can design networks with higher\nlower-bound and upper-bound. The gap between Transformer-\nbased and convolution-based networks can be revealed with\ntwo different training settings on ImageNet. The first one is\nthe base setting. It is the standard setting for convolution-\nbased models, i.e., the training schedule is shorter and the data\naugmentation only contains basic operators such as random-\nsize cropping [11] and flipping. The performance under this\nsetting is called base performance in this paper. The other\none is the training setting used in [10]. It is carefully tuned\nfor Transformer-based models, i.e., the training schedule is\nlonger and the data augmentation is stronger (e.g., RandAug-\nment [12], CutMix [13], etc., have been added). We use the\nelite performance to refer to the accuracy produced by it."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2407
                },
                {
                    "x": 2355,
                    "y": 2407
                },
                {
                    "x": 2355,
                    "y": 3056
                },
                {
                    "x": 1292,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:20px'>We take DeiT-S [10] and ResNet-50 [4] as the examples of<br>Transformer-based and convolution-based models. As shown<br>in Table I, Deit-S and ResNet-50 employ comparable FLOPs<br>and parameters. However, they behave very differently trained<br>on the full data under these two settings. Deit-S has higher elite<br>performance, but changing the setting from elite to base can<br>cause a 10%+ accuracy drop for DeiT-S. ResNet-50 performs<br>much better under the base setting, yet the improvement for<br>the elite setting is merely 1.3%. This motivates us to study<br>the difference between these models. With these two settings,<br>we can roughly estimate the lower-bound and upper-bound of<br>the models. The methodology we use is to perform step-by-<br>step operations to gradually transit one model into another, by</p>",
            "id": 17,
            "page": 1,
            "text": "We take DeiT-S [10] and ResNet-50 [4] as the examples of\nTransformer-based and convolution-based models. As shown\nin Table I, Deit-S and ResNet-50 employ comparable FLOPs\nand parameters. However, they behave very differently trained\non the full data under these two settings. Deit-S has higher elite\nperformance, but changing the setting from elite to base can\ncause a 10%+ accuracy drop for DeiT-S. ResNet-50 performs\nmuch better under the base setting, yet the improvement for\nthe elite setting is merely 1.3%. This motivates us to study\nthe difference between these models. With these two settings,\nwe can roughly estimate the lower-bound and upper-bound of\nthe models. The methodology we use is to perform step-by-\nstep operations to gradually transit one model into another, by"
        },
        {
            "bounding_box": [
                {
                    "x": 1028,
                    "y": 3079
                },
                {
                    "x": 1520,
                    "y": 3079
                },
                {
                    "x": 1520,
                    "y": 3116
                },
                {
                    "x": 1028,
                    "y": 3116
                }
            ],
            "category": "footer",
            "html": "<br><footer id='18' style='font-size:16px'>0000-0000/00$00.00 ⓒ 2021 IEEE</footer>",
            "id": 18,
            "page": 1,
            "text": "0000-0000/00$00.00 ⓒ 2021 IEEE"
        },
        {
            "bounding_box": [
                {
                    "x": 56,
                    "y": 879
                },
                {
                    "x": 150,
                    "y": 879
                },
                {
                    "x": 150,
                    "y": 2325
                },
                {
                    "x": 56,
                    "y": 2325
                }
            ],
            "category": "footer",
            "html": "<br><footer id='19' style='font-size:14px'>2021<br>Dec<br>18<br>[cs.CV]<br>arXiv:2104.12533v5</footer>",
            "id": 19,
            "page": 1,
            "text": "2021\nDec\n18\n[cs.CV]\narXiv:2104.12533v5"
        },
        {
            "bounding_box": [
                {
                    "x": 2325,
                    "y": 105
                },
                {
                    "x": 2350,
                    "y": 105
                },
                {
                    "x": 2350,
                    "y": 135
                },
                {
                    "x": 2325,
                    "y": 135
                }
            ],
            "category": "header",
            "html": "<header id='20' style='font-size:14px'>2</header>",
            "id": 20,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 233
                },
                {
                    "x": 1257,
                    "y": 233
                },
                {
                    "x": 1257,
                    "y": 378
                },
                {
                    "x": 196,
                    "y": 378
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:18px'>which we can identify the properties of modules and designs<br>in these two networks. The entire transition process, taking a<br>total of 8 steps, is illustrated in Figure 1.</p>",
            "id": 21,
            "page": 2,
            "text": "which we can identify the properties of modules and designs\nin these two networks. The entire transition process, taking a\ntotal of 8 steps, is illustrated in Figure 1."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 381
                },
                {
                    "x": 1257,
                    "y": 381
                },
                {
                    "x": 1257,
                    "y": 926
                },
                {
                    "x": 195,
                    "y": 926
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:22px'>Specifically, from DeiT-S to ResNet-50, one should (i)<br>use global average pooling (not the classification token),<br>(ii) introduce step-wise patch embeddings (not large patch<br>flattening), (iii) adopt the stage-wise backbone design, (iv) use<br>batch normalization [14] (not layer normalization [15]), (v)<br>leverage 3 x 3 convolutions, (vi) discard the position embed-<br>ding scheme, (vii) replace self-attention with convolution, and<br>finally (viii) adjust the network shape (e.g., depth, width, etc.).<br>After a thorough analysis on the reasons behind the results, we<br>absorb all the factors that are helpful to visual recognition and<br>derive the Visformer, i.e., the Vision-friendly Transformer.</p>",
            "id": 22,
            "page": 2,
            "text": "Specifically, from DeiT-S to ResNet-50, one should (i)\nuse global average pooling (not the classification token),\n(ii) introduce step-wise patch embeddings (not large patch\nflattening), (iii) adopt the stage-wise backbone design, (iv) use\nbatch normalization [14] (not layer normalization [15]), (v)\nleverage 3 x 3 convolutions, (vi) discard the position embed-\nding scheme, (vii) replace self-attention with convolution, and\nfinally (viii) adjust the network shape (e.g., depth, width, etc.).\nAfter a thorough analysis on the reasons behind the results, we\nabsorb all the factors that are helpful to visual recognition and\nderive the Visformer, i.e., the Vision-friendly Transformer."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 930
                },
                {
                    "x": 1256,
                    "y": 930
                },
                {
                    "x": 1256,
                    "y": 1474
                },
                {
                    "x": 195,
                    "y": 1474
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:20px'>Evaluated on ImageNet classification, Visformer claims<br>better performance than the competitors, DeiT and ResNet,<br>as shown in Table I. With the elite setting, the Visformer-<br>S model outperforms DeiT-S and ResNet-50 by 2.12% and<br>3.46%, respectively, under a comparable model complexity.<br>Different from Deit-S, Visformer-S also survives two extra<br>challenges, namely, when the model is trained with 10%<br>labels (images) and 10% classes. Visformer-S even performs<br>better than ResNet-50, which reveals the high lower-bound<br>of Visformer-S. Additionally, for tiny models, Visformer-Ti<br>significantly outperforms Deit-Ti by more than 6%.</p>",
            "id": 23,
            "page": 2,
            "text": "Evaluated on ImageNet classification, Visformer claims\nbetter performance than the competitors, DeiT and ResNet,\nas shown in Table I. With the elite setting, the Visformer-\nS model outperforms DeiT-S and ResNet-50 by 2.12% and\n3.46%, respectively, under a comparable model complexity.\nDifferent from Deit-S, Visformer-S also survives two extra\nchallenges, namely, when the model is trained with 10%\nlabels (images) and 10% classes. Visformer-S even performs\nbetter than ResNet-50, which reveals the high lower-bound\nof Visformer-S. Additionally, for tiny models, Visformer-Ti\nsignificantly outperforms Deit-Ti by more than 6%."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1476
                },
                {
                    "x": 1258,
                    "y": 1476
                },
                {
                    "x": 1258,
                    "y": 1921
                },
                {
                    "x": 196,
                    "y": 1921
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:18px'>The contribution of this paper is three-fold. First, for the<br>first time, we introduce the lower-bound and upper-bound<br>to investigate the performance of Transformer-based vision<br>models. Second, we close the gap between the Transformer-<br>based and convolution-based models by a gradual transition<br>process and thus identify the properties of the designs in<br>the Transformer-based and convolution-based models. Third,<br>we propose the Visformer as the final model that achieves<br>satisfying lower-bound and upper-bound.</p>",
            "id": 24,
            "page": 2,
            "text": "The contribution of this paper is three-fold. First, for the\nfirst time, we introduce the lower-bound and upper-bound\nto investigate the performance of Transformer-based vision\nmodels. Second, we close the gap between the Transformer-\nbased and convolution-based models by a gradual transition\nprocess and thus identify the properties of the designs in\nthe Transformer-based and convolution-based models. Third,\nwe propose the Visformer as the final model that achieves\nsatisfying lower-bound and upper-bound."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1923
                },
                {
                    "x": 1257,
                    "y": 1923
                },
                {
                    "x": 1257,
                    "y": 2168
                },
                {
                    "x": 196,
                    "y": 2168
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:18px'>The preliminary version of this paper appeared as [16]. In<br>the extended version, we further explore the recently proposed<br>work and provide more experiments and analysis. The main<br>improvements over the preliminary version are summarized as<br>follows:</p>",
            "id": 25,
            "page": 2,
            "text": "The preliminary version of this paper appeared as [16]. In\nthe extended version, we further explore the recently proposed\nwork and provide more experiments and analysis. The main\nimprovements over the preliminary version are summarized as\nfollows:"
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 2174
                },
                {
                    "x": 1259,
                    "y": 2174
                },
                {
                    "x": 1259,
                    "y": 2622
                },
                {
                    "x": 239,
                    "y": 2622
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:18px'>● We optimize the architecture of Visformer according to<br>the experimental observations and propose VisformerV2<br>which substantially outperforms the old version.<br>● We analyze the overflow problem when utilizing half-<br>precision in Transformers and propose an efficient<br>method to avoid overflow without degrading the perfor-<br>mance.<br>● We generalize Visformer to downstream vision tasks and<br>observe consistent improvements.</p>",
            "id": 26,
            "page": 2,
            "text": "● We optimize the architecture of Visformer according to\nthe experimental observations and propose VisformerV2\nwhich substantially outperforms the old version.\n● We analyze the overflow problem when utilizing half-\nprecision in Transformers and propose an efficient\nmethod to avoid overflow without degrading the perfor-\nmance.\n● We generalize Visformer to downstream vision tasks and\nobserve consistent improvements."
        },
        {
            "bounding_box": [
                {
                    "x": 543,
                    "y": 2660
                },
                {
                    "x": 904,
                    "y": 2660
                },
                {
                    "x": 904,
                    "y": 2707
                },
                {
                    "x": 543,
                    "y": 2707
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>II. RELATED WORK</p>",
            "id": 27,
            "page": 2,
            "text": "II. RELATED WORK"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 2724
                },
                {
                    "x": 1257,
                    "y": 2724
                },
                {
                    "x": 1257,
                    "y": 3124
                },
                {
                    "x": 196,
                    "y": 3124
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:16px'>Image classification is a fundamental task in computer<br>vision. In the deep learning era, the most popular method is to<br>use deep neural networks [2], [4], [17]. One of the fundamental<br>units to build such networks is convolution, where a number<br>of convolutional kernels are used to capture repeatable local<br>patterns in the input image and intermediate data. To reduce<br>the computational costs as well as alleviate the risk of over-<br>fitting, it was believed that the convolutional kernels should</p>",
            "id": 28,
            "page": 2,
            "text": "Image classification is a fundamental task in computer\nvision. In the deep learning era, the most popular method is to\nuse deep neural networks [2], [4], [17]. One of the fundamental\nunits to build such networks is convolution, where a number\nof convolutional kernels are used to capture repeatable local\npatterns in the input image and intermediate data. To reduce\nthe computational costs as well as alleviate the risk of over-\nfitting, it was believed that the convolutional kernels should"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 229
                },
                {
                    "x": 2353,
                    "y": 229
                },
                {
                    "x": 2353,
                    "y": 577
                },
                {
                    "x": 1291,
                    "y": 577
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:18px'>be of a small size, e.g., 3 x 3. However, this brings the<br>difficulty for faraway contexts in the image to communicate<br>with each other - this is partly the reason that the number<br>of layers has been increasing. Despite stacking more and<br>more layers, researchers consider another path which is to use<br>attention-based approaches to ease the propagation of visual<br>information.</p>",
            "id": 29,
            "page": 2,
            "text": "be of a small size, e.g., 3 x 3. However, this brings the\ndifficulty for faraway contexts in the image to communicate\nwith each other - this is partly the reason that the number\nof layers has been increasing. Despite stacking more and\nmore layers, researchers consider another path which is to use\nattention-based approaches to ease the propagation of visual\ninformation."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 581
                },
                {
                    "x": 2355,
                    "y": 581
                },
                {
                    "x": 2355,
                    "y": 1725
                },
                {
                    "x": 1294,
                    "y": 1725
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:20px'>Since Transformers achieved remarkable success in natural<br>language processing (NLP) [5]-[7], many efforts have been<br>made to introduce Transformers to vision tasks. These works<br>mainly fall into two categories. The first category consists<br>of pure attention models [8], [10], [18]-[22]. These models<br>usually only utilize self-attention and attempt to build vision<br>models without convolutions. However, it is computationally<br>expensive to relate all pixels with self-attention for realistic<br>full-sized images. Thus, there has some interest in forcing<br>self-attention to only concentrate on the pixels in local neigh-<br>borhoods (e.g., SASA [18], LRNet [19], SANet [20]). These<br>methods replace convolutions with local self-attentions to learn<br>local relations and achieve promising results. However, it<br>requires complex engineering to efficiently apply self-attention<br>to every local region in an image. Another way to solve<br>the complexity problem is to apply self-attention to reduced<br>resolution. These methods either reduce the resolution and<br>color space first [21] or regard image patches rather pixels as<br>tokens (i.e., words) [8], [10]. However, resolution reduction<br>and patch flattening usually make it more difficult to utilize<br>the local prior in natural images. Thus, these methods usually<br>obtain suboptimal results [21] or require huge dataset [8] and<br>heavy augmentation [10].</p>",
            "id": 30,
            "page": 2,
            "text": "Since Transformers achieved remarkable success in natural\nlanguage processing (NLP) [5]-[7], many efforts have been\nmade to introduce Transformers to vision tasks. These works\nmainly fall into two categories. The first category consists\nof pure attention models [8], [10], [18]-[22]. These models\nusually only utilize self-attention and attempt to build vision\nmodels without convolutions. However, it is computationally\nexpensive to relate all pixels with self-attention for realistic\nfull-sized images. Thus, there has some interest in forcing\nself-attention to only concentrate on the pixels in local neigh-\nborhoods (e.g., SASA [18], LRNet [19], SANet [20]). These\nmethods replace convolutions with local self-attentions to learn\nlocal relations and achieve promising results. However, it\nrequires complex engineering to efficiently apply self-attention\nto every local region in an image. Another way to solve\nthe complexity problem is to apply self-attention to reduced\nresolution. These methods either reduce the resolution and\ncolor space first [21] or regard image patches rather pixels as\ntokens (i.e., words) [8], [10]. However, resolution reduction\nand patch flattening usually make it more difficult to utilize\nthe local prior in natural images. Thus, these methods usually\nobtain suboptimal results [21] or require huge dataset [8] and\nheavy augmentation [10]."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1727
                },
                {
                    "x": 2354,
                    "y": 1727
                },
                {
                    "x": 2354,
                    "y": 3126
                },
                {
                    "x": 1295,
                    "y": 3126
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:20px'>The second category contains the networks built with not<br>only self-attentions but also convolutions. Self-attention was<br>first introduced to CNNs by non-local neural networks [23].<br>These networks aim to capture global dependencies in images<br>and videos. Note that non-local neural networks are inspired by<br>the classical non-local method in vision tasks [24] and unlike<br>those in Transformers, the self-attentions in non-local networks<br>are usually not equipped with multi-heads and position em-<br>bedding [23], [25], [26]. Afterwards, Transformers achieve<br>remarkable success in NLP tasks [6], [7] and, therefore,<br>self-attentions that inherits NLP settings (e.g., multi-heads,<br>position encodings, classification token, etc.) are combined<br>with convolutions to improve vision tasks [18], [27], [28].<br>A common combination is to utilize convolutions first and<br>apply self-attention afterwards [8], [29]. [8] builds hybrids of<br>self-attention and convolution by adding a ResNet backbone<br>before Transformers. Afterwards, more and more methods are<br>proposed to combine self-attention and convolution. Besides<br>utilizing convolution in early layers [30], BotNet [29] designs<br>bottleneck cells for self-attention. Conformer [31] fuses the<br>feature of a convolution neural network and a Transformer<br>with the feature coupling unit to combine the global and local<br>representations. CvT [32] introduces convolution to the feature<br>projection of self-attention, by which the query, key and value<br>in the self-attention can capture the local information. CoAt-<br>Net [33] unifies depthwise convolution with self-attention and<br>vertically stacks convolution layers and self-attention layers to<br>improve the generalization, capacity and efficiency. However,</p>",
            "id": 31,
            "page": 2,
            "text": "The second category contains the networks built with not\nonly self-attentions but also convolutions. Self-attention was\nfirst introduced to CNNs by non-local neural networks [23].\nThese networks aim to capture global dependencies in images\nand videos. Note that non-local neural networks are inspired by\nthe classical non-local method in vision tasks [24] and unlike\nthose in Transformers, the self-attentions in non-local networks\nare usually not equipped with multi-heads and position em-\nbedding [23], [25], [26]. Afterwards, Transformers achieve\nremarkable success in NLP tasks [6], [7] and, therefore,\nself-attentions that inherits NLP settings (e.g., multi-heads,\nposition encodings, classification token, etc.) are combined\nwith convolutions to improve vision tasks [18], [27], [28].\nA common combination is to utilize convolutions first and\napply self-attention afterwards [8], [29]. [8] builds hybrids of\nself-attention and convolution by adding a ResNet backbone\nbefore Transformers. Afterwards, more and more methods are\nproposed to combine self-attention and convolution. Besides\nutilizing convolution in early layers [30], BotNet [29] designs\nbottleneck cells for self-attention. Conformer [31] fuses the\nfeature of a convolution neural network and a Transformer\nwith the feature coupling unit to combine the global and local\nrepresentations. CvT [32] introduces convolution to the feature\nprojection of self-attention, by which the query, key and value\nin the self-attention can capture the local information. CoAt-\nNet [33] unifies depthwise convolution with self-attention and\nvertically stacks convolution layers and self-attention layers to\nimprove the generalization, capacity and efficiency. However,"
        },
        {
            "bounding_box": [
                {
                    "x": 2325,
                    "y": 105
                },
                {
                    "x": 2350,
                    "y": 105
                },
                {
                    "x": 2350,
                    "y": 134
                },
                {
                    "x": 2325,
                    "y": 134
                }
            ],
            "category": "header",
            "html": "<header id='32' style='font-size:14px'>3</header>",
            "id": 32,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 233
                },
                {
                    "x": 1257,
                    "y": 233
                },
                {
                    "x": 1257,
                    "y": 429
                },
                {
                    "x": 195,
                    "y": 429
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>these methods usually combine convolution and self-attention<br>empirically or heuristically. Our method, in contrast, explores<br>the full process of converting a Transformer to a convolution<br>neural network.</p>",
            "id": 33,
            "page": 3,
            "text": "these methods usually combine convolution and self-attention\nempirically or heuristically. Our method, in contrast, explores\nthe full process of converting a Transformer to a convolution\nneural network."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 434
                },
                {
                    "x": 1257,
                    "y": 434
                },
                {
                    "x": 1257,
                    "y": 1028
                },
                {
                    "x": 195,
                    "y": 1028
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:18px'>There is also work that studies scaling or training vision<br>Transformer. CaiT [34] find that it is very efficient to scale<br>up vision Transformer in depth dimension with LayerScale.<br>AutoFormer [35] builds a super Transformer network by which<br>they can evaluate the different designs and search efficient<br>Transformer architecture. Zhai et al. [36] observe that most<br>vision Transformers can benefit from increasing compute<br>resources and larger dataset. They suggest scaling up compute,<br>data and model together. Steiner et al. [37] further study data,<br>augmentation and regularization in vision Transformer and<br>finds that augmentation can yield the same performance as<br>that trained on an order of magnitude more data.</p>",
            "id": 34,
            "page": 3,
            "text": "There is also work that studies scaling or training vision\nTransformer. CaiT [34] find that it is very efficient to scale\nup vision Transformer in depth dimension with LayerScale.\nAutoFormer [35] builds a super Transformer network by which\nthey can evaluate the different designs and search efficient\nTransformer architecture. Zhai et al. [36] observe that most\nvision Transformers can benefit from increasing compute\nresources and larger dataset. They suggest scaling up compute,\ndata and model together. Steiner et al. [37] further study data,\naugmentation and regularization in vision Transformer and\nfinds that augmentation can yield the same performance as\nthat trained on an order of magnitude more data."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1034
                },
                {
                    "x": 1256,
                    "y": 1034
                },
                {
                    "x": 1256,
                    "y": 1828
                },
                {
                    "x": 195,
                    "y": 1828
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:20px'>Additionally, self-attention has been used in many down-<br>stream vision tasks (detection [38], segmentation [39]) and low<br>vision tasks [40]. These tasks usually utilize much larger input<br>resolution than classification. For example, the frameworks<br>in COCO [41] usually utilize 1280 x 800 inputs. This is a<br>critical problem for vision Transformer, since the complexity<br>increases quadratically with pixel numbers. The widely used<br>solution is adopting sliding windows to capture local patterns<br>and building extra pipelines for information exchange among<br>the windows. Swin Transformer [42] shifts the windows<br>alternately in different layers, by which the tokens can build<br>long-distance relations as the depth increases. CSWin Trans-<br>former [43] further develops the cross-shaped self-attention<br>mechanism to ensure that the windows can access the global<br>feature in one dimension. MSG-Transformer [44], by contrast,<br>exchanges the local information by messenger tokens.</p>",
            "id": 35,
            "page": 3,
            "text": "Additionally, self-attention has been used in many down-\nstream vision tasks (detection [38], segmentation [39]) and low\nvision tasks [40]. These tasks usually utilize much larger input\nresolution than classification. For example, the frameworks\nin COCO [41] usually utilize 1280 x 800 inputs. This is a\ncritical problem for vision Transformer, since the complexity\nincreases quadratically with pixel numbers. The widely used\nsolution is adopting sliding windows to capture local patterns\nand building extra pipelines for information exchange among\nthe windows. Swin Transformer [42] shifts the windows\nalternately in different layers, by which the tokens can build\nlong-distance relations as the depth increases. CSWin Trans-\nformer [43] further develops the cross-shaped self-attention\nmechanism to ensure that the windows can access the global\nfeature in one dimension. MSG-Transformer [44], by contrast,\nexchanges the local information by messenger tokens."
        },
        {
            "bounding_box": [
                {
                    "x": 538,
                    "y": 1890
                },
                {
                    "x": 911,
                    "y": 1890
                },
                {
                    "x": 911,
                    "y": 1936
                },
                {
                    "x": 538,
                    "y": 1936
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:16px'>III. METHODOLOGY</p>",
            "id": 36,
            "page": 3,
            "text": "III. METHODOLOGY"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 1960
                },
                {
                    "x": 1251,
                    "y": 1960
                },
                {
                    "x": 1251,
                    "y": 2054
                },
                {
                    "x": 197,
                    "y": 2054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:20px'>A. Transformer-based and convolution-based visual recogni-<br>tion models</p>",
            "id": 37,
            "page": 3,
            "text": "A. Transformer-based and convolution-based visual recogni-\ntion models"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 2074
                },
                {
                    "x": 1257,
                    "y": 2074
                },
                {
                    "x": 1257,
                    "y": 2321
                },
                {
                    "x": 197,
                    "y": 2321
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:20px'>Recognition is the fundamental task in computer vision.<br>This work mainly considers image classification, where the<br>input image is propagated through a deep network to derive<br>the output class label. Most deep networks are designed in a<br>hierarchical manner and composed of a series of layers.</p>",
            "id": 38,
            "page": 3,
            "text": "Recognition is the fundamental task in computer vision.\nThis work mainly considers image classification, where the\ninput image is propagated through a deep network to derive\nthe output class label. Most deep networks are designed in a\nhierarchical manner and composed of a series of layers."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2327
                },
                {
                    "x": 1257,
                    "y": 2327
                },
                {
                    "x": 1257,
                    "y": 2920
                },
                {
                    "x": 193,
                    "y": 2920
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:18px'>We consider two popular layers named convolution and<br>Transformer. Convolution originates from the intuition to<br>capture local patterns which are believed more repeatable<br>than global patterns. It uses a number of learnable kernels<br>to compute the responses of the input to different patterns,<br>for which a sliding window is moved along both axes of<br>the input data and the inner-product between the data and<br>kernel is calculated. In this paper, we constrain our study<br>in the scope of residual blocks, a combination of 2 or 3<br>convolutional layers and a skip-connection. Non-linearities<br>such as activation and normalization are inserted between the<br>neighboring convolutional layers.</p>",
            "id": 39,
            "page": 3,
            "text": "We consider two popular layers named convolution and\nTransformer. Convolution originates from the intuition to\ncapture local patterns which are believed more repeatable\nthan global patterns. It uses a number of learnable kernels\nto compute the responses of the input to different patterns,\nfor which a sliding window is moved along both axes of\nthe input data and the inner-product between the data and\nkernel is calculated. In this paper, we constrain our study\nin the scope of residual blocks, a combination of 2 or 3\nconvolutional layers and a skip-connection. Non-linearities\nsuch as activation and normalization are inserted between the\nneighboring convolutional layers."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 2923
                },
                {
                    "x": 1257,
                    "y": 2923
                },
                {
                    "x": 1257,
                    "y": 3122
                },
                {
                    "x": 196,
                    "y": 3122
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:20px'>On the other hand, Transformer originates from natural<br>language processing and aims to frequently formulate the<br>relationship between any two elements (called tokens) even<br>when they are far from each other. This is achieved by</p>",
            "id": 40,
            "page": 3,
            "text": "On the other hand, Transformer originates from natural\nlanguage processing and aims to frequently formulate the\nrelationship between any two elements (called tokens) even\nwhen they are far from each other. This is achieved by"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 233
                },
                {
                    "x": 2352,
                    "y": 233
                },
                {
                    "x": 2352,
                    "y": 580
                },
                {
                    "x": 1291,
                    "y": 580
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:20px'>generating three features for each token, named the query, key,<br>and value, respectively. Then, the response of each token is<br>calculated as a weighted sum over all the values, where the<br>weights are determined by the similarity between its query and<br>the corresponding keys. This is often referred to as multi-head<br>self-attention (MHSA), followed by other operations including<br>normalization and linear mapping.</p>",
            "id": 41,
            "page": 3,
            "text": "generating three features for each token, named the query, key,\nand value, respectively. Then, the response of each token is\ncalculated as a weighted sum over all the values, where the\nweights are determined by the similarity between its query and\nthe corresponding keys. This is often referred to as multi-head\nself-attention (MHSA), followed by other operations including\nnormalization and linear mapping."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 591
                },
                {
                    "x": 2352,
                    "y": 591
                },
                {
                    "x": 2352,
                    "y": 985
                },
                {
                    "x": 1291,
                    "y": 985
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:20px'>Throughout the remaining part, we consider DeiT-S [10]<br>and ResNet-50 [4] as the representative of Transformer-based<br>and convolution-based models, respectively. Besides the basic<br>building block, there are also differences in design, e.g.,<br>ResNet-50 has a few down-sampling layers that partition<br>the model into stages, but the number of tokens remains<br>unchanged throughout DeiT-S. The impact of these details will<br>be elaborated in Section III-C.</p>",
            "id": 42,
            "page": 3,
            "text": "Throughout the remaining part, we consider DeiT-S [10]\nand ResNet-50 [4] as the representative of Transformer-based\nand convolution-based models, respectively. Besides the basic\nbuilding block, there are also differences in design, e.g.,\nResNet-50 has a few down-sampling layers that partition\nthe model into stages, but the number of tokens remains\nunchanged throughout DeiT-S. The impact of these details will\nbe elaborated in Section III-C."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1117
                },
                {
                    "x": 2064,
                    "y": 1117
                },
                {
                    "x": 2064,
                    "y": 1165
                },
                {
                    "x": 1293,
                    "y": 1165
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:22px'>B. Settings: The base and elite performance</p>",
            "id": 43,
            "page": 3,
            "text": "B. Settings: The base and elite performance"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1206
                },
                {
                    "x": 2353,
                    "y": 1206
                },
                {
                    "x": 2353,
                    "y": 1702
                },
                {
                    "x": 1294,
                    "y": 1702
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:20px'>Although DeiT-S reports a 80.1% accuracy which is higher<br>than 78.7% of ResNet-50, we notice that DeiT-S has changed<br>the training strategy significantly, e.g., the number of epochs is<br>enlarged by more than 3x and the data augmentation becomes<br>much stronger. Interestingly, DeiT-S seems to heavily rely on<br>the carefully-tuned training strategy, and other Transformer-<br>based models including ViT [8] and PIT [40] also reported<br>their dependency on other factors, e.g., a large-scale training<br>set. In what follows, we provide a comprehensive study on<br>this phenomenon.</p>",
            "id": 44,
            "page": 3,
            "text": "Although DeiT-S reports a 80.1% accuracy which is higher\nthan 78.7% of ResNet-50, we notice that DeiT-S has changed\nthe training strategy significantly, e.g., the number of epochs is\nenlarged by more than 3x and the data augmentation becomes\nmuch stronger. Interestingly, DeiT-S seems to heavily rely on\nthe carefully-tuned training strategy, and other Transformer-\nbased models including ViT [8] and PIT [40] also reported\ntheir dependency on other factors, e.g., a large-scale training\nset. In what follows, we provide a comprehensive study on\nthis phenomenon."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1713
                },
                {
                    "x": 2351,
                    "y": 1713
                },
                {
                    "x": 2351,
                    "y": 1959
                },
                {
                    "x": 1292,
                    "y": 1959
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:20px'>We evaluate all classification models on the ImageNet<br>dataset [45] which has 1K classes, 1.28M training images and<br>50K testing images. Each class has roughly the same number<br>of training images. This is one of the most popular datasets<br>for visual recognition.</p>",
            "id": 45,
            "page": 3,
            "text": "We evaluate all classification models on the ImageNet\ndataset [45] which has 1K classes, 1.28M training images and\n50K testing images. Each class has roughly the same number\nof training images. This is one of the most popular datasets\nfor visual recognition."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1969
                },
                {
                    "x": 2352,
                    "y": 1969
                },
                {
                    "x": 2352,
                    "y": 2862
                },
                {
                    "x": 1295,
                    "y": 2862
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:20px'>There are two settings to optimize each recognition model.<br>The first one is named the base setting which is widely<br>adopted by convolution-based networks. Specifically, the<br>model is trained for 90 epochs with the SGD optimizer.<br>The learning rate starts with 0.2 for batch size 512 and<br>gradually decays to 0.00001 following the cosine annealing<br>function. A moderate data augmentation strategy with random-<br>size cropping [11] and flipping is used. The second one is<br>named the elite setting which has been verified effective to<br>improve the Transformer-based models. The Adamw optimizer<br>with an initial learning rate of 0.0005 for batch size 512 is<br>used. The data augmentation and regularization strategy is<br>made much stronger to avoid over-fitting, for which intensive<br>operations including RandAugment [12], Mixup [46], Cut-<br>Mix [13], Random Erasing [47], Repeated Augmentation [48],<br>[49] and Stochastic Depth [50] are used. Correspondingly, the<br>training lasts 300 epochs, much longer than that of the base<br>setting.</p>",
            "id": 46,
            "page": 3,
            "text": "There are two settings to optimize each recognition model.\nThe first one is named the base setting which is widely\nadopted by convolution-based networks. Specifically, the\nmodel is trained for 90 epochs with the SGD optimizer.\nThe learning rate starts with 0.2 for batch size 512 and\ngradually decays to 0.00001 following the cosine annealing\nfunction. A moderate data augmentation strategy with random-\nsize cropping [11] and flipping is used. The second one is\nnamed the elite setting which has been verified effective to\nimprove the Transformer-based models. The Adamw optimizer\nwith an initial learning rate of 0.0005 for batch size 512 is\nused. The data augmentation and regularization strategy is\nmade much stronger to avoid over-fitting, for which intensive\noperations including RandAugment [12], Mixup [46], Cut-\nMix [13], Random Erasing [47], Repeated Augmentation [48],\n[49] and Stochastic Depth [50] are used. Correspondingly, the\ntraining lasts 300 epochs, much longer than that of the base\nsetting."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2873
                },
                {
                    "x": 2353,
                    "y": 2873
                },
                {
                    "x": 2353,
                    "y": 3120
                },
                {
                    "x": 1292,
                    "y": 3120
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:18px'>Throughout the remaining part of this paper, we refer to<br>the classification accuracy under the base and elite settings as<br>base performance and elite performance, respectively. We<br>expect the numbers to provide complementary views for us to<br>understand the studied models.</p>",
            "id": 47,
            "page": 3,
            "text": "Throughout the remaining part of this paper, we refer to\nthe classification accuracy under the base and elite settings as\nbase performance and elite performance, respectively. We\nexpect the numbers to provide complementary views for us to\nunderstand the studied models."
        },
        {
            "bounding_box": [
                {
                    "x": 2325,
                    "y": 107
                },
                {
                    "x": 2348,
                    "y": 107
                },
                {
                    "x": 2348,
                    "y": 134
                },
                {
                    "x": 2325,
                    "y": 134
                }
            ],
            "category": "header",
            "html": "<header id='48' style='font-size:14px'>4</header>",
            "id": 48,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 236
                },
                {
                    "x": 2347,
                    "y": 236
                },
                {
                    "x": 2347,
                    "y": 1143
                },
                {
                    "x": 203,
                    "y": 1143
                }
            ],
            "category": "figure",
            "html": "<figure><img id='49' style='font-size:16px' alt=\"Classifier Classifier\nDeit in transformer view Deit in convolution view\nclassifier classifier L/3x L1 X MLP Bottleneck\nFF cell FF cell\nL X L X\n320,1x1, 384\n1536,1x1, 384\nLinear Conv, 1x1 MHSA cell FF cell\n320,3x3,320\n384,1x1,1536\nLinear Conv,1x1 Conv,2x2,s2 Conv,2x2,s2\n384,1x1,320\nL/3x L2x Norm\nNorm Norm Norm\nFF cell FF cell\nMHSA cell FF cell\nMHSA MHSA\nConv,2x2,s2 Conv,2x2,s2\nNorm Norm\nL/3 X L3x\nFF cell FF cell\n··· MHSA cell FF cell\nConv,4x4,s4 Conv,4x4,s4\nLinear Conv,16x16,s16\nConv,7x7,s2 Conv,7x7,s2\n□ 3x3 Convolution\n□ Global Self-attention\" data-coord=\"top-left:(203,236); bottom-right:(2347,1143)\" /></figure>",
            "id": 49,
            "page": 4,
            "text": "Classifier Classifier\nDeit in transformer view Deit in convolution view\nclassifier classifier L/3x L1 X MLP Bottleneck\nFF cell FF cell\nL X L X\n320,1x1, 384\n1536,1x1, 384\nLinear Conv, 1x1 MHSA cell FF cell\n320,3x3,320\n384,1x1,1536\nLinear Conv,1x1 Conv,2x2,s2 Conv,2x2,s2\n384,1x1,320\nL/3x L2x Norm\nNorm Norm Norm\nFF cell FF cell\nMHSA cell FF cell\nMHSA MHSA\nConv,2x2,s2 Conv,2x2,s2\nNorm Norm\nL/3 X L3x\nFF cell FF cell\n··· MHSA cell FF cell\nConv,4x4,s4 Conv,4x4,s4\nLinear Conv,16x16,s16\nConv,7x7,s2 Conv,7x7,s2\n□ 3x3 Convolution\n□ Global Self-attention"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1209
                },
                {
                    "x": 2354,
                    "y": 1209
                },
                {
                    "x": 2354,
                    "y": 1400
                },
                {
                    "x": 196,
                    "y": 1400
                }
            ],
            "category": "caption",
            "html": "<caption id='50' style='font-size:18px'>Fig. 1. The transition process that starts with DeiT and ends with ResNet-50. To save space, we only show three important movements. The first movement<br>converts DeiT from the Transformer to convolution view (Section III-C1). The second movement replaces the patch flattening module with step-wise patch<br>embedding (elaborated in Section III-C2) and introduces the stage-wise design (Section III-C3) The third movement replaces the self-attention module with<br>convolution (Section III-C7). The upper-right area shows a relatively minor modifications, inserting 3 x 3 convolution (Section III-C5). The lower-right area<br>compares the receptive fields of a 3 x 3 convolution and self-attention. This figure is best viewed in color.</caption>",
            "id": 50,
            "page": 4,
            "text": "Fig. 1. The transition process that starts with DeiT and ends with ResNet-50. To save space, we only show three important movements. The first movement\nconverts DeiT from the Transformer to convolution view (Section III-C1). The second movement replaces the patch flattening module with step-wise patch\nembedding (elaborated in Section III-C2) and introduces the stage-wise design (Section III-C3) The third movement replaces the self-attention module with\nconvolution (Section III-C7). The upper-right area shows a relatively minor modifications, inserting 3 x 3 convolution (Section III-C5). The lower-right area\ncompares the receptive fields of a 3 x 3 convolution and self-attention. This figure is best viewed in color."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1488
                },
                {
                    "x": 962,
                    "y": 1488
                },
                {
                    "x": 962,
                    "y": 1535
                },
                {
                    "x": 199,
                    "y": 1535
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:20px'>C. The transition from DeiT-S to ResNet-50</p>",
            "id": 51,
            "page": 4,
            "text": "C. The transition from DeiT-S to ResNet-50"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1564
                },
                {
                    "x": 1257,
                    "y": 1564
                },
                {
                    "x": 1257,
                    "y": 1810
                },
                {
                    "x": 196,
                    "y": 1810
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:20px'>This subsection displays a step-by-step process in which we<br>gradually transit a model from DeiT-S to ResNet-50. There are<br>eight steps in total. The key steps are illustrated in Figure 1,<br>and the results, including the base and elite performance and<br>the model statistics, are summarized in Table II.</p>",
            "id": 52,
            "page": 4,
            "text": "This subsection displays a step-by-step process in which we\ngradually transit a model from DeiT-S to ResNet-50. There are\neight steps in total. The key steps are illustrated in Figure 1,\nand the results, including the base and elite performance and\nthe model statistics, are summarized in Table II."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1821
                },
                {
                    "x": 1257,
                    "y": 1821
                },
                {
                    "x": 1257,
                    "y": 2315
                },
                {
                    "x": 195,
                    "y": 2315
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:22px'>1) Using global average pooling to replace the classifi-<br>cation token: The first step of the transition is to remove<br>the classification token and add global average pooling to<br>the Transformer-based models. Unlike the convolution-based<br>models, Transformers usually add a classification token to the<br>inputs and utilize the corresponding output token to perform<br>classification, which is inherited from NLP tasks [6]. As<br>a contrast, the classification features in convolution-based<br>models are obtained by conducting global average pooling in<br>the space dimension.</p>",
            "id": 53,
            "page": 4,
            "text": "1) Using global average pooling to replace the classifi-\ncation token: The first step of the transition is to remove\nthe classification token and add global average pooling to\nthe Transformer-based models. Unlike the convolution-based\nmodels, Transformers usually add a classification token to the\ninputs and utilize the corresponding output token to perform\nclassification, which is inherited from NLP tasks [6]. As\na contrast, the classification features in convolution-based\nmodels are obtained by conducting global average pooling in\nthe space dimension."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2322
                },
                {
                    "x": 1259,
                    "y": 2322
                },
                {
                    "x": 1259,
                    "y": 2770
                },
                {
                    "x": 195,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:20px'>By removing the classification token, the Transformer can<br>be equivalent translated to the convolutional version as shown<br>in Figure 1. Specifically, the patch embedding operation is<br>equivalent to a convolution whose kernel size and stride is the<br>patch size [8]. The shape of the intermediate features can be<br>naturally converted from a sequence of tokens (i.e., words) to<br>a bundle feature maps and the tokens become the vector in<br>channel dimension (illustrated in Figure 1). The linear layers<br>in MHSA and MLP blocks are equivalent to 1 x1 convolutions.</p>",
            "id": 54,
            "page": 4,
            "text": "By removing the classification token, the Transformer can\nbe equivalent translated to the convolutional version as shown\nin Figure 1. Specifically, the patch embedding operation is\nequivalent to a convolution whose kernel size and stride is the\npatch size [8]. The shape of the intermediate features can be\nnaturally converted from a sequence of tokens (i.e., words) to\na bundle feature maps and the tokens become the vector in\nchannel dimension (illustrated in Figure 1). The linear layers\nin MHSA and MLP blocks are equivalent to 1 x1 convolutions."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 2773
                },
                {
                    "x": 1258,
                    "y": 2773
                },
                {
                    "x": 1258,
                    "y": 3121
                },
                {
                    "x": 196,
                    "y": 3121
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:20px'>The performance of the obtained network (Net1) is shown<br>in Table II. As can be seen, this transition can substantially im-<br>prove the base performance. Our further experiments show that<br>adding global pooling itself can improve the base performance<br>from 64.17% to 69.44%. In other words, the global average<br>pooling operation which is widely used in convolution-based<br>models since NIN [51], enables the network to learn more</p>",
            "id": 55,
            "page": 4,
            "text": "The performance of the obtained network (Net1) is shown\nin Table II. As can be seen, this transition can substantially im-\nprove the base performance. Our further experiments show that\nadding global pooling itself can improve the base performance\nfrom 64.17% to 69.44%. In other words, the global average\npooling operation which is widely used in convolution-based\nmodels since NIN [51], enables the network to learn more"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1489
                },
                {
                    "x": 2349,
                    "y": 1489
                },
                {
                    "x": 2349,
                    "y": 1585
                },
                {
                    "x": 1294,
                    "y": 1585
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:22px'>efficiently under moderate augmentation. Furthermore, this<br>transition can slightly improve the elite performance.</p>",
            "id": 56,
            "page": 4,
            "text": "efficiently under moderate augmentation. Furthermore, this\ntransition can slightly improve the elite performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1604
                },
                {
                    "x": 2353,
                    "y": 1604
                },
                {
                    "x": 2353,
                    "y": 2201
                },
                {
                    "x": 1294,
                    "y": 2201
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:22px'>2) Replacing patch flattening with step-wise patch embed-<br>ding: DeiT and ViT models directly encode the image pixels<br>with a patch embedding layer which is equivalent to a convolu-<br>tion with a large kernel size and stride (e.g., 16). This operation<br>flattens the image patches to a sequence of tokens SO that<br>Transformers can handle images. However, patch flattening<br>impairs the position information within each patch and makes<br>it more difficult to extract the patterns within patches. To solve<br>this problem, existing methods usually attach a preprocessing<br>module before patch embedding. The preprocessing module<br>can be a feature extraction convnet [8] or a specially designed<br>Transformer [52].</p>",
            "id": 57,
            "page": 4,
            "text": "2) Replacing patch flattening with step-wise patch embed-\nding: DeiT and ViT models directly encode the image pixels\nwith a patch embedding layer which is equivalent to a convolu-\ntion with a large kernel size and stride (e.g., 16). This operation\nflattens the image patches to a sequence of tokens SO that\nTransformers can handle images. However, patch flattening\nimpairs the position information within each patch and makes\nit more difficult to extract the patterns within patches. To solve\nthis problem, existing methods usually attach a preprocessing\nmodule before patch embedding. The preprocessing module\ncan be a feature extraction convnet [8] or a specially designed\nTransformer [52]."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2214
                },
                {
                    "x": 2353,
                    "y": 2214
                },
                {
                    "x": 2353,
                    "y": 3011
                },
                {
                    "x": 1294,
                    "y": 3011
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:20px'>We found that there is a rather simple solution, which is<br>factorizing the large patch embedding to step-wise small patch<br>embeddings. Specifically, We first add the stem layer in ResNet<br>to the Transformer, which is a 7 x 7 convolution layer with a<br>stride of two. The stem layer can be seen as a 2 x 2 patching<br>embedding operation with pixel overlap (i.e., 7 x 7 kernel size).<br>Since the patch size in the original DeiT model is 16, we still<br>need to embed 8x8 patches after the stem. We further factorize<br>the 8 x 8 patch embedding to a 4 x 4 embedding and a 2 x 2<br>embedding, which are 4 x 4 and 2 x 2 convolution layers with<br>stride 4 and 2 in the perspective of convolution. Additionally,<br>we add an extra 2 x 2 convolution to further upgrade the<br>patch size from 16 x 16 to 32 x 32 before classification. These<br>patch embedding layers can also be seen as the down-sampling<br>layers and we double the channel numbers after embedding<br>following the practice in convolution-based models.</p>",
            "id": 58,
            "page": 4,
            "text": "We found that there is a rather simple solution, which is\nfactorizing the large patch embedding to step-wise small patch\nembeddings. Specifically, We first add the stem layer in ResNet\nto the Transformer, which is a 7 x 7 convolution layer with a\nstride of two. The stem layer can be seen as a 2 x 2 patching\nembedding operation with pixel overlap (i.e., 7 x 7 kernel size).\nSince the patch size in the original DeiT model is 16, we still\nneed to embed 8x8 patches after the stem. We further factorize\nthe 8 x 8 patch embedding to a 4 x 4 embedding and a 2 x 2\nembedding, which are 4 x 4 and 2 x 2 convolution layers with\nstride 4 and 2 in the perspective of convolution. Additionally,\nwe add an extra 2 x 2 convolution to further upgrade the\npatch size from 16 x 16 to 32 x 32 before classification. These\npatch embedding layers can also be seen as the down-sampling\nlayers and we double the channel numbers after embedding\nfollowing the practice in convolution-based models."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 3023
                },
                {
                    "x": 2351,
                    "y": 3023
                },
                {
                    "x": 2351,
                    "y": 3121
                },
                {
                    "x": 1293,
                    "y": 3121
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:22px'>By utilizing step-wise embeddings, the position prior within<br>patches is encoded into features. As a result, the model can</p>",
            "id": 59,
            "page": 4,
            "text": "By utilizing step-wise embeddings, the position prior within\npatches is encoded into features. As a result, the model can"
        },
        {
            "bounding_box": [
                {
                    "x": 2325,
                    "y": 105
                },
                {
                    "x": 2349,
                    "y": 105
                },
                {
                    "x": 2349,
                    "y": 135
                },
                {
                    "x": 2325,
                    "y": 135
                }
            ],
            "category": "header",
            "html": "<header id='60' style='font-size:14px'>5</header>",
            "id": 60,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 1201,
                    "y": 240
                },
                {
                    "x": 1348,
                    "y": 240
                },
                {
                    "x": 1348,
                    "y": 274
                },
                {
                    "x": 1201,
                    "y": 274
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:18px'>TABLE II</p>",
            "id": 61,
            "page": 5,
            "text": "TABLE II"
        },
        {
            "bounding_box": [
                {
                    "x": 213,
                    "y": 272
                },
                {
                    "x": 2339,
                    "y": 272
                },
                {
                    "x": 2339,
                    "y": 464
                },
                {
                    "x": 213,
                    "y": 464
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:16px'>THE CLASSIFICATION ACCURACY ON IMAGENET DURING THE TRANSITION PROCEDURE FROM DEIT-S TO RESNET-50. BOTH THE BASE SETTING AND<br>THE ELITE SETTING ARE CONSIDERED (FOR THE DETAILS, SEE SECTION III-B), AND WE MARK THE POSITIVE MODIFICATIONS IN RED AND THE<br>NEGATIVE MODIFICATIONS IN BLUE. NOTE THAT A MODIFICATION CAN IMPACT THE BASE AND ELITE PERFORMANCE DIFFERENTLY. THOUGH THE<br>NUMBER OF PARAMETERS INCREASES CONSIDERABLY AT THE INTERMEDIATE STATUS, THE COMPUTATIONAL COSTS MEASURED BY FLOPs DOES NOT<br>CHANGE SIGNIFICANTLY.</p>",
            "id": 62,
            "page": 5,
            "text": "THE CLASSIFICATION ACCURACY ON IMAGENET DURING THE TRANSITION PROCEDURE FROM DEIT-S TO RESNET-50. BOTH THE BASE SETTING AND\nTHE ELITE SETTING ARE CONSIDERED (FOR THE DETAILS, SEE SECTION III-B), AND WE MARK THE POSITIVE MODIFICATIONS IN RED AND THE\nNEGATIVE MODIFICATIONS IN BLUE. NOTE THAT A MODIFICATION CAN IMPACT THE BASE AND ELITE PERFORMANCE DIFFERENTLY. THOUGH THE\nNUMBER OF PARAMETERS INCREASES CONSIDERABLY AT THE INTERMEDIATE STATUS, THE COMPUTATIONAL COSTS MEASURED BY FLOPs DOES NOT\nCHANGE SIGNIFICANTLY."
        },
        {
            "bounding_box": [
                {
                    "x": 427,
                    "y": 490
                },
                {
                    "x": 2127,
                    "y": 490
                },
                {
                    "x": 2127,
                    "y": 897
                },
                {
                    "x": 427,
                    "y": 897
                }
            ],
            "category": "table",
            "html": "<table id='63' style='font-size:18px'><tr><td>Model Name</td><td>added</td><td>removed</td><td>base perf.</td><td>elite perf.</td><td>FLOPs (G)</td><td>Params (M)</td></tr><tr><td>DeiT-S</td><td colspan=\"2\"></td><td>64.17</td><td>80.07</td><td>4.60</td><td>22.1</td></tr><tr><td>Net1</td><td>global average pooling</td><td>classification token</td><td>69.81 (+5.64)</td><td>80.16 (+0.09)</td><td>4.57</td><td>22.0</td></tr><tr><td>Net2</td><td>step-wise embeddings</td><td>large patch embedding</td><td>73.01 (+3.20)</td><td>81.35 (+1.19)</td><td>4.77</td><td>23.9</td></tr><tr><td>Net3</td><td>stage-wise design</td><td>-</td><td>75.76 (+2.75)</td><td>80.19 (-1.14)</td><td>4.79</td><td>39.5</td></tr><tr><td>Net4</td><td>batch norm</td><td>layer norm</td><td>76.49 (+0.73)</td><td>80.97 (+0.78)</td><td>4.79</td><td>39.5</td></tr><tr><td>Net5</td><td>3 x 3 convolution</td><td>-</td><td>77.37 (+0.88)</td><td>80.15 (-0.82)</td><td>4.76</td><td>39.2</td></tr><tr><td>Net6</td><td></td><td>position embedding</td><td>77.31 (-0.06)</td><td>79.86 (-0.29)</td><td>4.76</td><td>39.0</td></tr><tr><td>Net7</td><td>convolution</td><td>self-attention</td><td>76.24 (-1.07)</td><td>79.01 (-0.85)</td><td>4.83</td><td>45.0</td></tr><tr><td>ResNet-50</td><td colspan=\"2\">network shape adjustment</td><td>77.43 (+1.19)</td><td>78.73 (-0.28)</td><td>4.09</td><td>25.6</td></tr></table>",
            "id": 63,
            "page": 5,
            "text": "Model Name added removed base perf. elite perf. FLOPs (G) Params (M)\n DeiT-S  64.17 80.07 4.60 22.1\n Net1 global average pooling classification token 69.81 (+5.64) 80.16 (+0.09) 4.57 22.0\n Net2 step-wise embeddings large patch embedding 73.01 (+3.20) 81.35 (+1.19) 4.77 23.9\n Net3 stage-wise design - 75.76 (+2.75) 80.19 (-1.14) 4.79 39.5\n Net4 batch norm layer norm 76.49 (+0.73) 80.97 (+0.78) 4.79 39.5\n Net5 3 x 3 convolution - 77.37 (+0.88) 80.15 (-0.82) 4.76 39.2\n Net6  position embedding 77.31 (-0.06) 79.86 (-0.29) 4.76 39.0\n Net7 convolution self-attention 76.24 (-1.07) 79.01 (-0.85) 4.83 45.0\n ResNet-50 network shape adjustment 77.43 (+1.19) 78.73 (-0.28) 4.09"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1008
                },
                {
                    "x": 1257,
                    "y": 1008
                },
                {
                    "x": 1257,
                    "y": 1353
                },
                {
                    "x": 195,
                    "y": 1353
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:22px'>learn patterns more efficiently. As can be seen in Table II,<br>this transition can significantly improve the base performance<br>and elite performance of the network. It indicates that step-<br>wise embedding is a better choice than larger patch embedding<br>in Transformer-based models. Additionally, this transition is<br>computationally efficient and only introduces about 4% extra<br>FLOPs.</p>",
            "id": 64,
            "page": 5,
            "text": "learn patterns more efficiently. As can be seen in Table II,\nthis transition can significantly improve the base performance\nand elite performance of the network. It indicates that step-\nwise embedding is a better choice than larger patch embedding\nin Transformer-based models. Additionally, this transition is\ncomputationally efficient and only introduces about 4% extra\nFLOPs."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1364
                },
                {
                    "x": 1257,
                    "y": 1364
                },
                {
                    "x": 1257,
                    "y": 2061
                },
                {
                    "x": 194,
                    "y": 2061
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:20px'>3) Stage-wise design: In this section, we split networks into<br>stages like ResNets. The blocks in the same stage share the<br>same feature resolution. Since step-wise embeddings in the<br>last transition have split the network into different stages, the<br>transition in this section is to reassign the blocks to different<br>stages as shown in Figure 1. However, unlike convolution<br>blocks, the complexity of self-attention blocks increases by<br>O(N4) with respect to the feature size. Thus we only insert<br>blocks to the 8 x 8, 16 x 16 and 32 x 32 patch embedding<br>stages, which correspond to 28 x 28, 14 x 14 and 7 x 7 feature<br>resolutions respectively for 224 x 224 inputs. Additionally, we<br>halve the head dimension and feature dimension before self-<br>attention in 28 x 28 stage to ensure that the blocks in different<br>stages utilize similar FLOPs.</p>",
            "id": 65,
            "page": 5,
            "text": "3) Stage-wise design: In this section, we split networks into\nstages like ResNets. The blocks in the same stage share the\nsame feature resolution. Since step-wise embeddings in the\nlast transition have split the network into different stages, the\ntransition in this section is to reassign the blocks to different\nstages as shown in Figure 1. However, unlike convolution\nblocks, the complexity of self-attention blocks increases by\nO(N4) with respect to the feature size. Thus we only insert\nblocks to the 8 x 8, 16 x 16 and 32 x 32 patch embedding\nstages, which correspond to 28 x 28, 14 x 14 and 7 x 7 feature\nresolutions respectively for 224 x 224 inputs. Additionally, we\nhalve the head dimension and feature dimension before self-\nattention in 28 x 28 stage to ensure that the blocks in different\nstages utilize similar FLOPs."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 2066
                },
                {
                    "x": 1257,
                    "y": 2066
                },
                {
                    "x": 1257,
                    "y": 2563
                },
                {
                    "x": 194,
                    "y": 2563
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:20px'>This transition leads to interesting results. The base perfor-<br>mance is further improved. It is conjectured that the stage-<br>wise design leverages the image local priors and thus can<br>perform better under moderate augmentation. However, the<br>elite performance of the network decreases markedly. To study<br>reasons, we conduct ablation experiments and find that self-<br>attention does not work well in very large resolutions. We<br>conjecture that large resolution contains too many tokens and<br>it is much more difficult for self-attention to learn relations<br>among them. We will detail it in section III-D.</p>",
            "id": 66,
            "page": 5,
            "text": "This transition leads to interesting results. The base perfor-\nmance is further improved. It is conjectured that the stage-\nwise design leverages the image local priors and thus can\nperform better under moderate augmentation. However, the\nelite performance of the network decreases markedly. To study\nreasons, we conduct ablation experiments and find that self-\nattention does not work well in very large resolutions. We\nconjecture that large resolution contains too many tokens and\nit is much more difficult for self-attention to learn relations\namong them. We will detail it in section III-D."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 2573
                },
                {
                    "x": 1256,
                    "y": 2573
                },
                {
                    "x": 1256,
                    "y": 3123
                },
                {
                    "x": 194,
                    "y": 3123
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:22px'>4) Replacing LayerNorm with BatchNorm: Transformer-<br>based models usually normalize the features with Layer-<br>Norm [15], which is inherited from NLP tasks [5], [6]. As<br>a contrast, convolution-based models like ResNets usually<br>utilize BatchNorm [14] to stabilize the training process. Lay-<br>erNorm is independent of batch size and more friendly for<br>specific tasks compared with BatchNorm, while BatchNorm<br>usually can achieve better performance given appropriate<br>batch size [53]. We replace all the LayerNorm layers with<br>BatchNorm layers and the results show that BatchNorm per-<br>forms better than LayerNorm. It can improve both the base</p>",
            "id": 67,
            "page": 5,
            "text": "4) Replacing LayerNorm with BatchNorm: Transformer-\nbased models usually normalize the features with Layer-\nNorm [15], which is inherited from NLP tasks [5], [6]. As\na contrast, convolution-based models like ResNets usually\nutilize BatchNorm [14] to stabilize the training process. Lay-\nerNorm is independent of batch size and more friendly for\nspecific tasks compared with BatchNorm, while BatchNorm\nusually can achieve better performance given appropriate\nbatch size [53]. We replace all the LayerNorm layers with\nBatchNorm layers and the results show that BatchNorm per-\nforms better than LayerNorm. It can improve both the base"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1009
                },
                {
                    "x": 2172,
                    "y": 1009
                },
                {
                    "x": 2172,
                    "y": 1054
                },
                {
                    "x": 1294,
                    "y": 1054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:20px'>performance and elite performance of the network.</p>",
            "id": 68,
            "page": 5,
            "text": "performance and elite performance of the network."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1063
                },
                {
                    "x": 2351,
                    "y": 1063
                },
                {
                    "x": 2351,
                    "y": 1357
                },
                {
                    "x": 1292,
                    "y": 1357
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:20px'>In addition, we also try to add BatchNorm to Net2 to<br>further improve the elite performance. However, this Net2-BN<br>network suffers from convergence problems. This may explain<br>why BatchNorm is not widely used in the pure self-attention<br>models. But for our mixed model, BatchNorm is a reliable<br>method to advance performance.</p>",
            "id": 69,
            "page": 5,
            "text": "In addition, we also try to add BatchNorm to Net2 to\nfurther improve the elite performance. However, this Net2-BN\nnetwork suffers from convergence problems. This may explain\nwhy BatchNorm is not widely used in the pure self-attention\nmodels. But for our mixed model, BatchNorm is a reliable\nmethod to advance performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1367
                },
                {
                    "x": 2354,
                    "y": 1367
                },
                {
                    "x": 2354,
                    "y": 2262
                },
                {
                    "x": 1294,
                    "y": 2262
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:20px'>5) Introducing 3 x 3 convolutions: Since the tokens of<br>the network are present as feature maps, it is natural to<br>introduce convolutions with kernel sizes larger than 1 x 1.<br>The specific meaning of large kernel convolution is illus-<br>trated at the bottom right of Figure 1. When global self-<br>attentions attempt to build the relations among all the tokens<br>(i.e., pixels), convolutions focus on relating the tokens within<br>local neighborhoods. We chose to insert 3 x 3 convolutions<br>between the 1 x 1 convolutions in feed-forward blocks, which<br>transforms the MLP blocks into bottleneck blocks as exhibited<br>at the top right of Figure 1. Note that the channel numbers<br>of the 3 x 3 convolution layers are tuned to ensure that the<br>FLOPs of the feed-forward blocks are nearly unchanged. The<br>obtained bottleneck blocks are similar to the bottleneck blocks<br>in ResNet-50, although they have different bottleneck ratios<br>(i.e., the factor of reducing the channel numbers before the<br>3x3 convolution). We replace the MLP blocks with bottleneck<br>blocks in all three stages.</p>",
            "id": 70,
            "page": 5,
            "text": "5) Introducing 3 x 3 convolutions: Since the tokens of\nthe network are present as feature maps, it is natural to\nintroduce convolutions with kernel sizes larger than 1 x 1.\nThe specific meaning of large kernel convolution is illus-\ntrated at the bottom right of Figure 1. When global self-\nattentions attempt to build the relations among all the tokens\n(i.e., pixels), convolutions focus on relating the tokens within\nlocal neighborhoods. We chose to insert 3 x 3 convolutions\nbetween the 1 x 1 convolutions in feed-forward blocks, which\ntransforms the MLP blocks into bottleneck blocks as exhibited\nat the top right of Figure 1. Note that the channel numbers\nof the 3 x 3 convolution layers are tuned to ensure that the\nFLOPs of the feed-forward blocks are nearly unchanged. The\nobtained bottleneck blocks are similar to the bottleneck blocks\nin ResNet-50, although they have different bottleneck ratios\n(i.e., the factor of reducing the channel numbers before the\n3x3 convolution). We replace the MLP blocks with bottleneck\nblocks in all three stages."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2268
                },
                {
                    "x": 2352,
                    "y": 2268
                },
                {
                    "x": 2352,
                    "y": 2913
                },
                {
                    "x": 1294,
                    "y": 2913
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:22px'>Not surprisingly, 3 x 3 convolutions which can leverage the<br>local priors in images further improve the network base perfor-<br>mance. The base performance (77.37%) becomes comparable<br>with ResNet-50 (77.43%). However, the elite performance<br>decreases by 0.82%. We conduct more experiments to study<br>the reasons. Instead of adding 3 x 3 convolutions to all stages,<br>we insert 3 x 3 convolutions to different stages separately.<br>We observe that 3 x 3 convolutions only work well on<br>the high-resolution features. We conjecture that leveraging<br>local relations is important for the high-resolution features in<br>natural images. For the low-resolution features, however, local<br>convolutions become unimportant when equipped with global<br>self-attention. We will detail it in section III-D.</p>",
            "id": 71,
            "page": 5,
            "text": "Not surprisingly, 3 x 3 convolutions which can leverage the\nlocal priors in images further improve the network base perfor-\nmance. The base performance (77.37%) becomes comparable\nwith ResNet-50 (77.43%). However, the elite performance\ndecreases by 0.82%. We conduct more experiments to study\nthe reasons. Instead of adding 3 x 3 convolutions to all stages,\nwe insert 3 x 3 convolutions to different stages separately.\nWe observe that 3 x 3 convolutions only work well on\nthe high-resolution features. We conjecture that leveraging\nlocal relations is important for the high-resolution features in\nnatural images. For the low-resolution features, however, local\nconvolutions become unimportant when equipped with global\nself-attention. We will detail it in section III-D."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2923
                },
                {
                    "x": 2353,
                    "y": 2923
                },
                {
                    "x": 2353,
                    "y": 3122
                },
                {
                    "x": 1292,
                    "y": 3122
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:22px'>6) Removing position embedding: In Transformer-based<br>models, position embedding is proposed to encode the position<br>information inter tokens. In the transition network, we utilize<br>learnable position embedding as in [6] and add them to</p>",
            "id": 72,
            "page": 5,
            "text": "6) Removing position embedding: In Transformer-based\nmodels, position embedding is proposed to encode the position\ninformation inter tokens. In the transition network, we utilize\nlearnable position embedding as in [6] and add them to"
        },
        {
            "bounding_box": [
                {
                    "x": 2326,
                    "y": 107
                },
                {
                    "x": 2349,
                    "y": 107
                },
                {
                    "x": 2349,
                    "y": 134
                },
                {
                    "x": 2326,
                    "y": 134
                }
            ],
            "category": "header",
            "html": "<header id='73' style='font-size:14px'>6</header>",
            "id": 73,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 236
                },
                {
                    "x": 1252,
                    "y": 236
                },
                {
                    "x": 1252,
                    "y": 327
                },
                {
                    "x": 198,
                    "y": 327
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:22px'>features after patch embeddings. To approaching ResNet-50,<br>position embedding should be removed.</p>",
            "id": 74,
            "page": 6,
            "text": "features after patch embeddings. To approaching ResNet-50,\nposition embedding should be removed."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 336
                },
                {
                    "x": 1256,
                    "y": 336
                },
                {
                    "x": 1256,
                    "y": 980
                },
                {
                    "x": 195,
                    "y": 980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:20px'>The results are exhibited in Table II. The base perfor-<br>mance is almost unchanged and the elite performance declines<br>slightly (0.29%). As a comparison, We test to remove the<br>position embedding of DeiT-S and elite performance decreases<br>significantly by 3.95%. It reveals that position embedding is<br>less important in the transition model than that in the pure<br>Transformer-based models. It is because that the position<br>prior inter tokens is preserved by the feature maps and<br>convolutions with spatial kernels can encode and leverage it.<br>Consequently, the harm of removing position embedding is<br>remarkably reduced in the transition network. It also explains<br>why convolution-based models do not need position embed-<br>ding.</p>",
            "id": 75,
            "page": 6,
            "text": "The results are exhibited in Table II. The base perfor-\nmance is almost unchanged and the elite performance declines\nslightly (0.29%). As a comparison, We test to remove the\nposition embedding of DeiT-S and elite performance decreases\nsignificantly by 3.95%. It reveals that position embedding is\nless important in the transition model than that in the pure\nTransformer-based models. It is because that the position\nprior inter tokens is preserved by the feature maps and\nconvolutions with spatial kernels can encode and leverage it.\nConsequently, the harm of removing position embedding is\nremarkably reduced in the transition network. It also explains\nwhy convolution-based models do not need position embed-\nding."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 985
                },
                {
                    "x": 1257,
                    "y": 985
                },
                {
                    "x": 1257,
                    "y": 1330
                },
                {
                    "x": 196,
                    "y": 1330
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:18px'>7) Replacing self-attention with feed-forward: In this sec-<br>tion, we remove the self-attention blocks in each stage and<br>utilize a feed-forward layer instead, SO that the network<br>becomes a pure convolution-based network. To keep the<br>FLOPs unchanged, several bottleneck blocks are added to each<br>stage. After the replacement, the obtained network consists of<br>bottleneck blocks like ResNet-50.</p>",
            "id": 76,
            "page": 6,
            "text": "7) Replacing self-attention with feed-forward: In this sec-\ntion, we remove the self-attention blocks in each stage and\nutilize a feed-forward layer instead, SO that the network\nbecomes a pure convolution-based network. To keep the\nFLOPs unchanged, several bottleneck blocks are added to each\nstage. After the replacement, the obtained network consists of\nbottleneck blocks like ResNet-50."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1336
                },
                {
                    "x": 1257,
                    "y": 1336
                },
                {
                    "x": 1257,
                    "y": 1728
                },
                {
                    "x": 195,
                    "y": 1728
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:18px'>The performance of the obtained network (Net7) is shown<br>in Table II. The pure convolution-based network performs<br>much worse both in base performance and elite performance.<br>It indicates that self-attentions do drive neural networks to<br>higher elite performance and is not responsible for the poor<br>base performance in ViT or DeiT. It is possible to design a<br>self-attention network with high base performance and elite<br>performance.</p>",
            "id": 77,
            "page": 6,
            "text": "The performance of the obtained network (Net7) is shown\nin Table II. The pure convolution-based network performs\nmuch worse both in base performance and elite performance.\nIt indicates that self-attentions do drive neural networks to\nhigher elite performance and is not responsible for the poor\nbase performance in ViT or DeiT. It is possible to design a\nself-attention network with high base performance and elite\nperformance."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1736
                },
                {
                    "x": 1257,
                    "y": 1736
                },
                {
                    "x": 1257,
                    "y": 2427
                },
                {
                    "x": 195,
                    "y": 2427
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:20px'>8) Adjusting the shape of network: There are still many<br>differences between Net7 and ResNet-50. First, the shape<br>of Net7 is different from ResNet-50. Their depths, widths,<br>bottleneck ratios and block numbers in network stages are<br>different. Second, they normalize the features in different<br>positions. Net7 only normalizes input features in a block,<br>while ResNet-50 normalizes features after each convolutional<br>layer. Third, ResNet-50 down-samples the features with bot-<br>tleneck blocks but Net7 utilizes a single convolution layer<br>(i.e., patch embedding layer). In addition, Net7 employs a<br>few more FLOPs. Nevertheless, both these two networks are<br>convolution-based networks. The performance gap between<br>these two networks can be attributed to architecture design<br>strategy.</p>",
            "id": 78,
            "page": 6,
            "text": "8) Adjusting the shape of network: There are still many\ndifferences between Net7 and ResNet-50. First, the shape\nof Net7 is different from ResNet-50. Their depths, widths,\nbottleneck ratios and block numbers in network stages are\ndifferent. Second, they normalize the features in different\npositions. Net7 only normalizes input features in a block,\nwhile ResNet-50 normalizes features after each convolutional\nlayer. Third, ResNet-50 down-samples the features with bot-\ntleneck blocks but Net7 utilizes a single convolution layer\n(i.e., patch embedding layer). In addition, Net7 employs a\nfew more FLOPs. Nevertheless, both these two networks are\nconvolution-based networks. The performance gap between\nthese two networks can be attributed to architecture design\nstrategy."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 2434
                },
                {
                    "x": 1257,
                    "y": 2434
                },
                {
                    "x": 1257,
                    "y": 2780
                },
                {
                    "x": 196,
                    "y": 2780
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:18px'>As shown in Table II, the base performance is improved<br>after transition. It demonstrates that ResNet-50 has better net-<br>work architecture and can perform better with fewer FLOPs.<br>However, ResNet-50 obtains worse elite performance. It indi-<br>cates that the inconsistencies between base performance and<br>elite performance exist not only in self-attention models but<br>also in pure convolution-based networks.</p>",
            "id": 79,
            "page": 6,
            "text": "As shown in Table II, the base performance is improved\nafter transition. It demonstrates that ResNet-50 has better net-\nwork architecture and can perform better with fewer FLOPs.\nHowever, ResNet-50 obtains worse elite performance. It indi-\ncates that the inconsistencies between base performance and\nelite performance exist not only in self-attention models but\nalso in pure convolution-based networks."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2856
                },
                {
                    "x": 800,
                    "y": 2856
                },
                {
                    "x": 800,
                    "y": 2900
                },
                {
                    "x": 199,
                    "y": 2900
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:22px'>D. Summary: the Visformer model</p>",
            "id": 80,
            "page": 6,
            "text": "D. Summary: the Visformer model"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 2923
                },
                {
                    "x": 1257,
                    "y": 2923
                },
                {
                    "x": 1257,
                    "y": 3123
                },
                {
                    "x": 197,
                    "y": 3123
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:18px'>We aim to build a network with high base performance<br>and elite performance. The transition study has shown that<br>there are some inconsistencies between base performance and<br>elite performance. The first problem is the stage-wise design,</p>",
            "id": 81,
            "page": 6,
            "text": "We aim to build a network with high base performance\nand elite performance. The transition study has shown that\nthere are some inconsistencies between base performance and\nelite performance. The first problem is the stage-wise design,"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 248
                },
                {
                    "x": 2323,
                    "y": 248
                },
                {
                    "x": 2323,
                    "y": 387
                },
                {
                    "x": 1323,
                    "y": 387
                }
            ],
            "category": "caption",
            "html": "<br><caption id='82' style='font-size:16px'>TABLE III<br>IMPACT OF REPLACING THE SELF-ATTENTION BLOCKS WITH THE<br>BOTTLENECK BLOCKS IN EACH STAGE OF NET5. THESE EXPERIMENTS<br>ARE PERFORMED INDIVIDUALLY.</caption>",
            "id": 82,
            "page": 6,
            "text": "TABLE III\nIMPACT OF REPLACING THE SELF-ATTENTION BLOCKS WITH THE\nBOTTLENECK BLOCKS IN EACH STAGE OF NET5. THESE EXPERIMENTS\nARE PERFORMED INDIVIDUALLY."
        },
        {
            "bounding_box": [
                {
                    "x": 1490,
                    "y": 418
                },
                {
                    "x": 2152,
                    "y": 418
                },
                {
                    "x": 2152,
                    "y": 633
                },
                {
                    "x": 1490,
                    "y": 633
                }
            ],
            "category": "table",
            "html": "<table id='83' style='font-size:18px'><tr><td>Network</td><td>base perf.(%)</td><td>elite perf.(%)</td></tr><tr><td>Net5</td><td>77.37</td><td>80.15</td></tr><tr><td>Net5-DS1</td><td>77.29 (-0.08)</td><td>80.13 (-0.02)</td></tr><tr><td>Net5-DS2</td><td>77.34 (-0.02)</td><td>79.75 (-0.40)</td></tr><tr><td>Net5-DS3</td><td>77.05 (-0.32)</td><td>79.59 (-0.56)</td></tr></table>",
            "id": 83,
            "page": 6,
            "text": "Network base perf.(%) elite perf.(%)\n Net5 77.37 80.15\n Net5-DS1 77.29 (-0.08) 80.13 (-0.02)\n Net5-DS2 77.34 (-0.02) 79.75 (-0.40)\n Net5-DS3 77.05 (-0.32)"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 723
                },
                {
                    "x": 2350,
                    "y": 723
                },
                {
                    "x": 2350,
                    "y": 870
                },
                {
                    "x": 1293,
                    "y": 870
                }
            ],
            "category": "caption",
            "html": "<caption id='84' style='font-size:14px'>TABLE IV<br>IMPACT OF REPLACING THE MLP LAYERS WITH THE BOTTLENECK<br>BLOCKS IN EACH STAGE OF NET4. THESE EXPERIMENTS ARE PERFORMED<br>INDIVIDUALLY.</caption>",
            "id": 84,
            "page": 6,
            "text": "TABLE IV\nIMPACT OF REPLACING THE MLP LAYERS WITH THE BOTTLENECK\nBLOCKS IN EACH STAGE OF NET4. THESE EXPERIMENTS ARE PERFORMED\nINDIVIDUALLY."
        },
        {
            "bounding_box": [
                {
                    "x": 1491,
                    "y": 901
                },
                {
                    "x": 2148,
                    "y": 901
                },
                {
                    "x": 2148,
                    "y": 1156
                },
                {
                    "x": 1491,
                    "y": 1156
                }
            ],
            "category": "table",
            "html": "<table id='85' style='font-size:18px'><tr><td>Network</td><td>base perf.(%)</td><td>elite perf.(%)</td></tr><tr><td>Net4</td><td>76.49</td><td>80.97</td></tr><tr><td>Net4-S1</td><td>77.02 (+0.53)</td><td>81.10 (+0.13)</td></tr><tr><td>Net4-S2</td><td>76.55 (+0.06)</td><td>80.50 (-0.47)</td></tr><tr><td>Net4-S3</td><td>76.82 (+0.33)</td><td>80.44 (-0.53)</td></tr><tr><td>Net5</td><td>77.37 (+0.88)</td><td>80.15 (-0.82)</td></tr></table>",
            "id": 85,
            "page": 6,
            "text": "Network base perf.(%) elite perf.(%)\n Net4 76.49 80.97\n Net4-S1 77.02 (+0.53) 81.10 (+0.13)\n Net4-S2 76.55 (+0.06) 80.50 (-0.47)\n Net4-S3 76.82 (+0.33) 80.44 (-0.53)\n Net5 77.37 (+0.88)"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1275
                },
                {
                    "x": 2353,
                    "y": 1275
                },
                {
                    "x": 2353,
                    "y": 2069
                },
                {
                    "x": 1294,
                    "y": 2069
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>which increases the base performance but decreases the elite<br>performance. Stage-wise design re-arrange the blocks from<br>one stage to three stages. Thus, for elite performance, some<br>blocks in the new two stages must work less efficiently than<br>those in the original stage. We replace the self-attention blocks<br>with bottleneck blocks in each stage separately for Net5,<br>by which we can estimate the importance of self-attention<br>in different stages. The results are shown in Table III. The<br>replacement of self-attention in all three stages reduces both<br>the base performance and the elite performance. There is<br>a trend that self-attentions in lower resolutions play more<br>important roles than those in higher resolutions. Additionally,<br>replacing the self-attentions in the first stage almost has no<br>effect on the network performance. Larger resolutions contain<br>much more tokens and we conjecture that it is more difficult<br>for self-attentions to learn relations among them.</p>",
            "id": 86,
            "page": 6,
            "text": "which increases the base performance but decreases the elite\nperformance. Stage-wise design re-arrange the blocks from\none stage to three stages. Thus, for elite performance, some\nblocks in the new two stages must work less efficiently than\nthose in the original stage. We replace the self-attention blocks\nwith bottleneck blocks in each stage separately for Net5,\nby which we can estimate the importance of self-attention\nin different stages. The results are shown in Table III. The\nreplacement of self-attention in all three stages reduces both\nthe base performance and the elite performance. There is\na trend that self-attentions in lower resolutions play more\nimportant roles than those in higher resolutions. Additionally,\nreplacing the self-attentions in the first stage almost has no\neffect on the network performance. Larger resolutions contain\nmuch more tokens and we conjecture that it is more difficult\nfor self-attentions to learn relations among them."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2075
                },
                {
                    "x": 2351,
                    "y": 2075
                },
                {
                    "x": 2351,
                    "y": 2668
                },
                {
                    "x": 1294,
                    "y": 2668
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:18px'>The second problem is adding 3 x 3 convolutions to the<br>feed-forward blocks, which decreases the elite performance<br>by 0.82%. Based on Net4, we replace MLP blocks with<br>bottleneck blocks in each stage separately. As can be seen<br>in Table III-D, although all stages obtain improvements in<br>base performance, only the first stage benefits from bottleneck<br>blocks in elite performance. The 3 x 3 convolutions are not<br>necessary for the other two low-resolution stages when self-<br>attentions already have a global view in these positions. On the<br>high-resolution stage, for which self-attentions have difficulty<br>in handling all tokens, the 3 x 3 convolutions can provide<br>improvement.</p>",
            "id": 87,
            "page": 6,
            "text": "The second problem is adding 3 x 3 convolutions to the\nfeed-forward blocks, which decreases the elite performance\nby 0.82%. Based on Net4, we replace MLP blocks with\nbottleneck blocks in each stage separately. As can be seen\nin Table III-D, although all stages obtain improvements in\nbase performance, only the first stage benefits from bottleneck\nblocks in elite performance. The 3 x 3 convolutions are not\nnecessary for the other two low-resolution stages when self-\nattentions already have a global view in these positions. On the\nhigh-resolution stage, for which self-attentions have difficulty\nin handling all tokens, the 3 x 3 convolutions can provide\nimprovement."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2674
                },
                {
                    "x": 2352,
                    "y": 2674
                },
                {
                    "x": 2352,
                    "y": 3123
                },
                {
                    "x": 1292,
                    "y": 3123
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:20px'>Integrating the observation above, we propose the Vis-<br>former as vision-friendly, Transformer-based models. The<br>detailed architectures are shown in Table V. Besides the<br>positive transitions, Visformer adopts the stage-wise design for<br>higher base performance. But self-attentions are only utilized<br>in the last two stages, considered that self-attention in the high-<br>resolution stage is relatively inefficient. Visformer employs<br>bottleneck blocks in the first stage and utilizes group 3 x 3<br>convolutions in bottleneck blocks inspired by ResNeXt [54].</p>",
            "id": 88,
            "page": 6,
            "text": "Integrating the observation above, we propose the Vis-\nformer as vision-friendly, Transformer-based models. The\ndetailed architectures are shown in Table V. Besides the\npositive transitions, Visformer adopts the stage-wise design for\nhigher base performance. But self-attentions are only utilized\nin the last two stages, considered that self-attention in the high-\nresolution stage is relatively inefficient. Visformer employs\nbottleneck blocks in the first stage and utilizes group 3 x 3\nconvolutions in bottleneck blocks inspired by ResNeXt [54]."
        },
        {
            "bounding_box": [
                {
                    "x": 2326,
                    "y": 105
                },
                {
                    "x": 2350,
                    "y": 105
                },
                {
                    "x": 2350,
                    "y": 134
                },
                {
                    "x": 2326,
                    "y": 134
                }
            ],
            "category": "header",
            "html": "<header id='89' style='font-size:14px'>7</header>",
            "id": 89,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 1200,
                    "y": 240
                },
                {
                    "x": 1349,
                    "y": 240
                },
                {
                    "x": 1349,
                    "y": 273
                },
                {
                    "x": 1200,
                    "y": 273
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:16px'>TABLE V</p>",
            "id": 90,
            "page": 7,
            "text": "TABLE V"
        },
        {
            "bounding_box": [
                {
                    "x": 214,
                    "y": 275
                },
                {
                    "x": 2336,
                    "y": 275
                },
                {
                    "x": 2336,
                    "y": 352
                },
                {
                    "x": 214,
                    "y": 352
                }
            ],
            "category": "caption",
            "html": "<br><caption id='91' style='font-size:14px'>THE CONFIGURATION FOR CONSTRUCTING THE VISFORMER-TI AND VISFORMER-S MODELS, WHERE 'EMB. , STANDS FOR FEATURE EMBEDDING, AND<br>'s0'-'s3' INDICATE THE FOUR STAGES WITH DIFFERENT SPATIAL RESOLUTIONS.</caption>",
            "id": 91,
            "page": 7,
            "text": "THE CONFIGURATION FOR CONSTRUCTING THE VISFORMER-TI AND VISFORMER-S MODELS, WHERE 'EMB. , STANDS FOR FEATURE EMBEDDING, AND\n's0'-'s3' INDICATE THE FOUR STAGES WITH DIFFERENT SPATIAL RESOLUTIONS."
        },
        {
            "bounding_box": [
                {
                    "x": 527,
                    "y": 382
                },
                {
                    "x": 2026,
                    "y": 382
                },
                {
                    "x": 2026,
                    "y": 1396
                },
                {
                    "x": 527,
                    "y": 1396
                }
            ],
            "category": "table",
            "html": "<table id='92' style='font-size:16px'><tr><td></td><td>output size</td><td colspan=\"2\">Visformer-Ti</td><td colspan=\"2\">Visformer-S</td><td colspan=\"2\">VisformerV2-Ti</td><td colspan=\"2\">VisformerV2-S</td></tr><tr><td>stem</td><td>112 x 112</td><td colspan=\"2\">7 x 7, 16, stride 2</td><td colspan=\"2\">7 x 7, 32, stride 2</td><td colspan=\"2\">7 x 7, 24, stride 2</td><td colspan=\"2\">7 x7, 32, stride 2</td></tr><tr><td>emb.</td><td>56 x 56</td><td colspan=\"2\">-</td><td colspan=\"2\">-</td><td colspan=\"2\">2 x 2, 48, stride 2</td><td colspan=\"2\">2 x 2, 64, stride 2</td></tr><tr><td>s0</td><td>56 x 56</td><td colspan=\"2\">-</td><td colspan=\"2\">-</td><td>1 x 1, 96 3 x 3, 96 (group = 8) 1 x 1, 48</td><td>x1</td><td>「1 x 1, 128 3 x 3, 128 (group = 8) 1 x 1, 64</td><td>x1</td></tr><tr><td>emb.</td><td>28 x 28</td><td colspan=\"2\">4 x 4, 96, stride 4</td><td colspan=\"2\">4 x 4, 192, stride 4</td><td colspan=\"2\">2x 2, 96, stride 2</td><td colspan=\"2\">2 x 2, 128, stride 2</td></tr><tr><td>s1</td><td>28 x 28</td><td>「1 x 1, 1927 3 x 3, 192 (group = 8) 1 x 1, 96</td><td>x7</td><td>「1 x 1, 384 □ 3 x 3, 384 (group = 8) 1 x 1, 192</td><td>x7</td><td>「1 x 1, 1927 3 x 3, 192 (group = 8) 1 x 1, 96</td><td>x4</td><td>「1 x 1, 2567 3 x 3, 256 (group = 8) 1 x 1, 128</td><td>x10</td></tr><tr><td>emb.</td><td>14 x 14</td><td colspan=\"2\">2x 2, 192, stride 2</td><td colspan=\"2\">2x 2, 384, stride 2</td><td colspan=\"4\"></td></tr><tr><td>s2</td><td>14 x 14</td><td>「MHSA, 192 1 x 1, 768 1x 1, 192</td><td>x4</td><td>MHSA, 384 1 x 1, 1536 1x 1, 384</td><td>x4</td><td>MHSA, 192 1 x 1, 768 1 x 1, 192</td><td>x6</td><td>MHSA, 256 1 x 1, 1024 1 x 1, 256</td><td>x14</td></tr><tr><td>emb.</td><td>7 x7</td><td colspan=\"2\">2 x 2, 384, stride 2</td><td colspan=\"2\">2 x2, 768, stride 2</td><td colspan=\"4\"></td></tr><tr><td>s3</td><td>7x 7</td><td>MHSA, 384 1 x 1, 1536 1 x 1, 384</td><td>x4</td><td>MHSA, 768 1 x 1, 3072 1 x 1, 768</td><td>x4</td><td>「MHSA, 384 1 x 1, 1536 1 x 1, 384</td><td>x2</td><td>MHSA, 512 1 x 1, 2048 1 x 1, 512</td><td>x3</td></tr><tr><td></td><td>1 x 1</td><td colspan=\"8\">global average pool, 1000-d fc, softmax</td></tr><tr><td colspan=\"2\">FLOPs</td><td colspan=\"2\">1.3 x 109</td><td colspan=\"2\">4.9 x 109</td><td colspan=\"2\">1.3 x 109</td><td colspan=\"2\">4.3 x 109</td></tr></table>",
            "id": 92,
            "page": 7,
            "text": "output size Visformer-Ti Visformer-S VisformerV2-Ti VisformerV2-S\n stem 112 x 112 7 x 7, 16, stride 2 7 x 7, 32, stride 2 7 x 7, 24, stride 2 7 x7, 32, stride 2\n emb. 56 x 56 - - 2 x 2, 48, stride 2 2 x 2, 64, stride 2\n s0 56 x 56 - - 1 x 1, 96 3 x 3, 96 (group = 8) 1 x 1, 48 x1 「1 x 1, 128 3 x 3, 128 (group = 8) 1 x 1, 64 x1\n emb. 28 x 28 4 x 4, 96, stride 4 4 x 4, 192, stride 4 2x 2, 96, stride 2 2 x 2, 128, stride 2\n s1 28 x 28 「1 x 1, 1927 3 x 3, 192 (group = 8) 1 x 1, 96 x7 「1 x 1, 384 □ 3 x 3, 384 (group = 8) 1 x 1, 192 x7 「1 x 1, 1927 3 x 3, 192 (group = 8) 1 x 1, 96 x4 「1 x 1, 2567 3 x 3, 256 (group = 8) 1 x 1, 128 x10\n emb. 14 x 14 2x 2, 192, stride 2 2x 2, 384, stride 2 \n s2 14 x 14 「MHSA, 192 1 x 1, 768 1x 1, 192 x4 MHSA, 384 1 x 1, 1536 1x 1, 384 x4 MHSA, 192 1 x 1, 768 1 x 1, 192 x6 MHSA, 256 1 x 1, 1024 1 x 1, 256 x14\n emb. 7 x7 2 x 2, 384, stride 2 2 x2, 768, stride 2 \n s3 7x 7 MHSA, 384 1 x 1, 1536 1 x 1, 384 x4 MHSA, 768 1 x 1, 3072 1 x 1, 768 x4 「MHSA, 384 1 x 1, 1536 1 x 1, 384 x2 MHSA, 512 1 x 1, 2048 1 x 1, 512 x3\n  1 x 1 global average pool, 1000-d fc, softmax\n FLOPs 1.3 x 109 4.9 x 109 1.3 x 109"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1508
                },
                {
                    "x": 1257,
                    "y": 1508
                },
                {
                    "x": 1257,
                    "y": 1856
                },
                {
                    "x": 196,
                    "y": 1856
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:18px'>We also introduce BatchNorm to patch embedding modules<br>as in CNNs. We name Visformer-S to denote the model that<br>directly comes from DeiT-S. In addition, we can adjust the<br>complexity by changing the output dimensionality of multi-<br>head attentions. Here, we shrink the dimensionality by half<br>and derive the Visformer-Ti model, which requires around 1/4<br>computational costs of the Visformer-S model.</p>",
            "id": 93,
            "page": 7,
            "text": "We also introduce BatchNorm to patch embedding modules\nas in CNNs. We name Visformer-S to denote the model that\ndirectly comes from DeiT-S. In addition, we can adjust the\ncomplexity by changing the output dimensionality of multi-\nhead attentions. Here, we shrink the dimensionality by half\nand derive the Visformer-Ti model, which requires around 1/4\ncomputational costs of the Visformer-S model."
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1905
                },
                {
                    "x": 1193,
                    "y": 1905
                },
                {
                    "x": 1193,
                    "y": 1952
                },
                {
                    "x": 198,
                    "y": 1952
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:20px'>E. VisformerV2: optimizing the architecture configuration</p>",
            "id": 94,
            "page": 7,
            "text": "E. VisformerV2: optimizing the architecture configuration"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1967
                },
                {
                    "x": 1256,
                    "y": 1967
                },
                {
                    "x": 1256,
                    "y": 2663
                },
                {
                    "x": 195,
                    "y": 2663
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:18px'>Some architecture configurations of Visformer are not care-<br>fully tuned. For example, when splitting the network into<br>different stages, we averagely assign the 12 blocks to the<br>three stages, expect that we utilize 3 more blocks in the<br>first stage to compensate for the removal of self-attention. In<br>other words, the stage configuration ([7, 4, 4]) is not carefully<br>designed. Furthermore, depth and width, which are also not<br>polished in Visformer, have been demonstrated to be very<br>important configurations for network performance. Therefore,<br>we conduct many experiments to explore the architecture of<br>Visformer and propose VisformerV2. VisformerV2 is much<br>better than the original Visformer and the architecture is shown<br>in Table V. The detailed analysis and experiments are shown<br>in Section IV-C.</p>",
            "id": 95,
            "page": 7,
            "text": "Some architecture configurations of Visformer are not care-\nfully tuned. For example, when splitting the network into\ndifferent stages, we averagely assign the 12 blocks to the\nthree stages, expect that we utilize 3 more blocks in the\nfirst stage to compensate for the removal of self-attention. In\nother words, the stage configuration ([7, 4, 4]) is not carefully\ndesigned. Furthermore, depth and width, which are also not\npolished in Visformer, have been demonstrated to be very\nimportant configurations for network performance. Therefore,\nwe conduct many experiments to explore the architecture of\nVisformer and propose VisformerV2. VisformerV2 is much\nbetter than the original Visformer and the architecture is shown\nin Table V. The detailed analysis and experiments are shown\nin Section IV-C."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2711
                },
                {
                    "x": 808,
                    "y": 2711
                },
                {
                    "x": 808,
                    "y": 2758
                },
                {
                    "x": 199,
                    "y": 2758
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:20px'>F. Transformer with Half-precision</p>",
            "id": 96,
            "page": 7,
            "text": "F. Transformer with Half-precision"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 2774
                },
                {
                    "x": 1257,
                    "y": 2774
                },
                {
                    "x": 1257,
                    "y": 3121
                },
                {
                    "x": 197,
                    "y": 3121
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:18px'>Recently, quantization has been widely used to accelerate<br>the training process and save GPU memory. Specifically, half-<br>precision floating-point (FP16), the lowest precision that can<br>preserve the network performance, has been adopted by many<br>researchers. However, some works [10], [55], [56] have shown<br>that half-precision can lead to overflows in Transformers and<br>we also observe this problem in Visformer.</p>",
            "id": 97,
            "page": 7,
            "text": "Recently, quantization has been widely used to accelerate\nthe training process and save GPU memory. Specifically, half-\nprecision floating-point (FP16), the lowest precision that can\npreserve the network performance, has been adopted by many\nresearchers. However, some works [10], [55], [56] have shown\nthat half-precision can lead to overflows in Transformers and\nwe also observe this problem in Visformer."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1509
                },
                {
                    "x": 2353,
                    "y": 1509
                },
                {
                    "x": 2353,
                    "y": 1702
                },
                {
                    "x": 1292,
                    "y": 1702
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='98' style='font-size:16px'>Based on our experimental analysis, we find that attention<br>score generation can cause overflow. With the queries (Q) and<br>keys (K), the standard self-attention scores can be computed<br>as:</p>",
            "id": 98,
            "page": 7,
            "text": "Based on our experimental analysis, we find that attention\nscore generation can cause overflow. With the queries (Q) and\nkeys (K), the standard self-attention scores can be computed\nas:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1881
                },
                {
                    "x": 2352,
                    "y": 1881
                },
                {
                    "x": 2352,
                    "y": 2129
                },
                {
                    "x": 1291,
                    "y": 2129
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:18px'>However, Q and K can be very large matrices and the elements<br>in QKT will be the dot-product of two very long vectors. As<br>a result, the scores can overflow easily while utilizing 16-bit<br>precision. To solve this problem, we first try to pre-normalize<br>Q and K:</p>",
            "id": 99,
            "page": 7,
            "text": "However, Q and K can be very large matrices and the elements\nin QKT will be the dot-product of two very long vectors. As\na result, the scores can overflow easily while utilizing 16-bit\nprecision. To solve this problem, we first try to pre-normalize\nQ and K:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2313
                },
                {
                    "x": 2353,
                    "y": 2313
                },
                {
                    "x": 2353,
                    "y": 2565
                },
                {
                    "x": 1291,
                    "y": 2565
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:16px'>Where d is the length of the vector. Nevertheless, the attention<br>scores still overflow sometimes. This is because the scores are<br>only normalized with Vd overall. As the dot product of two<br>vectors with length d, the scores are still under the risk of<br>overflow. Consequently, we try to normalize the score with d:</p>",
            "id": 100,
            "page": 7,
            "text": "Where d is the length of the vector. Nevertheless, the attention\nscores still overflow sometimes. This is because the scores are\nonly normalized with Vd overall. As the dot product of two\nvectors with length d, the scores are still under the risk of\noverflow. Consequently, we try to normalize the score with d:"
        },
        {
            "bounding_box": [
                {
                    "x": 1319,
                    "y": 2815
                },
                {
                    "x": 2332,
                    "y": 2815
                },
                {
                    "x": 2332,
                    "y": 2971
                },
                {
                    "x": 1319,
                    "y": 2971
                }
            ],
            "category": "caption",
            "html": "<caption id='101' style='font-size:14px'>TABLE VI<br>COMPARISON OF INFERENCE TIME FOR VISFORMERV2-S WITH<br>DIFFERENT SCORE GENERATING METHODS. THE TESTED GPU IS V100<br>AND THE BATCH SIZE IS 32.</caption>",
            "id": 101,
            "page": 7,
            "text": "TABLE VI\nCOMPARISON OF INFERENCE TIME FOR VISFORMERV2-S WITH\nDIFFERENT SCORE GENERATING METHODS. THE TESTED GPU IS V100\nAND THE BATCH SIZE IS 32."
        },
        {
            "bounding_box": [
                {
                    "x": 1453,
                    "y": 2996
                },
                {
                    "x": 2196,
                    "y": 2996
                },
                {
                    "x": 2196,
                    "y": 3083
                },
                {
                    "x": 1453,
                    "y": 3083
                }
            ],
            "category": "table",
            "html": "<table id='102' style='font-size:16px'><tr><td>Method</td><td>original</td><td>PB-Relax</td><td>ours</td></tr><tr><td>Batch Time (ms)</td><td>42.8</td><td>46.6</td><td>43.0</td></tr></table>",
            "id": 102,
            "page": 7,
            "text": "Method original PB-Relax ours\n Batch Time (ms) 42.8 46.6"
        },
        {
            "bounding_box": [
                {
                    "x": 2326,
                    "y": 106
                },
                {
                    "x": 2349,
                    "y": 106
                },
                {
                    "x": 2349,
                    "y": 133
                },
                {
                    "x": 2326,
                    "y": 133
                }
            ],
            "category": "header",
            "html": "<header id='103' style='font-size:14px'>8</header>",
            "id": 103,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 642,
                    "y": 240
                },
                {
                    "x": 812,
                    "y": 240
                },
                {
                    "x": 812,
                    "y": 273
                },
                {
                    "x": 642,
                    "y": 273
                }
            ],
            "category": "caption",
            "html": "<caption id='104' style='font-size:16px'>TABLE VII</caption>",
            "id": 104,
            "page": 8,
            "text": "TABLE VII"
        },
        {
            "bounding_box": [
                {
                    "x": 215,
                    "y": 280
                },
                {
                    "x": 1240,
                    "y": 280
                },
                {
                    "x": 1240,
                    "y": 349
                },
                {
                    "x": 215,
                    "y": 349
                }
            ],
            "category": "caption",
            "html": "<br><caption id='105' style='font-size:14px'>THE COMPARISON OF BASE AND ELITE PERFORMANCE AS WELL AS THE<br>FLOPs BETWEEN VISFORMER AND DEIT, THE DIRECT BASELINE.</caption>",
            "id": 105,
            "page": 8,
            "text": "THE COMPARISON OF BASE AND ELITE PERFORMANCE AS WELL AS THE\nFLOPs BETWEEN VISFORMER AND DEIT, THE DIRECT BASELINE."
        },
        {
            "bounding_box": [
                {
                    "x": 355,
                    "y": 383
                },
                {
                    "x": 1102,
                    "y": 383
                },
                {
                    "x": 1102,
                    "y": 636
                },
                {
                    "x": 355,
                    "y": 636
                }
            ],
            "category": "table",
            "html": "<table id='106' style='font-size:16px'><tr><td>Network</td><td>base perf. (%)</td><td>elite perf. (%)</td><td>FLOPs (G)</td></tr><tr><td>Visformer-Ti</td><td>74.34</td><td>78.62</td><td>1.3</td></tr><tr><td>DeiT-Ti</td><td>63.87</td><td>72.21</td><td>1.3</td></tr><tr><td>Visformer-S</td><td>77.20</td><td>82.19</td><td>4.9</td></tr><tr><td>DeiT-S</td><td>63.12</td><td>80.07</td><td>4.6</td></tr></table>",
            "id": 106,
            "page": 8,
            "text": "Network base perf. (%) elite perf. (%) FLOPs (G)\n Visformer-Ti 74.34 78.62 1.3\n DeiT-Ti 63.87 72.21 1.3\n Visformer-S 77.20 82.19 4.9\n DeiT-S 63.12 80.07"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 755
                },
                {
                    "x": 1256,
                    "y": 755
                },
                {
                    "x": 1256,
                    "y": 897
                },
                {
                    "x": 196,
                    "y": 897
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:18px'>In our experiments, we observed that it can effectively avoid<br>overflow during computing scores and will not degrade the<br>network performance.</p>",
            "id": 107,
            "page": 8,
            "text": "In our experiments, we observed that it can effectively avoid\noverflow during computing scores and will not degrade the\nnetwork performance."
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 907
                },
                {
                    "x": 1257,
                    "y": 907
                },
                {
                    "x": 1257,
                    "y": 1202
                },
                {
                    "x": 197,
                    "y": 1202
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:18px'>Note that CogView [55] also proposes PB-Relax to elim-<br>inate overflow in attention scores. PB-Relax pre-minuses the<br>maximum of the attention scores. However, this method needs<br>to tune a hyper-parameter and usually considerably increases<br>the network runtime, as shown in Table VI. As a contrast, our<br>method nearly does not introduce extra runtime.</p>",
            "id": 108,
            "page": 8,
            "text": "Note that CogView [55] also proposes PB-Relax to elim-\ninate overflow in attention scores. PB-Relax pre-minuses the\nmaximum of the attention scores. However, this method needs\nto tune a hyper-parameter and usually considerably increases\nthe network runtime, as shown in Table VI. As a contrast, our\nmethod nearly does not introduce extra runtime."
        },
        {
            "bounding_box": [
                {
                    "x": 354,
                    "y": 1279
                },
                {
                    "x": 1097,
                    "y": 1279
                },
                {
                    "x": 1097,
                    "y": 1322
                },
                {
                    "x": 354,
                    "y": 1322
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>IV. MORE EXPERIMENTS ON VISFORMER</p>",
            "id": 109,
            "page": 8,
            "text": "IV. MORE EXPERIMENTS ON VISFORMER"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1353
                },
                {
                    "x": 1229,
                    "y": 1353
                },
                {
                    "x": 1229,
                    "y": 1398
                },
                {
                    "x": 198,
                    "y": 1398
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>A. The improvements on the upper-bound and lower-bound</p>",
            "id": 110,
            "page": 8,
            "text": "A. The improvements on the upper-bound and lower-bound"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1422
                },
                {
                    "x": 1256,
                    "y": 1422
                },
                {
                    "x": 1256,
                    "y": 1921
                },
                {
                    "x": 195,
                    "y": 1921
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:20px'>We first compare Visformer against DeiT, the direct base-<br>line. Results are summarized in Table VII. Using comparable<br>computational costs, the Visformer models outperform the cor-<br>responding DeiT models significantly. Specifically, the advan-<br>tages of Visformer-S and Visformer-Ti over DeiT-S and DeiT-<br>Ti under the elite setting are 2.12% and 6.41%, while under<br>the base setting, the numbers grow to 14.08% and 10.47%,<br>respectively. In other words, the advantage becomes more<br>significant under the base setting, which is more frequently<br>used for visual recognition.</p>",
            "id": 111,
            "page": 8,
            "text": "We first compare Visformer against DeiT, the direct base-\nline. Results are summarized in Table VII. Using comparable\ncomputational costs, the Visformer models outperform the cor-\nresponding DeiT models significantly. Specifically, the advan-\ntages of Visformer-S and Visformer-Ti over DeiT-S and DeiT-\nTi under the elite setting are 2.12% and 6.41%, while under\nthe base setting, the numbers grow to 14.08% and 10.47%,\nrespectively. In other words, the advantage becomes more\nsignificant under the base setting, which is more frequently\nused for visual recognition."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2003
                },
                {
                    "x": 711,
                    "y": 2003
                },
                {
                    "x": 711,
                    "y": 2049
                },
                {
                    "x": 199,
                    "y": 2049
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:20px'>B. Training with limited data</p>",
            "id": 112,
            "page": 8,
            "text": "B. Training with limited data"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2072
                },
                {
                    "x": 1256,
                    "y": 2072
                },
                {
                    "x": 1256,
                    "y": 2267
                },
                {
                    "x": 198,
                    "y": 2267
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='113' style='font-size:20px'>We evaluate the performance of Visformer in the scenario<br>with limited training data, which we consider is an important<br>ability of being vision-friendly, while prior Transformer-based<br>models mostly required abundant training data [8].</p>",
            "id": 113,
            "page": 8,
            "text": "We evaluate the performance of Visformer in the scenario\nwith limited training data, which we consider is an important\nability of being vision-friendly, while prior Transformer-based\nmodels mostly required abundant training data [8]."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2275
                },
                {
                    "x": 1255,
                    "y": 2275
                },
                {
                    "x": 1255,
                    "y": 2817
                },
                {
                    "x": 195,
                    "y": 2817
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:20px'>Four subsets of ImageNet are used, with 10% and 1%<br>randomly chosen classes (all data), and with 10% and 1%<br>randomly chosen images (all classes), respectively. To chal-<br>lenge the models, we still use the elite setting with 300 epochs<br>(not extended). As shown in Table VIII, it is observed that<br>the DeiT-S model reports dramatic accuracy drops in all the<br>four tests (note that the accuracy of using only 10% and<br>1% classes should be much higher if epochs are extended).<br>In comparison, Visformer remains robust in these scenarios,<br>showing its potential of being used for visual recognition with<br>limited data.</p>",
            "id": 114,
            "page": 8,
            "text": "Four subsets of ImageNet are used, with 10% and 1%\nrandomly chosen classes (all data), and with 10% and 1%\nrandomly chosen images (all classes), respectively. To chal-\nlenge the models, we still use the elite setting with 300 epochs\n(not extended). As shown in Table VIII, it is observed that\nthe DeiT-S model reports dramatic accuracy drops in all the\nfour tests (note that the accuracy of using only 10% and\n1% classes should be much higher if epochs are extended).\nIn comparison, Visformer remains robust in these scenarios,\nshowing its potential of being used for visual recognition with\nlimited data."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 2824
                },
                {
                    "x": 1257,
                    "y": 2824
                },
                {
                    "x": 1257,
                    "y": 3120
                },
                {
                    "x": 196,
                    "y": 3120
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:20px'>In tiny level, ResNet-50-55% is obtained by reducing the<br>channel numbers (like other tiny models) to 55% (so that the<br>FLOPs, 1.3G, is similar to Visformer-Ti and Deit-Ti). The<br>conclusion is similar: Visformer-Ti is still the best overall<br>model, and the advantage is slightly enlarged because the risk<br>of over-fitting has been reduced.</p>",
            "id": 115,
            "page": 8,
            "text": "In tiny level, ResNet-50-55% is obtained by reducing the\nchannel numbers (like other tiny models) to 55% (so that the\nFLOPs, 1.3G, is similar to Visformer-Ti and Deit-Ti). The\nconclusion is similar: Visformer-Ti is still the best overall\nmodel, and the advantage is slightly enlarged because the risk\nof over-fitting has been reduced."
        },
        {
            "bounding_box": [
                {
                    "x": 1317,
                    "y": 246
                },
                {
                    "x": 2328,
                    "y": 246
                },
                {
                    "x": 2328,
                    "y": 388
                },
                {
                    "x": 1317,
                    "y": 388
                }
            ],
            "category": "caption",
            "html": "<br><caption id='116' style='font-size:14px'>TABLE VIII<br>COMPARISON AMONG VISFORMER, DEIT, AND RESNET, IN TERMS OF<br>CLASSIFICATION ACCURACY (%) USING LIMITED TRAINING DATA. THE<br>ELITE SETTING WITH 300 EPOCHS IS USED FOR ALL MODELS.</caption>",
            "id": 116,
            "page": 8,
            "text": "TABLE VIII\nCOMPARISON AMONG VISFORMER, DEIT, AND RESNET, IN TERMS OF\nCLASSIFICATION ACCURACY (%) USING LIMITED TRAINING DATA. THE\nELITE SETTING WITH 300 EPOCHS IS USED FOR ALL MODELS."
        },
        {
            "bounding_box": [
                {
                    "x": 1385,
                    "y": 418
                },
                {
                    "x": 2257,
                    "y": 418
                },
                {
                    "x": 2257,
                    "y": 743
                },
                {
                    "x": 1385,
                    "y": 743
                }
            ],
            "category": "table",
            "html": "<table id='117' style='font-size:16px'><tr><td>Network</td><td>100% classes</td><td>10% classes</td><td>1% classes</td><td>10% images</td><td>1% images</td></tr><tr><td>DeiT-S</td><td>80.07</td><td>80.06</td><td>73.40</td><td>40.41</td><td>6.94</td></tr><tr><td>ResNet-50</td><td>78.73</td><td>89.90</td><td>93.20</td><td>58.37</td><td>13.59</td></tr><tr><td>Visformer-S</td><td>82.19</td><td>90.06</td><td>91.60</td><td>58.74</td><td>16.56</td></tr><tr><td>Deit-Ti</td><td>72.33</td><td>78.72</td><td>74.40</td><td>38.44</td><td>6.53</td></tr><tr><td>ResNet-50-55%</td><td>72.84</td><td>87.10</td><td>91.40</td><td>51.48</td><td>10.68</td></tr><tr><td>Visformer-Ti</td><td>78.62</td><td>89.48</td><td>90.60</td><td>55.14</td><td>11.79</td></tr></table>",
            "id": 117,
            "page": 8,
            "text": "Network 100% classes 10% classes 1% classes 10% images 1% images\n DeiT-S 80.07 80.06 73.40 40.41 6.94\n ResNet-50 78.73 89.90 93.20 58.37 13.59\n Visformer-S 82.19 90.06 91.60 58.74 16.56\n Deit-Ti 72.33 78.72 74.40 38.44 6.53\n ResNet-50-55% 72.84 87.10 91.40 51.48 10.68\n Visformer-Ti 78.62 89.48 90.60 55.14"
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 851
                },
                {
                    "x": 1757,
                    "y": 851
                },
                {
                    "x": 1757,
                    "y": 896
                },
                {
                    "x": 1295,
                    "y": 896
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:22px'>C. Designing VisformerV2</p>",
            "id": 118,
            "page": 8,
            "text": "C. Designing VisformerV2"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 914
                },
                {
                    "x": 2353,
                    "y": 914
                },
                {
                    "x": 2353,
                    "y": 2105
                },
                {
                    "x": 1294,
                    "y": 2105
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:20px'>We design VisformerV2 by polishing the original Visformer.<br>First, we apply relative position bias [42] to Visformer, which<br>improves the results to 82.39% as shown in Table IV-C. Then<br>we test whether we need to utilize an extra early stage. As<br>shown in Table IX, assigning a block to the new stage does not<br>improve the performance and furthermore, the performance<br>decreases when more blocks are assigned to it. However, we<br>find that this stage can improve the detection and segmentation<br>results, which will be detailed in Section IV-F. Therefore,<br>we decide to utilize one block in the new stage. Next, we<br>test to utilize deep and narrow architecture [43]. We first<br>narrow down the network and directly assign blocks to the<br>self-attention stages (the last two stages). The tested stage<br>configurations are {1, 3, 11, 11} and {1, 6, 10, 10}. These<br>settings degrade the performance. Then we try to assign more<br>blocks to the third stage ( i.e., {1, 3, 18, 3} and {1, 6, 16,<br>3}), which is the default setting for many convolution [4] and<br>Transformer networks [42], [43]. It improves the networks<br>significantly. We also find that the second pure convolution<br>stage is very important. Moving the blocks from this stage<br>to the other stages will substantially degrade the network.<br>Therefore we assign more blocks to this stage and obtain<br>VisformerV2-S. With a similar study, we design VisformerV2-<br>Ti. The detailed architecture is shown in Table V.</p>",
            "id": 119,
            "page": 8,
            "text": "We design VisformerV2 by polishing the original Visformer.\nFirst, we apply relative position bias [42] to Visformer, which\nimproves the results to 82.39% as shown in Table IV-C. Then\nwe test whether we need to utilize an extra early stage. As\nshown in Table IX, assigning a block to the new stage does not\nimprove the performance and furthermore, the performance\ndecreases when more blocks are assigned to it. However, we\nfind that this stage can improve the detection and segmentation\nresults, which will be detailed in Section IV-F. Therefore,\nwe decide to utilize one block in the new stage. Next, we\ntest to utilize deep and narrow architecture [43]. We first\nnarrow down the network and directly assign blocks to the\nself-attention stages (the last two stages). The tested stage\nconfigurations are {1, 3, 11, 11} and {1, 6, 10, 10}. These\nsettings degrade the performance. Then we try to assign more\nblocks to the third stage ( i.e., {1, 3, 18, 3} and {1, 6, 16,\n3}), which is the default setting for many convolution [4] and\nTransformer networks [42], [43]. It improves the networks\nsignificantly. We also find that the second pure convolution\nstage is very important. Moving the blocks from this stage\nto the other stages will substantially degrade the network.\nTherefore we assign more blocks to this stage and obtain\nVisformerV2-S. With a similar study, we design VisformerV2-\nTi. The detailed architecture is shown in Table V."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2109
                },
                {
                    "x": 2352,
                    "y": 2109
                },
                {
                    "x": 2352,
                    "y": 2553
                },
                {
                    "x": 1292,
                    "y": 2553
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:18px'>Note that the deep and narrow architecture significantly<br>increases the runtime on GPU. This is because that the wide<br>and shallow architecture has a better parallelization property.<br>To compensate for the loss in runtime, we utilize fewer<br>FLOPs for 'deep-narrow' networks. More importantly, we<br>find that when the input resolution is enlarged (detection and<br>segmentation tasks in Section IV-F) or the model is scaled up,<br>the parallelization property will be improved and the runtime<br>on GPU becomes more consistent with FLOPs.</p>",
            "id": 120,
            "page": 8,
            "text": "Note that the deep and narrow architecture significantly\nincreases the runtime on GPU. This is because that the wide\nand shallow architecture has a better parallelization property.\nTo compensate for the loss in runtime, we utilize fewer\nFLOPs for 'deep-narrow' networks. More importantly, we\nfind that when the input resolution is enlarged (detection and\nsegmentation tasks in Section IV-F) or the model is scaled up,\nthe parallelization property will be improved and the runtime\non GPU becomes more consistent with FLOPs."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2613
                },
                {
                    "x": 1963,
                    "y": 2613
                },
                {
                    "x": 1963,
                    "y": 2658
                },
                {
                    "x": 1294,
                    "y": 2658
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:20px'>D. Comparison to the state-of-the-arts</p>",
            "id": 121,
            "page": 8,
            "text": "D. Comparison to the state-of-the-arts"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2675
                },
                {
                    "x": 2353,
                    "y": 2675
                },
                {
                    "x": 2353,
                    "y": 3118
                },
                {
                    "x": 1292,
                    "y": 3118
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:18px'>We then compare Visformer and VisformerV2 to other<br>Transformer-based approaches in Table X. At the tiny level,<br>Visformer-Ti and VisformerV2-Ti outperform other vision<br>Transformers that with similar FLOPs. For larger models,<br>Visformer-S performs much better than most of the models<br>with similar FLOPs. VisformerV2-S further improves the per-<br>formance and outperforms other vision Transformer models.<br>Note that VisformerV2-S utilize fewer FLOPs and parameters<br>than Visformer-S.</p>",
            "id": 122,
            "page": 8,
            "text": "We then compare Visformer and VisformerV2 to other\nTransformer-based approaches in Table X. At the tiny level,\nVisformer-Ti and VisformerV2-Ti outperform other vision\nTransformers that with similar FLOPs. For larger models,\nVisformer-S performs much better than most of the models\nwith similar FLOPs. VisformerV2-S further improves the per-\nformance and outperforms other vision Transformer models.\nNote that VisformerV2-S utilize fewer FLOPs and parameters\nthan Visformer-S."
        },
        {
            "bounding_box": [
                {
                    "x": 2325,
                    "y": 107
                },
                {
                    "x": 2350,
                    "y": 107
                },
                {
                    "x": 2350,
                    "y": 134
                },
                {
                    "x": 2325,
                    "y": 134
                }
            ],
            "category": "header",
            "html": "<header id='123' style='font-size:14px'>9</header>",
            "id": 123,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 238
                },
                {
                    "x": 2327,
                    "y": 238
                },
                {
                    "x": 2327,
                    "y": 353
                },
                {
                    "x": 221,
                    "y": 353
                }
            ],
            "category": "caption",
            "html": "<caption id='124' style='font-size:16px'>TABLE IX<br>THE ELITE PERFORMANCE AND INFERENCE TIME OF DIFFERENT VISFORMER MODELS. THE BATCH TIME IS TESTED ON A V100 GPU WITH A BATCH<br>SIZE OF 32.</caption>",
            "id": 124,
            "page": 9,
            "text": "TABLE IX\nTHE ELITE PERFORMANCE AND INFERENCE TIME OF DIFFERENT VISFORMER MODELS. THE BATCH TIME IS TESTED ON A V100 GPU WITH A BATCH\nSIZE OF 32."
        },
        {
            "bounding_box": [
                {
                    "x": 382,
                    "y": 379
                },
                {
                    "x": 2168,
                    "y": 379
                },
                {
                    "x": 2168,
                    "y": 750
                },
                {
                    "x": 382,
                    "y": 750
                }
            ],
            "category": "table",
            "html": "<table id='125' style='font-size:18px'><tr><td>Network</td><td>block numbers</td><td>channel numbers</td><td>elite perf.(%)</td><td>FLOPs (G)</td><td>Params (M)</td><td>Batch Time (ms)</td></tr><tr><td>Visformer-S</td><td>{0, 7, 4, 4}</td><td>{96, 192, 384, 768}</td><td>82.39</td><td>4.9</td><td>40.2</td><td>36.9</td></tr><tr><td rowspan=\"6\"></td><td>{1,6, 4, 4}</td><td rowspan=\"2\">{96, 192, 384, 768}</td><td>82.37</td><td>4.9</td><td>40.3</td><td>37.3</td></tr><tr><td>{3,4,4,4}</td><td>81.70</td><td>4.9</td><td>39.3</td><td>38.4</td></tr><tr><td>{1,3, 11, 11}</td><td rowspan=\"4\">{64, 128, 256, 512}</td><td>81.73</td><td>4.2</td><td>45.4</td><td>42.1</td></tr><tr><td>{1,6, 10, 10}</td><td>82.20</td><td>4.2</td><td>41.9</td><td>41.7</td></tr><tr><td>{1,3, 18, 3}</td><td>82.51</td><td>4.2</td><td>25.8</td><td>43.4</td></tr><tr><td>{1,6, 16, 3}</td><td>82.89</td><td>4.2</td><td>24.6</td><td>42.8</td></tr><tr><td>VisformerV2-S</td><td>{1, 10, 14, 3}</td><td>{64, 128, 256, 512}</td><td>82.97</td><td>4.3</td><td>23.6</td><td>43.0</td></tr></table>",
            "id": 125,
            "page": 9,
            "text": "Network block numbers channel numbers elite perf.(%) FLOPs (G) Params (M) Batch Time (ms)\n Visformer-S {0, 7, 4, 4} {96, 192, 384, 768} 82.39 4.9 40.2 36.9\n  {1,6, 4, 4} {96, 192, 384, 768} 82.37 4.9 40.3 37.3\n {3,4,4,4} 81.70 4.9 39.3 38.4\n {1,3, 11, 11} {64, 128, 256, 512} 81.73 4.2 45.4 42.1\n {1,6, 10, 10} 82.20 4.2 41.9 41.7\n {1,3, 18, 3} 82.51 4.2 25.8 43.4\n {1,6, 16, 3} 82.89 4.2 24.6 42.8\n VisformerV2-S {1, 10, 14, 3} {64, 128, 256, 512} 82.97 4.3 23.6"
        },
        {
            "bounding_box": [
                {
                    "x": 652,
                    "y": 865
                },
                {
                    "x": 800,
                    "y": 865
                },
                {
                    "x": 800,
                    "y": 900
                },
                {
                    "x": 652,
                    "y": 900
                }
            ],
            "category": "caption",
            "html": "<caption id='126' style='font-size:16px'>TABLE X</caption>",
            "id": 126,
            "page": 9,
            "text": "TABLE X"
        },
        {
            "bounding_box": [
                {
                    "x": 209,
                    "y": 906
                },
                {
                    "x": 1252,
                    "y": 906
                },
                {
                    "x": 1252,
                    "y": 1014
                },
                {
                    "x": 209,
                    "y": 1014
                }
            ],
            "category": "caption",
            "html": "<br><caption id='127' style='font-size:14px'>COMPARISON AMONG OUR METHOD AND OTHER TRANSFORMER-BASED<br>VISION MODELS. '*' INDICATES THAT WE RE-RUN THE MODEL USING THE<br>ELITE SETTING. 'KD' STANDS FOR KNOWLEDGE DISTILLATION [57].</caption>",
            "id": 127,
            "page": 9,
            "text": "COMPARISON AMONG OUR METHOD AND OTHER TRANSFORMER-BASED\nVISION MODELS. '*' INDICATES THAT WE RE-RUN THE MODEL USING THE\nELITE SETTING. 'KD' STANDS FOR KNOWLEDGE DISTILLATION [57]."
        },
        {
            "bounding_box": [
                {
                    "x": 304,
                    "y": 1044
                },
                {
                    "x": 1160,
                    "y": 1044
                },
                {
                    "x": 1160,
                    "y": 2374
                },
                {
                    "x": 304,
                    "y": 2374
                }
            ],
            "category": "table",
            "html": "<table id='128' style='font-size:18px'><tr><td>Methods</td><td>Top-1(%)</td><td>FLOPs (G)</td><td>Params (M)</td></tr><tr><td>ResNet-18 [4]</td><td>69.8</td><td>1.8</td><td>11.7</td></tr><tr><td>DeiT-Ti [10]</td><td>72.2</td><td>1.3</td><td>5.7</td></tr><tr><td>DeiT-Ti (KD) [10]</td><td>74.6</td><td>1.3</td><td>5.7</td></tr><tr><td>AutoFormer-Ti [35]</td><td>74.7</td><td>1.3</td><td>5.7</td></tr><tr><td>PVT-Ti [22]</td><td>75.1</td><td>1.9</td><td>13.2</td></tr><tr><td>PVTv2-B1 [58]</td><td>78.7</td><td>2.1</td><td>13.1</td></tr><tr><td>Visformer-Ti (ours)</td><td>78.6</td><td>1.3</td><td>10.3</td></tr><tr><td>VisformerV2-Ti (ours)</td><td>79.6</td><td>1.3</td><td>9.4</td></tr><tr><td>ResNet-50 [4]</td><td>76.2</td><td>4.1</td><td>25.6</td></tr><tr><td>ResNet-50* [4]</td><td>78.7</td><td>4.1</td><td>25.6</td></tr><tr><td>RegNetY-4GF [59]</td><td>79.4</td><td>4.0</td><td>20.6</td></tr><tr><td>RegNetY-8GF [59]</td><td>79.9</td><td>8.0</td><td>39.2</td></tr><tr><td>RegNetY-4GF* [59]</td><td>80.0</td><td>4.0</td><td>20.6</td></tr><tr><td>DeiT-S [10]</td><td>79.8</td><td>4.6</td><td>21.8</td></tr><tr><td>DeiT-S* [10]</td><td>80.1</td><td>4.6</td><td>21.8</td></tr><tr><td>DeiT-B [10]</td><td>81.8</td><td>17.4</td><td>86.3</td></tr><tr><td>PVT-S [22]</td><td>79.8</td><td>3.8</td><td>24.5</td></tr><tr><td>PVT-Medium [22]</td><td>81.2</td><td>6.7</td><td>44.2</td></tr><tr><td>PVTv2-B2-Li [58]</td><td>82.1</td><td>3.9</td><td>22.6</td></tr><tr><td>PVTv2-B2 [58]</td><td>82.0</td><td>4.0</td><td>25.4</td></tr><tr><td>Swin-T [42]</td><td>81.3</td><td>4.5</td><td>29</td></tr><tr><td>CvT-13 [32]</td><td>81.6</td><td>4.5</td><td>20</td></tr><tr><td>CvT-13-NAS [32]</td><td>82.2</td><td>4.1</td><td>18</td></tr><tr><td>CvT-13(384) [32]</td><td>83.0</td><td>16.3</td><td>20</td></tr><tr><td>Conformer-Ti [31]</td><td>81.3</td><td>5.2</td><td>23.5</td></tr><tr><td>T2T-ViTt-14 [52]</td><td>80.7</td><td>5.2</td><td>21.5</td></tr><tr><td>T2T-ViTt-19 [52]</td><td>81.4</td><td>8.4</td><td>39.0</td></tr><tr><td>BoTNet-S1-59 [29]</td><td>81.7</td><td>7.3</td><td>33.5</td></tr><tr><td>CSWin-T [43]</td><td>82.7</td><td>4.3</td><td>23</td></tr><tr><td>AutoFormer-S [35]</td><td>81.7</td><td>5.1</td><td>22.9</td></tr><tr><td>Visformer-S (ours)</td><td>82.2</td><td>4.9</td><td>40.2</td></tr><tr><td>VisformerV2-S (ours)</td><td>83.0</td><td>4.3</td><td>23.6</td></tr></table>",
            "id": 128,
            "page": 9,
            "text": "Methods Top-1(%) FLOPs (G) Params (M)\n ResNet-18 [4] 69.8 1.8 11.7\n DeiT-Ti [10] 72.2 1.3 5.7\n DeiT-Ti (KD) [10] 74.6 1.3 5.7\n AutoFormer-Ti [35] 74.7 1.3 5.7\n PVT-Ti [22] 75.1 1.9 13.2\n PVTv2-B1 [58] 78.7 2.1 13.1\n Visformer-Ti (ours) 78.6 1.3 10.3\n VisformerV2-Ti (ours) 79.6 1.3 9.4\n ResNet-50 [4] 76.2 4.1 25.6\n ResNet-50* [4] 78.7 4.1 25.6\n RegNetY-4GF [59] 79.4 4.0 20.6\n RegNetY-8GF [59] 79.9 8.0 39.2\n RegNetY-4GF* [59] 80.0 4.0 20.6\n DeiT-S [10] 79.8 4.6 21.8\n DeiT-S* [10] 80.1 4.6 21.8\n DeiT-B [10] 81.8 17.4 86.3\n PVT-S [22] 79.8 3.8 24.5\n PVT-Medium [22] 81.2 6.7 44.2\n PVTv2-B2-Li [58] 82.1 3.9 22.6\n PVTv2-B2 [58] 82.0 4.0 25.4\n Swin-T [42] 81.3 4.5 29\n CvT-13 [32] 81.6 4.5 20\n CvT-13-NAS [32] 82.2 4.1 18\n CvT-13(384) [32] 83.0 16.3 20\n Conformer-Ti [31] 81.3 5.2 23.5\n T2T-ViTt-14 [52] 80.7 5.2 21.5\n T2T-ViTt-19 [52] 81.4 8.4 39.0\n BoTNet-S1-59 [29] 81.7 7.3 33.5\n CSWin-T [43] 82.7 4.3 23\n AutoFormer-S [35] 81.7 5.1 22.9\n Visformer-S (ours) 82.2 4.9 40.2\n VisformerV2-S (ours) 83.0 4.3"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2519
                },
                {
                    "x": 590,
                    "y": 2519
                },
                {
                    "x": 590,
                    "y": 2567
                },
                {
                    "x": 199,
                    "y": 2567
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:22px'>E. Inference efficiency</p>",
            "id": 129,
            "page": 9,
            "text": "E. Inference efficiency"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2622
                },
                {
                    "x": 1256,
                    "y": 2622
                },
                {
                    "x": 1256,
                    "y": 3120
                },
                {
                    "x": 195,
                    "y": 3120
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:20px'>Although VisformerV2-S is not as efficient as Visformer-S<br>in runtime, it is still much faster than most vision Transformer<br>models as shown in Table XI. As for the state-of-the-art<br>EfficientNet convnets, Visformer-S are below the EfficientNets<br>with similar FLOPs. However, EfficientNets are computing in-<br>efficient on GPUs. It is shown that Visformer-S is significantly<br>faster than EfficientNet-B3 which performance is slightly<br>worse than our model. VisformerV2-S and EfficientNet-B4<br>have similar FLOPs and performance, but VisformerV2-S is<br>significantly faster than EfficientNet-B4.</p>",
            "id": 130,
            "page": 9,
            "text": "Although VisformerV2-S is not as efficient as Visformer-S\nin runtime, it is still much faster than most vision Transformer\nmodels as shown in Table XI. As for the state-of-the-art\nEfficientNet convnets, Visformer-S are below the EfficientNets\nwith similar FLOPs. However, EfficientNets are computing in-\nefficient on GPUs. It is shown that Visformer-S is significantly\nfaster than EfficientNet-B3 which performance is slightly\nworse than our model. VisformerV2-S and EfficientNet-B4\nhave similar FLOPs and performance, but VisformerV2-S is\nsignificantly faster than EfficientNet-B4."
        },
        {
            "bounding_box": [
                {
                    "x": 1745,
                    "y": 865
                },
                {
                    "x": 1903,
                    "y": 865
                },
                {
                    "x": 1903,
                    "y": 899
                },
                {
                    "x": 1745,
                    "y": 899
                }
            ],
            "category": "caption",
            "html": "<br><caption id='131' style='font-size:18px'>TABLE XI</caption>",
            "id": 131,
            "page": 9,
            "text": "TABLE XI"
        },
        {
            "bounding_box": [
                {
                    "x": 1335,
                    "y": 885
                },
                {
                    "x": 2314,
                    "y": 885
                },
                {
                    "x": 2314,
                    "y": 1055
                },
                {
                    "x": 1335,
                    "y": 1055
                }
            ],
            "category": "caption",
            "html": "<br><caption id='132' style='font-size:14px'>COMPARISON OF INFERENCE EFFICIENCY AMONG VISFORMER AND<br>OTHER MODELS ON A 32G- V100. A BATCH SIZE OF 32 IS USED FOR<br>TESTING. '*' INDICATES THAT THE MODEL IS RE-TRAINED WITH THE<br>ELITE SETTING.</caption>",
            "id": 132,
            "page": 9,
            "text": "COMPARISON OF INFERENCE EFFICIENCY AMONG VISFORMER AND\nOTHER MODELS ON A 32G- V100. A BATCH SIZE OF 32 IS USED FOR\nTESTING. '*' INDICATES THAT THE MODEL IS RE-TRAINED WITH THE\nELITE SETTING."
        },
        {
            "bounding_box": [
                {
                    "x": 1400,
                    "y": 1080
                },
                {
                    "x": 2244,
                    "y": 1080
                },
                {
                    "x": 2244,
                    "y": 1632
                },
                {
                    "x": 1400,
                    "y": 1632
                }
            ],
            "category": "table",
            "html": "<table id='133' style='font-size:18px'><tr><td>Methods</td><td>Top-1 (%)</td><td>FLOPs (G)</td><td>Batch Time (ms)</td></tr><tr><td>ResNet-50*</td><td>78.7</td><td>4.1</td><td>34.2</td></tr><tr><td>DeiT-S*</td><td>80.1</td><td>4.6</td><td>36.9</td></tr><tr><td>RegNetY-4GF*</td><td>80.0</td><td>4.0</td><td>40.2</td></tr><tr><td>Swin-T</td><td>81.3</td><td>4.5</td><td>47.6</td></tr><tr><td>CSwin-T</td><td>82.7</td><td>4.3</td><td>57.5</td></tr><tr><td>PVT-S</td><td>79.8</td><td>3.8</td><td>47.6</td></tr><tr><td>PVTv2-B2</td><td>82.0</td><td>4.0</td><td>57.1</td></tr><tr><td>PVTv2-B2-Li</td><td>82.1</td><td>3.9</td><td>56.8</td></tr><tr><td>EfficientNet-B3 [60]</td><td>81.6</td><td>1.8</td><td>48.3</td></tr><tr><td>EfficientNet-B4 [60]</td><td>82.9</td><td>4.2</td><td>81.7</td></tr><tr><td>Visformer-S (ours)</td><td>82.2</td><td>4.9</td><td>36.7</td></tr><tr><td>VisformerV2-S (ours)</td><td>83.0</td><td>4.3</td><td>43.0</td></tr></table>",
            "id": 133,
            "page": 9,
            "text": "Methods Top-1 (%) FLOPs (G) Batch Time (ms)\n ResNet-50* 78.7 4.1 34.2\n DeiT-S* 80.1 4.6 36.9\n RegNetY-4GF* 80.0 4.0 40.2\n Swin-T 81.3 4.5 47.6\n CSwin-T 82.7 4.3 57.5\n PVT-S 79.8 3.8 47.6\n PVTv2-B2 82.0 4.0 57.1\n PVTv2-B2-Li 82.1 3.9 56.8\n EfficientNet-B3 [60] 81.6 1.8 48.3\n EfficientNet-B4 [60] 82.9 4.2 81.7\n Visformer-S (ours) 82.2 4.9 36.7\n VisformerV2-S (ours) 83.0 4.3"
        },
        {
            "bounding_box": [
                {
                    "x": 1296,
                    "y": 1752
                },
                {
                    "x": 1772,
                    "y": 1752
                },
                {
                    "x": 1772,
                    "y": 1797
                },
                {
                    "x": 1296,
                    "y": 1797
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:20px'>F. COCO Object Detection</p>",
            "id": 134,
            "page": 9,
            "text": "F. COCO Object Detection"
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1822
                },
                {
                    "x": 2352,
                    "y": 1822
                },
                {
                    "x": 2352,
                    "y": 2367
                },
                {
                    "x": 1295,
                    "y": 2367
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:20px'>Last but not least, we evaluate our models on the COCO<br>object detection task. Since the standard self-attention in<br>Visformer models is not efficient for high-resolution inputs,<br>we simply replace self-attention with the shifted window<br>(Swin) self-attention [42] to apply our models to the detection<br>task. Therefore, Swin Transformers are our important baseline<br>models. It should be emphasized that the self-attention in Vis-<br>former can also be replaced with other resolution-friendly self-<br>attentions like CSWin self-attention and MSG self-attention.<br>We just utilize the widely used Swin self-attention to show<br>the superiority of Visformer architecture.</p>",
            "id": 135,
            "page": 9,
            "text": "Last but not least, we evaluate our models on the COCO\nobject detection task. Since the standard self-attention in\nVisformer models is not efficient for high-resolution inputs,\nwe simply replace self-attention with the shifted window\n(Swin) self-attention [42] to apply our models to the detection\ntask. Therefore, Swin Transformers are our important baseline\nmodels. It should be emphasized that the self-attention in Vis-\nformer can also be replaced with other resolution-friendly self-\nattentions like CSWin self-attention and MSG self-attention.\nWe just utilize the widely used Swin self-attention to show\nthe superiority of Visformer architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2373
                },
                {
                    "x": 2352,
                    "y": 2373
                },
                {
                    "x": 2352,
                    "y": 2819
                },
                {
                    "x": 1291,
                    "y": 2819
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:20px'>The models are evaluated with two frameworks: Mask-<br>RCNN [61] and Cascade Mask-RCNN [62]. We train the<br>models on COCO 2017 dataset and report the results on COCO<br>val2017. We inherit the training settings in [42]: the AdamW<br>optimizer with a learning rate of 0.0001 and the weight decay<br>of 0.05. The batch size is 16 and we show the results of 1x (12<br>epochs) and 3x (36 epochs) schedule. The FPS is measured on<br>a V100 GPU with a batch size of 1. The FLOPs are computed<br>with 1280 x 800 resolution.</p>",
            "id": 136,
            "page": 9,
            "text": "The models are evaluated with two frameworks: Mask-\nRCNN [61] and Cascade Mask-RCNN [62]. We train the\nmodels on COCO 2017 dataset and report the results on COCO\nval2017. We inherit the training settings in [42]: the AdamW\noptimizer with a learning rate of 0.0001 and the weight decay\nof 0.05. The batch size is 16 and we show the results of 1x (12\nepochs) and 3x (36 epochs) schedule. The FPS is measured on\na V100 GPU with a batch size of 1. The FLOPs are computed\nwith 1280 x 800 resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2823
                },
                {
                    "x": 2353,
                    "y": 2823
                },
                {
                    "x": 2353,
                    "y": 3121
                },
                {
                    "x": 1292,
                    "y": 3121
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:20px'>We first test different methods with the Mask R-CNN<br>1x schedule. As shown at the top of Table XII, Visformer-<br>S slightly outperform Swin-T. When we assign a block to<br>the first stage (Visformer-S-F), although the classification<br>performance is not improved (illustrated in Section IV-C),<br>the detection result becomes better. We conjecture that the</p>",
            "id": 137,
            "page": 9,
            "text": "We first test different methods with the Mask R-CNN\n1x schedule. As shown at the top of Table XII, Visformer-\nS slightly outperform Swin-T. When we assign a block to\nthe first stage (Visformer-S-F), although the classification\nperformance is not improved (illustrated in Section IV-C),\nthe detection result becomes better. We conjecture that the"
        },
        {
            "bounding_box": [
                {
                    "x": 2315,
                    "y": 107
                },
                {
                    "x": 2349,
                    "y": 107
                },
                {
                    "x": 2349,
                    "y": 135
                },
                {
                    "x": 2315,
                    "y": 135
                }
            ],
            "category": "header",
            "html": "<header id='138' style='font-size:14px'>10</header>",
            "id": 138,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 358,
                    "y": 207
                },
                {
                    "x": 2193,
                    "y": 207
                },
                {
                    "x": 2193,
                    "y": 957
                },
                {
                    "x": 358,
                    "y": 957
                }
            ],
            "category": "table",
            "html": "<table id='139' style='font-size:14px'><tr><td>Method</td><td>Backbone</td><td>APbox</td><td>APbox 50</td><td>APbox 75</td><td>APmask</td><td>APmask 50</td><td>APmask 75</td><td>FLOPs</td><td>Params</td><td>FPS</td></tr><tr><td rowspan=\"5\">Mask R-CNN 1x schedule</td><td>R-50</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td><td>260</td><td>44</td><td>18.6</td></tr><tr><td>Swin-T</td><td>42.6</td><td>65.1</td><td>46.2</td><td>39.3</td><td>62.0</td><td>42.1</td><td>267</td><td>48</td><td>14.8</td></tr><tr><td>Visformer-S</td><td>43.0</td><td>65.3</td><td>47.2</td><td>39.6</td><td>62.4</td><td>42.4</td><td>275</td><td>60</td><td>13.1</td></tr><tr><td>Visformer-S-F</td><td>43.5</td><td>65.9</td><td>47.7</td><td>39.8</td><td>62.5</td><td>42.6</td><td>275</td><td>60</td><td>13.0</td></tr><tr><td>VisformerV2-S</td><td>44.8</td><td>66.8</td><td>49.4</td><td>40.7</td><td>63.9</td><td>43.7</td><td>262</td><td>43</td><td>15.2</td></tr><tr><td rowspan=\"3\">Mask R-CNN 3x + MS schedule</td><td>R-50</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td><td>260</td><td>44</td><td>18.6</td></tr><tr><td>Swin-T</td><td>46.0</td><td>68.2</td><td>50.2</td><td>41.6</td><td>65.1</td><td>44.8</td><td>267</td><td>48</td><td>14.8</td></tr><tr><td>VisformerV2-S</td><td>47.8</td><td>69.5</td><td>52.6</td><td>42.5</td><td>66.4</td><td>45.8</td><td>262</td><td>43</td><td>15.2</td></tr><tr><td rowspan=\"3\">Cascade Mask R-CNN 1x + MS schedule</td><td>R-50</td><td>43.7</td><td>61.7</td><td>47.5</td><td>38.0</td><td>58.8</td><td>41.0</td><td>739</td><td>82</td><td>10.6</td></tr><tr><td>Swin-T</td><td>48.1</td><td>67.1</td><td>52.2</td><td>41.7</td><td>64.4</td><td>45.0</td><td>745</td><td>86</td><td>9.5</td></tr><tr><td>VisformerV2-S</td><td>49.3</td><td>68.1</td><td>53.6</td><td>42.3</td><td>65.1</td><td>45.7</td><td>740</td><td>81</td><td>9.6</td></tr><tr><td rowspan=\"7\">Cascade Mask R-CNN 3x + MS schedule</td><td>R-50</td><td>46.3</td><td>64.3</td><td>50.5</td><td>40.1</td><td>61.7</td><td>43.4</td><td>739</td><td>82</td><td>10.6</td></tr><tr><td>DeiT-S</td><td>48.0</td><td>67.2</td><td>51.7</td><td>41.4</td><td>64.2</td><td>44.3</td><td>889</td><td>80</td><td>-</td></tr><tr><td>Swin-T</td><td>50.5</td><td>69.3</td><td>54.9</td><td>43.7</td><td>66.6</td><td>47.1</td><td>745</td><td>86</td><td>9.5</td></tr><tr><td>MSG-T [44]</td><td>50.3</td><td>69.0</td><td>54.7</td><td>43.6</td><td>66.5</td><td>47.5</td><td>758</td><td>86</td><td>9.5</td></tr><tr><td>PVTv2-B2-Li [58]</td><td>50.9</td><td>69.5</td><td>55.2</td><td>-</td><td>-</td><td>-</td><td>725</td><td>80</td><td>8.2</td></tr><tr><td>PVTv2-B2 [58]</td><td>51.1</td><td>69.8</td><td>55.3</td><td>-</td><td>-</td><td>-</td><td>788</td><td>83</td><td>7.1</td></tr><tr><td>VisformerV2-S</td><td>51.6</td><td>70.1</td><td>56.4</td><td>44.1</td><td>67.5</td><td>47.8</td><td>740</td><td>81</td><td>9.6</td></tr></table>",
            "id": 139,
            "page": 10,
            "text": "Method Backbone APbox APbox 50 APbox 75 APmask APmask 50 APmask 75 FLOPs Params FPS\n Mask R-CNN 1x schedule R-50 38.0 58.6 41.4 34.4 55.1 36.7 260 44 18.6\n Swin-T 42.6 65.1 46.2 39.3 62.0 42.1 267 48 14.8\n Visformer-S 43.0 65.3 47.2 39.6 62.4 42.4 275 60 13.1\n Visformer-S-F 43.5 65.9 47.7 39.8 62.5 42.6 275 60 13.0\n VisformerV2-S 44.8 66.8 49.4 40.7 63.9 43.7 262 43 15.2\n Mask R-CNN 3x + MS schedule R-50 41.0 61.7 44.9 37.1 58.4 40.1 260 44 18.6\n Swin-T 46.0 68.2 50.2 41.6 65.1 44.8 267 48 14.8\n VisformerV2-S 47.8 69.5 52.6 42.5 66.4 45.8 262 43 15.2\n Cascade Mask R-CNN 1x + MS schedule R-50 43.7 61.7 47.5 38.0 58.8 41.0 739 82 10.6\n Swin-T 48.1 67.1 52.2 41.7 64.4 45.0 745 86 9.5\n VisformerV2-S 49.3 68.1 53.6 42.3 65.1 45.7 740 81 9.6\n Cascade Mask R-CNN 3x + MS schedule R-50 46.3 64.3 50.5 40.1 61.7 43.4 739 82 10.6\n DeiT-S 48.0 67.2 51.7 41.4 64.2 44.3 889 80 -\n Swin-T 50.5 69.3 54.9 43.7 66.6 47.1 745 86 9.5\n MSG-T [44] 50.3 69.0 54.7 43.6 66.5 47.5 758 86 9.5\n PVTv2-B2-Li [58] 50.9 69.5 55.2 - - - 725 80 8.2\n PVTv2-B2 [58] 51.1 69.8 55.3 - - - 788 83 7.1\n VisformerV2-S 51.6 70.1 56.4 44.1 67.5 47.8 740 81"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 967
                },
                {
                    "x": 2342,
                    "y": 967
                },
                {
                    "x": 2342,
                    "y": 1086
                },
                {
                    "x": 205,
                    "y": 1086
                }
            ],
            "category": "caption",
            "html": "<br><caption id='140' style='font-size:14px'>TABLE XII<br>OBJECT DETECTION AND INSTANCE SEGMENTATION PERFORMANCE ON COCO 2017. THE FPS IS MEASURED ON A V100 GPU WITH A BATCH SIZE OF<br>1. THE FLOPs ARE COMPUTED WITH 1280 x 800 RESOLUTION. 'MS' INDICATES MULTI-SCALE TRAINING [38], [64]</caption>",
            "id": 140,
            "page": 10,
            "text": "TABLE XII\nOBJECT DETECTION AND INSTANCE SEGMENTATION PERFORMANCE ON COCO 2017. THE FPS IS MEASURED ON A V100 GPU WITH A BATCH SIZE OF\n1. THE FLOPs ARE COMPUTED WITH 1280 x 800 RESOLUTION. 'MS' INDICATES MULTI-SCALE TRAINING [38], [64]"
        },
        {
            "bounding_box": [
                {
                    "x": 1701,
                    "y": 1246
                },
                {
                    "x": 1943,
                    "y": 1246
                },
                {
                    "x": 1943,
                    "y": 1289
                },
                {
                    "x": 1701,
                    "y": 1289
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>REFERENCES</p>",
            "id": 141,
            "page": 10,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1550
                },
                {
                    "x": 1257,
                    "y": 1550
                },
                {
                    "x": 1257,
                    "y": 1896
                },
                {
                    "x": 196,
                    "y": 1896
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:18px'>For the Cascade Mask R-CNN framework, VisformerV2-<br>S still outperforms Swin-T by a large margin. We compare<br>VisformerV2-S with more methods for 3x and multi-scale<br>schedule [38], [64], and our model still performs better than<br>the other methods. As for FPS, our method is as efficient as<br>Swin-T and MSG-T, and is faster than other vision Trans-<br>former Methods.</p>",
            "id": 142,
            "page": 10,
            "text": "For the Cascade Mask R-CNN framework, VisformerV2-\nS still outperforms Swin-T by a large margin. We compare\nVisformerV2-S with more methods for 3x and multi-scale\nschedule [38], [64], and our model still performs better than\nthe other methods. As for FPS, our method is as efficient as\nSwin-T and MSG-T, and is faster than other vision Trans-\nformer Methods."
        },
        {
            "bounding_box": [
                {
                    "x": 563,
                    "y": 1998
                },
                {
                    "x": 889,
                    "y": 1998
                },
                {
                    "x": 889,
                    "y": 2039
                },
                {
                    "x": 563,
                    "y": 2039
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:20px'>V. CONCLUSIONS</p>",
            "id": 143,
            "page": 10,
            "text": "V. CONCLUSIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 2084
                },
                {
                    "x": 1254,
                    "y": 2084
                },
                {
                    "x": 1254,
                    "y": 2685
                },
                {
                    "x": 194,
                    "y": 2685
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>This paper presents Visformer, a Transformer-based model<br>that is friendly to visual recognition. We propose to use two<br>protocols, the base and elite setting, to evaluate the perfor-<br>mance of each model. To study the reason why Transformer-<br>based models and convolution-based models behave differ-<br>ently, we decompose the gap between these models and design<br>an eight-step transition procedure that bridges the gap between<br>DeiT-S and ResNet-50. By absorbing the advantages and<br>discarding the disadvantages, we obtain the Visformer-S model<br>that outperforms both DeiT-S and ResNet-50. Visformer also<br>shows a promising ability when it is transferred to a compact<br>model and when it is evaluated on small datasets.</p>",
            "id": 144,
            "page": 10,
            "text": "This paper presents Visformer, a Transformer-based model\nthat is friendly to visual recognition. We propose to use two\nprotocols, the base and elite setting, to evaluate the perfor-\nmance of each model. To study the reason why Transformer-\nbased models and convolution-based models behave differ-\nently, we decompose the gap between these models and design\nan eight-step transition procedure that bridges the gap between\nDeiT-S and ResNet-50. By absorbing the advantages and\ndiscarding the disadvantages, we obtain the Visformer-S model\nthat outperforms both DeiT-S and ResNet-50. Visformer also\nshows a promising ability when it is transferred to a compact\nmodel and when it is evaluated on small datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 491,
                    "y": 2783
                },
                {
                    "x": 961,
                    "y": 2783
                },
                {
                    "x": 961,
                    "y": 2824
                },
                {
                    "x": 491,
                    "y": 2824
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:18px'>VI. ACKNOWLEDGMENTS</p>",
            "id": 145,
            "page": 10,
            "text": "VI. ACKNOWLEDGMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 2874
                },
                {
                    "x": 1256,
                    "y": 2874
                },
                {
                    "x": 1256,
                    "y": 3119
                },
                {
                    "x": 196,
                    "y": 3119
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:22px'>This work was supported by the National Key R&D Program<br>of China (2017YFB1301100), National Natural Science Foun-<br>dation of China (61772060, U1536107, 61472024, 61572060,<br>61976012, 61602024), and the CERNET Innovation Project<br>(NGII20160316).</p>",
            "id": 146,
            "page": 10,
            "text": "This work was supported by the National Key R&D Program\nof China (2017YFB1301100), National Natural Science Foun-\ndation of China (61772060, U1536107, 61472024, 61572060,\n61976012, 61602024), and the CERNET Innovation Project\n(NGII20160316)."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1245
                },
                {
                    "x": 1256,
                    "y": 1245
                },
                {
                    "x": 1256,
                    "y": 1542
                },
                {
                    "x": 196,
                    "y": 1542
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:20px'>block in the first stage can help the FPN [63] to explore the<br>low-level features. The VisformerV2-S further improves the<br>performance and outperforms Swin-T by 2.2%. Additionally,<br>because of the improvement on parallelization property, the<br>FPS becomes consistent with FLOPs and VisformerV2-S is<br>faster than Visformer-S.</p>",
            "id": 147,
            "page": 10,
            "text": "block in the first stage can help the FPN [63] to explore the\nlow-level features. The VisformerV2-S further improves the\nperformance and outperforms Swin-T by 2.2%. Additionally,\nbecause of the improvement on parallelization property, the\nFPS becomes consistent with FLOPs and VisformerV2-S is\nfaster than Visformer-S."
        },
        {
            "bounding_box": [
                {
                    "x": 1303,
                    "y": 1323
                },
                {
                    "x": 2354,
                    "y": 1323
                },
                {
                    "x": 2354,
                    "y": 3119
                },
                {
                    "x": 1303,
                    "y": 3119
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:16px'>[1] Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" nature, vol. 521,<br>no. 7553, pp. 436-444, 2015.<br>[2] K. Simonyan and A. Zisserman, \"Very deep convolutional networks for<br>large-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014.<br>[3] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,<br>V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\"<br>in Proceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition, 2015, pp. 1-9.<br>[4] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image<br>recognition,\" in Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition, 2016, pp. 770-778.<br>[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,<br>L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" arXiv preprint<br>arXiv:1706.03762, 2017.<br>[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training<br>of deep bidirectional transformers for language understanding,\" arXiv<br>preprint arXiv:1810.04805, 2018.<br>[7] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, \"Improving<br>language understanding by generative pre-training,\" 2018.<br>[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,<br>T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,<br>\"An image is worth 16x16 words: Transformers for image recognition<br>at scale,\" arXiv preprint arXiv:2010.11929, 2020.<br>[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet:<br>A large-scale hierarchical image database,\" in Proceedings of the IEEE<br>conference on computer vision and pattern recognition, 2009, pp. 248-<br>255.<br>[10] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and<br>H. Jegou, \"Training data-efficient image transformers & distillation<br>through attention,\" arXiv preprint arXiv:2012.12877, 2020.<br>[11] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking<br>the inception architecture for computer vision,\" in Proceedings of the<br>IEEE Conference on Computer Vision and Pattern Recognition, 2016,<br>pp. 2818-2826.<br>[12] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, \"Randaugment:<br>Practical automated data augmentation with a reduced search space,\"<br>in Proceedings of the IEEE/CVF Conference on Computer Vision and<br>Pattern Recognition Workshops, 2020, pp. 702-703.<br>[13] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, \"Cutmix: Reg-<br>ularization strategy to train strong classifiers with localizable features,\"<br>in Proceedings of the IEEE/CVF International Conference on Computer<br>Vision, 2019, pp. 6023-6032.<br>[14] S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep<br>network training by reducing internal covariate shift,\" arXiv preprint<br>arXiv:1502.03167, 2015.<br>[15] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv<br>preprint arXiv:1607.06450, 2016.</p>",
            "id": 148,
            "page": 10,
            "text": "[1] Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" nature, vol. 521,\nno. 7553, pp. 436-444, 2015.\n[2] K. Simonyan and A. Zisserman, \"Very deep convolutional networks for\nlarge-scale image recognition,\" arXiv preprint arXiv:1409.1556, 2014.\n[3] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\nV. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\"\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015, pp. 1-9.\n[4] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image\nrecognition,\" in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2016, pp. 770-778.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, \"Attention is all you need,\" arXiv preprint\narXiv:1706.03762, 2017.\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training\nof deep bidirectional transformers for language understanding,\" arXiv\npreprint arXiv:1810.04805, 2018.\n[7] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, \"Improving\nlanguage understanding by generative pre-training,\" 2018.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n\"An image is worth 16x16 words: Transformers for image recognition\nat scale,\" arXiv preprint arXiv:2010.11929, 2020.\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet:\nA large-scale hierarchical image database,\" in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2009, pp. 248-\n255.\n[10] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. Jegou, \"Training data-efficient image transformers & distillation\nthrough attention,\" arXiv preprint arXiv:2012.12877, 2020.\n[11] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking\nthe inception architecture for computer vision,\" in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2016,\npp. 2818-2826.\n[12] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, \"Randaugment:\nPractical automated data augmentation with a reduced search space,\"\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops, 2020, pp. 702-703.\n[13] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, \"Cutmix: Reg-\nularization strategy to train strong classifiers with localizable features,\"\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 6023-6032.\n[14] S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,\" arXiv preprint\narXiv:1502.03167, 2015.\n[15] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv\npreprint arXiv:1607.06450, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 2314,
                    "y": 106
                },
                {
                    "x": 2348,
                    "y": 106
                },
                {
                    "x": 2348,
                    "y": 136
                },
                {
                    "x": 2314,
                    "y": 136
                }
            ],
            "category": "header",
            "html": "<header id='149' style='font-size:14px'>11</header>",
            "id": 149,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 212
                },
                {
                    "x": 1260,
                    "y": 212
                },
                {
                    "x": 1260,
                    "y": 3127
                },
                {
                    "x": 202,
                    "y": 3127
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:18px'>[16] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian, \"Visformer:<br>The vision-friendly transformer,\" in Proceedings of the IEEE/CVF<br>International Conference on Computer Vision (ICCV), October 2021,<br>pp. 589-598.<br>[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification<br>with deep convolutional neural networks,\" in Advances in neural infor-<br>mation processing systems, 2012, pp. 1097-1105.<br>[18] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and<br>J. Shlens, \"Stand-alone self-attention in vision models,\" arXiv preprint<br>arXiv:1906.05909, 2019.<br>[19] H. Hu, Z. Zhang, Z. Xie, and S. Lin, \"Local relation networks for image<br>recognition,\" in Proceedings of the IEEE/CVF International Conference<br>on Computer Vision, 2019, pp. 3464-3473.<br>[20] H. Zhao, J. Jia, and V. Koltun, \"Exploring self-attention for image<br>recognition,' in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, 2020, pp. 10076-10085.<br>[21] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,<br>\"Generative pretraining from pixels,\" in International Conference on<br>Machine Learning. PMLR, 2020, pp. 1691-1703.<br>[22] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and<br>L. Shao, \"Pyramid vision transformer: A versatile backbone for dense<br>prediction without convolutions,\" arXiv preprint arXiv:2102.12122,<br>2021.<br>[23] X. Wang, R. Girshick, A. Gupta, and K. He, \"Non-local neural net-<br>works,\" in Proceedings of the IEEE conference on computer vision and<br>pattern recognition, 2018, pp. 7794-7803.<br>[24] A. Buades, B. Coll, and J.-M. Morel, \"A non-local algorithm for image<br>denoising,\" in 2005 IEEE Computer Society Conference on Computer<br>Vision and Pattern Recognition (CVPR'05), vol. 2. IEEE, 2005, pp.<br>60-65.<br>[25] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, \"Gcnet: Non-local networks<br>meet squeeze-excitation networks and beyond,\" in Proceedings of the<br>IEEE/CVF International Conference on Computer Vision Workshops,<br>2019, pp. 0-0.<br>[26] Y. Li, X. Jin, J. Mei, X. Lian, L. Yang, C. Xie, Q. Yu, Y. Zhou, S. Bai,<br>and A. L. Yuille, \"Neural architecture search for lightweight non-local<br>networks,\" in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, 2020, pp. 10297-10306.<br>[27] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \"Attention<br>augmented convolutional networks,\" in Proceedings of the IEEE/CVF<br>international conference on computer vision, 2019, pp. 3286-3295.<br>[28] I. Bello, \"Lambdanetworks: Modeling long-range interactions without<br>attention,' arXiv preprint arXiv:2102.08602, 2021.<br>[29] A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel, and<br>A. Vaswani, \"Bottleneck transformers for visual recognition,\" arXiv<br>preprint arXiv:2101.11605, 2021.<br>[30] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman,<br>and J. Shlens, \"Scaling local self-attention for parameter efficient visual<br>backbones,\" in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, 2021, pp. 12 894-12904.<br>[31] Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye,<br>\"Conformer: Local features coupling global representations for visual<br>recognition,\" arXiv preprint arXiv:2105.03889, 2021.<br>[32] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,<br>\"Cvt: Introducing convolutions to vision transformers,\" arXiv preprint<br>arXiv:2103.15808, 2021.<br>[33] Z. Dai, H. Liu, Q. V. Le, and M. Tan, \"Coatnet: Marrying convolution<br>and attention for all data sizes,\" arXiv preprint arXiv:2106.04803, 2021.<br>[34] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jegou, \"Go-<br>ing deeper with image transformers,\" arXiv preprint arXiv:2103.17239,<br>2021.<br>[35] M. Chen, H. Peng, J. Fu, and H. Ling, \"Autoformer: Searching<br>transformers for visual recognition,\" in Proceedings of the IEEE/CVF<br>International Conference on Computer Vision, 2021, pp. 12 270-12280.<br>[36] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, \"Scaling vision<br>transformers,\" arXiv preprint arXiv:2106.04560, 2021.<br>[37] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and<br>L. Beyer, \"How to train your vit? data, augmentation, and regularization<br>in vision transformers,\" arXiv preprint arXiv:2106.10270, 2021.<br>[38] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and<br>S. Zagoruyko, \"End-to-end object detection with transformers,\" in<br>European Conference on Computer Vision. Springer, 2020, pp. 213-<br>229.<br>[39] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and<br>Y. Zhou, \"Transunet: Transformers make strong encoders for medical<br>image segmentation,\" arXiv preprint arXiv:2102.04306, 2021.</p>",
            "id": 150,
            "page": 11,
            "text": "[16] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian, \"Visformer:\nThe vision-friendly transformer,\" in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), October 2021,\npp. 589-598.\n[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification\nwith deep convolutional neural networks,\" in Advances in neural infor-\nmation processing systems, 2012, pp. 1097-1105.\n[18] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and\nJ. Shlens, \"Stand-alone self-attention in vision models,\" arXiv preprint\narXiv:1906.05909, 2019.\n[19] H. Hu, Z. Zhang, Z. Xie, and S. Lin, \"Local relation networks for image\nrecognition,\" in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2019, pp. 3464-3473.\n[20] H. Zhao, J. Jia, and V. Koltun, \"Exploring self-attention for image\nrecognition,' in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 10076-10085.\n[21] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n\"Generative pretraining from pixels,\" in International Conference on\nMachine Learning. PMLR, 2020, pp. 1691-1703.\n[22] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, \"Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,\" arXiv preprint arXiv:2102.12122,\n2021.\n[23] X. Wang, R. Girshick, A. Gupta, and K. He, \"Non-local neural net-\nworks,\" in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7794-7803.\n[24] A. Buades, B. Coll, and J.-M. Morel, \"A non-local algorithm for image\ndenoising,\" in 2005 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR'05), vol. 2. IEEE, 2005, pp.\n60-65.\n[25] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, \"Gcnet: Non-local networks\nmeet squeeze-excitation networks and beyond,\" in Proceedings of the\nIEEE/CVF International Conference on Computer Vision Workshops,\n2019, pp. 0-0.\n[26] Y. Li, X. Jin, J. Mei, X. Lian, L. Yang, C. Xie, Q. Yu, Y. Zhou, S. Bai,\nand A. L. Yuille, \"Neural architecture search for lightweight non-local\nnetworks,\" in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 10297-10306.\n[27] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \"Attention\naugmented convolutional networks,\" in Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2019, pp. 3286-3295.\n[28] I. Bello, \"Lambdanetworks: Modeling long-range interactions without\nattention,' arXiv preprint arXiv:2102.08602, 2021.\n[29] A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel, and\nA. Vaswani, \"Bottleneck transformers for visual recognition,\" arXiv\npreprint arXiv:2101.11605, 2021.\n[30] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman,\nand J. Shlens, \"Scaling local self-attention for parameter efficient visual\nbackbones,\" in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021, pp. 12 894-12904.\n[31] Z. Peng, W. Huang, S. Gu, L. Xie, Y. Wang, J. Jiao, and Q. Ye,\n\"Conformer: Local features coupling global representations for visual\nrecognition,\" arXiv preprint arXiv:2105.03889, 2021.\n[32] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n\"Cvt: Introducing convolutions to vision transformers,\" arXiv preprint\narXiv:2103.15808, 2021.\n[33] Z. Dai, H. Liu, Q. V. Le, and M. Tan, \"Coatnet: Marrying convolution\nand attention for all data sizes,\" arXiv preprint arXiv:2106.04803, 2021.\n[34] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jegou, \"Go-\ning deeper with image transformers,\" arXiv preprint arXiv:2103.17239,\n2021.\n[35] M. Chen, H. Peng, J. Fu, and H. Ling, \"Autoformer: Searching\ntransformers for visual recognition,\" in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 12 270-12280.\n[36] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, \"Scaling vision\ntransformers,\" arXiv preprint arXiv:2106.04560, 2021.\n[37] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and\nL. Beyer, \"How to train your vit? data, augmentation, and regularization\nin vision transformers,\" arXiv preprint arXiv:2106.10270, 2021.\n[38] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, \"End-to-end object detection with transformers,\" in\nEuropean Conference on Computer Vision. Springer, 2020, pp. 213-\n229.\n[39] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and\nY. Zhou, \"Transunet: Transformers make strong encoders for medical\nimage segmentation,\" arXiv preprint arXiv:2102.04306, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 223
                },
                {
                    "x": 2354,
                    "y": 223
                },
                {
                    "x": 2354,
                    "y": 3125
                },
                {
                    "x": 1290,
                    "y": 3125
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='151' style='font-size:18px'>[40] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,<br>C. Xu, and W. Gao, \"Pre-trained image processing transformer,\" arXiv<br>preprint arXiv:2012.00364, 2020.<br>[41] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,<br>P. Dollar, and C. L. Zitnick, \"Microsoft COCO: Common objects in<br>context,\" in European conference on computer vision. Springer, 2014,<br>pp. 740-755.<br>[42] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and<br>B. Guo, \"Swin transformer: Hierarchical vision transformer using shifted<br>windows,' , arXiv preprint arXiv:2103.14030, 2021.<br>[43] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and<br>B. Guo, \"Cswin transformer: A general vision transformer backbone<br>with cross-shaped windows,\" arXiv preprint arXiv:2107.00652, 2021.<br>[44] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian, \"Msg-<br>transformer: Exchanging local spatial information by manipulating mes-<br>senger tokens,' , arXiv preprint arXiv:2105.15168, 2021.<br>[45] 0. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,<br>Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \"Imagenet large<br>scale visual recognition challenge,\" International Journal of Computer<br>Vision, vol. 115, no. 3, pp. 211-252, 2015.<br>[46] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \"mixup: Beyond<br>empirical risk minimization,' , 2017.<br>[47] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, \"Random erasing<br>data augmentation,\" in Proceedings of the AAAI Conference on Artificial<br>Intelligence, vol. 34, no. 07, 2020, pp. 13 001-13 008.<br>[48] M. Berman, H. Jegou, A. Vedaldi, I. Kokkinos, and M. Douze, \"Multi-<br>grain: a unified image embedding for classes and instances,\" arXiv<br>preprint arXiv:1902.05509, 2019.<br>[49] E. Hoffer, T. Ben-Nun, I. Hubara, N. Giladi, T. Hoefler, and D. Soudry,<br>\"Augment your batch: Improving generalization through instance repeti-<br>tion,\" in Proceedings of the IEEE/CVF Conference on Computer Vision<br>and Pattern Recognition, 2020, pp. 8129-8138.<br>[50] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \"Deep<br>networks with stochastic depth,\" in European Conference on Computer<br>Vision. Springer, 2016, pp. 646-661.<br>[51] M. Lin, Q. Chen, and S. Yan, \"Network in network,\" arXiv preprint<br>arXiv:1312.4400, 2013.<br>[52] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and<br>S. Yan, \"Tokens-to-token vit: Training vision transformers from scratch<br>on imagenet,\" arXiv preprint arXiv:2101.11986, 2021.<br>[53] Y. Wu and K. He, \"Group normalization,\" in Proceedings of the<br>European conference on computer vision (ECCV), 2018, pp. 3-19.<br>[54] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, \"Aggregated<br>residual transformations for deep neural networks,\" arXiv preprint<br>arXiv:1611.05431, 2016.<br>[55] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou,<br>Z. Shao, H. Yang et al., \"Cogview: Mastering text-to-image generation<br>via transformers,\" arXiv preprint arXiv:2105.13290, 2021.<br>[56] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao,<br>Z. Zhang, L. Dong et al., \"Swin transformer v2: Scaling up capacity<br>and resolution,\" arXiv preprint arXiv:2111.09883, 2021.<br>[57] G. Hinton, 0. Vinyals, and J. Dean, \"Distilling the knowledge in a neural<br>network,\" arXiv preprint arXiv:1503.02531, 2015.<br>[58] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and<br>L. Shao, \"Pvtv2: Improved baselines with pyramid vision transformer,\"<br>arXiv preprint arXiv:2106.13797, 2021.<br>[59] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollar,<br>\"Designing network design spaces,' , in Proceedings of the IEEE/CVF<br>Conference on Computer Vision and Pattern Recognition, 2020, pp.<br>10428-10436.<br>[60] M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for con-<br>volutional neural networks,\" in International Conference on Machine<br>Learning. PMLR, 2019, pp. 6105-6114.<br>[61] K. He, G. Gkioxari, P. Dollar, and R. Girshick, \"Mask r-cnn,\" in<br>International Conference on Computer Vision. IEEE, 2017.<br>[62] Z. Cai and N. Vasconcelos, \"Cascade r-cnn: Delving into high quality<br>object detection,\" in Proceedings of the IEEE conference on computer<br>vision and pattern recognition, 2018, pp. 6154-6162.<br>[63] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie,<br>\"Feature pyramid networks for object detection,\" in Proceedings of the<br>IEEE conference on computer vision and pattern recognition, 2017, pp.<br>2117-2125.<br>[64] P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka,<br>L. Li, Z. Yuan, C. Wang et al., \"Sparse r-cnn: End-to-end object<br>detection with learnable proposals,\" in Proceedings of the IEEE/CVF<br>Conference on Computer Vision and Pattern Recognition, 2021, pp.<br>14454-14 463.</p>",
            "id": 151,
            "page": 11,
            "text": "[40] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, \"Pre-trained image processing transformer,\" arXiv\npreprint arXiv:2012.00364, 2020.\n[41] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Dollar, and C. L. Zitnick, \"Microsoft COCO: Common objects in\ncontext,\" in European conference on computer vision. Springer, 2014,\npp. 740-755.\n[42] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and\nB. Guo, \"Swin transformer: Hierarchical vision transformer using shifted\nwindows,' , arXiv preprint arXiv:2103.14030, 2021.\n[43] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and\nB. Guo, \"Cswin transformer: A general vision transformer backbone\nwith cross-shaped windows,\" arXiv preprint arXiv:2107.00652, 2021.\n[44] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian, \"Msg-\ntransformer: Exchanging local spatial information by manipulating mes-\nsenger tokens,' , arXiv preprint arXiv:2105.15168, 2021.\n[45] 0. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \"Imagenet large\nscale visual recognition challenge,\" International Journal of Computer\nVision, vol. 115, no. 3, pp. 211-252, 2015.\n[46] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \"mixup: Beyond\nempirical risk minimization,' , 2017.\n[47] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, \"Random erasing\ndata augmentation,\" in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 34, no. 07, 2020, pp. 13 001-13 008.\n[48] M. Berman, H. Jegou, A. Vedaldi, I. Kokkinos, and M. Douze, \"Multi-\ngrain: a unified image embedding for classes and instances,\" arXiv\npreprint arXiv:1902.05509, 2019.\n[49] E. Hoffer, T. Ben-Nun, I. Hubara, N. Giladi, T. Hoefler, and D. Soudry,\n\"Augment your batch: Improving generalization through instance repeti-\ntion,\" in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 8129-8138.\n[50] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \"Deep\nnetworks with stochastic depth,\" in European Conference on Computer\nVision. Springer, 2016, pp. 646-661.\n[51] M. Lin, Q. Chen, and S. Yan, \"Network in network,\" arXiv preprint\narXiv:1312.4400, 2013.\n[52] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and\nS. Yan, \"Tokens-to-token vit: Training vision transformers from scratch\non imagenet,\" arXiv preprint arXiv:2101.11986, 2021.\n[53] Y. Wu and K. He, \"Group normalization,\" in Proceedings of the\nEuropean conference on computer vision (ECCV), 2018, pp. 3-19.\n[54] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, \"Aggregated\nresidual transformations for deep neural networks,\" arXiv preprint\narXiv:1611.05431, 2016.\n[55] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou,\nZ. Shao, H. Yang et al., \"Cogview: Mastering text-to-image generation\nvia transformers,\" arXiv preprint arXiv:2105.13290, 2021.\n[56] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao,\nZ. Zhang, L. Dong et al., \"Swin transformer v2: Scaling up capacity\nand resolution,\" arXiv preprint arXiv:2111.09883, 2021.\n[57] G. Hinton, 0. Vinyals, and J. Dean, \"Distilling the knowledge in a neural\nnetwork,\" arXiv preprint arXiv:1503.02531, 2015.\n[58] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, \"Pvtv2: Improved baselines with pyramid vision transformer,\"\narXiv preprint arXiv:2106.13797, 2021.\n[59] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollar,\n\"Designing network design spaces,' , in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n10428-10436.\n[60] M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for con-\nvolutional neural networks,\" in International Conference on Machine\nLearning. PMLR, 2019, pp. 6105-6114.\n[61] K. He, G. Gkioxari, P. Dollar, and R. Girshick, \"Mask r-cnn,\" in\nInternational Conference on Computer Vision. IEEE, 2017.\n[62] Z. Cai and N. Vasconcelos, \"Cascade r-cnn: Delving into high quality\nobject detection,\" in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2018, pp. 6154-6162.\n[63] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n\"Feature pyramid networks for object detection,\" in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2017, pp.\n2117-2125.\n[64] P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka,\nL. Li, Z. Yuan, C. Wang et al., \"Sparse r-cnn: End-to-end object\ndetection with learnable proposals,\" in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2021, pp.\n14454-14 463."
        }
    ]
}