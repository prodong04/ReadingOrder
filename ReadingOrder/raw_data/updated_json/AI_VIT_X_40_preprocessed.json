{
    "id": "32b57266-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2302.04166v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 791,
                    "y": 367
                },
                {
                    "x": 1695,
                    "y": 367
                },
                {
                    "x": 1695,
                    "y": 438
                },
                {
                    "x": 791,
                    "y": 438
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>GPTScore: Evaluate as You Desire</p>",
            "id": 0,
            "page": 1,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 691,
                    "y": 572
                },
                {
                    "x": 1774,
                    "y": 572
                },
                {
                    "x": 1774,
                    "y": 635
                },
                {
                    "x": 691,
                    "y": 635
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:18px'>Jinlan Fu 1 See-Kiong Ng 1 Zhengbao Jiang 2 Pengfei Liu 2</p>",
            "id": 1,
            "page": 1,
            "text": "Jinlan Fu 1 See-Kiong Ng 1 Zhengbao Jiang 2 Pengfei Liu 2"
        },
        {
            "bounding_box": [
                {
                    "x": 619,
                    "y": 718
                },
                {
                    "x": 817,
                    "y": 718
                },
                {
                    "x": 817,
                    "y": 771
                },
                {
                    "x": 619,
                    "y": 771
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:20px'>Abstract</p>",
            "id": 2,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 306,
                    "y": 806
                },
                {
                    "x": 1140,
                    "y": 806
                },
                {
                    "x": 1140,
                    "y": 2060
                },
                {
                    "x": 306,
                    "y": 2060
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:16px'>Generative Artificial Intelligence (AI) has enabled<br>the development of sophisticated models that are<br>capable of producing high-caliber text, images,<br>and other outputs through the utilization of large<br>pre-trained models. Nevertheless, assessing the<br>quality of the generation is an even more ardu-<br>ous task than the generation itself, and this is-<br>sue has not been given adequate consideration<br>recently. This paper proposes a novel evalua-<br>tion framework, GPTSCORE, which utilizes the<br>emergent abilities (e.g., zero-shot instruction) of<br>generative pre-trained models to score generated<br>texts. There are 19 pre-trained models explored<br>in this paper, ranging in size from 80M (e.g.,<br>FLAN-T5-small) to 175B (e.g., GPT3). Exper-<br>imental results on four text generation tasks, 22<br>evaluation aspects, and corresponding 37 datasets<br>demonstrate that this approach can effectively<br>allow us to achieve what one desires to evalu-<br>ate for texts simply by natural language instruc-<br>tions. This nature helps us overcome several<br>long-standing challenges in text evaluation-how<br>to achieve customized, multi-faceted evaluation<br>without the need for annotated samples. We make<br>our code publicly available. 1</p>",
            "id": 3,
            "page": 1,
            "text": "Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTSCORE, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation-how to achieve customized, multi-faceted evaluation without the need for annotated samples. We make our code publicly available. 1"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2167
                },
                {
                    "x": 554,
                    "y": 2167
                },
                {
                    "x": 554,
                    "y": 2223
                },
                {
                    "x": 226,
                    "y": 2223
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>1. Introduction</p>",
            "id": 4,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2252
                },
                {
                    "x": 1216,
                    "y": 2252
                },
                {
                    "x": 1216,
                    "y": 2705
                },
                {
                    "x": 222,
                    "y": 2705
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>The advent of generative pre-trained models, such as GPT3<br>(Brown et al., 2020), has precipitated a shift from analyti-<br>cal AI to generative AI across multiple domains (Sequoia,<br>2022). Take text as an example: the use of a large pre-<br>trained model with appropriate prompts (Liu et al., 2021)<br>has achieved superior performance in tasks defined both in<br>academia (Sanh et al., 2021) and scenarios from the real<br>world (Ouyang et al., 2022). While text generation tech-<br>nology is advancing rapidly, techniques for evaluating the</p>",
            "id": 5,
            "page": 1,
            "text": "The advent of generative pre-trained models, such as GPT3 (Brown , 2020), has precipitated a shift from analytical AI to generative AI across multiple domains (Sequoia, 2022). Take text as an example: the use of a large pretrained model with appropriate prompts (Liu , 2021) has achieved superior performance in tasks defined both in academia (Sanh , 2021) and scenarios from the real world (Ouyang , 2022). While text generation technology is advancing rapidly, techniques for evaluating the"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2733
                },
                {
                    "x": 1212,
                    "y": 2733
                },
                {
                    "x": 1212,
                    "y": 2863
                },
                {
                    "x": 225,
                    "y": 2863
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>1National University of Singapore 2Carnegie Mellon University.<br>Correspondence to: Jinlan Fu <jinlanjonna@gmail.com>, Pengfei<br>Liu <pliu3@cs.cmu.edu>.</p>",
            "id": 6,
            "page": 1,
            "text": "1National University of Singapore 2Carnegie Mellon University. Correspondence to: Jinlan Fu <jinlanjonna@gmail.com>, Pengfei Liu <pliu3@cs.cmu.edu>."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2902
                },
                {
                    "x": 584,
                    "y": 2902
                },
                {
                    "x": 584,
                    "y": 2943
                },
                {
                    "x": 226,
                    "y": 2943
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:16px'>Preprint. Under review.</p>",
            "id": 7,
            "page": 1,
            "text": "Preprint. Under review."
        },
        {
            "bounding_box": [
                {
                    "x": 277,
                    "y": 2947
                },
                {
                    "x": 1111,
                    "y": 2947
                },
                {
                    "x": 1111,
                    "y": 2991
                },
                {
                    "x": 277,
                    "y": 2991
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:16px'>1https : / / github · com/ ·n inlanfu/ GPTScore</p>",
            "id": 8,
            "page": 1,
            "text": "1https : / / github · com/ ·n inlanfu/ GPTScore"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 721
                },
                {
                    "x": 2269,
                    "y": 721
                },
                {
                    "x": 2269,
                    "y": 824
                },
                {
                    "x": 1274,
                    "y": 824
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>quality of these texts lag far behind. This is especially evi-<br>dent in the following ways:</p>",
            "id": 9,
            "page": 1,
            "text": "quality of these texts lag far behind. This is especially evident in the following ways:"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 865
                },
                {
                    "x": 2245,
                    "y": 865
                },
                {
                    "x": 2245,
                    "y": 1247
                },
                {
                    "x": 1278,
                    "y": 1247
                }
            ],
            "category": "figure",
            "html": "<figure><img id='10' style='font-size:14px' alt=\"F1 Ya Ya\nYa\na\nX\n力 B\nRecall B\nX YB YB 2 3\n(a) Single Aspect (b) Multi-Aspect (c) GPTScore\nx : evaluated text a, � : evaluation aspect : training-free\nI* : instruction Y*: score f: evaluator fine-tuning based\" data-coord=\"top-left:(1278,865); bottom-right:(2245,1247)\" /></figure>",
            "id": 10,
            "page": 1,
            "text": "F1 Ya Ya Ya a X 力 B Recall B X YB YB 2 3 (a) Single Aspect (b) Multi-Aspect (c) GPTScore x : evaluated text a, � : evaluation aspect : training-free I* : instruction Y*: score f: evaluator fine-tuning based"
        },
        {
            "bounding_box": [
                {
                    "x": 1359,
                    "y": 1253
                },
                {
                    "x": 2164,
                    "y": 1253
                },
                {
                    "x": 2164,
                    "y": 1301
                },
                {
                    "x": 1359,
                    "y": 1301
                }
            ],
            "category": "caption",
            "html": "<br><caption id='11' style='font-size:16px'>Figure 1. An overview of text evaluation approaches.</caption>",
            "id": 11,
            "page": 1,
            "text": "Figure 1. An overview of text evaluation approaches."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1345
                },
                {
                    "x": 2269,
                    "y": 1345
                },
                {
                    "x": 2269,
                    "y": 2246
                },
                {
                    "x": 1270,
                    "y": 2246
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:18px'>(a) Existing studies evaluate text quality with limited aspects<br>(e.g., semantic equivalence, fluency) (Fig. 1-(a)), which are<br>usually customized prohibitively, making it harder for users<br>to evaluate aspects as they need (Freitag et al., 2021). (b)<br>A handful of studies have examined multi-aspect evalua-<br>tion (Yuan et al., 2021; Scialom et al., 2021 ; Zhong et al.,<br>2022) but have not given adequate attention to the defi-<br>nition of the evaluation aspect and the latent relationship<br>among them. Instead, the evaluation of an aspect is ei-<br>ther empirically bound with metric variants (Yuan et al.,<br>2021) or learned by supervised signals (Zhong et al., 2022).<br>(c) Recently proposed evaluation methods (Mehri & Eske-<br>nazi, 2020; Rei et al., 2020; Li et al., 2021; Zhong et al.,<br>2022) usually necessitate a complicated training procedure<br>or costly manual annotation of samples (Fig. 1-(a,b)), which<br>makes it hard to use these methods in industrial settings due<br>to the amount of time needed for annotation and training to<br>accommodate a new evaluation demand from the user.</p>",
            "id": 12,
            "page": 1,
            "text": "(a) Existing studies evaluate text quality with limited aspects (e.g., semantic equivalence, fluency) (Fig. 1-(a)), which are usually customized prohibitively, making it harder for users to evaluate aspects as they need (Freitag , 2021). (b) A handful of studies have examined multi-aspect evaluation (Yuan , 2021; Scialom , 2021 ; Zhong , 2022) but have not given adequate attention to the definition of the evaluation aspect and the latent relationship among them. Instead, the evaluation of an aspect is either empirically bound with metric variants (Yuan , 2021) or learned by supervised signals (Zhong , 2022). (c) Recently proposed evaluation methods (Mehri & Eskenazi, 2020; Rei , 2020; Li , 2021; Zhong , 2022) usually necessitate a complicated training procedure or costly manual annotation of samples (Fig. 1-(a,b)), which makes it hard to use these methods in industrial settings due to the amount of time needed for annotation and training to accommodate a new evaluation demand from the user."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2269
                },
                {
                    "x": 2266,
                    "y": 2269
                },
                {
                    "x": 2266,
                    "y": 2720
                },
                {
                    "x": 1272,
                    "y": 2720
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:18px'>In this paper, we demonstrated the talent of the super large<br>pre-trained language model (e.g., GPT-3) in achieving multi-<br>aspect, customized, and training-free evaluation (Fig. 1-<br>(c)). In essence, it skillfully uses the pre-trained model's<br>zero-shot instruction (Chung et al., 2022), and in-context<br>learning (Brown et al., 2020; Min et al., 2022) ability to deal<br>with complex and ever-changing evaluation needs SO as to<br>solve multiple evaluation challenges that have been plagued<br>for many years at the same time.</p>",
            "id": 13,
            "page": 1,
            "text": "In this paper, we demonstrated the talent of the super large pre-trained language model (e.g., GPT-3) in achieving multiaspect, customized, and training-free evaluation (Fig. 1(c)). In essence, it skillfully uses the pre-trained model's zero-shot instruction (Chung , 2022), and in-context learning (Brown , 2020; Min , 2022) ability to deal with complex and ever-changing evaluation needs SO as to solve multiple evaluation challenges that have been plagued for many years at the same time."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2743
                },
                {
                    "x": 2268,
                    "y": 2743
                },
                {
                    "x": 2268,
                    "y": 2994
                },
                {
                    "x": 1273,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:16px'>Specifically, given a text generated from a certain context,<br>and desirable evaluation aspects (e.g., fluency), the high-<br>level idea of the proposed framework is that the higher-<br>quality text of a certain aspect will be more likely generated<br>than unqualified ones based on the given context, where the</p>",
            "id": 14,
            "page": 1,
            "text": "Specifically, given a text generated from a certain context, and desirable evaluation aspects (e.g., fluency), the highlevel idea of the proposed framework is that the higherquality text of a certain aspect will be more likely generated than unqualified ones based on the given context, where the"
        },
        {
            "bounding_box": [
                {
                    "x": 59,
                    "y": 886
                },
                {
                    "x": 149,
                    "y": 886
                },
                {
                    "x": 149,
                    "y": 2333
                },
                {
                    "x": 59,
                    "y": 2333
                }
            ],
            "category": "footer",
            "html": "<br><footer id='15' style='font-size:14px'>2023<br>Feb<br>13<br>[cs.CL]<br>arXiv:2302.04166v2</footer>",
            "id": 15,
            "page": 1,
            "text": "2023 Feb 13 [cs.CL] arXiv:2302.04166v2"
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='16' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 16,
            "page": 2,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 266,
                    "y": 275
                },
                {
                    "x": 2229,
                    "y": 275
                },
                {
                    "x": 2229,
                    "y": 919
                },
                {
                    "x": 266,
                    "y": 919
                }
            ],
            "category": "figure",
            "html": "<figure><img id='17' style='font-size:14px' alt=\"Task Specification Evaluation Sample\nINF\nGenerate a summary Text: · ··\nTemplate Summ:\nfor the following text.\nREL GPTScore\n{Task Specification}\nGenerate a relevant summary\nAspect Definition {Aspect_ Definition]}\nwith consistent details for the\nText: {Text}\nDemonstrated Samples following text.\nTl;dr: {Summ}\nREL The details pr- Text: {Text} Tl;dr: {Summ} Demo INF REL\nINF\novided by the genera- Text: {Text} Tl;dr: {Summ}\nREL\nted text are consistent\n0.8 0.9\nwith the details in the Sample\nText: {Text} Tl;dr: {Summ}\nsource text.\nText: {Text} Tl;dr: {Summ}\nText:\nSumm:\nINF\nEvaluation Protocol Input Scoring\" data-coord=\"top-left:(266,275); bottom-right:(2229,919)\" /></figure>",
            "id": 17,
            "page": 2,
            "text": "Task Specification Evaluation Sample INF Generate a summary Text: · ·· Template Summ: for the following text. REL GPTScore {Task Specification} Generate a relevant summary Aspect Definition {Aspect_ Definition]} with consistent details for the Text: {Text} Demonstrated Samples following text. Tl;dr: {Summ} REL The details pr- Text: {Text} Tl;dr: {Summ} Demo INF REL INF ovided by the genera- Text: {Text} Tl;dr: {Summ} REL ted text are consistent 0.8 0.9 with the details in the Sample Text: {Text} Tl;dr: {Summ} source text. Text: {Text} Tl;dr: {Summ} Text: Summ: INF Evaluation Protocol Input Scoring"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 950
                },
                {
                    "x": 2263,
                    "y": 950
                },
                {
                    "x": 2263,
                    "y": 1046
                },
                {
                    "x": 223,
                    "y": 1046
                }
            ],
            "category": "caption",
            "html": "<caption id='18' style='font-size:14px'>Figure 2. The framework of GPTSCORE. We include two evaluation aspects relevance (REL) and informative (INF) in this figure and use<br>the evaluation of relevance (REL) of the text summarization task to exemplify our framework.</caption>",
            "id": 18,
            "page": 2,
            "text": "Figure 2. The framework of GPTSCORE. We include two evaluation aspects relevance (REL) and informative (INF) in this figure and use the evaluation of relevance (REL) of the text summarization task to exemplify our framework."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1121
                },
                {
                    "x": 1218,
                    "y": 1121
                },
                {
                    "x": 1218,
                    "y": 2126
                },
                {
                    "x": 223,
                    "y": 2126
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:18px'>\"likely\" can be measured by the conditional generation prob-<br>ability. As illustrated in Fig. 2, to capture users' true desires,<br>an evaluation protocol will be initially established based on<br>(a) the task specification, which typically outlines how the<br>textis generated (e.g., generate a response for a human based<br>on the conversation.) (b) aspect definition that documents<br>the details of desirable evaluation aspects (e.g., the response<br>should be intuitive to understand). Subsequently, each eval-<br>uation sample will be presented with the evaluated protocol<br>with optionally moderate exemplar samples, which could<br>facilitate the model's learning. Lastly, a large generative<br>pre-trained model will be used to calculate how likely the<br>text could be generated based on the above evaluation pro-<br>tocol, thus giving rise to our model's name: GPTSCORE.<br>Given the plethora of pre-trained models, we instantiate our<br>framework with different backbones: GPT2 (Radford et al.,<br>2019), OPT (Zhang et al., 2022b), FLAN (Chung et al.,<br>2022), and GPT3 (instruction-based (Ouyang et al., 2022))<br>due to their superior capacity for zero-shot instruction and<br>their aptitude for in-context learning.</p>",
            "id": 19,
            "page": 2,
            "text": "\"likely\" can be measured by the conditional generation probability. As illustrated in Fig. 2, to capture users' true desires, an evaluation protocol will be initially established based on (a) the task specification, which typically outlines how the textis generated (e.g., generate a response for a human based on the conversation.) (b) aspect definition that documents the details of desirable evaluation aspects (e.g., the response should be intuitive to understand). Subsequently, each evaluation sample will be presented with the evaluated protocol with optionally moderate exemplar samples, which could facilitate the model's learning. Lastly, a large generative pre-trained model will be used to calculate how likely the text could be generated based on the above evaluation protocol, thus giving rise to our model's name: GPTSCORE. Given the plethora of pre-trained models, we instantiate our framework with different backbones: GPT2 (Radford , 2019), OPT (Zhang , 2022b), FLAN (Chung , 2022), and GPT3 (instruction-based (Ouyang , 2022)) due to their superior capacity for zero-shot instruction and their aptitude for in-context learning."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2146
                },
                {
                    "x": 1219,
                    "y": 2146
                },
                {
                    "x": 1219,
                    "y": 2947
                },
                {
                    "x": 222,
                    "y": 2947
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:16px'>Experimentally, we ran through almost all common nat-<br>ural language generation tasks in NLP, and the results<br>showed the power of this new paradigm. The main ob-<br>servations are listed as follows: (1) Evaluating texts with<br>generative pre-training models can be more reliable when<br>instructed by the definition of task and aspect, provid-<br>ing a degree of flexibility to accommodate various eval-<br>uation criteria. Furthermore, incorporating exemplified<br>samples with in-context learning will further enhance the<br>process. (2) Different evaluation aspects exhibit certain<br>correlations. Combining definitions with other highly<br>correlated aspects can improve evaluation performance.<br>(3) The performance of GPT3-text-davinci-003,<br>which is tuned based on human feedback, is inferior to<br>GPT3-text-davinci -001 in the majority of the eval-<br>uation settings, necessitating deep explorations on the work-</p>",
            "id": 20,
            "page": 2,
            "text": "Experimentally, we ran through almost all common natural language generation tasks in NLP, and the results showed the power of this new paradigm. The main observations are listed as follows: (1) Evaluating texts with generative pre-training models can be more reliable when instructed by the definition of task and aspect, providing a degree of flexibility to accommodate various evaluation criteria. Furthermore, incorporating exemplified samples with in-context learning will further enhance the process. (2) Different evaluation aspects exhibit certain correlations. Combining definitions with other highly correlated aspects can improve evaluation performance. (3) The performance of GPT3-text-davinci-003, which is tuned based on human feedback, is inferior to GPT3-text-davinci -001 in the majority of the evaluation settings, necessitating deep explorations on the work-"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1126
                },
                {
                    "x": 2267,
                    "y": 1126
                },
                {
                    "x": 2267,
                    "y": 1228
                },
                {
                    "x": 1272,
                    "y": 1228
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:20px'>ing mechanism of human feedback-based instruction learn-<br>ing (e.g., when it will fail).</p>",
            "id": 21,
            "page": 2,
            "text": "ing mechanism of human feedback-based instruction learning (e.g., when it will fail)."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1289
                },
                {
                    "x": 1621,
                    "y": 1289
                },
                {
                    "x": 1621,
                    "y": 1342
                },
                {
                    "x": 1274,
                    "y": 1342
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:22px'>2. Preliminaries</p>",
            "id": 22,
            "page": 2,
            "text": "2. Preliminaries"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1376
                },
                {
                    "x": 1639,
                    "y": 1376
                },
                {
                    "x": 1639,
                    "y": 1423
                },
                {
                    "x": 1273,
                    "y": 1423
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:16px'>2.1. Text Evaluation</p>",
            "id": 23,
            "page": 2,
            "text": "2.1. Text Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1453
                },
                {
                    "x": 2267,
                    "y": 1453
                },
                {
                    "x": 2267,
                    "y": 1804
                },
                {
                    "x": 1272,
                    "y": 1804
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>Text evaluation aims to assess the quality of hypothesis<br>text h in terms of certain aspect a (e.g., fluency), which is<br>either measured manually with different protocols (Nenkova<br>& Passonneau, 2004; Bhandari et al., 2020; Fabbri et al.,<br>2021; Liu et al., 2022) or quantified by diverse automated<br>metrics (Lin, 2004; Papineni et al., 2002; Zhao et al., 2019;<br>Zhang et al., 2020; Yuan et al., 2021).</p>",
            "id": 24,
            "page": 2,
            "text": "Text evaluation aims to assess the quality of hypothesis text h in terms of certain aspect a (e.g., fluency), which is either measured manually with different protocols (Nenkova & Passonneau, 2004; Bhandari , 2020; Fabbri , 2021; Liu , 2022) or quantified by diverse automated metrics (Lin, 2004; Papineni , 2002; Zhao , 2019; Zhang , 2020; Yuan , 2021)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1963
                },
                {
                    "x": 2264,
                    "y": 1963
                },
                {
                    "x": 2264,
                    "y": 2364
                },
                {
                    "x": 1272,
                    "y": 2364
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>where (1) h represents the text to be evaluated (hypothesis<br>text, e.g., generated summary in text summarization task).<br>(2) a denotes the evaluation aspect (e.g., fluency). (3) S is a<br>collection of additional texts that are optionally used based<br>on different scenarios. For example, it could be a source<br>document or a reference summary in the text summarization<br>task. (4) Function f(·) could be instantiated as a human<br>evaluation process or automated evaluation metrics.</p>",
            "id": 25,
            "page": 2,
            "text": "where (1) h represents the text to be evaluated (hypothesis text, e.g., generated summary in text summarization task). (2) a denotes the evaluation aspect (e.g., fluency). (3) S is a collection of additional texts that are optionally used based on different scenarios. For example, it could be a source document or a reference summary in the text summarization task. (4) Function f(·) could be instantiated as a human evaluation process or automated evaluation metrics."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2417
                },
                {
                    "x": 1655,
                    "y": 2417
                },
                {
                    "x": 1655,
                    "y": 2464
                },
                {
                    "x": 1272,
                    "y": 2464
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:16px'>2.2. Meta Evaluation</p>",
            "id": 26,
            "page": 2,
            "text": "2.2. Meta Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2493
                },
                {
                    "x": 2268,
                    "y": 2493
                },
                {
                    "x": 2268,
                    "y": 2992
                },
                {
                    "x": 1272,
                    "y": 2992
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:18px'>Meta evaluation aims to evaluate the reliability of auto-<br>mated metrics by calculating how well automated scores<br>(Yauto) correlate with human judgment (Yhuman) using correla-<br>tion functions g(Yauto, Yhuman) such as spearman correlation.<br>In this work, we adopt two widely-used correlation mea-<br>sures: (1) Spearman correlation (p) (Zar, 2005) measures<br>the monotonic relationship between two variables based on<br>their ranked values. (2) Pearson correlation (r) (Mukaka,<br>2012) measures the linear relationship based on the raw data<br>values of two variables.</p>",
            "id": 27,
            "page": 2,
            "text": "Meta evaluation aims to evaluate the reliability of automated metrics by calculating how well automated scores (Yauto) correlate with human judgment (Yhuman) using correlation functions g(Yauto, Yhuman) such as spearman correlation. In this work, we adopt two widely-used correlation measures: (1) Spearman correlation (p) (Zar, 2005) measures the monotonic relationship between two variables based on their ranked values. (2) Pearson correlation (r) (Mukaka, 2012) measures the linear relationship based on the raw data values of two variables."
        },
        {
            "bounding_box": [
                {
                    "x": 958,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 958,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='28' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 28,
            "page": 3,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 283
                },
                {
                    "x": 659,
                    "y": 283
                },
                {
                    "x": 659,
                    "y": 335
                },
                {
                    "x": 223,
                    "y": 335
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:20px'>2.3. Evaluation Strategy</p>",
            "id": 29,
            "page": 3,
            "text": "2.3. Evaluation Strategy"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 361
                },
                {
                    "x": 1217,
                    "y": 361
                },
                {
                    "x": 1217,
                    "y": 811
                },
                {
                    "x": 224,
                    "y": 811
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>Evaluation strategies define different aggregation methods<br>when we calculate the correlation scores. Specifically, sup-<br>pose that for each source text Si, i E [1, 2, · · · n] (e.g.,<br>,<br>documents in text summarization task or dialogue histories<br>for dialogue generation task), there are J system outputs<br>hi,j, where j E [1, 2, · · · J]. fauto is an automatic scoring<br>,<br>function (e.g., ROUGE (Lin, 2004)), and fhuman is the gold<br>human scoring function. For a given evaluation aspect a,<br>the meta-evaluation metric F can be formulated as follows.</p>",
            "id": 30,
            "page": 3,
            "text": "Evaluation strategies define different aggregation methods when we calculate the correlation scores. Specifically, suppose that for each source text Si, i E [1, 2, · · · n] (e.g., , documents in text summarization task or dialogue histories for dialogue generation task), there are J system outputs hi,j, where j E [1, 2, · · · J]. fauto is an automatic scoring , function (e.g., ROUGE (Lin, 2004)), and fhuman is the gold human scoring function. For a given evaluation aspect a, the meta-evaluation metric F can be formulated as follows."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 866
                },
                {
                    "x": 1213,
                    "y": 866
                },
                {
                    "x": 1213,
                    "y": 1018
                },
                {
                    "x": 224,
                    "y": 1018
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>Sample-level defines that a correlation value is calculated<br>for each sample separately based on outputs of multiple<br>systems, then averaged across all samples.</p>",
            "id": 31,
            "page": 3,
            "text": "Sample-level defines that a correlation value is calculated for each sample separately based on outputs of multiple systems, then averaged across all samples."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1317
                },
                {
                    "x": 1214,
                    "y": 1317
                },
                {
                    "x": 1214,
                    "y": 1410
                },
                {
                    "x": 223,
                    "y": 1410
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>where g can be instantiated as Spearman or Pearson correla-<br>tion.</p>",
            "id": 32,
            "page": 3,
            "text": "where g can be instantiated as Spearman or Pearson correlation."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1468
                },
                {
                    "x": 1216,
                    "y": 1468
                },
                {
                    "x": 1216,
                    "y": 1568
                },
                {
                    "x": 224,
                    "y": 1568
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>Dataset-level indicates that the correlation value is calcu-<br>lated on system outputs of all n samples.</p>",
            "id": 33,
            "page": 3,
            "text": "Dataset-level indicates that the correlation value is calculated on system outputs of all n samples."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1818
                },
                {
                    "x": 1215,
                    "y": 1818
                },
                {
                    "x": 1215,
                    "y": 2118
                },
                {
                    "x": 222,
                    "y": 2118
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:18px'>In this work, we select the evaluation strategy for a specific<br>task based on previous works (Yuan et al., 2021; Zhang et al.,<br>2022a). We use the sample-level evaluation strategy for text<br>summarization, data-to-text, and machine translation tasks.<br>For the dialogue response generation task, the dataset-level<br>evaluation strategy is utilized.</p>",
            "id": 34,
            "page": 3,
            "text": "In this work, we select the evaluation strategy for a specific task based on previous works (Yuan , 2021; Zhang , 2022a). We use the sample-level evaluation strategy for text summarization, data-to-text, and machine translation tasks. For the dialogue response generation task, the dataset-level evaluation strategy is utilized."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2178
                },
                {
                    "x": 550,
                    "y": 2178
                },
                {
                    "x": 550,
                    "y": 2234
                },
                {
                    "x": 224,
                    "y": 2234
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:22px'>3. GPTSCORE</p>",
            "id": 35,
            "page": 3,
            "text": "3. GPTSCORE"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2266
                },
                {
                    "x": 1042,
                    "y": 2266
                },
                {
                    "x": 1042,
                    "y": 2316
                },
                {
                    "x": 223,
                    "y": 2316
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:20px'>3.1. Generative Pre-trained Language Models</p>",
            "id": 36,
            "page": 3,
            "text": "3.1. Generative Pre-trained Language Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2344
                },
                {
                    "x": 1218,
                    "y": 2344
                },
                {
                    "x": 1218,
                    "y": 2996
                },
                {
                    "x": 223,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:20px'>Existing pre-trained language models could be classified<br>into the following three categories: (a) encoder-only models<br>(e.g., BERT (Devlin et al., 2019), RoBerta (Liu et al., 2019))<br>that encode inputs with bidirectional attention; (b) encoder-<br>decoder models (e.g., BART (Lewis et al., 2020), T5 (Raffel<br>et al., 2020)) that encode inputs with bidirectional atten-<br>tion and generate outputs autoregressively; (c) decoder-only<br>models (e.g., GPT2 (Radford et al., 2019), GPT3 (Brown<br>et al., 2020), PaLM (Chowdhery et al., 2022)) that gen-<br>erate the entire text sequence autoregressively, where pre-<br>trained models with decoding abilities (b, c) have caught<br>much attention since they show impressive performance on<br>zero-shot instruction and in-context learning. Specifically,</p>",
            "id": 37,
            "page": 3,
            "text": "Existing pre-trained language models could be classified into the following three categories: (a) encoder-only models (e.g., BERT (Devlin , 2019), RoBerta (Liu , 2019)) that encode inputs with bidirectional attention; (b) encoderdecoder models (e.g., BART (Lewis , 2020), T5 (Raffel , 2020)) that encode inputs with bidirectional attention and generate outputs autoregressively; (c) decoder-only models (e.g., GPT2 (Radford , 2019), GPT3 (Brown , 2020), PaLM (Chowdhery , 2022)) that generate the entire text sequence autoregressively, where pretrained models with decoding abilities (b, c) have caught much attention since they show impressive performance on zero-shot instruction and in-context learning. Specifically,"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 283
                },
                {
                    "x": 2266,
                    "y": 283
                },
                {
                    "x": 2266,
                    "y": 485
                },
                {
                    "x": 1272,
                    "y": 485
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:14px'>given a prompt text x = {x1, x2, · · · xn}, a generative<br>,<br>pre-training language model can generate a textual continu-<br>ation y = {y1, Y2, · · · , ym} with the following generation<br>probability:</p>",
            "id": 38,
            "page": 3,
            "text": "given a prompt text x = {x1, x2, · · · xn}, a generative , pre-training language model can generate a textual continuation y = {y1, Y2, · · · , ym} with the following generation probability:"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 660
                },
                {
                    "x": 2267,
                    "y": 660
                },
                {
                    "x": 2267,
                    "y": 1261
                },
                {
                    "x": 1270,
                    "y": 1261
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>Emergent Ability Recent works progressively reveal a va-<br>riety of emergent abilities of generative pre-trained lan-<br>guage models with appropriate tuning or prompting meth-<br>ods, such as in-context learning (Min et al., 2022), chain-<br>of-thought reasoning (Wei et al., 2022), and zero-shot in-<br>struction (Ouyang et al., 2022). One core commonality of<br>these abilities is to allow for handling customized require-<br>ments with a few or even zero annotated examples. It's<br>the appearance of these abilities that allows us to re-invent<br>a new way for text evaluation-evaluating from the textual<br>description, which can achieve customizable, multi-faceted,<br>and train-free evaluation.</p>",
            "id": 39,
            "page": 3,
            "text": "Emergent Ability Recent works progressively reveal a variety of emergent abilities of generative pre-trained language models with appropriate tuning or prompting methods, such as in-context learning (Min , 2022), chainof-thought reasoning (Wei , 2022), and zero-shot instruction (Ouyang , 2022). One core commonality of these abilities is to allow for handling customized requirements with a few or even zero annotated examples. It's the appearance of these abilities that allows us to re-invent a new way for text evaluation-evaluating from the textual description, which can achieve customizable, multi-faceted, and train-free evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1312
                },
                {
                    "x": 2100,
                    "y": 1312
                },
                {
                    "x": 2100,
                    "y": 1362
                },
                {
                    "x": 1272,
                    "y": 1362
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:20px'>3.2. Generative Pretraining Score (GPTScore)</p>",
            "id": 40,
            "page": 3,
            "text": "3.2. Generative Pretraining Score (GPTScore)"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1392
                },
                {
                    "x": 2267,
                    "y": 1392
                },
                {
                    "x": 2267,
                    "y": 1840
                },
                {
                    "x": 1272,
                    "y": 1840
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>The core idea of GPTSCORE is that a generative pre-training<br>model will assign a higher probability of high-quality gen-<br>erated text following a given instruction and context. In<br>our method, the instruction is composed of the task descrip-<br>tion d and the aspect definition a. Specifically, suppose<br>that the text to be evaluated is h = {h1, h2, · · · hm}, the<br>,<br>context information is S (e.g., source text or reference text),<br>then GPTSCORE is defined as the following conditional<br>probability:</p>",
            "id": 41,
            "page": 3,
            "text": "The core idea of GPTSCORE is that a generative pre-training model will assign a higher probability of high-quality generated text following a given instruction and context. In our method, the instruction is composed of the task description d and the aspect definition a. Specifically, suppose that the text to be evaluated is h = {h1, h2, · · · hm}, the , context information is S (e.g., source text or reference text), then GPTSCORE is defined as the following conditional probability:"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2017
                },
                {
                    "x": 2265,
                    "y": 2017
                },
                {
                    "x": 2265,
                    "y": 2266
                },
                {
                    "x": 1273,
                    "y": 2266
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>where Wt is the weight of the token at position t. In our<br>work, we treat each token equally. T(·) is a prompt tem-<br>plate that defines the evaluation protocol, which is usually<br>task-dependent and specified manually through prompt en-<br>gineering.</p>",
            "id": 42,
            "page": 3,
            "text": "where Wt is the weight of the token at position t. In our work, we treat each token equally. T(·) is a prompt template that defines the evaluation protocol, which is usually task-dependent and specified manually through prompt engineering."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2290
                },
                {
                    "x": 2266,
                    "y": 2290
                },
                {
                    "x": 2266,
                    "y": 2541
                },
                {
                    "x": 1272,
                    "y": 2541
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>Few-shot with Demonstration The generative pre-<br>trained language model can better perform tasks when pre-<br>fixed with a few annotated samples (i.e., demonstrations).<br>Our proposed framework is flexible in supporting this by<br>extending the prompt template T with demonstrations.</p>",
            "id": 43,
            "page": 3,
            "text": "Few-shot with Demonstration The generative pretrained language model can better perform tasks when prefixed with a few annotated samples (i.e., demonstrations). Our proposed framework is flexible in supporting this by extending the prompt template T with demonstrations."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2564
                },
                {
                    "x": 2265,
                    "y": 2564
                },
                {
                    "x": 2265,
                    "y": 2916
                },
                {
                    "x": 1272,
                    "y": 2916
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>Choice of Prompt Template Prompt templates define<br>how task description, aspect definition, and context are<br>organized. Minging desirable prompts itself is a non-trivial<br>task and there are extensive research works there (Liu<br>et al., 2021; Fu et al., 2022). In this work, for the<br>GPT3-based model, we opt for prompts that are officially<br>provided by OpenAI.2 For instruction-based pre-trained</p>",
            "id": 44,
            "page": 3,
            "text": "Choice of Prompt Template Prompt templates define how task description, aspect definition, and context are organized. Minging desirable prompts itself is a non-trivial task and there are extensive research works there (Liu , 2021; Fu , 2022). In this work, for the GPT3-based model, we opt for prompts that are officially provided by OpenAI.2 For instruction-based pre-trained"
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1527,
                    "y": 190
                },
                {
                    "x": 1527,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='45' style='font-size:16px'>GPTScore: Evaluate as You Desire</header>",
            "id": 45,
            "page": 4,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 334,
                    "y": 281
                },
                {
                    "x": 2114,
                    "y": 281
                },
                {
                    "x": 2114,
                    "y": 1121
                },
                {
                    "x": 334,
                    "y": 1121
                }
            ],
            "category": "table",
            "html": "<table id='46' style='font-size:14px'><tr><td>Aspect</td><td>Task</td><td>Definition</td></tr><tr><td>Semantic Coverage (COV)</td><td>Summ</td><td>How many semantic content units from the reference text are covered by the generated text?</td></tr><tr><td>Factuality (FAC)</td><td>Summ</td><td>Does the generated text preserve the factual statements of the source text?</td></tr><tr><td>Consistency (CON)</td><td>Summ, Diag</td><td>Is the generated text consistent in the information it provides?</td></tr><tr><td>Informativeness (INF)</td><td>Summ, D2T, Diag</td><td>How well does the generated text capture the key ideas of its source text?</td></tr><tr><td>Coherence (COH)</td><td>Summ, Diag</td><td>How much does the generated text make sense?</td></tr><tr><td>Relevance (REL)</td><td>Diag, Summ, D2T</td><td>How well is the generated text relevant to its source text?</td></tr><tr><td>Fluency (FLU)</td><td>Diag, Summ, D2T, MT</td><td>Is the generated text well-written and grammatical?</td></tr><tr><td>Accuracy (ACC)</td><td>MT</td><td>Are there inaccuracies, missing, or unfactual content in the generated text?</td></tr><tr><td>Multidimensional Quality Metrics (MQM)</td><td>MT</td><td>How is the overall quality of the generated text?</td></tr><tr><td>Interest (INT)</td><td>Diag</td><td>Is the generated text interesting?</td></tr><tr><td>Engagement (ENG)</td><td>Diag</td><td>Is the generated text engaging?</td></tr><tr><td>Specific (SPE)</td><td>Diag</td><td>Is the generated text generic or specific to the source text?</td></tr><tr><td>Correctness (COR)</td><td>Diag</td><td>Is the generated text correct or was there a misunderstanding of the source text?</td></tr><tr><td>Semantically appropriate (SEM)</td><td>Diag</td><td>Is the generated text semantically appropriate?</td></tr><tr><td>Understandability (UND)</td><td>Diag</td><td>Is the generated text understandable?</td></tr><tr><td>Error Recovery (ERR)</td><td>Diag</td><td>Is the system able to recover from errors that it makes?</td></tr><tr><td>Diversity (DIV)</td><td>Diag</td><td>Is there diversity in the system responses?</td></tr><tr><td>Depth (DEP)</td><td>Diag</td><td>Does the system discuss topics in depth?</td></tr><tr><td>Likeability (LIK)</td><td>Diag</td><td>Does the system display a likeable personality?</td></tr><tr><td>Flexibility (FLE)</td><td>Diag</td><td>Is the system flexible and adaptable to the user and their interests?</td></tr><tr><td>Inquisitiveness (INQ)</td><td>Diag</td><td>Is the system inquisitive throughout the conversation?</td></tr></table>",
            "id": 46,
            "page": 4,
            "text": "Aspect Task Definition  Semantic Coverage (COV) Summ How many semantic content units from the reference text are covered by the generated text?  Factuality (FAC) Summ Does the generated text preserve the factual statements of the source text?  Consistency (CON) Summ, Diag Is the generated text consistent in the information it provides?  Informativeness (INF) Summ, D2T, Diag How well does the generated text capture the key ideas of its source text?  Coherence (COH) Summ, Diag How much does the generated text make sense?  Relevance (REL) Diag, Summ, D2T How well is the generated text relevant to its source text?  Fluency (FLU) Diag, Summ, D2T, MT Is the generated text well-written and grammatical?  Accuracy (ACC) MT Are there inaccuracies, missing, or unfactual content in the generated text?  Multidimensional Quality Metrics (MQM) MT How is the overall quality of the generated text?  Interest (INT) Diag Is the generated text interesting?  Engagement (ENG) Diag Is the generated text engaging?  Specific (SPE) Diag Is the generated text generic or specific to the source text?  Correctness (COR) Diag Is the generated text correct or was there a misunderstanding of the source text?  Semantically appropriate (SEM) Diag Is the generated text semantically appropriate?  Understandability (UND) Diag Is the generated text understandable?  Error Recovery (ERR) Diag Is the system able to recover from errors that it makes?  Diversity (DIV) Diag Is there diversity in the system responses?  Depth (DEP) Diag Does the system discuss topics in depth?  Likeability (LIK) Diag Does the system display a likeable personality?  Flexibility (FLE) Diag Is the system flexible and adaptable to the user and their interests?  Inquisitiveness (INQ) Diag"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1134
                },
                {
                    "x": 2261,
                    "y": 1134
                },
                {
                    "x": 2261,
                    "y": 1229
                },
                {
                    "x": 225,
                    "y": 1229
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:16px'>Table 1. The definition of aspects evaluated in this work. Semantic App. denotes semantically appropriate aspect. Diag, Summ, D2T, and<br>MT denote the dialogue response generation, text summarization, data to text and machine translation, respectively.</p>",
            "id": 47,
            "page": 4,
            "text": "Table 1. The definition of aspects evaluated in this work. Semantic App. denotes semantically appropriate aspect. Diag, Summ, D2T, and MT denote the dialogue response generation, text summarization, data to text and machine translation, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1306
                },
                {
                    "x": 1216,
                    "y": 1306
                },
                {
                    "x": 1216,
                    "y": 2110
                },
                {
                    "x": 224,
                    "y": 2110
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>models, we use prompts from NaturalInstruction (Wang<br>et al., 2022) since it's the main training source for those<br>instruction-based pre-train models. Taking the evaluation of<br>the fluency of the text summarization task as an example,<br>based on the prompt provided by OpenAI,3 the task<br>the definition<br>prompt is \"{Text} Tl;dr {Summary}\", ,<br>of fluency is \"Is the generated text well-written and<br>grammatical?\" (in Tab. 1), and then the final prompt tem-<br>plate is \"Generate a fluent and grammat ical<br>summary for the following text : { Text }<br>T1 ; dr { Summary }\", where demonstrations could be<br>introduced by repeating instantiating { Text } T1; dr<br>{ Summary }\" In Appendix D, we list the prompts for<br>various aspects of all tasks studied in this work and leave a<br>more comprehensive exploration on prompt engineering as<br>a future work.</p>",
            "id": 48,
            "page": 4,
            "text": "models, we use prompts from NaturalInstruction (Wang , 2022) since it's the main training source for those instruction-based pre-train models. Taking the evaluation of the fluency of the text summarization task as an example, based on the prompt provided by OpenAI,3 the task the definition prompt is \"{Text} Tl;dr {Summary}\", , of fluency is \"Is the generated text well-written and grammatical?\" (in Tab. 1), and then the final prompt template is \"Generate a fluent and grammat ical summary for the following text : { Text } T1 ; dr { Summary }\", where demonstrations could be introduced by repeating instantiating { Text } T1; dr { Summary }\" In Appendix D, we list the prompts for various aspects of all tasks studied in this work and leave a more comprehensive exploration on prompt engineering as a future work."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2128
                },
                {
                    "x": 1216,
                    "y": 2128
                },
                {
                    "x": 1216,
                    "y": 2683
                },
                {
                    "x": 223,
                    "y": 2683
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:18px'>Selection of Scoring Dimension GPTSCORE exhibits<br>different variants in terms of diverse choices of texts being<br>calculated. For example, given a generated hypothesis, we<br>can calculate GPTSCORE either based on the source text<br>(i.e., src->hypo, p(hypo|src)) or based on the gold reference<br>(i.e., ref->hypo, p(hypo|ref)). In this paper, the criteria for<br>choosing GPTSCORE variants are mainly designed to align<br>the protocol of human judgments (Liu et al., 2022) that are<br>used to evaluate the reliability of automated metrics. We<br>will detail this based on different human judgment datasets<br>in the experiment section.</p>",
            "id": 49,
            "page": 4,
            "text": "Selection of Scoring Dimension GPTSCORE exhibits different variants in terms of diverse choices of texts being calculated. For example, given a generated hypothesis, we can calculate GPTSCORE either based on the source text (i.e., src->hypo, p(hypo|src)) or based on the gold reference (i.e., ref->hypo, p(hypo|ref)). In this paper, the criteria for choosing GPTSCORE variants are mainly designed to align the protocol of human judgments (Liu , 2022) that are used to evaluate the reliability of automated metrics. We will detail this based on different human judgment datasets in the experiment section."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2707
                },
                {
                    "x": 1041,
                    "y": 2707
                },
                {
                    "x": 1041,
                    "y": 2802
                },
                {
                    "x": 224,
                    "y": 2802
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:14px'>3https : / /beta · openai · com/ examples /<br>default-tldr-summary</p>",
            "id": 50,
            "page": 4,
            "text": "3https : / /beta · openai · com/ examples / default-tldr-summary"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1303
                },
                {
                    "x": 1808,
                    "y": 1303
                },
                {
                    "x": 1808,
                    "y": 1362
                },
                {
                    "x": 1273,
                    "y": 1362
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:22px'>4. Experimental Settings</p>",
            "id": 51,
            "page": 4,
            "text": "4. Experimental Settings"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1391
                },
                {
                    "x": 1862,
                    "y": 1391
                },
                {
                    "x": 1862,
                    "y": 1442
                },
                {
                    "x": 1272,
                    "y": 1442
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:20px'>4.1. Tasks, Datasets, and Aspects</p>",
            "id": 52,
            "page": 4,
            "text": "4.1. Tasks, Datasets, and Aspects"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1467
                },
                {
                    "x": 2267,
                    "y": 1467
                },
                {
                    "x": 2267,
                    "y": 1917
                },
                {
                    "x": 1273,
                    "y": 1917
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>To achieve a comprehensive evaluation, in this paper, we<br>cover a broad range of natural language generation tasks: Di-<br>alogue Response Generation, Text Summarization, Data-to-<br>Text, and Machine Translation, which involves 37 datasets<br>and 22 evaluation aspects in total. Tab. 8 summarizes the<br>tasks, datasets, and evaluation aspects considered by each<br>dataset. The definition of different aspects can be found in<br>Tab. 1. More detailed illustrations about the datasets can be<br>found in Appendix B.</p>",
            "id": 53,
            "page": 4,
            "text": "To achieve a comprehensive evaluation, in this paper, we cover a broad range of natural language generation tasks: Dialogue Response Generation, Text Summarization, Data-toText, and Machine Translation, which involves 37 datasets and 22 evaluation aspects in total. Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset. The definition of different aspects can be found in Tab. 1. More detailed illustrations about the datasets can be found in Appendix B."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1939
                },
                {
                    "x": 2265,
                    "y": 1939
                },
                {
                    "x": 2265,
                    "y": 2842
                },
                {
                    "x": 1269,
                    "y": 2842
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:18px'>(1) Dialogue Response Generation aims to automatically<br>generate an engaging and informative response based on the<br>dialogue history. Here, we choose to use the FED (Mehri<br>& Esk�nazi, 2020) datasets and consider both turn-level<br>and dialogue-level evaluations. (2) Text Summarization is<br>a task of automatically generating informative and fluent<br>summary for a given long text. Here, we consider the fol-<br>lowing four datasets, SummEval (Bhandari et al., 2020),<br>REALSumm (Bhandari et al., 2020), NEWSROOM (Grusky<br>et al., 2018), and QAGS_XSUM (Wang et al., 2020), covering<br>10 aspects. (3) Data-to-Text aims to automatically generate<br>a fluent and factual description for a given table. Our work<br>considered BAGEL (Mairesse et al., 2010) and SFRES (Wen<br>et al., 2015) datasets. (4) Machine Translation aims to<br>translate a sentence from one language to another. We<br>consider a subdatasets of Multidimensional Quality Metrics<br>(MQM) (Freitag et al., 2021), namely, MQM-2020 (Chinese-<br>>English).</p>",
            "id": 54,
            "page": 4,
            "text": "(1) Dialogue Response Generation aims to automatically generate an engaging and informative response based on the dialogue history. Here, we choose to use the FED (Mehri & Esk�nazi, 2020) datasets and consider both turn-level and dialogue-level evaluations. (2) Text Summarization is a task of automatically generating informative and fluent summary for a given long text. Here, we consider the following four datasets, SummEval (Bhandari , 2020), REALSumm (Bhandari , 2020), NEWSROOM (Grusky , 2018), and QAGS_XSUM (Wang , 2020), covering 10 aspects. (3) Data-to-Text aims to automatically generate a fluent and factual description for a given table. Our work considered BAGEL (Mairesse , 2010) and SFRES (Wen , 2015) datasets. (4) Machine Translation aims to translate a sentence from one language to another. We consider a subdatasets of Multidimensional Quality Metrics (MQM) (Freitag , 2021), namely, MQM-2020 (Chinese>English)."
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 191
                },
                {
                    "x": 1525,
                    "y": 191
                },
                {
                    "x": 1525,
                    "y": 234
                },
                {
                    "x": 959,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<header id='55' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 55,
            "page": 5,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 283
                },
                {
                    "x": 584,
                    "y": 283
                },
                {
                    "x": 584,
                    "y": 332
                },
                {
                    "x": 223,
                    "y": 332
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>4.2. Scoring Models</p>",
            "id": 56,
            "page": 5,
            "text": "4.2. Scoring Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 356
                },
                {
                    "x": 1218,
                    "y": 356
                },
                {
                    "x": 1218,
                    "y": 1758
                },
                {
                    "x": 223,
                    "y": 1758
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>ROUGE (Lin, 2004) is a popular automatic generation<br>evaluation metric. We consider three variants ROUGE-1,<br>ROUGE-2, and ROUGE-L. PRISM (Thompson & Post,<br>2020) is a reference-based evaluation method designed<br>for machine translation with pre-trained paraphrase sys-<br>tems. BERTScore (Zhang et al., 2020) uses contextual<br>representation from BERT to calculate the similarity be-<br>tween the generated text and the reference text. Mover-<br>Score (Zhao et al., 2019) considers both contextual rep-<br>resentation and Word Mover's Distance (WMD, (Kus-<br>ner et al., 2015)) DynaEval (Zhang et al., 2021) is a<br>unified automatic evaluation framework for dialogue re-<br>sponse generation tasks on the turn level and dialogue level.<br>BARTScore (Yuan et al., 2021) is a text-scoring model<br>based on BART (Lewis et al., 2020) without fine-tuning.<br>BARTScore+CNN (Yuan et al., 2021) is based on BART<br>fine-tuned on the CNNDM dataset (Hermann et al., 2015).<br>BARTScore+CNN+Para (Yuan et al., 2021) is based on<br>BART fine-tuned on CNNDM and Paraphrase2.0 (Hu et al.,<br>2019). GPTSCORE is our evaluation method, which is<br>designed based on different pre-trained language models.<br>Specifically, we considered GPT3, OPT, FLAN-T5, and<br>GPT2 in this work. Five variants are explored for each<br>framework. For a fair comparison with the decoder-only<br>model, such as GPT3 and OPT, only four variant models of<br>GPT2 with a parameter size of at least 350M are considered.<br>Tab. 2 shows all model variants we used in this paper and<br>their number of parameters.</p>",
            "id": 57,
            "page": 5,
            "text": "ROUGE (Lin, 2004) is a popular automatic generation evaluation metric. We consider three variants ROUGE-1, ROUGE-2, and ROUGE-L. PRISM (Thompson & Post, 2020) is a reference-based evaluation method designed for machine translation with pre-trained paraphrase systems. BERTScore (Zhang , 2020) uses contextual representation from BERT to calculate the similarity between the generated text and the reference text. MoverScore (Zhao , 2019) considers both contextual representation and Word Mover's Distance (WMD, (Kusner , 2015)) DynaEval (Zhang , 2021) is a unified automatic evaluation framework for dialogue response generation tasks on the turn level and dialogue level. BARTScore (Yuan , 2021) is a text-scoring model based on BART (Lewis , 2020) without fine-tuning. BARTScore+CNN (Yuan , 2021) is based on BART fine-tuned on the CNNDM dataset (Hermann , 2015). BARTScore+CNN+Para (Yuan , 2021) is based on BART fine-tuned on CNNDM and Paraphrase2.0 (Hu , 2019). GPTSCORE is our evaluation method, which is designed based on different pre-trained language models. Specifically, we considered GPT3, OPT, FLAN-T5, and GPT2 in this work. Five variants are explored for each framework. For a fair comparison with the decoder-only model, such as GPT3 and OPT, only four variant models of GPT2 with a parameter size of at least 350M are considered. Tab. 2 shows all model variants we used in this paper and their number of parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 277,
                    "y": 1792
                },
                {
                    "x": 1146,
                    "y": 1792
                },
                {
                    "x": 1146,
                    "y": 2393
                },
                {
                    "x": 277,
                    "y": 2393
                }
            ],
            "category": "table",
            "html": "<table id='58' style='font-size:14px'><tr><td>GPT3</td><td>Param.</td><td>OPT</td><td>Param.</td></tr><tr><td>text-ada-001</td><td>350M</td><td>OPT350M</td><td>350M</td></tr><tr><td>text-babbage-001</td><td>1.3B</td><td>OPT-1.3B</td><td>1.3B</td></tr><tr><td>text-curie-001</td><td>6.7B</td><td>OPT-6.7B</td><td>6.7B</td></tr><tr><td>text-davinci-001</td><td>175B</td><td>OPT-13B</td><td>13B</td></tr><tr><td>text-davinci-003</td><td>175B</td><td>OPT-66B</td><td>66B</td></tr><tr><td>FLAN-T5</td><td>Param.</td><td>GPT2</td><td>Param.</td></tr><tr><td>FT5-small</td><td>80M</td><td>GPT2-M</td><td>355M</td></tr><tr><td>FT5-base</td><td>250M</td><td>GPT2-L</td><td>774M</td></tr><tr><td>FT5-L</td><td>770M</td><td>GPT2-XL</td><td>1.5B</td></tr><tr><td>FT5-XL</td><td>3B</td><td>GPT-J-6B</td><td>6B</td></tr><tr><td>FT5-XXL</td><td>11B</td><td></td><td></td></tr></table>",
            "id": 58,
            "page": 5,
            "text": "GPT3 Param. OPT Param.  text-ada-001 350M OPT350M 350M  text-babbage-001 1.3B OPT-1.3B 1.3B  text-curie-001 6.7B OPT-6.7B 6.7B  text-davinci-001 175B OPT-13B 13B  text-davinci-003 175B OPT-66B 66B  FLAN-T5 Param. GPT2 Param.  FT5-small 80M GPT2-M 355M  FT5-base 250M GPT2-L 774M  FT5-L 770M GPT2-XL 1.5B  FT5-XL 3B GPT-J-6B 6B  FT5-XXL 11B"
        },
        {
            "bounding_box": [
                {
                    "x": 346,
                    "y": 2401
                },
                {
                    "x": 1091,
                    "y": 2401
                },
                {
                    "x": 1091,
                    "y": 2443
                },
                {
                    "x": 346,
                    "y": 2443
                }
            ],
            "category": "caption",
            "html": "<br><caption id='59' style='font-size:14px'>Table 2. Pre-trained backbones used in this work.</caption>",
            "id": 59,
            "page": 5,
            "text": "Table 2. Pre-trained backbones used in this work."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2517
                },
                {
                    "x": 643,
                    "y": 2517
                },
                {
                    "x": 643,
                    "y": 2564
                },
                {
                    "x": 223,
                    "y": 2564
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:20px'>4.3. Scoring Dimension</p>",
            "id": 60,
            "page": 5,
            "text": "4.3. Scoring Dimension"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2593
                },
                {
                    "x": 1217,
                    "y": 2593
                },
                {
                    "x": 1217,
                    "y": 2995
                },
                {
                    "x": 224,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>Specifically, (1) For aspects INT, ENG, SPC, REL, COR,<br>SEM, UND, and FLU of FED-Turn datasets from the open<br>domain dialogue generation task, we choose the src->hypo<br>variant since the human judgments of the evaluated dataset<br>(i.e., FED-Turn) are also created based on the source. (2)<br>For aspects COH, CON, and INF from SummEval and News-<br>room, since data annotators labeled the data based on source<br>and hypothesis texts, we chose src->hypo for these aspects.</p>",
            "id": 61,
            "page": 5,
            "text": "Specifically, (1) For aspects INT, ENG, SPC, REL, COR, SEM, UND, and FLU of FED-Turn datasets from the open domain dialogue generation task, we choose the src->hypo variant since the human judgments of the evaluated dataset (i.e., FED-Turn) are also created based on the source. (2) For aspects COH, CON, and INF from SummEval and Newsroom, since data annotators labeled the data based on source and hypothesis texts, we chose src->hypo for these aspects."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 282
                },
                {
                    "x": 2264,
                    "y": 282
                },
                {
                    "x": 2264,
                    "y": 834
                },
                {
                    "x": 1270,
                    "y": 834
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:16px'>(3) For aspects INF, NAT, and QUA from the data-to-text<br>task, we choose src->hypo. Because the source text of the<br>data-to-text task is not in the standard text format, which<br>will be hard to handle by the scoring function. (4) For<br>aspects ACC, FLU, and MQM from the machine translation<br>task, we also choose src->hypo. Because the source text<br>of the machine translation is a different language from the<br>translated text (hypo). In this work, we mainly consider the<br>evaluation of the English text. In the future, we can consider<br>designing a scoring function based on BLOOM (Scao et al.,<br>2022) that can evaluate texts in a cross-lingual setting.</p>",
            "id": 62,
            "page": 5,
            "text": "(3) For aspects INF, NAT, and QUA from the data-to-text task, we choose src->hypo. Because the source text of the data-to-text task is not in the standard text format, which will be hard to handle by the scoring function. (4) For aspects ACC, FLU, and MQM from the machine translation task, we also choose src->hypo. Because the source text of the machine translation is a different language from the translated text (hypo). In this work, we mainly consider the evaluation of the English text. In the future, we can consider designing a scoring function based on BLOOM (Scao , 2022) that can evaluate texts in a cross-lingual setting."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 888
                },
                {
                    "x": 1938,
                    "y": 888
                },
                {
                    "x": 1938,
                    "y": 935
                },
                {
                    "x": 1274,
                    "y": 935
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:18px'>4.4. Evaluation Dataset Construction</p>",
            "id": 63,
            "page": 5,
            "text": "4.4. Evaluation Dataset Construction"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 964
                },
                {
                    "x": 2265,
                    "y": 964
                },
                {
                    "x": 2265,
                    "y": 1415
                },
                {
                    "x": 1272,
                    "y": 1415
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>Unlike previous works (Matiana et al., 2021; Xu et al.,<br>2022a;b; Castricato et al., 2022) that only consider the over-<br>all text quality, we focus on evaluating multi-dimensional<br>text quality. In this work, we studied 37 datasets according<br>to 22 evaluation aspects. Due to the expensive API cost of<br>GPT3, we randomly extract and construct sub-datasets for<br>meta-evaluation. For the MQM dataset, since many aspects<br>of samples lack human scores, we extract samples with<br>human scores in ACC, MQM, and FLU as much as possible.</p>",
            "id": 64,
            "page": 5,
            "text": "Unlike previous works (Matiana , 2021; Xu , 2022a;b; Castricato , 2022) that only consider the overall text quality, we focus on evaluating multi-dimensional text quality. In this work, we studied 37 datasets according to 22 evaluation aspects. Due to the expensive API cost of GPT3, we randomly extract and construct sub-datasets for meta-evaluation. For the MQM dataset, since many aspects of samples lack human scores, we extract samples with human scores in ACC, MQM, and FLU as much as possible."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1475
                },
                {
                    "x": 1756,
                    "y": 1475
                },
                {
                    "x": 1756,
                    "y": 1532
                },
                {
                    "x": 1274,
                    "y": 1532
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:22px'>5. Experiment Results</p>",
            "id": 65,
            "page": 5,
            "text": "5. Experiment Results"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1562
                },
                {
                    "x": 2266,
                    "y": 1562
                },
                {
                    "x": 2266,
                    "y": 1910
                },
                {
                    "x": 1273,
                    "y": 1910
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:18px'>In this work, we focus on exploring whether language<br>models with different structures and sizes can work in<br>the following three scenarios. (a) vanilla (VAL): with<br>non-instruction and non-demonstration; (b) instruction<br>(IST): with instruction and non-demonstration; (c) instruc-<br>tion+demonstration (IDM): with instruction and demon-<br>stration.</p>",
            "id": 66,
            "page": 5,
            "text": "In this work, we focus on exploring whether language models with different structures and sizes can work in the following three scenarios. (a) vanilla (VAL): with non-instruction and non-demonstration; (b) instruction (IST): with instruction and non-demonstration; (c) instruction+demonstration (IDM): with instruction and demonstration."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1935
                },
                {
                    "x": 2263,
                    "y": 1935
                },
                {
                    "x": 2263,
                    "y": 2385
                },
                {
                    "x": 1273,
                    "y": 2385
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>Significance Tests To examine the reliability and validity<br>of the experiment results, we conducted the significance test<br>based on bootstrapping. 4 Our significance test is to check<br>(1) whether the performance of IST (IDM) is significantly<br>better than VAL, and values achieved with the IST (IDM)<br>settings will be marked t if it passes the significant test<br>(p-value <0.05). (2) whether the performance of IDM is<br>significantly better than IST, if yes, mark the value with<br>IDM setting with 1.</p>",
            "id": 67,
            "page": 5,
            "text": "Significance Tests To examine the reliability and validity of the experiment results, we conducted the significance test based on bootstrapping. 4 Our significance test is to check (1) whether the performance of IST (IDM) is significantly better than VAL, and values achieved with the IST (IDM) settings will be marked t if it passes the significant test (p-value <0.05). (2) whether the performance of IDM is significantly better than IST, if yes, mark the value with IDM setting with 1."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2411
                },
                {
                    "x": 2265,
                    "y": 2411
                },
                {
                    "x": 2265,
                    "y": 2610
                },
                {
                    "x": 1272,
                    "y": 2610
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>Average Performance Due to space limitations, we keep<br>the average performance of GPT3-based, GPT2-based, OPT-<br>based, and FT5-based models. The full results of various<br>variants can be found in Appendix E.</p>",
            "id": 68,
            "page": 5,
            "text": "Average Performance Due to space limitations, we keep the average performance of GPT3-based, GPT2-based, OPTbased, and FT5-based models. The full results of various variants can be found in Appendix E."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2663
                },
                {
                    "x": 1719,
                    "y": 2663
                },
                {
                    "x": 1719,
                    "y": 2712
                },
                {
                    "x": 1272,
                    "y": 2712
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>5.1. Text Summarization</p>",
            "id": 69,
            "page": 5,
            "text": "5.1. Text Summarization"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2741
                },
                {
                    "x": 2264,
                    "y": 2741
                },
                {
                    "x": 2264,
                    "y": 2841
                },
                {
                    "x": 1274,
                    "y": 2841
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:18px'>The evaluation results of 28 (9 baseline models (e.g.,<br>ROUGE-1) and 19 variants of GPTScore (e.g., GPT3-d01))</p>",
            "id": 70,
            "page": 5,
            "text": "The evaluation results of 28 (9 baseline models (e.g., ROUGE-1) and 19 variants of GPTScore (e.g., GPT3-d01))"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2873
                },
                {
                    "x": 2025,
                    "y": 2873
                },
                {
                    "x": 2025,
                    "y": 2960
                },
                {
                    "x": 1275,
                    "y": 2960
                }
            ],
            "category": "footer",
            "html": "<footer id='71' style='font-size:14px'>4https : / /en. wikipedia · org / wiki /<br>Bootstrapping_ (statistics)</footer>",
            "id": 71,
            "page": 5,
            "text": "4https : / /en. wikipedia · org / wiki / Bootstrapping_ (statistics)"
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='72' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 72,
            "page": 6,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 272
                },
                {
                    "x": 1206,
                    "y": 272
                },
                {
                    "x": 1206,
                    "y": 828
                },
                {
                    "x": 225,
                    "y": 828
                }
            ],
            "category": "table",
            "html": "<table id='73' style='font-size:14px'><tr><td></td><td colspan=\"10\">SummEval RSumm</td></tr><tr><td>Model</td><td colspan=\"2\">COH</td><td colspan=\"2\">CON</td><td colspan=\"2\">FLU</td><td colspan=\"2\">REL</td><td colspan=\"2\">COV</td></tr><tr><td></td><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td></tr><tr><td>ROUGE-1</td><td>14.1</td><td>-</td><td>20.8</td><td>-</td><td>14.8</td><td>-</td><td>26.2</td><td>-</td><td>46.4</td><td>-</td></tr><tr><td>ROUGE-2</td><td>9.1</td><td>-</td><td>17.2</td><td>-</td><td>12.0</td><td>-</td><td>17.4</td><td>-</td><td>37.3</td><td>-</td></tr><tr><td>ROUGE-L</td><td>12.9</td><td>-</td><td>19.8</td><td>-</td><td>17.6</td><td>-</td><td>24.7</td><td>-</td><td>45.1</td><td>-</td></tr><tr><td>BERTSc</td><td>25.9</td><td>-</td><td>19.7</td><td>-</td><td>23.7</td><td>-</td><td>34.7</td><td>-</td><td>38.4</td><td>-</td></tr><tr><td>MoverSc</td><td>11.5</td><td>-</td><td>18.0</td><td>-</td><td>15.7</td><td>-</td><td>24.8</td><td>-</td><td>34.4</td><td>-</td></tr><tr><td>PRISM</td><td>26.5</td><td>-</td><td>29.9</td><td>-</td><td>26.1</td><td>-</td><td>25.2</td><td>-</td><td>32.3</td><td>-</td></tr><tr><td>BARTSc</td><td>29.7</td><td>-</td><td>30.8</td><td>-</td><td>24.6</td><td>-</td><td>28.9</td><td>-</td><td>43.1</td><td></td></tr><tr><td>+CNN</td><td>42.5</td><td>-</td><td>35.8</td><td>-</td><td>38.1</td><td>-</td><td>35.9</td><td>-</td><td>42.9</td><td>-</td></tr><tr><td>+CNN+Pa</td><td>42.5</td><td>-</td><td>37.0</td><td>-</td><td>40.5</td><td>-</td><td>33.9</td><td>-</td><td>40.9</td><td>-</td></tr></table>",
            "id": 73,
            "page": 6,
            "text": "SummEval RSumm  Model COH CON FLU REL COV   VAL IST VAL IST VAL IST VAL IST VAL IST  ROUGE-1 14.1 - 20.8 - 14.8 - 26.2 - 46.4  ROUGE-2 9.1 - 17.2 - 12.0 - 17.4 - 37.3  ROUGE-L 12.9 - 19.8 - 17.6 - 24.7 - 45.1  BERTSc 25.9 - 19.7 - 23.7 - 34.7 - 38.4  MoverSc 11.5 - 18.0 - 15.7 - 24.8 - 34.4  PRISM 26.5 - 29.9 - 26.1 - 25.2 - 32.3  BARTSc 29.7 - 30.8 - 24.6 - 28.9 - 43.1   +CNN 42.5 - 35.8 - 38.1 - 35.9 - 42.9  +CNN+Pa 42.5 - 37.0 - 40.5 - 33.9 - 40.9"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 840
                },
                {
                    "x": 1209,
                    "y": 840
                },
                {
                    "x": 1209,
                    "y": 1826
                },
                {
                    "x": 227,
                    "y": 1826
                }
            ],
            "category": "table",
            "html": "<br><table id='74' style='font-size:14px'><tr><td>GPT3-a01</td><td>39.3 39.8t</td><td>39.7 40.5+</td><td>36.1</td><td>35.9</td><td>28.2 27.6 29.5 29.8t</td></tr><tr><td>GPT3-b01</td><td>42.7 45.2†</td><td>41.0 41.4+</td><td>37.1 39.1t</td><td>32.0 33.4t</td><td>35.0 35.2+</td></tr><tr><td>GPT3-c01</td><td>41.3 40.8</td><td>44.6 45.1+</td><td>38.9 39.5t</td><td>31.6 33.2t</td><td>36.1 45.1+</td></tr><tr><td>GPT3-d01</td><td>40.0 40.1</td><td>46.6 47.5�</td><td>40.5 41.0+</td><td>32.4 34.3t</td><td>36.0 33.9</td></tr><tr><td>GPT3-d03</td><td>43.7 43.4</td><td>45.2 44.9</td><td>41.1 40.3</td><td>36.3 38.1t</td><td>35.2 38.0+</td></tr><tr><td>GPT2-M</td><td>36.0 39.2+</td><td>34.6 35.3t</td><td>28.1 30.7t</td><td>28.3 28.3</td><td>41.8 43.3t</td></tr><tr><td>GPT2-L</td><td>36.4 39.8t</td><td>33.7 34.4t</td><td>29.4 31.5t</td><td>27.8 28.1t</td><td>39.641.3+</td></tr><tr><td>GPT2-XL</td><td>35.3 39.9†</td><td>35.9 36.1+</td><td>31.2 33.1t</td><td>28.1 28.0</td><td>40.4 41.0+</td></tr><tr><td>GPT-J-6B</td><td>35.5 39.5t</td><td>42.7 42.8t</td><td>35.5 37.4+</td><td>31.5 31.9†</td><td>42.8 43.7t</td></tr><tr><td>OPT350m</td><td>33.437.6+</td><td>34.9 35.5t</td><td>29.6 31.4+</td><td>29.5 28.6</td><td>40.2 42.3t</td></tr><tr><td>OPT-1.3B</td><td>35.0 37.8t</td><td>40.0 42.0+</td><td>33.6 35.9t</td><td>33.5 34.2+</td><td>42.0 39.7</td></tr><tr><td>OPT-6.7B</td><td>35.7 36.8t</td><td>42.1 45.7†</td><td>35.5 37.6t</td><td>35.4 35.4</td><td>38.0 41.9t</td></tr><tr><td>OPT-13B</td><td>33.5 34.7t</td><td>42.5 45.2+</td><td>35.6 37.3t</td><td>33.6 33.9</td><td>37.6 41.0+</td></tr><tr><td>OPT-66B</td><td>32.0 35.9t</td><td>44.0 45.3t</td><td>36.3 38.0+</td><td>33.4 33.7t</td><td>40.3 41.3t</td></tr><tr><td>FT5-small</td><td>35.0 35.4t</td><td>37.0 38.0+</td><td>35.6 34.7</td><td>27.3 28.0+</td><td>33.6 35.7t</td></tr><tr><td>FT5-base</td><td>39.2 39.9t</td><td>36.7 37.2+</td><td>37.3 36.5</td><td>29.5 31.2+</td><td>36.7 38.6t</td></tr><tr><td>FT5-L</td><td>42.3 45.1+</td><td>41.0 42.5t</td><td>39.3 41.6+</td><td>31.2 35.3t</td><td>31.4 39.3t</td></tr><tr><td>FT5-XL</td><td>42.8 47.0+</td><td>41.0 43.6t</td><td>39.7 42.1+</td><td>31.4 34.4t</td><td>34.8 43.8t</td></tr><tr><td>FT5-XXL</td><td>42.1 45.6+</td><td>43.7 43.8</td><td>39.8 42.4†</td><td>32.8 34.3t</td><td>40.2 41.1+</td></tr><tr><td>Avg.</td><td>38.0 40.2</td><td>40.4 41.4</td><td>35.8 37.2</td><td>31.3 32.2</td><td>37.4 39.8</td></tr></table>",
            "id": 74,
            "page": 6,
            "text": "GPT3-a01 39.3 39.8t 39.7 40.5+ 36.1 35.9 28.2 27.6 29.5 29.8t  GPT3-b01 42.7 45.2† 41.0 41.4+ 37.1 39.1t 32.0 33.4t 35.0 35.2+  GPT3-c01 41.3 40.8 44.6 45.1+ 38.9 39.5t 31.6 33.2t 36.1 45.1+  GPT3-d01 40.0 40.1 46.6 47.5� 40.5 41.0+ 32.4 34.3t 36.0 33.9  GPT3-d03 43.7 43.4 45.2 44.9 41.1 40.3 36.3 38.1t 35.2 38.0+  GPT2-M 36.0 39.2+ 34.6 35.3t 28.1 30.7t 28.3 28.3 41.8 43.3t  GPT2-L 36.4 39.8t 33.7 34.4t 29.4 31.5t 27.8 28.1t 39.641.3+  GPT2-XL 35.3 39.9† 35.9 36.1+ 31.2 33.1t 28.1 28.0 40.4 41.0+  GPT-J-6B 35.5 39.5t 42.7 42.8t 35.5 37.4+ 31.5 31.9† 42.8 43.7t  OPT350m 33.437.6+ 34.9 35.5t 29.6 31.4+ 29.5 28.6 40.2 42.3t  OPT-1.3B 35.0 37.8t 40.0 42.0+ 33.6 35.9t 33.5 34.2+ 42.0 39.7  OPT-6.7B 35.7 36.8t 42.1 45.7† 35.5 37.6t 35.4 35.4 38.0 41.9t  OPT-13B 33.5 34.7t 42.5 45.2+ 35.6 37.3t 33.6 33.9 37.6 41.0+  OPT-66B 32.0 35.9t 44.0 45.3t 36.3 38.0+ 33.4 33.7t 40.3 41.3t  FT5-small 35.0 35.4t 37.0 38.0+ 35.6 34.7 27.3 28.0+ 33.6 35.7t  FT5-base 39.2 39.9t 36.7 37.2+ 37.3 36.5 29.5 31.2+ 36.7 38.6t  FT5-L 42.3 45.1+ 41.0 42.5t 39.3 41.6+ 31.2 35.3t 31.4 39.3t  FT5-XL 42.8 47.0+ 41.0 43.6t 39.7 42.1+ 31.4 34.4t 34.8 43.8t  FT5-XXL 42.1 45.6+ 43.7 43.8 39.8 42.4† 32.8 34.3t 40.2 41.1+  Avg. 38.0 40.2 40.4 41.4 35.8 37.2 31.3 32.2"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1845
                },
                {
                    "x": 1212,
                    "y": 1845
                },
                {
                    "x": 1212,
                    "y": 2078
                },
                {
                    "x": 225,
                    "y": 2078
                }
            ],
            "category": "caption",
            "html": "<br><caption id='75' style='font-size:14px'>Table 3. Spearman correlation of different aspects on text summa-<br>rization datasets. VAL and IST is the abbreviation of vanilla and<br>instruction, respectively. Values with 1 denote the evaluator with<br>instruction significantly outperforms with vanilla. Values in bold<br>are the best performance in a set of variants (e.g., GPT3 family).</caption>",
            "id": 75,
            "page": 6,
            "text": "Table 3. Spearman correlation of different aspects on text summarization datasets. VAL and IST is the abbreviation of vanilla and instruction, respectively. Values with 1 denote the evaluator with instruction significantly outperforms with vanilla. Values in bold are the best performance in a set of variants (e.g., GPT3 family)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2172
                },
                {
                    "x": 1214,
                    "y": 2172
                },
                {
                    "x": 1214,
                    "y": 2618
                },
                {
                    "x": 223,
                    "y": 2618
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>scoring functions for the text summarization task on Sum-<br>mEval and RealSumm datasets are shown in Tab. 3. Due<br>to the space limitation, we move the performance of the<br>NEWSROOM and QXSUM datasets to the Appendix E.<br>Fig. 3 shows the evaluation results of five GPT3 variant<br>models on four text summarization datasets, where QX-<br>SUM uses the Pearson correlation and other datasets use<br>the Spearman correlation metric. The main observations are<br>summarized as follows:</p>",
            "id": 76,
            "page": 6,
            "text": "scoring functions for the text summarization task on SummEval and RealSumm datasets are shown in Tab. 3. Due to the space limitation, we move the performance of the NEWSROOM and QXSUM datasets to the Appendix E. Fig. 3 shows the evaluation results of five GPT3 variant models on four text summarization datasets, where QXSUM uses the Pearson correlation and other datasets use the Spearman correlation metric. The main observations are summarized as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2643
                },
                {
                    "x": 1214,
                    "y": 2643
                },
                {
                    "x": 1214,
                    "y": 2993
                },
                {
                    "x": 224,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:20px'>(1) Evaluator with instruction significantly improves<br>the performance (values with t in Tab. 3). What's<br>more, small models with instruction demonstrate compa-<br>rable performance to supervised learning models. For ex-<br>ample, OPT350m, FT5-small, and FT5-base outperform<br>BARTScore+CNN on the CON aspect when using the in-<br>structions. (2) The benefit from instruction is more sta-</p>",
            "id": 77,
            "page": 6,
            "text": "(1) Evaluator with instruction significantly improves the performance (values with t in Tab. 3). What's more, small models with instruction demonstrate comparable performance to supervised learning models. For example, OPT350m, FT5-small, and FT5-base outperform BARTScore+CNN on the CON aspect when using the instructions. (2) The benefit from instruction is more sta-"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 281
                },
                {
                    "x": 2264,
                    "y": 281
                },
                {
                    "x": 2264,
                    "y": 882
                },
                {
                    "x": 1272,
                    "y": 882
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:20px'>ble for the decoder-only models. In Tab. 3, the average<br>Spearman score of both the GPT2 and OPT models, 9 out of<br>10 aspects are better than the vanilla setting (VAL) by using<br>instruction (IST), while the equipment of instruction (IST)<br>to the encoder-decoder model of FT5 on the NEWSROOM<br>dataset fails to achieve gains. (3) As for the GPT3-based<br>models, (a) the performance of GPT3-d01 is barely sig-<br>nificantly better than GPT3-c01, which tries to balance<br>power and speed. (b) GPT3-d03 performs better than<br>GPT3-d01 significantly. We can observe these conclu-<br>sions from Fig. 3, and both conclusions have passed the<br>significance test at p < 0.05.</p>",
            "id": 78,
            "page": 6,
            "text": "ble for the decoder-only models. In Tab. 3, the average Spearman score of both the GPT2 and OPT models, 9 out of 10 aspects are better than the vanilla setting (VAL) by using instruction (IST), while the equipment of instruction (IST) to the encoder-decoder model of FT5 on the NEWSROOM dataset fails to achieve gains. (3) As for the GPT3-based models, (a) the performance of GPT3-d01 is barely significantly better than GPT3-c01, which tries to balance power and speed. (b) GPT3-d03 performs better than GPT3-d01 significantly. We can observe these conclusions from Fig. 3, and both conclusions have passed the significance test at p < 0.05."
        },
        {
            "bounding_box": [
                {
                    "x": 1337,
                    "y": 911
                },
                {
                    "x": 2183,
                    "y": 911
                },
                {
                    "x": 2183,
                    "y": 1904
                },
                {
                    "x": 1337,
                    "y": 1904
                }
            ],
            "category": "figure",
            "html": "<figure><img id='79' style='font-size:22px' alt=\"SummEval RSumm\nCOH CON FLU REI COV\n46 50 50\n42 40\n44\n40\n45 35 40\n42\n38\n40 30\n40 36 30\nVAL IS AV 15 AV 15 VAL 15 VAL 15\nNEWSROOM QXSUM\nCOH FLU REL INF FAC\n70 76 30\n74\n74\n70 68\n20\n72\n72\n66 70\n68 10\nVAL IS VAI IS VAL IS VAL IS VAL 15\" data-coord=\"top-left:(1337,911); bottom-right:(2183,1904)\" /></figure>",
            "id": 79,
            "page": 6,
            "text": "SummEval RSumm COH CON FLU REI COV 46 50 50 42 40 44 40 45 35 40 42 38 40 30 40 36 30 VAL IS AV 15 AV 15 VAL 15 VAL 15 NEWSROOM QXSUM COH FLU REL INF FAC 70 76 30 74 74 70 68 20 72 72 66 70 68 10 VAL IS VAI IS VAL IS VAL IS VAL 15"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1914
                },
                {
                    "x": 2265,
                    "y": 1914
                },
                {
                    "x": 2265,
                    "y": 2144
                },
                {
                    "x": 1272,
                    "y": 2144
                }
            ],
            "category": "caption",
            "html": "<br><caption id='80' style='font-size:14px'>Figure 3. Experimental results for GPT3-based variants in text<br>summarization task. Here, blue, orange, green, pink, and cyan dot<br>denote that GPTSCORE is built based on a01 ), b01 ), c01<br>), d01 ●), and d03 (●), respectively. The red lines (一) denote<br>the average performance of GPT3-based variants.</caption>",
            "id": 80,
            "page": 6,
            "text": "Figure 3. Experimental results for GPT3-based variants in text summarization task. Here, blue, orange, green, pink, and cyan dot denote that GPTSCORE is built based on a01 ), b01 ), c01 ), d01 ●), and d03 (●), respectively. The red lines (一) denote the average performance of GPT3-based variants."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2217
                },
                {
                    "x": 1728,
                    "y": 2217
                },
                {
                    "x": 1728,
                    "y": 2265
                },
                {
                    "x": 1272,
                    "y": 2265
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:18px'>5.2. Machine Translation</p>",
            "id": 81,
            "page": 6,
            "text": "5.2. Machine Translation"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2294
                },
                {
                    "x": 2266,
                    "y": 2294
                },
                {
                    "x": 2266,
                    "y": 2992
                },
                {
                    "x": 1270,
                    "y": 2992
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:18px'>The average sample-level Spearman (p) scores of GPT3-<br>based, GPT2-based, OPT-based, and FT5-based models<br>on the MQM-2020 machine translation dataset are shown<br>in Tab. 4, where values with 1 denote that the evaluator<br>equipped with IST (or IDM) significantly outperforms the<br>VAL setting, and : indicate that the evaluator equipped with<br>IDM (the combination of IST and DM) significantly out-<br>performs the IST setting. The Spearman correlations for<br>the GPT3-based variants are shown in Fig. 4. For the full<br>evaluation results of 28 models (including 9 baseline scor-<br>ing models, such as ROUGE-1) can be found in Tab. 14.<br>Following Thompson & Post (2020) and Yuan et al. (2021),<br>we treat the evaluation of machine translation as the para-<br>phrasing task. The main observations are listed as follows:</p>",
            "id": 82,
            "page": 6,
            "text": "The average sample-level Spearman (p) scores of GPT3based, GPT2-based, OPT-based, and FT5-based models on the MQM-2020 machine translation dataset are shown in Tab. 4, where values with 1 denote that the evaluator equipped with IST (or IDM) significantly outperforms the VAL setting, and : indicate that the evaluator equipped with IDM (the combination of IST and DM) significantly outperforms the IST setting. The Spearman correlations for the GPT3-based variants are shown in Fig. 4. For the full evaluation results of 28 models (including 9 baseline scoring models, such as ROUGE-1) can be found in Tab. 14. Following Thompson & Post (2020) and Yuan  (2021), we treat the evaluation of machine translation as the paraphrasing task. The main observations are listed as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 191
                },
                {
                    "x": 1525,
                    "y": 191
                },
                {
                    "x": 1525,
                    "y": 234
                },
                {
                    "x": 959,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<header id='83' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 83,
            "page": 7,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 281
                },
                {
                    "x": 1216,
                    "y": 281
                },
                {
                    "x": 1216,
                    "y": 1031
                },
                {
                    "x": 224,
                    "y": 1031
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:18px'>(1) The introduction of instruction (IST) significantly<br>improve the performance in three different aspects of<br>ACC, FLU, and MQM. In Tab. 4, the average performance<br>of 19 GPTSCORE based evaluators with instruction (IST)<br>significantly outperforms vanilla (VAL). (2) The combi-<br>nation of instruction and demonstration (IDM) brings<br>gains for the evaluator with different model structures.<br>In Tab. 4, the performance of GPT3, GPT2, OPT, and FT5<br>improves a lot when instruction and demonstration (IDM)<br>are introduced. (3) The evaluator built based on GPT3-<br>c01 achieves comparable performance with GPT3-d01<br>and GPT3-d03. This can be found in Fig. 4. Since the<br>GPT3-d01 and GPT3-d03 are most expensive variant of<br>GPT3, the cheaper and comparative GPT3-c01 is a good<br>choice for machine translation task.</p>",
            "id": 84,
            "page": 7,
            "text": "(1) The introduction of instruction (IST) significantly improve the performance in three different aspects of ACC, FLU, and MQM. In Tab. 4, the average performance of 19 GPTSCORE based evaluators with instruction (IST) significantly outperforms vanilla (VAL). (2) The combination of instruction and demonstration (IDM) brings gains for the evaluator with different model structures. In Tab. 4, the performance of GPT3, GPT2, OPT, and FT5 improves a lot when instruction and demonstration (IDM) are introduced. (3) The evaluator built based on GPT3c01 achieves comparable performance with GPT3-d01 and GPT3-d03. This can be found in Fig. 4. Since the GPT3-d01 and GPT3-d03 are most expensive variant of GPT3, the cheaper and comparative GPT3-c01 is a good choice for machine translation task."
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 1070
                },
                {
                    "x": 1199,
                    "y": 1070
                },
                {
                    "x": 1199,
                    "y": 1473
                },
                {
                    "x": 227,
                    "y": 1473
                }
            ],
            "category": "table",
            "html": "<table id='85' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">ACC</td><td colspan=\"3\">FLU</td><td colspan=\"3\">MQM</td></tr><tr><td>VAL</td><td>IST</td><td>IDM</td><td>VAL</td><td>IST</td><td>IDM</td><td>VAL</td><td>IST</td><td>IDM</td></tr><tr><td>GPT3</td><td>27.2</td><td>27.1</td><td>29.7t,‡</td><td>11.3</td><td>10.4</td><td>16.4+,‡</td><td>30.3</td><td>31.2+</td><td>32.3+,‡</td></tr><tr><td>GPT2</td><td>25.8</td><td>27.0+</td><td>30.3+,‡</td><td>9.8</td><td>10.8+</td><td>15.8+,‡</td><td>30.1</td><td>30.3t</td><td>33.5+,‡</td></tr><tr><td>OPT</td><td>28.7</td><td>29.4t</td><td>30.3+,‡</td><td>10.0</td><td>12.2+</td><td>16.3+,‡</td><td>32.5</td><td>34.6+</td><td>35.1+,‡</td></tr><tr><td>FT5</td><td>27.7</td><td>27.8t</td><td>28.3+,‡</td><td>9.6</td><td>11.0+</td><td>15.4+,‡</td><td>31.0</td><td>32.3t</td><td>32.3</td></tr><tr><td>Avg.</td><td>27.4</td><td>27.8t</td><td>29.7†,‡</td><td>10.2</td><td>11.1 十</td><td>16.0t,‡</td><td>31.0</td><td>32.1†</td><td>33.3t,‡</td></tr></table>",
            "id": 85,
            "page": 7,
            "text": "Model ACC FLU MQM  VAL IST IDM VAL IST IDM VAL IST IDM  GPT3 27.2 27.1 29.7t,‡ 11.3 10.4 16.4+,‡ 30.3 31.2+ 32.3+,‡  GPT2 25.8 27.0+ 30.3+,‡ 9.8 10.8+ 15.8+,‡ 30.1 30.3t 33.5+,‡  OPT 28.7 29.4t 30.3+,‡ 10.0 12.2+ 16.3+,‡ 32.5 34.6+ 35.1+,‡  FT5 27.7 27.8t 28.3+,‡ 9.6 11.0+ 15.4+,‡ 31.0 32.3t 32.3  Avg. 27.4 27.8t 29.7†,‡ 10.2 11.1 十 16.0t,‡ 31.0 32.1†"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1481
                },
                {
                    "x": 1215,
                    "y": 1481
                },
                {
                    "x": 1215,
                    "y": 1620
                },
                {
                    "x": 226,
                    "y": 1620
                }
            ],
            "category": "caption",
            "html": "<br><caption id='86' style='font-size:14px'>Table 4. The average Spearman correlation of the GPT3-based,<br>GPT2-based, OPT-based, and FT5-based models in machine trans-<br>lation task of MQM-2020 dataset.</caption>",
            "id": 86,
            "page": 7,
            "text": "Table 4. The average Spearman correlation of the GPT3-based, GPT2-based, OPT-based, and FT5-based models in machine translation task of MQM-2020 dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 389,
                    "y": 1687
                },
                {
                    "x": 1036,
                    "y": 1687
                },
                {
                    "x": 1036,
                    "y": 2191
                },
                {
                    "x": 389,
                    "y": 2191
                }
            ],
            "category": "table",
            "html": "<table id='87' style='font-size:22px'><tr><td colspan=\"2\">MQM-2020</td></tr><tr><td>ACC</td><td>FLU MQM</td></tr><tr><td>20 30 25 10 VAL IS IDM T</td><td>40 35 30 25 VAL 15 IDM VAL 15 M D</td></tr></table>",
            "id": 87,
            "page": 7,
            "text": "MQM-2020  ACC FLU MQM  20 30 25 10 VAL IS IDM T"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2196
                },
                {
                    "x": 1217,
                    "y": 2196
                },
                {
                    "x": 1217,
                    "y": 2427
                },
                {
                    "x": 222,
                    "y": 2427
                }
            ],
            "category": "caption",
            "html": "<br><caption id='88' style='font-size:14px'>Figure 4. Experimental results for GPT3-based variants in the ma-<br>chine translation task. Here, blue, orange, green, pink, and cyan<br>dot denote that GPTSCORE is built based on a01 ), b01<br>c01 ), d01 ), and d03 (●), respectively. The red lines ( -)<br>denote the average performance of GPT3-based variants.</caption>",
            "id": 88,
            "page": 7,
            "text": "Figure 4. Experimental results for GPT3-based variants in the machine translation task. Here, blue, orange, green, pink, and cyan dot denote that GPTSCORE is built based on a01 ), b01 c01 ), d01 ), and d03 (●), respectively. The red lines ( -) denote the average performance of GPT3-based variants."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2515
                },
                {
                    "x": 526,
                    "y": 2515
                },
                {
                    "x": 526,
                    "y": 2565
                },
                {
                    "x": 223,
                    "y": 2565
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>5.3. Data to Text</p>",
            "id": 89,
            "page": 7,
            "text": "5.3. Data to Text"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2593
                },
                {
                    "x": 1216,
                    "y": 2593
                },
                {
                    "x": 1216,
                    "y": 2995
                },
                {
                    "x": 223,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:18px'>We consider the BAGEL and SFRES datasets for the eval-<br>uation of data to text task. The average Spearman corre-<br>lations of the GPT3-based, GPT2-based, OPT-based, and<br>FT5-based models are listed in Tab. 5. VAL, IST, and IDM<br>denote the vanilla, using instruction, and using both instruc-<br>tion and demonstration settings, respectively. Due to the<br>space limitation, the detailed performance of each evaluator<br>considered in this work can be found in Tab. 15 and Tab. 16.</p>",
            "id": 90,
            "page": 7,
            "text": "We consider the BAGEL and SFRES datasets for the evaluation of data to text task. The average Spearman correlations of the GPT3-based, GPT2-based, OPT-based, and FT5-based models are listed in Tab. 5. VAL, IST, and IDM denote the vanilla, using instruction, and using both instruction and demonstration settings, respectively. Due to the space limitation, the detailed performance of each evaluator considered in this work can be found in Tab. 15 and Tab. 16."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 285
                },
                {
                    "x": 2013,
                    "y": 285
                },
                {
                    "x": 2013,
                    "y": 331
                },
                {
                    "x": 1274,
                    "y": 331
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:14px'>The main observations are listed as follows:</p>",
            "id": 91,
            "page": 7,
            "text": "The main observations are listed as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 360
                },
                {
                    "x": 2268,
                    "y": 360
                },
                {
                    "x": 2268,
                    "y": 1306
                },
                {
                    "x": 1270,
                    "y": 1306
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:18px'>(1) Introducing instruction (IST) can significantly im-<br>prove performance, and introducing demonstration<br>(DM) will further improve performance. In Tab. 5, the<br>average performance on the three aspects is significantly<br>improved when adapting to the instruction, and the perfor-<br>mance of using demonstration on NAT and FLU has further<br>significantly improved. (2) The decoder-only model is<br>better at utilizing demonstration to achieve high perfor-<br>mance. In Tab. 5, compare to the encoder-decoder model<br>FT5, the performance has a more significant improvement<br>for the decoder-only model of GPT2 and OPT on NAT and<br>FLU aspects after introducing DM, which holds for both<br>BAGEL and SFRES. (3) GPT3 has strong compatibility<br>with unformatted text. Named entities of the BAGEL<br>dataset are replaced with a special token (e.g, X and Y ). For<br>example, \"X is a cafe restaurant\", where \"X\" denotes the<br>name of the cafe. When introducing IST and DM (IDM), the<br>variants of GPT3 achieve much higher average performance<br>than GPT2, OPT, and FT5.</p>",
            "id": 92,
            "page": 7,
            "text": "(1) Introducing instruction (IST) can significantly improve performance, and introducing demonstration (DM) will further improve performance. In Tab. 5, the average performance on the three aspects is significantly improved when adapting to the instruction, and the performance of using demonstration on NAT and FLU has further significantly improved. (2) The decoder-only model is better at utilizing demonstration to achieve high performance. In Tab. 5, compare to the encoder-decoder model FT5, the performance has a more significant improvement for the decoder-only model of GPT2 and OPT on NAT and FLU aspects after introducing DM, which holds for both BAGEL and SFRES. (3) GPT3 has strong compatibility with unformatted text. Named entities of the BAGEL dataset are replaced with a special token (e.g, X and Y ). For example, \"X is a cafe restaurant\", where \"X\" denotes the name of the cafe. When introducing IST and DM (IDM), the variants of GPT3 achieve much higher average performance than GPT2, OPT, and FT5."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1347
                },
                {
                    "x": 2275,
                    "y": 1347
                },
                {
                    "x": 2275,
                    "y": 2150
                },
                {
                    "x": 1272,
                    "y": 2150
                }
            ],
            "category": "table",
            "html": "<table id='93' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">INF</td><td colspan=\"3\">NAT</td><td colspan=\"3\">FLU</td></tr><tr><td>VAL</td><td>IST</td><td>IDM</td><td>VAL</td><td>IST</td><td>IDM</td><td>VAL</td><td>IST</td><td>IDM</td></tr><tr><td colspan=\"10\">BAGEL</td></tr><tr><td>GPT3</td><td>35.4</td><td>38.3t</td><td>43.6+,‡</td><td>21.7</td><td>26.5t</td><td>36.9+,‡</td><td>30.5</td><td>32.9t</td><td>43.4+,‡</td></tr><tr><td>GPT2</td><td>40.8</td><td>43.2+</td><td>40.2</td><td>31.4</td><td>33.0+</td><td>33.5+,‡</td><td>36.7</td><td>39.3t</td><td>41.3+,‡</td></tr><tr><td>OPT</td><td>38.7</td><td>39.3t</td><td>38.6</td><td>31.4</td><td>30.0</td><td>33.7t,‡</td><td>37.7</td><td>37.1t</td><td>41.5+,‡</td></tr><tr><td>FT5</td><td>41.5</td><td>41.5</td><td>39.1</td><td>26.5</td><td>29.7t</td><td>28.6+</td><td>38.1</td><td>41.1+</td><td>40.3t</td></tr><tr><td>Avg.</td><td>39.1</td><td>40.6+</td><td>40.3t</td><td>27.7</td><td>29.8t</td><td>33.2+,‡</td><td>35.8</td><td>37.6t</td><td>41.6t,‡</td></tr><tr><td colspan=\"10\">SFRES</td></tr><tr><td>GPT3</td><td>30.4</td><td>25.1</td><td>31.5+,‡</td><td>25.0</td><td>30.4+</td><td>26.5t</td><td>31.2</td><td>30.9</td><td>26.1</td></tr><tr><td>GPT2</td><td>22.5</td><td>25.1+</td><td>20.5</td><td>31.0</td><td>31.9t</td><td>37.0+,‡</td><td>20.0</td><td>33.1t</td><td>36.2+,‡</td></tr><tr><td>OPT</td><td>25.2</td><td>26.9t</td><td>24.3</td><td>26.2</td><td>30.0+</td><td>36.6+,‡</td><td>21.3</td><td>25.6+</td><td>30.6+,‡</td></tr><tr><td>FT5</td><td>24.0</td><td>21.9</td><td>19.7</td><td>34.3</td><td>34.6+</td><td>36.8+,‡</td><td>22.0</td><td>17.8</td><td>19.7±</td></tr><tr><td>Avg.</td><td>25.5</td><td>24.7</td><td>24.0</td><td>29.1</td><td>31.7t</td><td>34.2+,‡</td><td>23.6</td><td>26.8+</td><td>28.2+,‡</td></tr></table>",
            "id": 93,
            "page": 7,
            "text": "Model INF NAT FLU  VAL IST IDM VAL IST IDM VAL IST IDM  BAGEL  GPT3 35.4 38.3t 43.6+,‡ 21.7 26.5t 36.9+,‡ 30.5 32.9t 43.4+,‡  GPT2 40.8 43.2+ 40.2 31.4 33.0+ 33.5+,‡ 36.7 39.3t 41.3+,‡  OPT 38.7 39.3t 38.6 31.4 30.0 33.7t,‡ 37.7 37.1t 41.5+,‡  FT5 41.5 41.5 39.1 26.5 29.7t 28.6+ 38.1 41.1+ 40.3t  Avg. 39.1 40.6+ 40.3t 27.7 29.8t 33.2+,‡ 35.8 37.6t 41.6t,‡  SFRES  GPT3 30.4 25.1 31.5+,‡ 25.0 30.4+ 26.5t 31.2 30.9 26.1  GPT2 22.5 25.1+ 20.5 31.0 31.9t 37.0+,‡ 20.0 33.1t 36.2+,‡  OPT 25.2 26.9t 24.3 26.2 30.0+ 36.6+,‡ 21.3 25.6+ 30.6+,‡  FT5 24.0 21.9 19.7 34.3 34.6+ 36.8+,‡ 22.0 17.8 19.7±  Avg. 25.5 24.7 24.0 29.1 31.7t 34.2+,‡ 23.6 26.8+"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2176
                },
                {
                    "x": 2264,
                    "y": 2176
                },
                {
                    "x": 2264,
                    "y": 2313
                },
                {
                    "x": 1274,
                    "y": 2313
                }
            ],
            "category": "caption",
            "html": "<caption id='94' style='font-size:14px'>Table 5. The average of Spearman correlation the models based on<br>GPT3, GPT2, OPT, and FT5 on BAGEL and SFRES datasets in<br>data-to-text task.</caption>",
            "id": 94,
            "page": 7,
            "text": "Table 5. The average of Spearman correlation the models based on GPT3, GPT2, OPT, and FT5 on BAGEL and SFRES datasets in data-to-text task."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2390
                },
                {
                    "x": 1904,
                    "y": 2390
                },
                {
                    "x": 1904,
                    "y": 2441
                },
                {
                    "x": 1275,
                    "y": 2441
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:20px'>5.4. Dialogue Response Generation</p>",
            "id": 95,
            "page": 7,
            "text": "5.4. Dialogue Response Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2470
                },
                {
                    "x": 2265,
                    "y": 2470
                },
                {
                    "x": 2265,
                    "y": 2818
                },
                {
                    "x": 1273,
                    "y": 2818
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:18px'>To test if GPTSCORE can generalize to more aspects, we<br>choose the task of dialogue response generation as a testbed,<br>which usually requires evaluating generated texts from a<br>variety of dimensions (i.e., \"interesting\" and \"fluent\"). To<br>reduce the computational cost, in this experiment, we focus<br>on GPT3-based metrics since they have achieved superior<br>performance as we observed in the previous experiments.</p>",
            "id": 96,
            "page": 7,
            "text": "To test if GPTSCORE can generalize to more aspects, we choose the task of dialogue response generation as a testbed, which usually requires evaluating generated texts from a variety of dimensions (i.e., \"interesting\" and \"fluent\"). To reduce the computational cost, in this experiment, we focus on GPT3-based metrics since they have achieved superior performance as we observed in the previous experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2846
                },
                {
                    "x": 2266,
                    "y": 2846
                },
                {
                    "x": 2266,
                    "y": 2991
                },
                {
                    "x": 1275,
                    "y": 2991
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:16px'>Tab. 6 shows the Spearman correlation of different aspects<br>on FED turn- and dialogue-level datasets. The main obser-<br>vations are listed as follows.</p>",
            "id": 97,
            "page": 7,
            "text": "Tab. 6 shows the Spearman correlation of different aspects on FED turn- and dialogue-level datasets. The main observations are listed as follows."
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='98' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 98,
            "page": 8,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 279
                },
                {
                    "x": 1198,
                    "y": 279
                },
                {
                    "x": 1198,
                    "y": 719
                },
                {
                    "x": 227,
                    "y": 719
                }
            ],
            "category": "figure",
            "html": "<figure><img id='99' style='font-size:20px' alt=\"BAGEL SFRES\nINF NAT FLU INF NAT FLU\n50 50 40 40\n40\n40\n40 40 30 30\n30 30\n30\n30 20 20 20\n20\nVAL 15 DDM VAL 15 M VAL 15 IDM VAL 15 IDM VAI 15 IDM VAIL 15 DM\nD\" data-coord=\"top-left:(227,279); bottom-right:(1198,719)\" /></figure>",
            "id": 99,
            "page": 8,
            "text": "BAGEL SFRES INF NAT FLU INF NAT FLU 50 50 40 40 40 40 40 40 30 30 30 30 30 30 20 20 20 20 VAL 15 DDM VAL 15 M VAL 15 IDM VAL 15 IDM VAI 15 IDM VAIL 15 DM D"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 723
                },
                {
                    "x": 1216,
                    "y": 723
                },
                {
                    "x": 1216,
                    "y": 954
                },
                {
                    "x": 224,
                    "y": 954
                }
            ],
            "category": "caption",
            "html": "<br><caption id='100' style='font-size:14px'>Figure 5. Experimental results for GPT3-based variants in data-to-<br>text task. Here, blue, orange, green, pink, and cyan dot denote that<br>GPTSCORE is built based on a01 (●), b01 ( ), c01 (●), d01 (●),<br>and d03 (●), respectively. The red lines (一) denote the average<br>performance of GPT3-based variants.</caption>",
            "id": 100,
            "page": 8,
            "text": "Figure 5. Experimental results for GPT3-based variants in data-totext task. Here, blue, orange, green, pink, and cyan dot denote that GPTSCORE is built based on a01 (●), b01 ( ), c01 (●), d01 (●), and d03 (●), respectively. The red lines (一) denote the average performance of GPT3-based variants."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1060
                },
                {
                    "x": 1218,
                    "y": 1060
                },
                {
                    "x": 1218,
                    "y": 1612
                },
                {
                    "x": 224,
                    "y": 1612
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:18px'>(1) The performance of GPT3-d01 is much better than<br>GPT3-d03, even though both of them have the same<br>model size. The average Spearman correlation of GPT3-<br>d01 outperforms GPT3-d03 by 40.8 on the FED Turn-level<br>dataset, and 5.5 on the FED dialogue-level. (2) The GPT3-<br>based model demonstrate stronger generalization abil-<br>ity. BART-based models failed in the evaluation of the<br>dialogue generation task, while the GPT3-a01 with 350M<br>parameters achieved comparable performance to FED and<br>DE models on both the FED turn-level and dialogue-level<br>datasets.</p>",
            "id": 101,
            "page": 8,
            "text": "(1) The performance of GPT3-d01 is much better than GPT3-d03, even though both of them have the same model size. The average Spearman correlation of GPT3d01 outperforms GPT3-d03 by 40.8 on the FED Turn-level dataset, and 5.5 on the FED dialogue-level. (2) The GPT3based model demonstrate stronger generalization ability. BART-based models failed in the evaluation of the dialogue generation task, while the GPT3-a01 with 350M parameters achieved comparable performance to FED and DE models on both the FED turn-level and dialogue-level datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1674
                },
                {
                    "x": 603,
                    "y": 1674
                },
                {
                    "x": 603,
                    "y": 1730
                },
                {
                    "x": 225,
                    "y": 1730
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:22px'>6. Ablation Study</p>",
            "id": 102,
            "page": 8,
            "text": "6. Ablation Study"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1761
                },
                {
                    "x": 859,
                    "y": 1761
                },
                {
                    "x": 859,
                    "y": 1810
                },
                {
                    "x": 225,
                    "y": 1810
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:18px'>6.1. Effectiveness of Demonstration</p>",
            "id": 103,
            "page": 8,
            "text": "6.1. Effectiveness of Demonstration"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1839
                },
                {
                    "x": 1216,
                    "y": 1839
                },
                {
                    "x": 1216,
                    "y": 2086
                },
                {
                    "x": 223,
                    "y": 2086
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:18px'>To investigate the relationship between the demonstration<br>sample size (denote as K) and the evaluation performance,<br>we choose the machine translation task and the GPT3-based<br>variants with model sizes ranging from 350M to 175B for<br>further study.</p>",
            "id": 104,
            "page": 8,
            "text": "To investigate the relationship between the demonstration sample size (denote as K) and the evaluation performance, we choose the machine translation task and the GPT3-based variants with model sizes ranging from 350M to 175B for further study."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2111
                },
                {
                    "x": 1217,
                    "y": 2111
                },
                {
                    "x": 1217,
                    "y": 2666
                },
                {
                    "x": 224,
                    "y": 2666
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:18px'>The change of Spearman correlation on the MQM-2020<br>dataset with different demonstration sample size are shown<br>in Fig. 6. The main observations are summarized as follows:<br>(1) The utilization of demonstration significantly improves<br>the evaluation performance, which holds for these three as-<br>pects. (2) There is an upper bound on the performance gains<br>from the introduction of the demonstration. For example,<br>when K>4, the performance of ACC is hard to improve fur-<br>ther. (3) When DM has only a few samples (such as K=1),<br>small models (e.g., GPT3-a01) are prone to performance<br>degradation due to the one-sidedness of the given examples.</p>",
            "id": 105,
            "page": 8,
            "text": "The change of Spearman correlation on the MQM-2020 dataset with different demonstration sample size are shown in Fig. 6. The main observations are summarized as follows: (1) The utilization of demonstration significantly improves the evaluation performance, which holds for these three aspects. (2) There is an upper bound on the performance gains from the introduction of the demonstration. For example, when K>4, the performance of ACC is hard to improve further. (3) When DM has only a few samples (such as K=1), small models (e.g., GPT3-a01) are prone to performance degradation due to the one-sidedness of the given examples."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2715
                },
                {
                    "x": 929,
                    "y": 2715
                },
                {
                    "x": 929,
                    "y": 2766
                },
                {
                    "x": 224,
                    "y": 2766
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:20px'>6.2. Partial Order of Evaluation Aspect</p>",
            "id": 106,
            "page": 8,
            "text": "6.2. Partial Order of Evaluation Aspect"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2794
                },
                {
                    "x": 1215,
                    "y": 2794
                },
                {
                    "x": 1215,
                    "y": 2994
                },
                {
                    "x": 224,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:18px'>To explore the correlation between aspects, we conducted<br>an empirical analysis with INT (interesting) on the dialogue<br>response generation task of the FED-Turn dataset. Specif-<br>ically, take INT as the target aspect and then combine the</p>",
            "id": 107,
            "page": 8,
            "text": "To explore the correlation between aspects, we conducted an empirical analysis with INT (interesting) on the dialogue response generation task of the FED-Turn dataset. Specifically, take INT as the target aspect and then combine the"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 270
                },
                {
                    "x": 2252,
                    "y": 270
                },
                {
                    "x": 2252,
                    "y": 1446
                },
                {
                    "x": 1274,
                    "y": 1446
                }
            ],
            "category": "table",
            "html": "<br><table id='108' style='font-size:14px'><tr><td rowspan=\"2\">Aspect</td><td colspan=\"5\">Baseline</td><td colspan=\"5\">GPTScore</td></tr><tr><td>BT</td><td>BTC</td><td>BTCP</td><td>FED</td><td>DE</td><td>a01</td><td>b01</td><td>c01</td><td>d01</td><td>d03</td></tr><tr><td colspan=\"11\">FED dialogue-level</td></tr><tr><td>COH</td><td>1.7</td><td>-14.9</td><td>-18.9</td><td>25.7</td><td>43.7</td><td>18.7</td><td>15.0</td><td>22.5</td><td>56.9</td><td>13.4</td></tr><tr><td>ERR</td><td>9.4</td><td>-12.2</td><td>-13.7</td><td>12.0</td><td>30.2</td><td>35.2</td><td>16.8</td><td>21.3</td><td>45.7</td><td>9.40</td></tr><tr><td>CON</td><td>2.6</td><td>-6.7</td><td>-10.2</td><td>11.6</td><td>36.7</td><td>33.7</td><td>9.9</td><td>18.4</td><td>32.9</td><td>18.1</td></tr><tr><td>DIV</td><td>13.3</td><td>-2.5</td><td>-13.9</td><td>13.7</td><td>37.8</td><td>14.9</td><td>5.20</td><td>21.5</td><td>62.8</td><td>-6.6</td></tr><tr><td>DEP</td><td>8.2</td><td>-6.6</td><td>-17.6</td><td>10.9</td><td>49.8</td><td>9.00</td><td>12.9</td><td>28.2</td><td>66.9</td><td>34.1</td></tr><tr><td>LIK</td><td>9.9</td><td>-6.3</td><td>-11.8</td><td>37.4</td><td>41.6</td><td>26.2</td><td>22.0</td><td>32.1</td><td>63.4</td><td>18.4</td></tr><tr><td>UND</td><td>-11.5</td><td>-17.6</td><td>-18.2</td><td>-0.3</td><td>36.5</td><td>31.2</td><td>40.0</td><td>40.0</td><td>52.4</td><td>19.6</td></tr><tr><td>FLE</td><td>9.3</td><td>-10.2</td><td>-10.3</td><td>24.9</td><td>38.3</td><td>32.7</td><td>44.9</td><td>34.6</td><td>51.5</td><td>7.20</td></tr><tr><td>INF</td><td>9.2</td><td>-7.5</td><td>-10.5</td><td>42.9</td><td>42.6</td><td>6.80</td><td>8.0</td><td>18.8</td><td>60.2</td><td>31.7</td></tr><tr><td>INQ</td><td>6.2</td><td>-0.6</td><td>-14.8</td><td>24.7</td><td>41.0</td><td>44.2</td><td>38.7</td><td>49.2</td><td>50.3</td><td>-10.1</td></tr><tr><td>Avg.</td><td>5.8</td><td>-8.5</td><td>-14.0</td><td>20.4</td><td>39.8</td><td>25.3</td><td>21.3</td><td>28.6</td><td>54.3</td><td>13.5</td></tr><tr><td colspan=\"11\">FED turn-level</td></tr><tr><td>INT</td><td>15.9</td><td>-3.3</td><td>-10.1</td><td>32.4</td><td>32.7</td><td>16.6</td><td>6.4</td><td>30.8</td><td>50.1</td><td>22.4</td></tr><tr><td>ENG</td><td>22.6</td><td>1.1</td><td>-2.5</td><td>24.0</td><td>30.0</td><td>10.2</td><td>6.2</td><td>29.4</td><td>49.6</td><td>35.5</td></tr><tr><td>SPE</td><td>8.3</td><td>-7.9</td><td>-16.2</td><td>14.1</td><td>34.6</td><td>33.7</td><td>16.1</td><td>31.7</td><td>21.4</td><td>15.1</td></tr><tr><td>REL</td><td>11.9</td><td>10.0</td><td>19.4</td><td>19.9</td><td>26.3</td><td>8.6</td><td>10.3</td><td>23.8</td><td>45.2</td><td>38.0</td></tr><tr><td>COR</td><td>7.6</td><td>1.8</td><td>12.4</td><td>26.2</td><td>24.2</td><td>29.7</td><td>11.2</td><td>27.0</td><td>43.4</td><td>42.8</td></tr><tr><td>SEM</td><td>10.0</td><td>18.8</td><td>26.1</td><td>-9.4</td><td>20.2</td><td>6.8</td><td>8.1</td><td>23.1</td><td>44.4</td><td>40.5</td></tr><tr><td>UND</td><td>12.0</td><td>8.1</td><td>4.5</td><td>1.3</td><td>20.0</td><td>6.6</td><td>14.8</td><td>23.4</td><td>36.5</td><td>31.1</td></tr><tr><td>FLU</td><td>14.0</td><td>17.2</td><td>28.4</td><td>-13.4</td><td>17.1</td><td>16.5</td><td>5.7</td><td>14.0</td><td>16.0</td><td>36.7</td></tr><tr><td>Avg.</td><td>12.8</td><td>5.7</td><td>7.7</td><td>11.9</td><td>25.6</td><td>16.1</td><td>9.9</td><td>25.4</td><td>38.3</td><td>32.8</td></tr></table>",
            "id": 108,
            "page": 8,
            "text": "Aspect Baseline GPTScore  BT BTC BTCP FED DE a01 b01 c01 d01 d03  FED dialogue-level  COH 1.7 -14.9 -18.9 25.7 43.7 18.7 15.0 22.5 56.9 13.4  ERR 9.4 -12.2 -13.7 12.0 30.2 35.2 16.8 21.3 45.7 9.40  CON 2.6 -6.7 -10.2 11.6 36.7 33.7 9.9 18.4 32.9 18.1  DIV 13.3 -2.5 -13.9 13.7 37.8 14.9 5.20 21.5 62.8 -6.6  DEP 8.2 -6.6 -17.6 10.9 49.8 9.00 12.9 28.2 66.9 34.1  LIK 9.9 -6.3 -11.8 37.4 41.6 26.2 22.0 32.1 63.4 18.4  UND -11.5 -17.6 -18.2 -0.3 36.5 31.2 40.0 40.0 52.4 19.6  FLE 9.3 -10.2 -10.3 24.9 38.3 32.7 44.9 34.6 51.5 7.20  INF 9.2 -7.5 -10.5 42.9 42.6 6.80 8.0 18.8 60.2 31.7  INQ 6.2 -0.6 -14.8 24.7 41.0 44.2 38.7 49.2 50.3 -10.1  Avg. 5.8 -8.5 -14.0 20.4 39.8 25.3 21.3 28.6 54.3 13.5  FED turn-level  INT 15.9 -3.3 -10.1 32.4 32.7 16.6 6.4 30.8 50.1 22.4  ENG 22.6 1.1 -2.5 24.0 30.0 10.2 6.2 29.4 49.6 35.5  SPE 8.3 -7.9 -16.2 14.1 34.6 33.7 16.1 31.7 21.4 15.1  REL 11.9 10.0 19.4 19.9 26.3 8.6 10.3 23.8 45.2 38.0  COR 7.6 1.8 12.4 26.2 24.2 29.7 11.2 27.0 43.4 42.8  SEM 10.0 18.8 26.1 -9.4 20.2 6.8 8.1 23.1 44.4 40.5  UND 12.0 8.1 4.5 1.3 20.0 6.6 14.8 23.4 36.5 31.1  FLU 14.0 17.2 28.4 -13.4 17.1 16.5 5.7 14.0 16.0 36.7  Avg. 12.8 5.7 7.7 11.9 25.6 16.1 9.9 25.4 38.3"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1460
                },
                {
                    "x": 2265,
                    "y": 1460
                },
                {
                    "x": 2265,
                    "y": 1687
                },
                {
                    "x": 1276,
                    "y": 1687
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:14px'>Table 6. Spearman correlation of different aspects on the FED turn-<br>and dialogue-level datasets. BT, BTC, BTCP, and DE denote<br>BARTSCORE, BARTSCORE+CNN, BARTSCORE+CNN+Para,<br>and DynaEval model, respectively. Values in bold indicate the best<br>performance.</p>",
            "id": 109,
            "page": 8,
            "text": "Table 6. Spearman correlation of different aspects on the FED turnand dialogue-level datasets. BT, BTC, BTCP, and DE denote BARTSCORE, BARTSCORE+CNN, BARTSCORE+CNN+Para, and DynaEval model, respectively. Values in bold indicate the best performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1801
                },
                {
                    "x": 2265,
                    "y": 1801
                },
                {
                    "x": 2265,
                    "y": 2099
                },
                {
                    "x": 1275,
                    "y": 2099
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:16px'>definitions of other aspects with the definition of INT as the<br>final evaluation protocols. The x-axis of Fig. 7-(a) is the<br>aspect order achieved based on the Spearman correlation<br>between INT and that aspect's human score. Fig. 7-(b) is<br>the Spearman correlation 0 INT as the modification of the<br>INT definition, and the scoring function is GPT3-c01.</p>",
            "id": 110,
            "page": 8,
            "text": "definitions of other aspects with the definition of INT as the final evaluation protocols. The x-axis of Fig. 7-(a) is the aspect order achieved based on the Spearman correlation between INT and that aspect's human score. Fig. 7-(b) is the Spearman correlation 0 INT as the modification of the INT definition, and the scoring function is GPT3-c01."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2126
                },
                {
                    "x": 2261,
                    "y": 2126
                },
                {
                    "x": 2261,
                    "y": 2221
                },
                {
                    "x": 1275,
                    "y": 2221
                }
            ],
            "category": "caption",
            "html": "<br><caption id='111' style='font-size:20px'>The following table illustrates the definition composition<br>process, where Sp denotes Spearman.</caption>",
            "id": 111,
            "page": 8,
            "text": "The following table illustrates the definition composition process, where Sp denotes Spearman."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2248
                },
                {
                    "x": 2229,
                    "y": 2248
                },
                {
                    "x": 2229,
                    "y": 2523
                },
                {
                    "x": 1278,
                    "y": 2523
                }
            ],
            "category": "table",
            "html": "<table id='112' style='font-size:16px'><tr><td>X Aspect</td><td>Aspect Definition</td><td>Sp</td></tr><tr><td>1 INT</td><td>Is this response interesting to the conversation?</td><td>30.8</td></tr><tr><td>3 INT, SPE</td><td>ENG, Is this an interesting response that is specific and engaging?</td><td>48.6</td></tr></table>",
            "id": 112,
            "page": 8,
            "text": "X Aspect Aspect Definition Sp  1 INT Is this response interesting to the conversation? 30.8  3 INT, SPE ENG, Is this an interesting response that is specific and engaging?"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2545
                },
                {
                    "x": 2266,
                    "y": 2545
                },
                {
                    "x": 2266,
                    "y": 2993
                },
                {
                    "x": 1274,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>Specifically, the definition of INT is \"Is this response inter-<br>esting to the conversation? \" x=1 in Fig. 7-(b). When<br>at<br>INT combines with ENG, SPE (at x=3 in Fig. 7-(b)), its<br>definition can be \"Is this an interesting response that is spe-<br>cific and engaging?\" · And the new aspect definition boosts<br>the performance from 30.8 (at x=1 in Fig. 7-(b)) to 48.6 (at<br>x=3 in Fig. 7-(b)). The best performance of 51.4 (x=5 in<br>Fig. 7-(b)) is achieved after combining five aspects (INT,<br>ENG, SPE, COR, REL), which already exceeded 50.1</p>",
            "id": 113,
            "page": 8,
            "text": "Specifically, the definition of INT is \"Is this response interesting to the conversation? \" x=1 in Fig. 7-(b). When at INT combines with ENG, SPE (at x=3 in Fig. 7-(b)), its definition can be \"Is this an interesting response that is specific and engaging?\" · And the new aspect definition boosts the performance from 30.8 (at x=1 in Fig. 7-(b)) to 48.6 (at x=3 in Fig. 7-(b)). The best performance of 51.4 (x=5 in Fig. 7-(b)) is achieved after combining five aspects (INT, ENG, SPE, COR, REL), which already exceeded 50.1"
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='114' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 114,
            "page": 9,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 234,
                    "y": 297
                },
                {
                    "x": 1203,
                    "y": 297
                },
                {
                    "x": 1203,
                    "y": 669
                },
                {
                    "x": 234,
                    "y": 669
                }
            ],
            "category": "figure",
            "html": "<figure><img id='115' style='font-size:14px' alt=\"20\n30 35\n15\n30\n25 10\n25\n5\n0 1 2 4 8 12 0 1 2 4 8 12 0 1 2 4 8 12\n(a) ACC (b) FLU (c) MQM\" data-coord=\"top-left:(234,297); bottom-right:(1203,669)\" /></figure>",
            "id": 115,
            "page": 9,
            "text": "20 30 35 15 30 25 10 25 5 0 1 2 4 8 12 0 1 2 4 8 12 0 1 2 4 8 12 (a) ACC (b) FLU (c) MQM"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 679
                },
                {
                    "x": 1217,
                    "y": 679
                },
                {
                    "x": 1217,
                    "y": 908
                },
                {
                    "x": 222,
                    "y": 908
                }
            ],
            "category": "caption",
            "html": "<br><caption id='116' style='font-size:14px'>Figure 6. Results of the GPT3 family models with different num-<br>bers of examples (K) in the demonstration on the MQM-2020<br>dataset. Here, blue, orange, green, red, and cyan lines denote<br>that GPTSCORE is built based on GPT3-a01 (▲), GPT3-b01 ),<br>GPT3-c01 ), GPT3-d01 (x), and GPT3-d03 (+), respectively.</caption>",
            "id": 116,
            "page": 9,
            "text": "Figure 6. Results of the GPT3 family models with different numbers of examples (K) in the demonstration on the MQM-2020 dataset. Here, blue, orange, green, red, and cyan lines denote that GPTSCORE is built based on GPT3-a01 (▲), GPT3-b01 ), GPT3-c01 ), GPT3-d01 (x), and GPT3-d03 (+), respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 999
                },
                {
                    "x": 1215,
                    "y": 999
                },
                {
                    "x": 1215,
                    "y": 1198
                },
                {
                    "x": 223,
                    "y": 1198
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>of the most potent scoring model GPT3-d01 with aspect def-<br>inition built only on INT. Therefore, combining definitions<br>with other highly correlated aspects can improve evaluation<br>performance.</p>",
            "id": 117,
            "page": 9,
            "text": "of the most potent scoring model GPT3-d01 with aspect definition built only on INT. Therefore, combining definitions with other highly correlated aspects can improve evaluation performance."
        },
        {
            "bounding_box": [
                {
                    "x": 236,
                    "y": 1263
                },
                {
                    "x": 1180,
                    "y": 1263
                },
                {
                    "x": 1180,
                    "y": 1612
                },
                {
                    "x": 236,
                    "y": 1612
                }
            ],
            "category": "figure",
            "html": "<figure><img id='118' style='font-size:14px' alt=\"100\nSpearman\n50\n50 Spearman 40\nINENSPEORENBENLU 30\n1 2 3 4 5 6 7 8\n(a) Aspect order (b) INT performance\" data-coord=\"top-left:(236,1263); bottom-right:(1180,1612)\" /></figure>",
            "id": 118,
            "page": 9,
            "text": "100 Spearman 50 50 Spearman 40 INENSPEORENBENLU 30 1 2 3 4 5 6 7 8 (a) Aspect order (b) INT performance"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1630
                },
                {
                    "x": 1215,
                    "y": 1630
                },
                {
                    "x": 1215,
                    "y": 1815
                },
                {
                    "x": 223,
                    "y": 1815
                }
            ],
            "category": "caption",
            "html": "<br><caption id='119' style='font-size:16px'>Figure 7. (a) Descending order of Spearman correlation between<br>INT and other aspects' human scoring. (b) The Spearman cor-<br>relation of INT changes as its aspect definition is modified in<br>combination with other aspects. The scoring model is GPT3-c01.</caption>",
            "id": 119,
            "page": 9,
            "text": "Figure 7. (a) Descending order of Spearman correlation between INT and other aspects' human scoring. (b) The Spearman correlation of INT changes as its aspect definition is modified in combination with other aspects. The scoring model is GPT3-c01."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1908
                },
                {
                    "x": 523,
                    "y": 1908
                },
                {
                    "x": 523,
                    "y": 1964
                },
                {
                    "x": 223,
                    "y": 1964
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:20px'>7. Conclusion</p>",
            "id": 120,
            "page": 9,
            "text": "7. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1994
                },
                {
                    "x": 1215,
                    "y": 1994
                },
                {
                    "x": 1215,
                    "y": 2597
                },
                {
                    "x": 224,
                    "y": 2597
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:16px'>In this paper, we propose to leverage the emergent abilities<br>from generative pre-training models to address intricate<br>and ever-changing evaluation requirements. The proposed<br>framework, GPTSCORE, is studied on multiple pre-trained<br>language models with different structures, including the<br>GPT3 with a model size of 175B. GPTSCORE has multiple<br>benefits: customizability, multi-faceted evaluation, and train-<br>free, which enable us to flexibly craft a metric that can<br>support 22 evaluation aspects on 37 datasets without any<br>learning process yet attain competitive performance. This<br>work opens a new way to audit generative AI by utilizing<br>generative AI.</p>",
            "id": 121,
            "page": 9,
            "text": "In this paper, we propose to leverage the emergent abilities from generative pre-training models to address intricate and ever-changing evaluation requirements. The proposed framework, GPTSCORE, is studied on multiple pre-trained language models with different structures, including the GPT3 with a model size of 175B. GPTSCORE has multiple benefits: customizability, multi-faceted evaluation, and trainfree, which enable us to flexibly craft a metric that can support 22 evaluation aspects on 37 datasets without any learning process yet attain competitive performance. This work opens a new way to audit generative AI by utilizing generative AI."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2656
                },
                {
                    "x": 649,
                    "y": 2656
                },
                {
                    "x": 649,
                    "y": 2714
                },
                {
                    "x": 224,
                    "y": 2714
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:22px'>Acknowledgements</p>",
            "id": 122,
            "page": 9,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2742
                },
                {
                    "x": 1217,
                    "y": 2742
                },
                {
                    "x": 1217,
                    "y": 2994
                },
                {
                    "x": 224,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:18px'>We thank Chen Zhang for helpful discussion and feedback.<br>This research / projectis supported by the National Research<br>Foundation, Singapore under its Industry Alignment Fund -<br>Pre-positioning (IAF-PP) Funding Initiative. Any opinions,<br>findings and conclusions or recommendations expressed in</p>",
            "id": 123,
            "page": 9,
            "text": "We thank Chen Zhang for helpful discussion and feedback. This research / projectis supported by the National Research Foundation, Singapore under its Industry Alignment Fund Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 285
                },
                {
                    "x": 2264,
                    "y": 285
                },
                {
                    "x": 2264,
                    "y": 485
                },
                {
                    "x": 1273,
                    "y": 485
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='124' style='font-size:18px'>this material are those of the author(s) and do not reflect the<br>views of National Research Foundation, Singapore. Pengfei<br>Liu is supported by a grant from the Singapore Defence<br>Science and Technology Agency.</p>",
            "id": 124,
            "page": 9,
            "text": "this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. Pengfei Liu is supported by a grant from the Singapore Defence Science and Technology Agency."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 545
                },
                {
                    "x": 1518,
                    "y": 545
                },
                {
                    "x": 1518,
                    "y": 599
                },
                {
                    "x": 1276,
                    "y": 599
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:22px'>References</p>",
            "id": 125,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 628
                },
                {
                    "x": 2268,
                    "y": 628
                },
                {
                    "x": 2268,
                    "y": 876
                },
                {
                    "x": 1272,
                    "y": 876
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:18px'>Adiwardana, D., Luong, M., So, D. R., Hall, J., Fiedel,<br>N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade,<br>G., Lu, Y., and Le, Q. V. Towards a human-like open-<br>domain chatbot. CoRR, abs/2001.09977, 2020. URL<br>https : / / arxiv · org/ abs/2001 · 09977.</p>",
            "id": 126,
            "page": 9,
            "text": "Adiwardana, D., Luong, M., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., and Le, Q. V. Towards a human-like opendomain chatbot. CoRR, abs/2001.09977, 2020. URL https : / / arxiv · org/ abs/2001 · 09977."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 920
                },
                {
                    "x": 2270,
                    "y": 920
                },
                {
                    "x": 2270,
                    "y": 1368
                },
                {
                    "x": 1272,
                    "y": 1368
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>Bhandari, M., Gour, P. N., Ashfaq, A., Liu, P., and Neubig,<br>G. Re-evaluating evaluation in text summarization. In<br>Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Pro-<br>ceedings of the 2020 Conference on Empirical Methods<br>in Natural Language Processing, EMNLP 2020, Online,<br>November 16-20, 2020, pp. 9347-9359. Association for<br>Computational Linguistics, 2020. doi: 10.18653/v1/<br>2020.emnlp-main.751. URL https : / / doi · org/10<br>18653/v1/2020 · emnlp-main · 751.</p>",
            "id": 127,
            "page": 9,
            "text": "Bhandari, M., Gour, P. N., Ashfaq, A., Liu, P., and Neubig, G. Re-evaluating evaluation in text summarization. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 9347-9359. Association for Computational Linguistics, 2020. doi: 10.18653/v1/ 2020.emnlp-main.751. URL https : / / doi · org/10 18653/v1/2020 · emnlp-main · 751."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1408
                },
                {
                    "x": 2269,
                    "y": 1408
                },
                {
                    "x": 2269,
                    "y": 1907
                },
                {
                    "x": 1272,
                    "y": 1907
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:18px'>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,<br>J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,<br>Askell, A., Agarwal, S., Herbert- Voss, A., Krueger, G.,<br>Henighan, T., Child, R., Ramesh, A., Ziegler, D. M.,<br>Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,<br>Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C.,<br>McCandlish, S., Radford, A., Sutskever, I., and Amodei,<br>D. Language models are few-shot learners. CoRR,<br>abs/2005.14165, 2020. URL https : / / arxiv · org/<br>abs/ 2005 · 14165.</p>",
            "id": 128,
            "page": 9,
            "text": "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert- Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. URL https : / / arxiv · org/ abs/ 2005 · 14165."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1953
                },
                {
                    "x": 2271,
                    "y": 1953
                },
                {
                    "x": 2271,
                    "y": 2252
                },
                {
                    "x": 1273,
                    "y": 2252
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>Castricato, L., Havrilla, A., Matiana, S., Pieler, M., Ye,<br>A., Yang, I., Frazier, S., and Riedl, M. 0. Robust<br>preference learning for storytelling via contrastive re-<br>inforcement learning. CoRR, abs/2210.07792, 2022.<br>doi: 10.48550/arXiv.2210.07792. URL https : / / doi .<br>org/10 · 48550/arXiv · 2210 · 07792.</p>",
            "id": 129,
            "page": 9,
            "text": "Castricato, L., Havrilla, A., Matiana, S., Pieler, M., Ye, A., Yang, I., Frazier, S., and Riedl, M. 0. Robust preference learning for storytelling via contrastive reinforcement learning. CoRR, abs/2210.07792, 2022. doi: 10.48550/arXiv.2210.07792. URL https : / / doi . org/10 · 48550/arXiv · 2210 · 07792."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2296
                },
                {
                    "x": 2269,
                    "y": 2296
                },
                {
                    "x": 2269,
                    "y": 2994
                },
                {
                    "x": 1268,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:20px'>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,<br>G., Roberts, A., Barham, P., Chung, H. W., Sutton,<br>C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,<br>S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,<br>N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,<br>Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,<br>G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,<br>S., Michalewski, H., Garcia, X., Misra, V., Robinson,<br>K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,<br>H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,<br>Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pel-<br>lat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov,<br>0., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M.,<br>Firat, 0., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,</p>",
            "id": 130,
            "page": 9,
            "text": "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, 0., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, 0., Catasta, M., Wei, J., Meier-Hellstern, K., Eck,"
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='131' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 131,
            "page": 10,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 265,
                    "y": 284
                },
                {
                    "x": 1219,
                    "y": 284
                },
                {
                    "x": 1219,
                    "y": 481
                },
                {
                    "x": 265,
                    "y": 481
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:14px'>D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling lan-<br>guage modeling with pathways. CoRR, abs/2204.02311,<br>2022. doi: 10.48550/arXiv.2204.02311. URL https :<br>/ / doi · org/1 0 · 48550/arxiv · 2204 · 02311.</p>",
            "id": 132,
            "page": 10,
            "text": "D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022. doi: 10.48550/arXiv.2204.02311. URL https : / / doi · org/1 0 · 48550/arxiv · 2204 · 02311."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 512
                },
                {
                    "x": 1217,
                    "y": 512
                },
                {
                    "x": 1217,
                    "y": 713
                },
                {
                    "x": 224,
                    "y": 713
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:22px'>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,<br>Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,<br>S., et al. Scaling instruction-finetuned language models.<br>arXiv preprint arXiv:2210.11416, 2022.</p>",
            "id": 133,
            "page": 10,
            "text": "Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S.,  Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 744
                },
                {
                    "x": 1217,
                    "y": 744
                },
                {
                    "x": 1217,
                    "y": 1292
                },
                {
                    "x": 226,
                    "y": 1292
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:18px'>Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:<br>pre-training of deep bidirectional transformers for lan-<br>guage understanding. In Burstein, J., Doran, C., and<br>Solorio, T. (eds.), Proceedings of the 2019 Conference of<br>the North American Chapter of the Associationfor Com-<br>putational Linguistics: Human Language Technologies,<br>NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,<br>2019, Volume 1 (Long and Short Papers), pp. 4171-4186.<br>Association for Computational Linguistics, 2019. doi:<br>10.18653/v1/n19-1423. URL https : / / doi · org/<br>10 · 18653/v1/n19-1423.</p>",
            "id": 134,
            "page": 10,
            "text": "Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https : / / doi · org/ 10 · 18653/v1/n19-1423."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1323
                },
                {
                    "x": 1218,
                    "y": 1323
                },
                {
                    "x": 1218,
                    "y": 1622
                },
                {
                    "x": 225,
                    "y": 1622
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>Fabbri, A. R., Kryscinski, W., McCann, B., Xiong,<br>C., Socher, R., and Radev, D. R. Summeval: Re-<br>evaluating summarization evaluation. Trans. Assoc. Com-<br>put. Linguistics, 9:391-409, 2021. doi: 10.1162/tacl\\<br>_al_00373. URL https : / / doi · org/10 · 1162/<br>tacl_a_ 00373.</p>",
            "id": 135,
            "page": 10,
            "text": "Fabbri, A. R., Kryscinski, W., McCann, B., Xiong, C., Socher, R., and Radev, D. R. Summeval: Reevaluating summarization evaluation. Trans. Assoc. Comput. Linguistics, 9:391-409, 2021. doi: 10.1162/tacl\\ _al_00373. URL https : / / doi · org/10 · 1162/ tacl_a_ 00373."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1655
                },
                {
                    "x": 1218,
                    "y": 1655
                },
                {
                    "x": 1218,
                    "y": 1902
                },
                {
                    "x": 224,
                    "y": 1902
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Freitag, M., Foster, G.F., Grangier, D., Ratnakar, V., Tan, Q.,<br>and Macherey, W. Experts, errors, and context: A large-<br>scale study of human evaluation for machine translation.<br>CoRR, abs/2104.14478, 2021. URL https : / / arxiv.<br>org/ abs/2104 · 14478.</p>",
            "id": 136,
            "page": 10,
            "text": "Freitag, M., Foster, G.F., Grangier, D., Ratnakar, V., Tan, Q., and Macherey, W. Experts, errors, and context: A largescale study of human evaluation for machine translation. CoRR, abs/2104.14478, 2021. URL https : / / arxiv. org/ abs/2104 · 14478."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1934
                },
                {
                    "x": 1217,
                    "y": 1934
                },
                {
                    "x": 1217,
                    "y": 2081
                },
                {
                    "x": 224,
                    "y": 2081
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:20px'>Fu, J., Ng, S.-K., and Liu, P. Polyglot prompt: Mul-<br>tilingual multitask promptraining. arXiv preprint<br>arXiv:2204.14264, 2022.</p>",
            "id": 137,
            "page": 10,
            "text": "Fu, J., Ng, S.-K., and Liu, P. Polyglot prompt: Multilingual multitask promptraining. arXiv preprint arXiv:2204.14264, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2114
                },
                {
                    "x": 1217,
                    "y": 2114
                },
                {
                    "x": 1217,
                    "y": 2664
                },
                {
                    "x": 225,
                    "y": 2664
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:18px'>Grusky, M., Naaman, M., and Artzi, Y. Newsroom: A<br>dataset of 1.3 million summaries with diverse extrac-<br>tive strategies. In Walker, M. A., Ji, H., and Stent,<br>A. (eds.), Proceedings of the 2018 Conference of the<br>North American Chapter of the Association for Com-<br>putational Linguistics: Human Language Technologies,<br>NAACL-HLT 2018, New Orleans, Louisiana, USA, June<br>1-6, 2018, Volume 1 (Long Papers), pp. 708-719. As-<br>sociation for Computational Linguistics, 2018. doi:<br>10.18653/v1/n18-1065. URL https : / / doi · org/<br>10 · 18653/v1/n18-1065.</p>",
            "id": 138,
            "page": 10,
            "text": "Grusky, M., Naaman, M., and Artzi, Y. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Walker, M. A., Ji, H., and Stent, A. (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 708-719. Association for Computational Linguistics, 2018. doi: 10.18653/v1/n18-1065. URL https : / / doi · org/ 10 · 18653/v1/n18-1065."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2691
                },
                {
                    "x": 1217,
                    "y": 2691
                },
                {
                    "x": 1217,
                    "y": 2993
                },
                {
                    "x": 225,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:20px'>Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt,<br>L., Kay, W., Suleyman, M., and Blunsom, P. Teaching<br>machines to read and comprehend. In Cortes, C.,<br>Lawrence, N. D., Lee, D. D., Sugiyama, M., and<br>Garnett, R. (eds.), Advances in Neural Information<br>Processing Systems 28: Annual Conference on Neural</p>",
            "id": 139,
            "page": 10,
            "text": "Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. Teaching machines to read and comprehend. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 284
                },
                {
                    "x": 2371,
                    "y": 284
                },
                {
                    "x": 2371,
                    "y": 581
                },
                {
                    "x": 1312,
                    "y": 581
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:18px'>Information Processing Systems 2015, December<br>7-12, 2015, Montreal, Quebec, Canada, pp. 1693-<br>1701, 2015. URL https : / /proceedings.<br>neurips · cc/paper /2015/hash/<br>afdec7005oc5614802pd04745dif0c96-Aber/rect.<br>html.</p>",
            "id": 140,
            "page": 10,
            "text": "Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 16931701, 2015. URL https : / /proceedings. neurips · cc/paper /2015/hash/ afdec7005oc5614802pd04745dif0c96-Aber/rect. html."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 617
                },
                {
                    "x": 2269,
                    "y": 617
                },
                {
                    "x": 2269,
                    "y": 1068
                },
                {
                    "x": 1272,
                    "y": 1068
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>Hu, J. E., Singh, A., Holzenberger, N., Post, M., and Durme,<br>B. V. Large-scale, diverse, paraphrastic bitexts via sam-<br>pling and clustering. In Bansal, M. and Villavicencio,<br>A. (eds.), Proceedings of the 23rd Conference on Com-<br>putational Natural Language Learning, CoNLL 2019,<br>Hong Kong, China, November 3-4, 2019, pp. 44-54.<br>Association for Computational Linguistics, 2019. doi:<br>10.18653/v1/K19-1005. URL https : / / doi · org/<br>10 · 18653/v1 /K1 9-1005.</p>",
            "id": 141,
            "page": 10,
            "text": "Hu, J. E., Singh, A., Holzenberger, N., Post, M., and Durme, B. V. Large-scale, diverse, paraphrastic bitexts via sampling and clustering. In Bansal, M. and Villavicencio, A. (eds.), Proceedings of the 23rd Conference on Computational Natural Language Learning, CoNLL 2019, Hong Kong, China, November 3-4, 2019, pp. 44-54. Association for Computational Linguistics, 2019. doi: 10.18653/v1/K19-1005. URL https : / / doi · org/ 10 · 18653/v1 /K1 9-1005."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1102
                },
                {
                    "x": 2270,
                    "y": 1102
                },
                {
                    "x": 2270,
                    "y": 1501
                },
                {
                    "x": 1275,
                    "y": 1501
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Kusner, M. J., Sun, Y., Kolkin, N. I., and Weinberger, K. Q.<br>From word embeddings to document distances. In Bach,<br>F. R. and Blei, D. M. (eds.), Proceedings of the 32nd<br>International Conference on Machine Learning, ICML<br>2015, Lille, France, 6-11 July 2015, volume 37 of JMLR<br>Workshop and Conference Proceedings, pp. 957-966.<br>JMLR.org, 2015. URL http : / /proceedings · mlr.<br>press/v37 / kusnerb15 · html.</p>",
            "id": 142,
            "page": 10,
            "text": "Kusner, M. J., Sun, Y., Kolkin, N. I., and Weinberger, K. Q. From word embeddings to document distances. In Bach, F. R. and Blei, D. M. (eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 957-966. JMLR.org, 2015. URL http : / /proceedings · mlr. press/v37 / kusnerb15 · html."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1538
                },
                {
                    "x": 2269,
                    "y": 1538
                },
                {
                    "x": 2269,
                    "y": 2089
                },
                {
                    "x": 1269,
                    "y": 2089
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:18px'>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-<br>hamed, A., Levy, 0., Stoyanov, V., and Zettlemoyer,<br>L. BART: denoising sequence-to-sequence pre-training<br>for natural language generation, translation, and com-<br>prehension. In Jurafsky, D., Chai, J., Schluter, N., and<br>Tetreault, J. R. (eds.), Proceedings of the 58th Annual<br>Meeting of the Association for Computational Linguis-<br>tics, ACL 2020, Online, July 5-10, 2020, pp. 7871-7880.<br>Association for Computational Linguistics, 2020. doi:<br>10.18653/v1/2020.acl-main. 703. URL https : / / doi ·<br>org/10 · 18653/v1/2020 · acl -main · 703.</p>",
            "id": 143,
            "page": 10,
            "text": "Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, 0., Stoyanov, V., and Zettlemoyer, L. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. R. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871-7880. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main. 703. URL https : / / doi · org/10 · 18653/v1/2020 · acl -main · 703."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2120
                },
                {
                    "x": 2266,
                    "y": 2120
                },
                {
                    "x": 2266,
                    "y": 2673
                },
                {
                    "x": 1267,
                    "y": 2673
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>Li, Z., Zhang, J., Fei, Z., Feng, Y., and Zhou, J. Conversa-<br>tions are not flat: Modeling the dynamic information flow<br>across dialogue utterances. In Zong, C., Xia, F., Li, W.,<br>and Navigli, R. (eds.), Proceedings of the 59th Annual<br>Meeting of the Association for Computational Linguistics<br>and the 11th International Joint Conference on Natural<br>Language Processing, ACL/IJCNLP 2021, (Volume 1:<br>Long Papers), Virtual Event, August 1-6, 2021, pp. 128-<br>138. Association for Computational Linguistics, 2021.<br>doi: 10.18653/v1/2021.acl-long.11. URL https : / /<br>doi · org/10 · 18653/v1/2021 · acl-long · 11.</p>",
            "id": 144,
            "page": 10,
            "text": "Li, Z., Zhang, J., Fei, Z., Feng, Y., and Zhou, J. Conversations are not flat: Modeling the dynamic information flow across dialogue utterances. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 128138. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.11. URL https : / / doi · org/10 · 18653/v1/2021 · acl-long · 11."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2706
                },
                {
                    "x": 2266,
                    "y": 2706
                },
                {
                    "x": 2266,
                    "y": 2855
                },
                {
                    "x": 1274,
                    "y": 2855
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:16px'>Lin, C.-Y. Rouge: A package for automatic evaluation<br>of summaries. In Text summarization branches out, pp.<br>74-81, 2004.</p>",
            "id": 145,
            "page": 10,
            "text": "Lin, C.-Y. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2892
                },
                {
                    "x": 2267,
                    "y": 2892
                },
                {
                    "x": 2267,
                    "y": 2994
                },
                {
                    "x": 1274,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:20px'>Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,<br>G. Pre-train, prompt, and predict: A systematic survey of</p>",
            "id": 146,
            "page": 10,
            "text": "Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. Pre-train, prompt, and predict: A systematic survey of"
        },
        {
            "bounding_box": [
                {
                    "x": 958,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 234
                },
                {
                    "x": 958,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<header id='147' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 147,
            "page": 11,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 263,
                    "y": 287
                },
                {
                    "x": 1213,
                    "y": 287
                },
                {
                    "x": 1213,
                    "y": 382
                },
                {
                    "x": 263,
                    "y": 382
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:20px'>prompting methods in natural language processing. arXiv<br>preprint arXiv:2107.13586, 2021.</p>",
            "id": 148,
            "page": 11,
            "text": "prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 416
                },
                {
                    "x": 1217,
                    "y": 416
                },
                {
                    "x": 1217,
                    "y": 664
                },
                {
                    "x": 223,
                    "y": 664
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:18px'>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,<br>0., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta:<br>A robustly optimized BERT pretraining approach. CoRR,<br>abs/1907.11692, 2019. URL http : / / arxiv · org/<br>abs/1907 · 11692.</p>",
            "id": 149,
            "page": 11,
            "text": "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, 0., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http : / / arxiv · org/ abs/1907 · 11692."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 698
                },
                {
                    "x": 1218,
                    "y": 698
                },
                {
                    "x": 1218,
                    "y": 948
                },
                {
                    "x": 222,
                    "y": 948
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:20px'>Liu, Y., Fabbri, A. R., Liu, P., Zhao, Y., Nan, L., Han,<br>R., Han, S., Joty, S., Wu, C.-S., Xiong, C., et al. Re-<br>visiting the gold standard: Grounding summarization<br>evaluation with robust human evaluation. arXiv preprint<br>arXiv:2212.07981, 2022.</p>",
            "id": 150,
            "page": 11,
            "text": "Liu, Y., Fabbri, A. R., Liu, P., Zhao, Y., Nan, L., Han, R., Han, S., Joty, S., Wu, C.-S., Xiong, C.,  Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. arXiv preprint arXiv:2212.07981, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 981
                },
                {
                    "x": 1217,
                    "y": 981
                },
                {
                    "x": 1217,
                    "y": 1431
                },
                {
                    "x": 225,
                    "y": 1431
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:20px'>Mairesse, F., Gasic, M., Jurc�cek, F., Keizer, S., Thom-<br>son, B., Yu, K., and Young, S. J. Phrase-based statis-<br>tical language generation using graphical models and<br>active learning. In Hajic, J., Carberry, S., and Clark,<br>S. (eds.), ACL 2010, Proceedings of the 48th Annual<br>Meeting of the Association for Computational Linguis-<br>tics, July 11-16, 2010, Uppsala, Sweden, pp. 1552-1561.<br>The Association for Computer Linguistics, 2010. URL<br>https : / / acl anthology · org/P10-1157/.</p>",
            "id": 151,
            "page": 11,
            "text": "Mairesse, F., Gasic, M., Jurc�cek, F., Keizer, S., Thomson, B., Yu, K., and Young, S. J. Phrase-based statistical language generation using graphical models and active learning. In Hajic, J., Carberry, S., and Clark, S. (eds.), ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden, pp. 1552-1561. The Association for Computer Linguistics, 2010. URL https : / / acl anthology · org/P10-1157/."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1464
                },
                {
                    "x": 1216,
                    "y": 1464
                },
                {
                    "x": 1216,
                    "y": 1712
                },
                {
                    "x": 224,
                    "y": 1712
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:16px'>Matiana, S., Smith, J. R., Teehan, R., Castricato, L.,<br>Biderman, S., Gao, L., and Frazier, S. Cut the<br>CARP: fishing for zero-shot story evaluation. CoRR,<br>abs/2110.03111, 2021. URL https : / / arxiv · org/<br>abs / 2110 · 03111.</p>",
            "id": 152,
            "page": 11,
            "text": "Matiana, S., Smith, J. R., Teehan, R., Castricato, L., Biderman, S., Gao, L., and Frazier, S. Cut the CARP: fishing for zero-shot story evaluation. CoRR, abs/2110.03111, 2021. URL https : / / arxiv · org/ abs / 2110 · 03111."
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 1746
                },
                {
                    "x": 1218,
                    "y": 1746
                },
                {
                    "x": 1218,
                    "y": 2197
                },
                {
                    "x": 227,
                    "y": 2197
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>Mehri, S. and Eskenazi, M. Unsupervised evaluation of<br>interactive dialog with dialogpt. In Pietquin, 0., Mure-<br>san, S., Chen, V., Kennington, C., Vandyke, D., Dethlefs,<br>N., Inoue, K., Ekstedt, E., and Ultes, S. (eds.), Proceed-<br>ings of the 21th Annual Meeting of the Special Interest<br>Group on Discourse and Dialogue, SIGdial 2020, 1st<br>virtual meeting, July 1-3, 2020, pp. 225-235. Associa-<br>tion for Computational Linguistics, 2020. URL https :<br>/ / aclanthology · org/2020 · sigdial - 1 · 28/.</p>",
            "id": 153,
            "page": 11,
            "text": "Mehri, S. and Eskenazi, M. Unsupervised evaluation of interactive dialog with dialogpt. In Pietquin, 0., Muresan, S., Chen, V., Kennington, C., Vandyke, D., Dethlefs, N., Inoue, K., Ekstedt, E., and Ultes, S. (eds.), Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGdial 2020, 1st virtual meeting, July 1-3, 2020, pp. 225-235. Association for Computational Linguistics, 2020. URL https : / / aclanthology · org/2020 · sigdial - 1 · 28/."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2227
                },
                {
                    "x": 1221,
                    "y": 2227
                },
                {
                    "x": 1221,
                    "y": 2475
                },
                {
                    "x": 223,
                    "y": 2475
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,<br>Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of<br>demonstrations: What makes in-context learning work?<br>CoRR, abs/2202.12837, 2022. URL https : / / arxiv.<br>org/ abs / 2202 · 12837.</p>",
            "id": 154,
            "page": 11,
            "text": "Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? CoRR, abs/2202.12837, 2022. URL https : / / arxiv. org/ abs / 2202 · 12837."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2510
                },
                {
                    "x": 1218,
                    "y": 2510
                },
                {
                    "x": 1218,
                    "y": 2658
                },
                {
                    "x": 224,
                    "y": 2658
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:16px'>Mukaka, M. M. A guide to appropriate use of correlation<br>coefficient in medical research. Malawi medical journal,<br>24(3):69-71, 2012.</p>",
            "id": 155,
            "page": 11,
            "text": "Mukaka, M. M. A guide to appropriate use of correlation coefficient in medical research. Malawi medical journal, 24(3):69-71, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2692
                },
                {
                    "x": 1216,
                    "y": 2692
                },
                {
                    "x": 1216,
                    "y": 2993
                },
                {
                    "x": 225,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:20px'>Nenkova, A. and Passonneau, R. Evaluating content se-<br>lection in summarization: The pyramid method. In Pro-<br>ceedings of the Human Language Technology Confer-<br>ence of the North American Chapter of the Association<br>for Computational Linguistics: HLT-NAACL 2004, pp.<br>145-152, Boston, Massachusetts, USA, May 2 - May 7</p>",
            "id": 156,
            "page": 11,
            "text": "Nenkova, A. and Passonneau, R. Evaluating content selection in summarization: The pyramid method. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pp. 145-152, Boston, Massachusetts, USA, May 2 - May 7"
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 285
                },
                {
                    "x": 2263,
                    "y": 285
                },
                {
                    "x": 2263,
                    "y": 383
                },
                {
                    "x": 1316,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='157' style='font-size:14px'>2004. Association for Computational Linguistics. URL<br>https : / / aclanthology · org/N04-1019.</p>",
            "id": 157,
            "page": 11,
            "text": "2004. Association for Computational Linguistics. URL https : / / aclanthology · org/N04-1019."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 417
                },
                {
                    "x": 2268,
                    "y": 417
                },
                {
                    "x": 2268,
                    "y": 665
                },
                {
                    "x": 1273,
                    "y": 665
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:22px'>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,<br>C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,<br>K., Ray, A., et al. Training language models to fol-<br>low instructions with human feedback. arXiv preprint<br>arXiv:2203.02155, 2022.</p>",
            "id": 158,
            "page": 11,
            "text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,  Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 697
                },
                {
                    "x": 2269,
                    "y": 697
                },
                {
                    "x": 2269,
                    "y": 1196
                },
                {
                    "x": 1272,
                    "y": 1196
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:18px'>Pang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and<br>Tu, K. Towards holistic and automatic evaluation of<br>open-domain dialogue generation. In Jurafsky, D.,<br>Chai, J., Schluter, N., and Tetreault, J. R. (eds.), Pro-<br>ceedings of the 58th Annual Meeting of the Associ-<br>ation for Computational Linguistics, ACL 2020, On-<br>line, July 5-10, 2020, pp. 3619-3629. Association for<br>Computational Linguistics, 2020. doi: 10.18653/v1/<br>2020.acl-main.333. URL https : / / doi · org/10.<br>18653/v1 /2020 · acl-main · 333.</p>",
            "id": 159,
            "page": 11,
            "text": "Pang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. Towards holistic and automatic evaluation of open-domain dialogue generation. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. R. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 3619-3629. Association for Computational Linguistics, 2020. doi: 10.18653/v1/ 2020.acl-main.333. URL https : / / doi · org/10. 18653/v1 /2020 · acl-main · 333."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1231
                },
                {
                    "x": 2269,
                    "y": 1231
                },
                {
                    "x": 2269,
                    "y": 1582
                },
                {
                    "x": 1275,
                    "y": 1582
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>Papineni, K., Roukos, S., Ward, T., and Zhu, W. Bleu:<br>a method for automatic evaluation of machine transla-<br>tion. In Proceedings of the 40th Annual Meeting of<br>the Association for Computational Linguistics, July 6-<br>12, 2002, Philadelphia, PA, USA, pp. 311-318. ACL,<br>2002. doi: 10.3115/1073083.1073135. URL https:<br>/ / aclanthology · org/P02-1040/.</p>",
            "id": 160,
            "page": 11,
            "text": "Papineni, K., Roukos, S., Ward, T., and Zhu, W. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 612, 2002, Philadelphia, PA, USA, pp. 311-318. ACL, 2002. doi: 10.3115/1073083.1073135. URL https: / / aclanthology · org/P02-1040/."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1613
                },
                {
                    "x": 2267,
                    "y": 1613
                },
                {
                    "x": 2267,
                    "y": 1961
                },
                {
                    "x": 1274,
                    "y": 1961
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:16px'>Popovic, M. chrf: character n-gram f-score for automatic<br>MT evaluation. In Proceedings of the Tenth Workshop on<br>Statistical Machine Translation, WMT@EMNLP 2015,<br>17-18 September 2015, Lisbon, Portugal, pp. 392-395.<br>The Association for Computer Linguistics, 2015. doi:<br>10.18653/v1/w15-3049. URL https : / /doi · org/<br>10 · 18653/v1 /w15- 3049.</p>",
            "id": 161,
            "page": 11,
            "text": "Popovic, M. chrf: character n-gram f-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal, pp. 392-395. The Association for Computer Linguistics, 2015. doi: 10.18653/v1/w15-3049. URL https : / /doi · org/ 10 · 18653/v1 /w15- 3049."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1995
                },
                {
                    "x": 2266,
                    "y": 1995
                },
                {
                    "x": 2266,
                    "y": 2145
                },
                {
                    "x": 1275,
                    "y": 2145
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:20px'>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,<br>Sutskever, I., et al. Language models are unsupervised<br>multitask learners. OpenAI blog, 1(8):9, 2019.</p>",
            "id": 162,
            "page": 11,
            "text": "Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.,  Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2175
                },
                {
                    "x": 2270,
                    "y": 2175
                },
                {
                    "x": 2270,
                    "y": 2475
                },
                {
                    "x": 1273,
                    "y": 2475
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:18px'>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,<br>Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the<br>limits of transfer learning with a unified text-to-text trans-<br>former. J. Mach. Learn. Res., 21:140:1-140:67, 2020.<br>URL http: / / jmlr · org/papers/v21/20-074.<br>html.</p>",
            "id": 163,
            "page": 11,
            "text": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67, 2020. URL http: / / jmlr · org/papers/v21/20-074. html."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2511
                },
                {
                    "x": 2267,
                    "y": 2511
                },
                {
                    "x": 2267,
                    "y": 2708
                },
                {
                    "x": 1274,
                    "y": 2708
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:14px'>Rei, R., Stewart, C., Farinha, A. C., and Lavie, A.<br>COMET: A neural framework for MT evaluation. CoRR,<br>abs/2009.09025, 2020. URL https : / / arxiv · org/<br>abs/ 2009 · 09025.</p>",
            "id": 164,
            "page": 11,
            "text": "Rei, R., Stewart, C., Farinha, A. C., and Lavie, A. COMET: A neural framework for MT evaluation. CoRR, abs/2009.09025, 2020. URL https : / / arxiv · org/ abs/ 2009 · 09025."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2742
                },
                {
                    "x": 2267,
                    "y": 2742
                },
                {
                    "x": 2267,
                    "y": 2990
                },
                {
                    "x": 1276,
                    "y": 2990
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:20px'>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,<br>Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,<br>A., et al. Multitask prompted training enables zero-shot<br>task generalization. arXiv preprint arXiv:2110.08207,<br>2021.</p>",
            "id": 165,
            "page": 11,
            "text": "Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A.,  Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 191
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='166' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 166,
            "page": 12,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 280
                },
                {
                    "x": 1220,
                    "y": 280
                },
                {
                    "x": 1220,
                    "y": 1034
                },
                {
                    "x": 222,
                    "y": 1034
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow,<br>D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M.,<br>Tow, J., Rush, A. M., Biderman, S., Webson, A., Am-<br>manamanchi, P. S., Wang, T., Sagot, B., Muennighoff,<br>N., del Moral, A. V., Ruwase, O., Bawden, R., Bek-<br>man, S., McMillan-Major, A., Beltagy, I., Nguyen, H.,<br>Saulnier, L., Tan, S., Suarez, P. 0., Sanh, V., Lauren�on,<br>H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C.,<br>Gokaslan, A., Simhi, A., Soroa, A., Aji, A. F., Alfassy, A.,<br>Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C.,<br>Klamm, C., Leong, C., van Strien, D., Adelani, D. I.,<br>and et al. BLOOM: A 176b-parameter open-access<br>multilingual language model. CoRR, abs/2211.05100,<br>2022. doi: 10.48550/arXiv.2211.05100. URL https :<br>/ / doi · org/10 · 48550 /arXiv · 2211 · 05100.</p>",
            "id": 167,
            "page": 12,
            "text": "Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P. 0., Sanh, V., Lauren�on, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A. F., Alfassy, A., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C., Klamm, C., Leong, C., van Strien, D., Adelani, D. I., and  BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. doi: 10.48550/arXiv.2211.05100. URL https : / / doi · org/10 · 48550 /arXiv · 2211 · 05100."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1062
                },
                {
                    "x": 1219,
                    "y": 1062
                },
                {
                    "x": 1219,
                    "y": 1613
                },
                {
                    "x": 224,
                    "y": 1613
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:18px'>Scialom, T., Dray, P., Lamprier, S., Piwowarski, B., Sta-<br>iano, J., Wang, A., and Gallinari, P. Questeval: Sum-<br>marization asks for fact-based evaluation. In Moens,<br>M., Huang, X., Specia, L., and Yih, S. W. (eds.), Pro-<br>ceedings of the 2021 Conference on Empirical Meth-<br>ods in Natural Language Processing, EMNLP 2021,<br>Virtual Event / Punta Cana, Dominican Republic, 7-<br>11 November, 2021, pp. 6594-6604. Association for<br>Computational Linguistics, 2021. doi: 10.18653/v1/<br>2021.emnlp-main.529. URL https : / / doi · org/10.<br>18653/v1/2021 · emnlp-main · 529.</p>",
            "id": 168,
            "page": 12,
            "text": "Scialom, T., Dray, P., Lamprier, S., Piwowarski, B., Staiano, J., Wang, A., and Gallinari, P. Questeval: Summarization asks for fact-based evaluation. In Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 711 November, 2021, pp. 6594-6604. Association for Computational Linguistics, 2021. doi: 10.18653/v1/ 2021.emnlp-main.529. URL https : / / doi · org/10. 18653/v1/2021 · emnlp-main · 529."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1645
                },
                {
                    "x": 1220,
                    "y": 1645
                },
                {
                    "x": 1220,
                    "y": 2095
                },
                {
                    "x": 224,
                    "y": 2095
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:18px'>Sellam, T., Das, D., and Parikh, A. P. BLEURT: learn-<br>ing robust metrics for text generation. In Jurafsky,<br>D., Chai, J., Schluter, N., and Tetreault, J. R. (eds.),<br>Proceedings of the 58th Annual Meeting of the Asso-<br>ciation for Computational Linguistics, ACL 2020, On-<br>line, July 5-10, 2020, pp. 7881-7892. Association for<br>Computational Linguistics, 2020. doi: 10.18653/v1/<br>2020.acl-main.704. URL https : / / doi · org/10.<br>18653/v1/2020 · acl-main · 704.</p>",
            "id": 169,
            "page": 12,
            "text": "Sellam, T., Das, D., and Parikh, A. P. BLEURT: learning robust metrics for text generation. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. R. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7881-7892. Association for Computational Linguistics, 2020. doi: 10.18653/v1/ 2020.acl-main.704. URL https : / / doi · org/10. 18653/v1/2020 · acl-main · 704."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2127
                },
                {
                    "x": 1216,
                    "y": 2127
                },
                {
                    "x": 1216,
                    "y": 2325
                },
                {
                    "x": 223,
                    "y": 2325
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:14px'>Sequoia, T. Generative ai: A creative new world.<br>https : / /www · sequoiacap · com/ article/<br>generative-ai-e-creative-new-vod16/.<br>2022.</p>",
            "id": 170,
            "page": 12,
            "text": "Sequoia, T. Generative ai: A creative new world. https : / /www · sequoiacap · com/ article/ generative-ai-e-creative-new-vod16/. 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2361
                },
                {
                    "x": 1222,
                    "y": 2361
                },
                {
                    "x": 1222,
                    "y": 2809
                },
                {
                    "x": 225,
                    "y": 2809
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:18px'>Thompson, B. and Post, M. Automatic machine transla-<br>tion evaluation in many languages via zero-shot para-<br>phrasing. In Webber, B., Cohn, T., He, Y., and Liu, Y.<br>(eds.), Proceedings of the 2020 Conference on Empiri-<br>cal Methods in Natural Language Processing, EMNLP<br>2020, Online, November 16-20, 2020, pp. 90-121. As-<br>sociation for Computational Linguistics, 2020. doi:<br>10.18653/v1/2020.emnlp-main.8. URL https : / /doi .<br>org/10 · 18653/v1/2020 · emnlp-main . 8.</p>",
            "id": 171,
            "page": 12,
            "text": "Thompson, B. and Post, M. Automatic machine translation evaluation in many languages via zero-shot paraphrasing. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 90-121. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.8. URL https : / /doi . org/10 · 18653/v1/2020 · emnlp-main . 8."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2841
                },
                {
                    "x": 1219,
                    "y": 2841
                },
                {
                    "x": 1219,
                    "y": 2994
                },
                {
                    "x": 224,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:20px'>Wang, A., Cho, K., and Lewis, M. Asking and answer-<br>ing questions to evaluate the factual consistency of sum-<br>maries. In Jurafsky, D., Chai, J., Schluter, N., and</p>",
            "id": 172,
            "page": 12,
            "text": "Wang, A., Cho, K., and Lewis, M. Asking and answering questions to evaluate the factual consistency of summaries. In Jurafsky, D., Chai, J., Schluter, N., and"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 283
                },
                {
                    "x": 2267,
                    "y": 283
                },
                {
                    "x": 2267,
                    "y": 583
                },
                {
                    "x": 1313,
                    "y": 583
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='173' style='font-size:18px'>Tetreault, J. R. (eds.), Proceedings of the 58th Annual<br>Meeting of the Association for Computational Linguis-<br>tics, ACL 2020, Online, July 5-10, 2020, pp. 5008-5020.<br>Association for Computational Linguistics, 2020. doi:<br>10.18653/v1/2020.acl-main.450. URL https : / / doi<br>org/10 · 18653/v1/2020 · acl -main · 450.</p>",
            "id": 173,
            "page": 12,
            "text": "Tetreault, J. R. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 5008-5020. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.450. URL https : / / doi org/10 · 18653/v1/2020 · acl -main · 450."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 621
                },
                {
                    "x": 2268,
                    "y": 621
                },
                {
                    "x": 2268,
                    "y": 873
                },
                {
                    "x": 1274,
                    "y": 873
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:22px'>Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,<br>Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran,<br>A. S., Naik, A., Stap, D., et al. Super-naturalinstructions:<br>Generalization via declarative instructions on 1600+ nlp<br>tasks. URL https://arxiv. org/abs/2204.07705, 2022.</p>",
            "id": 174,
            "page": 12,
            "text": "Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D.,  Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. URL https://arxiv. org/abs/2204.07705, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 910
                },
                {
                    "x": 2267,
                    "y": 910
                },
                {
                    "x": 2267,
                    "y": 1158
                },
                {
                    "x": 1276,
                    "y": 1158
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:18px'>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi,<br>E. H., Le, Q., and Zhou, D. Chain of thought prompt-<br>ing elicits reasoning in large language models. CoRR,<br>abs/2201.11903, 2022. URL https : / / arxiv · org/<br>abs/ 2201 · 11903.</p>",
            "id": 175,
            "page": 12,
            "text": "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E. H., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903, 2022. URL https : / / arxiv · org/ abs/ 2201 · 11903."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1197
                },
                {
                    "x": 2268,
                    "y": 1197
                },
                {
                    "x": 2268,
                    "y": 1697
                },
                {
                    "x": 1276,
                    "y": 1697
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:18px'>Wen, T., Gasic, M., Mrksic, N., Su, P., Vandyke, D., and<br>Young, S. J. Semantically conditioned lstm-based nat-<br>ural language generation for spoken dialogue systems.<br>In Marquez, L., Callison-Burch, C., Su, J., Pighin, D.,<br>and Marton, Y. (eds.), Proceedings of the 2015 Confer-<br>ence on Empirical Methods in Natural Language Process-<br>ing, EMNLP 2015, Lisbon, Portugal, September 17-21,<br>2015, pp. 1711-1721. The Association for Computational<br>Linguistics, 2015. doi: 10.18653/v1/d15-1199. URL<br>https : / / doi · org/10 · 18653 /v1 / d15-1199.</p>",
            "id": 176,
            "page": 12,
            "text": "Wen, T., Gasic, M., Mrksic, N., Su, P., Vandyke, D., and Young, S. J. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. In Marquez, L., Callison-Burch, C., Su, J., Pighin, D., and Marton, Y. (eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pp. 1711-1721. The Association for Computational Linguistics, 2015. doi: 10.18653/v1/d15-1199. URL https : / / doi · org/10 · 18653 /v1 / d15-1199."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1738
                },
                {
                    "x": 2268,
                    "y": 1738
                },
                {
                    "x": 2268,
                    "y": 1985
                },
                {
                    "x": 1276,
                    "y": 1985
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:16px'>Xu, W., Qian, X., Wang, M., Li, L., and Wang, W. Y.<br>Sescore2: Retrieval augmented pretraining for text gener-<br>ation evaluation. CoRR, abs/2212.09305, 2022a. doi: 10.<br>48550/arXiv.2212.09305. URL https : / / doi · org/<br>10 · 48550/arXiv · 2212 · 09305.</p>",
            "id": 177,
            "page": 12,
            "text": "Xu, W., Qian, X., Wang, M., Li, L., and Wang, W. Y. Sescore2: Retrieval augmented pretraining for text generation evaluation. CoRR, abs/2212.09305, 2022a. doi: 10. 48550/arXiv.2212.09305. URL https : / / doi · org/ 10 · 48550/arXiv · 2212 · 09305."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2023
                },
                {
                    "x": 2268,
                    "y": 2023
                },
                {
                    "x": 2268,
                    "y": 2474
                },
                {
                    "x": 1274,
                    "y": 2474
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:18px'>Xu, W., Tuan, Y., Lu, Y., Saxon, M., Li, L., and Wang,<br>W. Y. Not all errors are equal: Learning text generation<br>metrics using stratified error synthesis. In Goldberg, Y.,<br>Kozareva, Z., and Zhang, Y. (eds.), Findings of the As-<br>sociation for Computational Linguistics: EMNLP 2022,<br>Abu Dhabi, United Arab Emirates, December 7-11, 2022,<br>pp. 6559-6574. Association for Computational Linguis-<br>tics, 2022b. URL https : / / aclanthology · org/<br>2022 · findings-emnlp · 489.</p>",
            "id": 178,
            "page": 12,
            "text": "Xu, W., Tuan, Y., Lu, Y., Saxon, M., Li, L., and Wang, W. Y. Not all errors are equal: Learning text generation metrics using stratified error synthesis. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 6559-6574. Association for Computational Linguistics, 2022b. URL https : / / aclanthology · org/ 2022 · findings-emnlp · 489."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2514
                },
                {
                    "x": 2265,
                    "y": 2514
                },
                {
                    "x": 2265,
                    "y": 2665
                },
                {
                    "x": 1276,
                    "y": 2665
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:20px'>Yuan, W., Neubig, G., and Liu, P. Bartscore: Evaluating<br>generated text as text generation. Advances in Neural<br>Information Processing Systems, 34:27263-27277, 2021.</p>",
            "id": 179,
            "page": 12,
            "text": "Yuan, W., Neubig, G., and Liu, P. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263-27277, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2703
                },
                {
                    "x": 2266,
                    "y": 2703
                },
                {
                    "x": 2266,
                    "y": 2802
                },
                {
                    "x": 1275,
                    "y": 2802
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:20px'>Zar, J. H. Spearman rank correlation. Encyclopedia of<br>biostatistics, 7, 2005.</p>",
            "id": 180,
            "page": 12,
            "text": "Zar, J. H. Spearman rank correlation. Encyclopedia of biostatistics, 7, 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2843
                },
                {
                    "x": 2268,
                    "y": 2843
                },
                {
                    "x": 2268,
                    "y": 2994
                },
                {
                    "x": 1274,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:22px'>Zhang, C., Chen, Y., D'Haro, L. F., Zhang, Y., Friedrichs,<br>T., Lee, G., and Li, H. Dynaeval: Unifying turn and dia-<br>logue level evaluation. In Zong, C., Xia, F., Li, W., and</p>",
            "id": 181,
            "page": 12,
            "text": "Zhang, C., Chen, Y., D'Haro, L. F., Zhang, Y., Friedrichs, T., Lee, G., and Li, H. Dynaeval: Unifying turn and dialogue level evaluation. In Zong, C., Xia, F., Li, W., and"
        },
        {
            "bounding_box": [
                {
                    "x": 960,
                    "y": 191
                },
                {
                    "x": 1525,
                    "y": 191
                },
                {
                    "x": 1525,
                    "y": 235
                },
                {
                    "x": 960,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='182' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 182,
            "page": 13,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 263,
                    "y": 285
                },
                {
                    "x": 1220,
                    "y": 285
                },
                {
                    "x": 1220,
                    "y": 683
                },
                {
                    "x": 263,
                    "y": 683
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:18px'>Navigli, R. (eds.), Proceedings of the 59th Annual Meet-<br>ing of the Association for Computational Linguistics and<br>the 11th International Joint Conference on Natural Lan-<br>guage Processing, ACL/IJCNLP 2021, (Volume 1: Long<br>Papers), Virtual Event, August 1-6, 2021, pp. 5676-5689.<br>Association for Computational Linguistics, 2021. doi:<br>10.18653/v1/2021.acl-long.441. URL https : / /doi<br>org/10 · 18653/v1/2021 · acl-long · 441.</p>",
            "id": 183,
            "page": 13,
            "text": "Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 5676-5689. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.441. URL https : / /doi org/10 · 18653/v1/2021 · acl-long · 441."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 714
                },
                {
                    "x": 1217,
                    "y": 714
                },
                {
                    "x": 1217,
                    "y": 964
                },
                {
                    "x": 224,
                    "y": 964
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:16px'>Zhang, C., D'Haro, L. F., Zhang, Q., Friedrichs, T., and Li,<br>H. Fined-eval: Fine-grained automatic dialogue-level<br>evaluation. CoRR, abs/2210.13832, 2022a. doi: 10.<br>48550/arXiv.2210.13832. URL https : / / doi · org/<br>10 · 48550/arxiv 2210 · 13832.</p>",
            "id": 184,
            "page": 13,
            "text": "Zhang, C., D'Haro, L. F., Zhang, Q., Friedrichs, T., and Li, H. Fined-eval: Fine-grained automatic dialogue-level evaluation. CoRR, abs/2210.13832, 2022a. doi: 10. 48550/arXiv.2210.13832. URL https : / / doi · org/ 10 · 48550/arxiv 2210 · 13832."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 997
                },
                {
                    "x": 1218,
                    "y": 997
                },
                {
                    "x": 1218,
                    "y": 1197
                },
                {
                    "x": 223,
                    "y": 1197
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:20px'>Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,<br>Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,<br>et al. Opt: Open pre-trained transformer language models.<br>arXiv preprint arXiv:2205.01068, 2022b.</p>",
            "id": 185,
            "page": 13,
            "text": "Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,  Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022b."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1229
                },
                {
                    "x": 1219,
                    "y": 1229
                },
                {
                    "x": 1219,
                    "y": 1530
                },
                {
                    "x": 224,
                    "y": 1530
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:18px'>Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and<br>Artzi, Y. Bertscore: Evaluating text generation with<br>BERT. In 8th International Conference on Learning<br>Representations, ICLR 2020, Addis Ababa, Ethiopia,<br>April 26-30, 2020. OpenReview.net, 2020. URL https :<br>/ / openreview · net /forum?id=SkeHuCVFDr.</p>",
            "id": 186,
            "page": 13,
            "text": "Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https : / / openreview · net /forum?id=SkeHuCVFDr."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1560
                },
                {
                    "x": 1216,
                    "y": 1560
                },
                {
                    "x": 1216,
                    "y": 2112
                },
                {
                    "x": 225,
                    "y": 2112
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:18px'>Zhao, W., Peyrard, M., Liu, F., Gao, Y., Meyer, C. M.,<br>and Eger, S. Moverscore: Text generation evaluating<br>with contextualized embeddings and earth mover dis-<br>tance. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.),<br>Proceedings of the 2019 Conference on Empirical Meth-<br>ods in Natural Language Processing and the 9th Interna-<br>tional Joint Conference on Natural Language Processing,<br>EMNLP-IJCNLP 2019, Hong Kong, China, November<br>3-7, 2019, pp. 563-578. Association for Computational<br>Linguistics, 2019. doi: 10.18653/y1/D19-1053. URL<br>https: / / doi · org/10 · 18653/v1/D19-1053.</p>",
            "id": 187,
            "page": 13,
            "text": "Zhao, W., Peyrard, M., Liu, F., Gao, Y., Meyer, C. M., and Eger, S. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 563-578. Association for Computational Linguistics, 2019. doi: 10.18653/y1/D19-1053. URL https: / / doi · org/10 · 18653/v1/D19-1053."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2142
                },
                {
                    "x": 1219,
                    "y": 2142
                },
                {
                    "x": 1219,
                    "y": 2444
                },
                {
                    "x": 223,
                    "y": 2444
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:16px'>Zhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu,<br>P., Zhu, C., Ji, H., and Han, J. Towards a uni-<br>fied multi-dimensional evaluator for text generation.<br>CoRR, abs/2210.07197, 2022. doi: 10.48550/arXiv.<br>2210.07197. URL https : / /doi · org/10 · 48550/<br>arXiv 2210 07197.</p>",
            "id": 188,
            "page": 13,
            "text": "Zhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P., Zhu, C., Ji, H., and Han, J. Towards a unified multi-dimensional evaluator for text generation. CoRR, abs/2210.07197, 2022. doi: 10.48550/arXiv. 2210.07197. URL https : / /doi · org/10 · 48550/ arXiv 2210 07197."
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='189' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 189,
            "page": 14,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 278
                },
                {
                    "x": 715,
                    "y": 278
                },
                {
                    "x": 715,
                    "y": 333
                },
                {
                    "x": 225,
                    "y": 333
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:20px'>A. Metric Comparison</p>",
            "id": 190,
            "page": 14,
            "text": "A. Metric Comparison"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 363
                },
                {
                    "x": 1379,
                    "y": 363
                },
                {
                    "x": 1379,
                    "y": 416
                },
                {
                    "x": 223,
                    "y": 416
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:16px'>Tab. 7 summarize several popular generated text evaluation methods.</p>",
            "id": 191,
            "page": 14,
            "text": "Tab. 7 summarize several popular generated text evaluation methods."
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 451
                },
                {
                    "x": 2187,
                    "y": 451
                },
                {
                    "x": 2187,
                    "y": 1189
                },
                {
                    "x": 296,
                    "y": 1189
                }
            ],
            "category": "table",
            "html": "<table id='192' style='font-size:16px'><tr><td rowspan=\"2\">Metrics</td><td rowspan=\"2\">Custom</td><td colspan=\"2\">Function (f)</td><td colspan=\"2\">Additional text (S)</td><td rowspan=\"2\" colspan=\"2\">Training-free Application</td></tr><tr><td>Representation</td><td>Formulation</td><td>Source</td><td>Reference</td></tr><tr><td>ROUGE (Lin, 2004)</td><td>X</td><td>Token</td><td>Matching</td><td>No</td><td>Required</td><td></td><td>SUM</td></tr><tr><td>BLEU (Papineni et al., 2002)</td><td>X</td><td>Token</td><td>Matching</td><td>No</td><td>Required</td><td></td><td>MT</td></tr><tr><td>CHRF (Popovic, 2015)</td><td>X</td><td>Character</td><td>Matching</td><td>No</td><td>Required</td><td></td><td>MT</td></tr><tr><td>BERTScore (Zhang et al., 2020)</td><td>X</td><td>BERT</td><td>Matching</td><td>No</td><td>Required</td><td></td><td>MUL(2)</td></tr><tr><td>MoverScore (Zhao et al., 2019)</td><td>X</td><td>BERT</td><td>Matching</td><td>No</td><td>Required</td><td></td><td>MUL(4)</td></tr><tr><td>BLEURT (Sellam et al., 2020)</td><td>X</td><td>BERT</td><td>Regression</td><td>No</td><td>Required</td><td></td><td>MT</td></tr><tr><td>PRISM (Thompson & Post, 2020)</td><td>X</td><td>Embedding</td><td>Paraphrase</td><td>Optional</td><td>Optional</td><td>V</td><td>MT</td></tr><tr><td>UNIEVAL (Zhong et al., 2022)</td><td>X</td><td>T5</td><td>Boolean QA</td><td>Optional</td><td>Optional</td><td>X</td><td>MUL(2)</td></tr><tr><td>COMET (Rei et al., 2020)</td><td>X</td><td>BERT</td><td>Regress, Rank</td><td>Optional</td><td>Optional</td><td>X</td><td>MT</td></tr><tr><td>BARTScore (Yuan et al., 2021)</td><td>X</td><td>BART</td><td>Generation</td><td>Optional</td><td>Optional</td><td>V</td><td>MUL(3)</td></tr><tr><td>FED (Mehri & Eskenazi, 2020)</td><td>X</td><td>DialoGPT</td><td>Generation</td><td>Required</td><td>Optional</td><td>V</td><td>Dialogue</td></tr><tr><td>HolisticEval (Pang et al., 2020)</td><td>X</td><td>GPT2</td><td>Generation</td><td>Optional</td><td>Optional</td><td>V</td><td>Dialogue</td></tr><tr><td>GPTScore</td><td>V</td><td>GPT3/OPT</td><td>Any</td><td>Optional</td><td>Optional</td><td>V</td><td>MUL(5)</td></tr></table>",
            "id": 192,
            "page": 14,
            "text": "Metrics Custom Function (f) Additional text (S) Training-free Application  Representation Formulation Source Reference  ROUGE (Lin, 2004) X Token Matching No Required  SUM  BLEU (Papineni , 2002) X Token Matching No Required  MT  CHRF (Popovic, 2015) X Character Matching No Required  MT  BERTScore (Zhang , 2020) X BERT Matching No Required  MUL(2)  MoverScore (Zhao , 2019) X BERT Matching No Required  MUL(4)  BLEURT (Sellam , 2020) X BERT Regression No Required  MT  PRISM (Thompson & Post, 2020) X Embedding Paraphrase Optional Optional V MT  UNIEVAL (Zhong , 2022) X T5 Boolean QA Optional Optional X MUL(2)  COMET (Rei , 2020) X BERT Regress, Rank Optional Optional X MT  BARTScore (Yuan , 2021) X BART Generation Optional Optional V MUL(3)  FED (Mehri & Eskenazi, 2020) X DialoGPT Generation Required Optional V Dialogue  HolisticEval (Pang , 2020) X GPT2 Generation Optional Optional V Dialogue  GPTScore V GPT3/OPT Any Optional Optional V"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1223
                },
                {
                    "x": 2264,
                    "y": 1223
                },
                {
                    "x": 2264,
                    "y": 1321
                },
                {
                    "x": 223,
                    "y": 1321
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:16px'>Table 7. A comprehensive comparison of existing research on automated evaluation of generated texts. MUL(k) denotes multiple (k)<br>applications explored. Custom denotes Custom Aspects.</p>",
            "id": 193,
            "page": 14,
            "text": "Table 7. A comprehensive comparison of existing research on automated evaluation of generated texts. MUL(k) denotes multiple (k) applications explored. Custom denotes Custom Aspects."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1402
                },
                {
                    "x": 899,
                    "y": 1402
                },
                {
                    "x": 899,
                    "y": 1461
                },
                {
                    "x": 224,
                    "y": 1461
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:22px'>B. Tasks, Datasets, and Aspects</p>",
            "id": 194,
            "page": 14,
            "text": "B. Tasks, Datasets, and Aspects"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1488
                },
                {
                    "x": 2268,
                    "y": 1488
                },
                {
                    "x": 2268,
                    "y": 1689
                },
                {
                    "x": 221,
                    "y": 1689
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:16px'>To achieve a more comprehensive evaluation, in this paper, we cover a broad range of natural language generation tasks:<br>Dialogue Response Generation, Text Summarization, Data-to-Text, and Machine Translation, which involves 9 datasets and<br>22 evaluation aspects in total. Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset. The<br>definition of different aspects can be found in Tab. 1.</p>",
            "id": 195,
            "page": 14,
            "text": "To achieve a more comprehensive evaluation, in this paper, we cover a broad range of natural language generation tasks: Dialogue Response Generation, Text Summarization, Data-to-Text, and Machine Translation, which involves 9 datasets and 22 evaluation aspects in total. Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset. The definition of different aspects can be found in Tab. 1."
        },
        {
            "bounding_box": [
                {
                    "x": 801,
                    "y": 1725
                },
                {
                    "x": 1674,
                    "y": 1725
                },
                {
                    "x": 1674,
                    "y": 2363
                },
                {
                    "x": 801,
                    "y": 2363
                }
            ],
            "category": "table",
            "html": "<table id='196' style='font-size:16px'><tr><td>Tasks</td><td>Dataset</td><td>Aspect</td></tr><tr><td rowspan=\"2\">Diag</td><td>FED-Diag</td><td>COH, DIV, FLE, UND,INQ CON, INF, LIK, DEP, ERR</td></tr><tr><td>FED-Turn</td><td>INT, ENG, SPE, REL, COR, SEM, UND, FLU</td></tr><tr><td>Summ</td><td>SummEval Newsroom REALSumm Q-XSUM</td><td>COH, CON, FLU,REL FLU, REL, INF, COH COV FAC</td></tr><tr><td>D2T</td><td>BAGEL SFRES</td><td>FLU, REL, INF FLU, REL, INF</td></tr><tr><td>MT</td><td>MQM-2020</td><td>FLU, COH, INF</td></tr></table>",
            "id": 196,
            "page": 14,
            "text": "Tasks Dataset Aspect  Diag FED-Diag COH, DIV, FLE, UND,INQ CON, INF, LIK, DEP, ERR  FED-Turn INT, ENG, SPE, REL, COR, SEM, UND, FLU  Summ SummEval Newsroom REALSumm Q-XSUM COH, CON, FLU,REL FLU, REL, INF, COH COV FAC  D2T BAGEL SFRES FLU, REL, INF FLU, REL, INF  MT MQM-2020"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2395
                },
                {
                    "x": 2265,
                    "y": 2395
                },
                {
                    "x": 2265,
                    "y": 2490
                },
                {
                    "x": 223,
                    "y": 2490
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:14px'>Table 8. An overview of tasks, datasets, and evaluation aspects. Summ. denote the text summarization task, D2T denotes the Data-to-Text<br>task, MT denotes the machine translation. Tab. 1 summarized the definitions of the aspects explored in this work.</p>",
            "id": 197,
            "page": 14,
            "text": "Table 8. An overview of tasks, datasets, and evaluation aspects. Summ. denote the text summarization task, D2T denotes the Data-to-Text task, MT denotes the machine translation. Tab. 1 summarized the definitions of the aspects explored in this work."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2560
                },
                {
                    "x": 2268,
                    "y": 2560
                },
                {
                    "x": 2268,
                    "y": 2764
                },
                {
                    "x": 222,
                    "y": 2764
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:18px'>Dialogue Response Generation aims to automatically generate an engaging and informative response based on the<br>dialogue history. (1) FED (Mehri & Eskenazi, 2020) collects 124 conversations, including both human-machine (Meena (Adi-<br>wardana et al., 2020), Mitsuku') and human-human dialogues, and manually annotated 9 and 11 evaluation aspects at the<br>turn- and dialogue-level, respectively.</p>",
            "id": 198,
            "page": 14,
            "text": "Dialogue Response Generation aims to automatically generate an engaging and informative response based on the dialogue history. (1) FED (Mehri & Eskenazi, 2020) collects 124 conversations, including both human-machine (Meena (Adiwardana , 2020), Mitsuku') and human-human dialogues, and manually annotated 9 and 11 evaluation aspects at the turn- and dialogue-level, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2812
                },
                {
                    "x": 2268,
                    "y": 2812
                },
                {
                    "x": 2268,
                    "y": 2917
                },
                {
                    "x": 223,
                    "y": 2917
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:18px'>Text Summarization is a task of automatically generating an informative and fluent summary for a given long text. Here,<br>we consider the following four datasets covering 6 evaluation aspects: semantic coverage, informativeness, relevance,</p>",
            "id": 199,
            "page": 14,
            "text": "Text Summarization is a task of automatically generating an informative and fluent summary for a given long text. Here, we consider the following four datasets covering 6 evaluation aspects: semantic coverage, informativeness, relevance,"
        },
        {
            "bounding_box": [
                {
                    "x": 274,
                    "y": 2942
                },
                {
                    "x": 2108,
                    "y": 2942
                },
                {
                    "x": 2108,
                    "y": 2995
                },
                {
                    "x": 274,
                    "y": 2995
                }
            ],
            "category": "footer",
            "html": "<br><footer id='200' style='font-size:20px'>5https: / /medium. com/perdoators-diog/hismity-dix-brage-pros- 3e8d98c5f2a7</footer>",
            "id": 200,
            "page": 14,
            "text": "5https: / /medium. com/perdoators-diog/hismity-dix-brage-pros- 3e8d98c5f2a7"
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='201' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 201,
            "page": 15,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 282
                },
                {
                    "x": 2266,
                    "y": 282
                },
                {
                    "x": 2266,
                    "y": 635
                },
                {
                    "x": 222,
                    "y": 635
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:18px'>fluency, coherence, and factuality. (1) SummEval (Bhandari et al., 2020) collects human judgments on 16 model-generated<br>summaries on the CNN/Daily Mail dataset, covering aspects of coherence, consistency, fluency, and relevance. (2)<br>REALSumm (Bhandari et al., 2020) evaluates the reliability of automatic metrics by measuring the pyramid recall of text<br>generated by 25 systems. (3) NEWSROOM (Grusky et al., 2018) covers news, sports, entertainment, finance, and other topics<br>and evaluates the quality of summaries generated by 7 systems, including informativeness, relevance, fluency, and coherence.<br>(4) QAGS_XSUM (Wang et al., 2020) is another dataset focusing on the factuality aspect. It has 239 samples from XSUM<br>and their summaries are generated by a fine-tuned BART model.</p>",
            "id": 202,
            "page": 15,
            "text": "fluency, coherence, and factuality. (1) SummEval (Bhandari , 2020) collects human judgments on 16 model-generated summaries on the CNN/Daily Mail dataset, covering aspects of coherence, consistency, fluency, and relevance. (2) REALSumm (Bhandari , 2020) evaluates the reliability of automatic metrics by measuring the pyramid recall of text generated by 25 systems. (3) NEWSROOM (Grusky , 2018) covers news, sports, entertainment, finance, and other topics and evaluates the quality of summaries generated by 7 systems, including informativeness, relevance, fluency, and coherence. (4) QAGS_XSUM (Wang , 2020) is another dataset focusing on the factuality aspect. It has 239 samples from XSUM and their summaries are generated by a fine-tuned BART model."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 682
                },
                {
                    "x": 2267,
                    "y": 682
                },
                {
                    "x": 2267,
                    "y": 888
                },
                {
                    "x": 221,
                    "y": 888
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:18px'>Data-to-Text aims to automatically generate a fluent and factual description for a given table. (1) BAGEL (Mairesse et al.,<br>2010) contains 202 samples about restaurants in Cambridge. (2) SFRES (Wen et al., 2015) contains 581 samples about<br>restaurants in San Francisco. These two datasets consider three evaluation aspects: informativeness, naturalness (relevance),<br>and quality (fluency).</p>",
            "id": 203,
            "page": 15,
            "text": "Data-to-Text aims to automatically generate a fluent and factual description for a given table. (1) BAGEL (Mairesse , 2010) contains 202 samples about restaurants in Cambridge. (2) SFRES (Wen , 2015) contains 581 samples about restaurants in San Francisco. These two datasets consider three evaluation aspects: informativeness, naturalness (relevance), and quality (fluency)."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 936
                },
                {
                    "x": 2266,
                    "y": 936
                },
                {
                    "x": 2266,
                    "y": 1090
                },
                {
                    "x": 222,
                    "y": 1090
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:18px'>Machine Translation aims to translate a sentence from one language to another. We consider a sub-datasets of Mul-<br>tidimensional Quality Metrics (MQM) (Freitag et al., 2021), namely, MQM-2020 (Chinese->English). Due to limited<br>annotations, here, we only consider three evaluation aspects: accuracy, fluency, and MQM with diverse scores.</p>",
            "id": 204,
            "page": 15,
            "text": "Machine Translation aims to translate a sentence from one language to another. We consider a sub-datasets of Multidimensional Quality Metrics (MQM) (Freitag , 2021), namely, MQM-2020 (Chinese->English). Due to limited annotations, here, we only consider three evaluation aspects: accuracy, fluency, and MQM with diverse scores."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1148
                },
                {
                    "x": 617,
                    "y": 1148
                },
                {
                    "x": 617,
                    "y": 1207
                },
                {
                    "x": 224,
                    "y": 1207
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:20px'>C. Ablation Study</p>",
            "id": 205,
            "page": 15,
            "text": "C. Ablation Study"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1235
                },
                {
                    "x": 868,
                    "y": 1235
                },
                {
                    "x": 868,
                    "y": 1288
                },
                {
                    "x": 226,
                    "y": 1288
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='206' style='font-size:18px'>C.1. Effectiveness of Demonstration</p>",
            "id": 206,
            "page": 15,
            "text": "C.1. Effectiveness of Demonstration"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1314
                },
                {
                    "x": 2265,
                    "y": 1314
                },
                {
                    "x": 2265,
                    "y": 1466
                },
                {
                    "x": 222,
                    "y": 1466
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:16px'>The in-context learning helps a lot to achieve a good performance. However, how does the number of samples in the<br>demonstration impact the performance? We conduct a case study on the five GPT3-based models explored in this work. The<br>experimental results are shown in Fig. 6, and the specific performance values can be seen in Tab. 9.</p>",
            "id": 207,
            "page": 15,
            "text": "The in-context learning helps a lot to achieve a good performance. However, how does the number of samples in the demonstration impact the performance? We conduct a case study on the five GPT3-based models explored in this work. The experimental results are shown in Fig. 6, and the specific performance values can be seen in Tab. 9."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1517
                },
                {
                    "x": 939,
                    "y": 1517
                },
                {
                    "x": 939,
                    "y": 1570
                },
                {
                    "x": 224,
                    "y": 1570
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:18px'>C.2. Partial Order of Evaluation Aspect</p>",
            "id": 208,
            "page": 15,
            "text": "C.2. Partial Order of Evaluation Aspect"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1595
                },
                {
                    "x": 2263,
                    "y": 1595
                },
                {
                    "x": 2263,
                    "y": 1699
                },
                {
                    "x": 222,
                    "y": 1699
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='209' style='font-size:18px'>We have investigated the combination of different evaluation aspects to achieve further performance gains in § 6.2. Tab. 10<br>summarizes the aspect definition and Spearman correlation changes for INT, with the introduction of other aspects.</p>",
            "id": 209,
            "page": 15,
            "text": "We have investigated the combination of different evaluation aspects to achieve further performance gains in § 6.2. Tab. 10 summarizes the aspect definition and Spearman correlation changes for INT, with the introduction of other aspects."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1760
                },
                {
                    "x": 614,
                    "y": 1760
                },
                {
                    "x": 614,
                    "y": 1818
                },
                {
                    "x": 225,
                    "y": 1818
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:22px'>D. Prompt Design</p>",
            "id": 210,
            "page": 15,
            "text": "D. Prompt Design"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1845
                },
                {
                    "x": 2266,
                    "y": 1845
                },
                {
                    "x": 2266,
                    "y": 2049
                },
                {
                    "x": 221,
                    "y": 2049
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:16px'>In this work, we have studied four popular text generation tasks: text summarization, machine translation, data-to-text, and<br>dialogue response generation. The instructions for these tasks on different evaluation aspects are summarized in Tab. 11 and<br>Tab. 12. Here, we convert the dialogue response generation task as a boolean question-answering task and incorporate the<br>aspect definition into the question of the boolean question-answering task.</p>",
            "id": 211,
            "page": 15,
            "text": "In this work, we have studied four popular text generation tasks: text summarization, machine translation, data-to-text, and dialogue response generation. The instructions for these tasks on different evaluation aspects are summarized in Tab. 11 and Tab. 12. Here, we convert the dialogue response generation task as a boolean question-answering task and incorporate the aspect definition into the question of the boolean question-answering task."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2108
                },
                {
                    "x": 713,
                    "y": 2108
                },
                {
                    "x": 713,
                    "y": 2166
                },
                {
                    "x": 225,
                    "y": 2166
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:20px'>E. Experiment Results</p>",
            "id": 212,
            "page": 15,
            "text": "E. Experiment Results"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2194
                },
                {
                    "x": 2268,
                    "y": 2194
                },
                {
                    "x": 2268,
                    "y": 2395
                },
                {
                    "x": 222,
                    "y": 2395
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:18px'>This section lists the full experimental results for the explored text generation tasks. The models considered here in-<br>clude the 9 baseline models: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, MoverScore, PRISM, BARTSCORE,<br>BARTSCORE+CNN, and BARTSCORE+CNN+Para, and 19 GPTScore models built based on the GPT3-based, GPT2-<br>based, OPT-based, and FLAN-T5-based pre-trained models.</p>",
            "id": 213,
            "page": 15,
            "text": "This section lists the full experimental results for the explored text generation tasks. The models considered here include the 9 baseline models: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, MoverScore, PRISM, BARTSCORE, BARTSCORE+CNN, and BARTSCORE+CNN+Para, and 19 GPTScore models built based on the GPT3-based, GPT2based, OPT-based, and FLAN-T5-based pre-trained models."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2419
                },
                {
                    "x": 2265,
                    "y": 2419
                },
                {
                    "x": 2265,
                    "y": 2568
                },
                {
                    "x": 221,
                    "y": 2568
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='214' style='font-size:14px'>Tab. 13 lists the results of the text summarization datasets. Tab. 14 lists the results of the machine translation datasets.<br>Tab. 15 shows the results of the data-to-text task on the BAGEL dataset. Tab. 16 shows the results of the data-to-text task on<br>the SFRES dataset.</p>",
            "id": 214,
            "page": 15,
            "text": "Tab. 13 lists the results of the text summarization datasets. Tab. 14 lists the results of the machine translation datasets. Tab. 15 shows the results of the data-to-text task on the BAGEL dataset. Tab. 16 shows the results of the data-to-text task on the SFRES dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 958,
                    "y": 190
                },
                {
                    "x": 1527,
                    "y": 190
                },
                {
                    "x": 1527,
                    "y": 236
                },
                {
                    "x": 958,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='215' style='font-size:16px'>GPTScore: Evaluate as You Desire</header>",
            "id": 215,
            "page": 16,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 825,
                    "y": 384
                },
                {
                    "x": 1654,
                    "y": 384
                },
                {
                    "x": 1654,
                    "y": 1817
                },
                {
                    "x": 825,
                    "y": 1817
                }
            ],
            "category": "table",
            "html": "<table id='216' style='font-size:14px'><tr><td>Model</td><td>K</td><td>ACC</td><td>FLU</td><td>MQM</td></tr><tr><td rowspan=\"6\">GPT3-ada</td><td>0</td><td>23.7</td><td>6.3</td><td>24.1</td></tr><tr><td>1</td><td>22.5</td><td>4.9</td><td>26.1</td></tr><tr><td>2</td><td>21.5</td><td>12.8</td><td>25.6</td></tr><tr><td>4</td><td>27.9</td><td>12.2</td><td>24.3</td></tr><tr><td>8</td><td>27.9</td><td>11.6</td><td>24.4</td></tr><tr><td>12</td><td>29.5</td><td>10.6</td><td>24.7</td></tr><tr><td rowspan=\"6\">GPT3-babbage</td><td>0</td><td>25.0</td><td>10.9</td><td>29.6</td></tr><tr><td>1</td><td>23.4</td><td>11.9</td><td>30.2</td></tr><tr><td>2</td><td>24.0</td><td>13.3</td><td>30.9</td></tr><tr><td>4</td><td>29.7</td><td>14.7</td><td>31.5</td></tr><tr><td>8</td><td>29.8</td><td>14.0</td><td>31.2</td></tr><tr><td>12</td><td>31.0</td><td>14.9</td><td>32.6</td></tr><tr><td rowspan=\"6\">GPT3-curie</td><td>0</td><td>30.3</td><td>9.3</td><td>34.8</td></tr><tr><td>1</td><td>29.8</td><td>12.5</td><td>31.9</td></tr><tr><td>2</td><td>30.2</td><td>16.4</td><td>32.9</td></tr><tr><td>4</td><td>33.1</td><td>15.8</td><td>33.2</td></tr><tr><td>8</td><td>30.2</td><td>17.9</td><td>34.5</td></tr><tr><td>12</td><td>32.3</td><td>18.8</td><td>34.3</td></tr><tr><td rowspan=\"6\">GPT3-davinci001</td><td>0</td><td>26.9</td><td>8.6</td><td>32.6</td></tr><tr><td>1</td><td>27.2</td><td>12.5</td><td>33.4</td></tr><tr><td>2</td><td>27.8</td><td>16.2</td><td>35.3</td></tr><tr><td>4</td><td>30.3</td><td>16.1</td><td>37.7</td></tr><tr><td>8</td><td>31.2</td><td>17.5</td><td>38.3</td></tr><tr><td>12</td><td>31.7</td><td>17.5</td><td>39.1</td></tr><tr><td rowspan=\"6\">GPT3-davinci003</td><td>0</td><td>29.5</td><td>21.3</td><td>32.8</td></tr><tr><td>1</td><td>30.7</td><td>19.3</td><td>31.4</td></tr><tr><td>2</td><td>30.1</td><td>21.6</td><td>32.9</td></tr><tr><td>4</td><td>29.5</td><td>19.1</td><td>33.5</td></tr><tr><td>8</td><td>29.3</td><td>21.5</td><td>32.2</td></tr><tr><td>12</td><td>29.8</td><td>21.8</td><td>32.5</td></tr></table>",
            "id": 216,
            "page": 16,
            "text": "Model K ACC FLU MQM  GPT3-ada 0 23.7 6.3 24.1  1 22.5 4.9 26.1  2 21.5 12.8 25.6  4 27.9 12.2 24.3  8 27.9 11.6 24.4  12 29.5 10.6 24.7  GPT3-babbage 0 25.0 10.9 29.6  1 23.4 11.9 30.2  2 24.0 13.3 30.9  4 29.7 14.7 31.5  8 29.8 14.0 31.2  12 31.0 14.9 32.6  GPT3-curie 0 30.3 9.3 34.8  1 29.8 12.5 31.9  2 30.2 16.4 32.9  4 33.1 15.8 33.2  8 30.2 17.9 34.5  12 32.3 18.8 34.3  GPT3-davinci001 0 26.9 8.6 32.6  1 27.2 12.5 33.4  2 27.8 16.2 35.3  4 30.3 16.1 37.7  8 31.2 17.5 38.3  12 31.7 17.5 39.1  GPT3-davinci003 0 29.5 21.3 32.8  1 30.7 19.3 31.4  2 30.1 21.6 32.9  4 29.5 19.1 33.5  8 29.3 21.5 32.2  12 29.8 21.8"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1847
                },
                {
                    "x": 2263,
                    "y": 1847
                },
                {
                    "x": 2263,
                    "y": 1944
                },
                {
                    "x": 222,
                    "y": 1944
                }
            ],
            "category": "caption",
            "html": "<caption id='217' style='font-size:16px'>Table 9. Spearman correlation of the GPT3-based models (e.g, text-ada-001 and text-davinci-001) with different demonstration sample<br>numbers on the MQM-2020 dataset .K denotes the number of samples in the demonstration.</caption>",
            "id": 217,
            "page": 16,
            "text": "Table 9. Spearman correlation of the GPT3-based models (e.g, text-ada-001 and text-davinci-001) with different demonstration sample numbers on the MQM-2020 dataset .K denotes the number of samples in the demonstration."
        },
        {
            "bounding_box": [
                {
                    "x": 273,
                    "y": 2194
                },
                {
                    "x": 2214,
                    "y": 2194
                },
                {
                    "x": 2214,
                    "y": 2750
                },
                {
                    "x": 273,
                    "y": 2750
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:20px'>X Aspect Aspect Definition Spear<br>1 Interesting (INT) Is this response interesting to the convsersation? 36.9<br>2 Engaging (ENG) Is this an interesting response that is engaging? 40.7<br>3 Specific (SPE) Is this an interesting response that is specific and engaging? 48.6<br>4 Correct (COR) Is this an interesting response that is engaging, specific, and correct? 50.0<br>5 Relevant (REL) Is this an interesting response that is specific, engaging, relevant, and correct? 51.3<br>6 Understandable (UND) Is this an interesting response that is specific, engaging, relevant, correct, 50.9<br>and understandable?<br>7 Semantically appropriate (SEM) Is this an interesting response that is specific, engaging, relevant, correct, 51.4<br>understandable, and semantically appropriate?<br>8 Fluent (FLU) Is this an interesting response that is specific, engaging, relevant, correct, 50.3<br>understandable, semantically appropriate, and fluent?</p>",
            "id": 218,
            "page": 16,
            "text": "X Aspect Aspect Definition Spear 1 Interesting (INT) Is this response interesting to the convsersation? 36.9 2 Engaging (ENG) Is this an interesting response that is engaging? 40.7 3 Specific (SPE) Is this an interesting response that is specific and engaging? 48.6 4 Correct (COR) Is this an interesting response that is engaging, specific, and correct? 50.0 5 Relevant (REL) Is this an interesting response that is specific, engaging, relevant, and correct? 51.3 6 Understandable (UND) Is this an interesting response that is specific, engaging, relevant, correct, 50.9 and understandable? 7 Semantically appropriate (SEM) Is this an interesting response that is specific, engaging, relevant, correct, 51.4 understandable, and semantically appropriate? 8 Fluent (FLU) Is this an interesting response that is specific, engaging, relevant, correct, 50.3 understandable, semantically appropriate, and fluent?"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2780
                },
                {
                    "x": 2262,
                    "y": 2780
                },
                {
                    "x": 2262,
                    "y": 2875
                },
                {
                    "x": 223,
                    "y": 2875
                }
            ],
            "category": "caption",
            "html": "<caption id='219' style='font-size:16px'>Table 10. The aspect definition and Spearman correlation of INT. X denotes the number of aspects combined with the INT. The scoring<br>model is GPT3-c01.</caption>",
            "id": 219,
            "page": 16,
            "text": "Table 10. The aspect definition and Spearman correlation of INT. X denotes the number of aspects combined with the INT. The scoring model is GPT3-c01."
        },
        {
            "bounding_box": [
                {
                    "x": 958,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 236
                },
                {
                    "x": 958,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='220' style='font-size:14px'>GPTScore: Evaluate as You Desire</header>",
            "id": 220,
            "page": 17,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 321,
                    "y": 722
                },
                {
                    "x": 2170,
                    "y": 722
                },
                {
                    "x": 2170,
                    "y": 2365
                },
                {
                    "x": 321,
                    "y": 2365
                }
            ],
            "category": "table",
            "html": "<table id='221' style='font-size:18px'><tr><td>Aspect</td><td>Function</td><td>Instruction</td></tr><tr><td colspan=\"3\">Text Summarization</td></tr><tr><td>FAC</td><td>src->hypo ref<->hypo</td><td>Generate a summary with consistent facts for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with consistent facts. {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>COV</td><td>src->hypo ref<->hypo</td><td>Generate a summary with as much semantic coverage as possible for the following text: {src }\\n\\nTl;dr{hypo} Rewrite the following text with the same semantics. {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>CON</td><td>src->hypo ref<->hypo</td><td>Generate factually consistent summary for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with consistent facts. {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>INF</td><td>src->hypo ref<->hypo</td><td>Generate an informative summary that captures the key points of the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with its core information. {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>COH</td><td>src->hypo ref<->hypo</td><td>Generate a coherent summary for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text into a coherent text. {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>REL</td><td>src->hypo ref<->hypo</td><td>Generate a relevant summary with consistent details for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with consistent details. {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>FLU</td><td>src->hypo ref<->hypo</td><td>Generate a fluent and grammatical summary for the following text: {src }\\n\\nTl;dr{hypo} Rewrite the following text into a fluent and grammatical text. {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td colspan=\"3\">Machine Translation</td></tr><tr><td>Acc</td><td>ref<->hypo</td><td>Rewrite the following text with its core information and consistent facts: {ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>FLU</td><td>ref<->hypo</td><td>Rewrite the following text to make it more grammatical and well-written:{ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>MQM</td><td>ref<->hypo</td><td>Rewrite the following text into high-quality text with its core information:{ref/hypo} In other words, {hypo/ref}</td></tr><tr><td colspan=\"3\">Data to Text</td></tr><tr><td>INF</td><td>ref<->hypo</td><td>Convert the following text to another expression that preserves key information:\\n\\n(ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>NAT</td><td>ref<->hypo</td><td>Convert the following text into another expression that is human-like and natural:\\n\\n{ref/hypo} In other words, {hypo/ref}</td></tr><tr><td>FLU</td><td>ref<->hypo</td><td>Convert the following text into another expression that preserves key information and is human-like and natural:\\n\\n{ref/hypo} In other words, {hypo/ref}</td></tr></table>",
            "id": 221,
            "page": 17,
            "text": "Aspect Function Instruction  Text Summarization  FAC src->hypo ref<->hypo Generate a summary with consistent facts for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with consistent facts. {ref/hypo} In other words, {hypo/ref}  COV src->hypo ref<->hypo Generate a summary with as much semantic coverage as possible for the following text: {src }\\n\\nTl;dr{hypo} Rewrite the following text with the same semantics. {ref/hypo} In other words, {hypo/ref}  CON src->hypo ref<->hypo Generate factually consistent summary for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with consistent facts. {ref/hypo} In other words, {hypo/ref}  INF src->hypo ref<->hypo Generate an informative summary that captures the key points of the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with its core information. {ref/hypo} In other words, {hypo/ref}  COH src->hypo ref<->hypo Generate a coherent summary for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text into a coherent text. {ref/hypo} In other words, {hypo/ref}  REL src->hypo ref<->hypo Generate a relevant summary with consistent details for the following text: {src}\\n\\nTl;dr{hypo} Rewrite the following text with consistent details. {ref/hypo} In other words, {hypo/ref}  FLU src->hypo ref<->hypo Generate a fluent and grammatical summary for the following text: {src }\\n\\nTl;dr{hypo} Rewrite the following text into a fluent and grammatical text. {ref/hypo} In other words, {hypo/ref}  Machine Translation  Acc ref<->hypo Rewrite the following text with its core information and consistent facts: {ref/hypo} In other words, {hypo/ref}  FLU ref<->hypo Rewrite the following text to make it more grammatical and well-written:{ref/hypo} In other words, {hypo/ref}  MQM ref<->hypo Rewrite the following text into high-quality text with its core information:{ref/hypo} In other words, {hypo/ref}  Data to Text  INF ref<->hypo Convert the following text to another expression that preserves key information:\\n\\n(ref/hypo} In other words, {hypo/ref}  NAT ref<->hypo Convert the following text into another expression that is human-like and natural:\\n\\n{ref/hypo} In other words, {hypo/ref}  FLU ref<->hypo"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2390
                },
                {
                    "x": 2268,
                    "y": 2390
                },
                {
                    "x": 2268,
                    "y": 2529
                },
                {
                    "x": 223,
                    "y": 2529
                }
            ],
            "category": "caption",
            "html": "<caption id='222' style='font-size:14px'>Table 11. Instruction design on different aspects for text summarization, machine translation, and data-to-text tasks. src, hypo, and ref<br>denote the source text, hypothesis text, and reference text, respectively. a->b (a<-b) denotes to evaluate the quality of b (a) text based on<br>the given a (b) text.</caption>",
            "id": 222,
            "page": 17,
            "text": "Table 11. Instruction design on different aspects for text summarization, machine translation, and data-to-text tasks. src, hypo, and ref denote the source text, hypothesis text, and reference text, respectively. a->b (a<-b) denotes to evaluate the quality of b (a) text based on the given a (b) text."
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='223' style='font-size:16px'>GPTScore: Evaluate as You Desire</header>",
            "id": 223,
            "page": 18,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 383,
                    "y": 485
                },
                {
                    "x": 701,
                    "y": 485
                },
                {
                    "x": 701,
                    "y": 528
                },
                {
                    "x": 383,
                    "y": 528
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:20px'>Aspect Instruction</p>",
            "id": 224,
            "page": 18,
            "text": "Aspect Instruction"
        },
        {
            "bounding_box": [
                {
                    "x": 383,
                    "y": 546
                },
                {
                    "x": 654,
                    "y": 546
                },
                {
                    "x": 654,
                    "y": 588
                },
                {
                    "x": 383,
                    "y": 588
                }
            ],
            "category": "caption",
            "html": "<br><caption id='225' style='font-size:16px'>FED Turn-Level</caption>",
            "id": 225,
            "page": 18,
            "text": "FED Turn-Level"
        },
        {
            "bounding_box": [
                {
                    "x": 385,
                    "y": 600
                },
                {
                    "x": 2098,
                    "y": 600
                },
                {
                    "x": 2098,
                    "y": 1430
                },
                {
                    "x": 385,
                    "y": 1430
                }
            ],
            "category": "table",
            "html": "<br><table id='226' style='font-size:14px'><tr><td>INT</td><td>Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI interesting? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.</td></tr><tr><td>ENG</td><td>Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI engaging? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.</td></tr><tr><td>UND</td><td>Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI understandable? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.</td></tr><tr><td>REL</td><td>Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI relevant to the conversation? (a) Yes. (b) No.backslashnConversation: {History AnAnswer: Yes.</td></tr><tr><td>SPE</td><td>Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI generic or specific to the conversation? (a) Yes. (b) No.\\nConversation: {History } \\nAnswer: Yes.</td></tr><tr><td>COR</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI correct to conversations? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.]</td></tr><tr><td>SEM</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI semantically appropriate? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.</td></tr><tr><td>FLU</td><td>Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI fluently written? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.</td></tr></table>",
            "id": 226,
            "page": 18,
            "text": "INT Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI interesting? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.  ENG Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI engaging? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.  UND Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI understandable? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.  REL Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI relevant to the conversation? (a) Yes. (b) No.backslashnConversation: {History AnAnswer: Yes.  SPE Answer the question based on the conversation between a human and AI. \\nQuestion: Are the responses of AI generic or specific to the conversation? (a) Yes. (b) No.\\nConversation: {History } \\nAnswer: Yes.  COR Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI correct to conversations? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.]  SEM Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI semantically appropriate? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.  FLU"
        },
        {
            "bounding_box": [
                {
                    "x": 382,
                    "y": 1444
                },
                {
                    "x": 684,
                    "y": 1444
                },
                {
                    "x": 684,
                    "y": 1488
                },
                {
                    "x": 382,
                    "y": 1488
                }
            ],
            "category": "caption",
            "html": "<br><caption id='227' style='font-size:20px'>FED Dialog-Level</caption>",
            "id": 227,
            "page": 18,
            "text": "FED Dialog-Level"
        },
        {
            "bounding_box": [
                {
                    "x": 384,
                    "y": 1497
                },
                {
                    "x": 2101,
                    "y": 1497
                },
                {
                    "x": 2101,
                    "y": 2618
                },
                {
                    "x": 384,
                    "y": 2618
                }
            ],
            "category": "table",
            "html": "<br><table id='228' style='font-size:14px'><tr><td>COH</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI coherent and maintains a good conversation flow throughout the conversation? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.</td></tr><tr><td>DIV</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Is there diversity in the AI responses? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.</td></tr><tr><td>FLE</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI flexible and adaptable to human and their interests? (a) Yes. (b) No. \\nConversation: {History}\\nAnswer: Yes.</td></tr><tr><td>UND</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI seem to understand the human? (a) Yes. (b) No. \\nConversation: {History }\\nAnswer: Yes.</td></tr><tr><td>INQ</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI inquisitive throughout the conversation? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.</td></tr><tr><td>CON</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI consistent in the information it provides throughout the conversation? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.</td></tr><tr><td>INF</td><td>nswer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI informative throughout the conversation? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.</td></tr><tr><td>LIK</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI display a likeable personality? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.</td></tr><tr><td>DEP</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI discuss topics in depth? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.</td></tr><tr><td>ERR</td><td>Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI able to recover from errors that it makes? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.</td></tr></table>",
            "id": 228,
            "page": 18,
            "text": "COH Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI coherent and maintains a good conversation flow throughout the conversation? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.  DIV Answer the question based on the conversation between a human and AI.\\nQuestion: Is there diversity in the AI responses? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.  FLE Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI flexible and adaptable to human and their interests? (a) Yes. (b) No. \\nConversation: {History}\\nAnswer: Yes.  UND Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI seem to understand the human? (a) Yes. (b) No. \\nConversation: {History }\\nAnswer: Yes.  INQ Answer the question based on the conversation between a human and AI.\\nQuestion: Is the AI inquisitive throughout the conversation? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.  CON Answer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI consistent in the information it provides throughout the conversation? (a) Yes. (b) No.\\nConversation: {History}\\nAnswer: Yes.  INF nswer the question based on the conversation between a human and AI.\\nQuestion: Are the responses of AI informative throughout the conversation? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.  LIK Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI display a likeable personality? (a) Yes. (b) No.\\nConversation: {History }\\nAnswer: Yes.  DEP Answer the question based on the conversation between a human and AI.\\nQuestion: Does the AI discuss topics in depth? (a) Yes. (b) No.\\nConversation: {History AnAnswer: Yes.  ERR"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2650
                },
                {
                    "x": 2266,
                    "y": 2650
                },
                {
                    "x": 2266,
                    "y": 2791
                },
                {
                    "x": 221,
                    "y": 2791
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:16px'>Table 12. Instruction design on various aspects for dialogue response generation task at the turn- and dialogue-level. History indicates the<br>conversation history. We convert the evaluation of the response generation task as a question-answering task, and the aspect definition is<br>incorporated into the question of the question-answering task.</p>",
            "id": 229,
            "page": 18,
            "text": "Table 12. Instruction design on various aspects for dialogue response generation task at the turn- and dialogue-level. History indicates the conversation history. We convert the evaluation of the response generation task as a question-answering task, and the aspect definition is incorporated into the question of the question-answering task."
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='230' style='font-size:18px'>GPTScore: Evaluate as You Desire</header>",
            "id": 230,
            "page": 19,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 468,
                    "y": 485
                },
                {
                    "x": 2014,
                    "y": 485
                },
                {
                    "x": 2014,
                    "y": 2593
                },
                {
                    "x": 468,
                    "y": 2593
                }
            ],
            "category": "table",
            "html": "<table id='231' style='font-size:14px'><tr><td rowspan=\"3\">Model</td><td colspan=\"8\">NEWSROOM</td><td colspan=\"2\">QXSUM</td></tr><tr><td colspan=\"2\">COH</td><td colspan=\"2\">CON</td><td colspan=\"2\">FLU</td><td colspan=\"2\">REL</td><td colspan=\"2\">COV</td></tr><tr><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td><td>VAL</td><td>IST</td></tr><tr><td>ROUGE-1</td><td>27.3</td><td>-</td><td>26.1</td><td>-</td><td>25.9</td><td>-</td><td>34.4</td><td>-</td><td>3.6</td><td>-</td></tr><tr><td>ROUGE-2</td><td>10.9</td><td>-</td><td>11.7</td><td>-</td><td>11.2</td><td>-</td><td>14.4</td><td>-</td><td>9.9</td><td>-</td></tr><tr><td>ROUGE-L</td><td>24.7</td><td>-</td><td>25.7</td><td>-</td><td>24.4</td><td>-</td><td>32.5</td><td>-</td><td>5.2</td><td>-</td></tr><tr><td>BERTScore</td><td>31.7</td><td>-</td><td>31.7</td><td>-</td><td>27.2</td><td>-</td><td>33.7</td><td>-</td><td>-4.6</td><td>-</td></tr><tr><td>MoverScore</td><td>17.7</td><td>-</td><td>14.2</td><td>-</td><td>16.0</td><td>-</td><td>18.9</td><td>-</td><td>5.4</td><td>-</td></tr><tr><td>PRISM</td><td>60.7</td><td>-</td><td>56.5</td><td>-</td><td>59.2</td><td>-</td><td>61.9</td><td>-</td><td>2.5</td><td>-</td></tr><tr><td>BARTSCORE</td><td>70.3</td><td>-</td><td>67.2</td><td>-</td><td>63.1</td><td>-</td><td>68.8</td><td>-</td><td>0.9</td><td>-</td></tr><tr><td>+CNN</td><td>68.5</td><td>-</td><td>64.9</td><td>-</td><td>60.4</td><td>-</td><td>66.3</td><td>-</td><td>18.4</td><td>-</td></tr><tr><td>+CNN+Para</td><td>69.0</td><td>-</td><td>65.5</td><td>-</td><td>62.5</td><td>-</td><td>67.3</td><td>-</td><td>6.4</td><td>-</td></tr><tr><td>GPT3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT3-a01</td><td>71.6</td><td>71.9t</td><td>69.7</td><td>70.0+</td><td>66.0</td><td>67.0+</td><td>69.6</td><td>69.2</td><td>10.3</td><td>9.2</td></tr><tr><td>GPT3-b01</td><td>73.6</td><td>72.9</td><td>70.2</td><td>70.3</td><td>66.8</td><td>68.3t</td><td>71.5</td><td>71.2</td><td>8.5</td><td>14.2</td></tr><tr><td>GPT3-c01</td><td>73.8</td><td>72.8</td><td>70.5</td><td>70.9†</td><td>65.9</td><td>68.6+</td><td>71.0</td><td>71.1</td><td>15.2</td><td>22.1t</td></tr><tr><td>GPT3-d01</td><td>72.6</td><td>73.4t</td><td>68.5</td><td>70.0+</td><td>65.9</td><td>66.9†</td><td>71.1</td><td>72.1+</td><td>24.0</td><td>22.7</td></tr><tr><td>GPT3-d03</td><td>73.8</td><td>73.1</td><td>70.4</td><td>70.0</td><td>67.4</td><td>68.9†</td><td>74.1</td><td>73.3</td><td>21.7</td><td>22.0+</td></tr><tr><td>Avg.</td><td>73.1</td><td>72.8</td><td>69.9</td><td>70.2†</td><td>66.4</td><td>67.9†</td><td>71.4</td><td>71.4</td><td>15.9</td><td>18.0†</td></tr><tr><td>GPT2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT2-M</td><td>68.9</td><td>71.7t</td><td>66.4</td><td>68.0+</td><td>61.1</td><td>62.3t</td><td>67.0</td><td>66.8</td><td>18.1</td><td>18.7t</td></tr><tr><td>GPT2-L</td><td>70.5</td><td>72.3t</td><td>66.6</td><td>68.3t</td><td>60.2</td><td>61.4t</td><td>66.8</td><td>67.8t</td><td>19.2</td><td>19.6+</td></tr><tr><td>GPT2-XL</td><td>71.0</td><td>70.5</td><td>66.6</td><td>66.6</td><td>61.4</td><td>60.7</td><td>67.2</td><td>66.9</td><td>21.2</td><td>21.2</td></tr><tr><td>GPT-J-6B</td><td>71.8</td><td>71.4</td><td>69.8</td><td>69.5</td><td>65.5</td><td>65.5</td><td>69.4</td><td>69.3</td><td>21.6</td><td>22.0+</td></tr><tr><td>Avg.</td><td>70.5</td><td>71.5�</td><td>67.4</td><td>68.1†</td><td>62.0</td><td>62.5†</td><td>67.6</td><td>67.7</td><td>20.0</td><td>20.4†</td></tr><tr><td>OPT</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OPT-350M</td><td>70.6</td><td>71.5t</td><td>69.2</td><td>69.9t</td><td>67.3</td><td>68.1t</td><td>70.8</td><td>71.6+</td><td>13.5</td><td>13.3</td></tr><tr><td>OPT-1.3B</td><td>73.2</td><td>73.6t</td><td>70.9</td><td>71.3t</td><td>67.2</td><td>67.8t</td><td>72.5</td><td>72.4</td><td>21.1</td><td>19.9</td></tr><tr><td>OPT-6.7B</td><td>71.9</td><td>71.9</td><td>69.0</td><td>69.0</td><td>67.7</td><td>67.1</td><td>71.7</td><td>71.3</td><td>21.2</td><td>19.9</td></tr><tr><td>OPT-13B</td><td>71.9</td><td>71.9</td><td>68.9</td><td>69.6t</td><td>65.4</td><td>66.0+</td><td>71.2</td><td>71.5t</td><td>23.1</td><td>22.1</td></tr><tr><td>OPT-66B</td><td>72.8</td><td>72.8</td><td>70.0</td><td>69.5</td><td>66.0</td><td>65.9</td><td>71.9</td><td>71.9</td><td>24.0</td><td>23.1</td></tr><tr><td>Avg.</td><td>72.1</td><td>72.3�</td><td>69.6</td><td>69.9†</td><td>66.7</td><td>67.0+</td><td>71.6</td><td>71.8t</td><td>20.6</td><td>19.6</td></tr><tr><td>FLAN-T5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FT5-S</td><td>68.3</td><td>69.2t</td><td>64.6</td><td>64.1</td><td>59.8</td><td>60.4+</td><td>64.6</td><td>65.5+</td><td>14.4</td><td>15.1+</td></tr><tr><td>FT5-B</td><td>68.9</td><td>69.0</td><td>64.8</td><td>64.6</td><td>59.6</td><td>59.9t</td><td>66.5</td><td>66.5</td><td>13.6</td><td>16.3t</td></tr><tr><td>FT5-L</td><td>70.5</td><td>69.1</td><td>66.1</td><td>64.6</td><td>60.9</td><td>60.0</td><td>66.6</td><td>65.4</td><td>27.2</td><td>28.8�</td></tr><tr><td>FT5-XL</td><td>72.1</td><td>70.1</td><td>66.7</td><td>65.6</td><td>61.0</td><td>60.5</td><td>68.3</td><td>67.5</td><td>18.9</td><td>25.6+</td></tr><tr><td>FT5-XXL</td><td>70.7</td><td>69.3</td><td>65.7</td><td>65.2</td><td>60.2</td><td>60.4+</td><td>67.6</td><td>67.8t</td><td>23.9</td><td>27.8t</td></tr><tr><td>Avg.</td><td>70.1</td><td>69.3</td><td>65.6</td><td>64.8</td><td>60.3</td><td>60.2</td><td>66.7</td><td>66.5</td><td>19.6</td><td>22.7†</td></tr><tr><td>Overall Avg</td><td>71.5</td><td>71.5</td><td>68.1</td><td>68.3</td><td>64.0</td><td>64.5+</td><td>69.4</td><td>69.4</td><td>19.0</td><td>20.2+</td></tr></table>",
            "id": 231,
            "page": 19,
            "text": "Model NEWSROOM QXSUM  COH CON FLU REL COV  VAL IST VAL IST VAL IST VAL IST VAL IST  ROUGE-1 27.3 - 26.1 - 25.9 - 34.4 - 3.6  ROUGE-2 10.9 - 11.7 - 11.2 - 14.4 - 9.9  ROUGE-L 24.7 - 25.7 - 24.4 - 32.5 - 5.2  BERTScore 31.7 - 31.7 - 27.2 - 33.7 - -4.6  MoverScore 17.7 - 14.2 - 16.0 - 18.9 - 5.4  PRISM 60.7 - 56.5 - 59.2 - 61.9 - 2.5  BARTSCORE 70.3 - 67.2 - 63.1 - 68.8 - 0.9  +CNN 68.5 - 64.9 - 60.4 - 66.3 - 18.4  +CNN+Para 69.0 - 65.5 - 62.5 - 67.3 - 6.4  GPT3            GPT3-a01 71.6 71.9t 69.7 70.0+ 66.0 67.0+ 69.6 69.2 10.3 9.2  GPT3-b01 73.6 72.9 70.2 70.3 66.8 68.3t 71.5 71.2 8.5 14.2  GPT3-c01 73.8 72.8 70.5 70.9† 65.9 68.6+ 71.0 71.1 15.2 22.1t  GPT3-d01 72.6 73.4t 68.5 70.0+ 65.9 66.9† 71.1 72.1+ 24.0 22.7  GPT3-d03 73.8 73.1 70.4 70.0 67.4 68.9† 74.1 73.3 21.7 22.0+  Avg. 73.1 72.8 69.9 70.2† 66.4 67.9† 71.4 71.4 15.9 18.0†  GPT2            GPT2-M 68.9 71.7t 66.4 68.0+ 61.1 62.3t 67.0 66.8 18.1 18.7t  GPT2-L 70.5 72.3t 66.6 68.3t 60.2 61.4t 66.8 67.8t 19.2 19.6+  GPT2-XL 71.0 70.5 66.6 66.6 61.4 60.7 67.2 66.9 21.2 21.2  GPT-J-6B 71.8 71.4 69.8 69.5 65.5 65.5 69.4 69.3 21.6 22.0+  Avg. 70.5 71.5� 67.4 68.1† 62.0 62.5† 67.6 67.7 20.0 20.4†  OPT            OPT-350M 70.6 71.5t 69.2 69.9t 67.3 68.1t 70.8 71.6+ 13.5 13.3  OPT-1.3B 73.2 73.6t 70.9 71.3t 67.2 67.8t 72.5 72.4 21.1 19.9  OPT-6.7B 71.9 71.9 69.0 69.0 67.7 67.1 71.7 71.3 21.2 19.9  OPT-13B 71.9 71.9 68.9 69.6t 65.4 66.0+ 71.2 71.5t 23.1 22.1  OPT-66B 72.8 72.8 70.0 69.5 66.0 65.9 71.9 71.9 24.0 23.1  Avg. 72.1 72.3� 69.6 69.9† 66.7 67.0+ 71.6 71.8t 20.6 19.6  FLAN-T5            FT5-S 68.3 69.2t 64.6 64.1 59.8 60.4+ 64.6 65.5+ 14.4 15.1+  FT5-B 68.9 69.0 64.8 64.6 59.6 59.9t 66.5 66.5 13.6 16.3t  FT5-L 70.5 69.1 66.1 64.6 60.9 60.0 66.6 65.4 27.2 28.8�  FT5-XL 72.1 70.1 66.7 65.6 61.0 60.5 68.3 67.5 18.9 25.6+  FT5-XXL 70.7 69.3 65.7 65.2 60.2 60.4+ 67.6 67.8t 23.9 27.8t  Avg. 70.1 69.3 65.6 64.8 60.3 60.2 66.7 66.5 19.6 22.7†  Overall Avg 71.5 71.5 68.1 68.3 64.0 64.5+ 69.4 69.4 19.0"
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 2619
                },
                {
                    "x": 2266,
                    "y": 2619
                },
                {
                    "x": 2266,
                    "y": 2764
                },
                {
                    "x": 220,
                    "y": 2764
                }
            ],
            "category": "caption",
            "html": "<caption id='232' style='font-size:18px'>Table 13. Spearman correlations on NEWSROOM and QXSUM datasets for text summarization task. VAL and IST denote the evaluator<br>with vanilla and instruction, respectively. Values with 1 denote the evaluator with instruction significantly outperforms with vanilla. Values<br>in bold are the best performance in a set of variants (e.g., GPT3 family).</caption>",
            "id": 232,
            "page": 19,
            "text": "Table 13. Spearman correlations on NEWSROOM and QXSUM datasets for text summarization task. VAL and IST denote the evaluator with vanilla and instruction, respectively. Values with 1 denote the evaluator with instruction significantly outperforms with vanilla. Values in bold are the best performance in a set of variants (e.g., GPT3 family)."
        },
        {
            "bounding_box": [
                {
                    "x": 958,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 958,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='233' style='font-size:18px'>GPTScore: Evaluate as You Desire</header>",
            "id": 233,
            "page": 20,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 477,
                    "y": 476
                },
                {
                    "x": 1995,
                    "y": 476
                },
                {
                    "x": 1995,
                    "y": 2559
                },
                {
                    "x": 477,
                    "y": 2559
                }
            ],
            "category": "table",
            "html": "<table id='234' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">ACC</td><td colspan=\"3\">FLU</td><td colspan=\"3\">MQM</td></tr><tr><td>VAL</td><td>IST</td><td>IDM</td><td>VAL</td><td>IST</td><td>IDM</td><td>VAL</td><td>IST</td><td>IDM</td></tr><tr><td>ROUGE-1</td><td>21.3</td><td>-</td><td>-</td><td>1.7</td><td>-</td><td>-</td><td>17.5</td><td>-</td><td>-</td></tr><tr><td>ROUGE-2</td><td>15.0</td><td>-</td><td>-</td><td>5.8</td><td>-</td><td>-</td><td>15.4</td><td>-</td><td>-</td></tr><tr><td>ROUGE-L</td><td>16.6</td><td>-</td><td>-</td><td>8.7</td><td>-</td><td>-</td><td>15.7</td><td>-</td><td>-</td></tr><tr><td>BERTScore</td><td>26.1</td><td>-</td><td>-</td><td>8.2</td><td>-</td><td>-</td><td>23.6</td><td>-</td><td>-</td></tr><tr><td>MoverScore</td><td>18.2</td><td>-</td><td>-</td><td>1.2</td><td>-</td><td>-</td><td>17.2</td><td>-</td><td>-</td></tr><tr><td>PRISM</td><td>25.9</td><td>-</td><td>-</td><td>9.1</td><td>-</td><td>-</td><td>27.4</td><td>-</td><td>-</td></tr><tr><td>BARTSCORE</td><td>26.1</td><td>-</td><td>-</td><td>8.2</td><td>-</td><td>-</td><td>23.6</td><td>-</td><td>-</td></tr><tr><td>+CNN</td><td>26.2</td><td>-</td><td>-</td><td>8.1</td><td>-</td><td>-</td><td>28.7</td><td>-</td><td>-</td></tr><tr><td>+CNN+Para</td><td>31.0</td><td>-</td><td>-</td><td>10.8</td><td>-</td><td>-</td><td>29.9</td><td>-</td><td></td></tr><tr><td colspan=\"10\">GPT3</td></tr><tr><td>GPT3-a01</td><td>24.9</td><td>23.7</td><td>27.9t,‡</td><td>5.9</td><td>6.3t</td><td>11.6+,‡</td><td>27.0</td><td>24.1</td><td>24.4±</td></tr><tr><td>GPT3-b01</td><td>25.9</td><td>25.0</td><td>29.8�,‡</td><td>10.7</td><td>10.8</td><td>14.0+,‡</td><td>29.4</td><td>29.6</td><td>31.2+,‡</td></tr><tr><td>GPT3-c01</td><td>29.4</td><td>30.3†</td><td>30.2+</td><td>10.7</td><td>9.3</td><td>17.9t,‡</td><td>33.3</td><td>34.8t</td><td>34.5t</td></tr><tr><td>GPT3-d01</td><td>28.6</td><td>26.5</td><td>31.2+,‡</td><td>11.3</td><td>8.6</td><td>17.5+,‡</td><td>32.0</td><td>32.5t</td><td>38.3t,‡</td></tr><tr><td>GPT3-d03</td><td>27.2</td><td>30.1t</td><td>29.5t</td><td>18.0</td><td>17.1</td><td>21.3 t,‡</td><td>29.9</td><td>34.8t</td><td>32.8t</td></tr><tr><td>Avg.</td><td>27.2</td><td>27.1</td><td>29.7�,‡</td><td>11.3</td><td>10.4</td><td>16.4+,‡</td><td>30.3</td><td>31.2†</td><td>32.3 T ,‡</td></tr><tr><td colspan=\"10\">GPT2</td></tr><tr><td>GPT2-M</td><td>25.7</td><td>24.6</td><td>29.6+,‡</td><td>8.6</td><td>9.4t</td><td>15.1+,‡</td><td>32.1</td><td>29.4</td><td>34.1+,‡</td></tr><tr><td>GPT2-L</td><td>27.2</td><td>28.5t</td><td>32.2+,‡</td><td>11.1</td><td>10.4</td><td>14.9t,‡</td><td>31.2</td><td>30.9</td><td>33.9+,‡</td></tr><tr><td>GPT2-XL</td><td>24.2</td><td>27.6+</td><td>29.7+,‡</td><td>9.4</td><td>12.0+</td><td>17.4+,‡</td><td>28.6</td><td>32.2t</td><td>35.8+,‡</td></tr><tr><td>GPT-J-6B</td><td>26.2</td><td>27.2+</td><td>29.5+,‡</td><td>9.9</td><td>11.2+</td><td>15.9+,‡</td><td>28.5</td><td>28.8�</td><td>30.3+,‡</td></tr><tr><td>Avg.</td><td>25.8</td><td>27.0+</td><td>30.3t,‡</td><td>9.8</td><td>10.8+</td><td>15.8t,‡</td><td>30.1</td><td>30.3t</td><td>33.5t,‡</td></tr><tr><td colspan=\"10\">OPT</td></tr><tr><td>OPT-350M</td><td>29.3</td><td>28.1</td><td>28.6‡</td><td>11.7</td><td>11.9</td><td>15.7+,‡</td><td>31.5</td><td>32.5t</td><td>31.8</td></tr><tr><td>OPT-1.3B</td><td>27.9</td><td>27.7</td><td>28.0‡</td><td>8.8</td><td>13.3t</td><td>15.9+,‡</td><td>32.6</td><td>33.6t</td><td>32.9t</td></tr><tr><td>OPT-6.7B</td><td>29.6</td><td>30.7t</td><td>30.6+</td><td>10.7</td><td>12.2+</td><td>15.0+,‡</td><td>34.2</td><td>36.4t</td><td>36.9+,±</td></tr><tr><td>OPT-13B</td><td>27.5</td><td>29.5t</td><td>30.8+,‡</td><td>9.6</td><td>11.7t</td><td>17.9+,‡</td><td>31.9</td><td>35.5t</td><td>37.5+,‡</td></tr><tr><td>OPT-66B</td><td>29.5</td><td>31.0+</td><td>33.4+,‡</td><td>9.1</td><td>12.1t</td><td>16.8+,‡</td><td>32.1</td><td>35.3t</td><td>36.4+,‡</td></tr><tr><td>Avg.</td><td>28.7</td><td>29.4t</td><td>30.3t,‡</td><td>10.0</td><td>12.2†</td><td>16.3t,‡</td><td>32.5</td><td>34.6+</td><td>35.1 t,‡</td></tr><tr><td colspan=\"10\">FLAN-T5</td></tr><tr><td>FT5-S</td><td>27.6</td><td>28.7t</td><td>27.0</td><td>12.6</td><td>9.4</td><td>15.0+,±</td><td>33.5</td><td>33.3</td><td>31.3</td></tr><tr><td>FT5-B</td><td>25.5</td><td>25.4</td><td>27.4+,‡</td><td>10.4</td><td>10.2</td><td>15.9+,‡</td><td>29.8</td><td>29.6</td><td>30.0±</td></tr><tr><td>FT5-L</td><td>28.5</td><td>28.5</td><td>28.8+,‡</td><td>7.9</td><td>13.0+</td><td>15.6+,‡</td><td>30.7</td><td>31.6+</td><td>32.1+,‡</td></tr><tr><td>FT5-XL</td><td>28.1</td><td>27.0</td><td>28.1‡</td><td>9.4</td><td>10.2+</td><td>14.0+,±</td><td>30.4</td><td>33.5t</td><td>34.2+,‡</td></tr><tr><td>FT5-XXL</td><td>29.0</td><td>29.4t</td><td>30.5+,‡</td><td>7.6</td><td>12.2+</td><td>16.2+,‡</td><td>30.7</td><td>33.3t</td><td>33.8+,‡</td></tr><tr><td>Avg.</td><td>27.7</td><td>27.8</td><td>28.3�,‡</td><td>9.6</td><td>11.0+</td><td>15.4+,‡</td><td>31.0</td><td>32.3†</td><td>32.3†</td></tr><tr><td>Overall Avg</td><td>27.4</td><td>27.8t</td><td>29.7�,‡</td><td>10.2</td><td>11.1+</td><td>16.0+,‡</td><td>31.0</td><td>32.1t</td><td>33.3+,‡</td></tr></table>",
            "id": 234,
            "page": 20,
            "text": "Model ACC FLU MQM  VAL IST IDM VAL IST IDM VAL IST IDM  ROUGE-1 21.3 - - 1.7 - - 17.5 -  ROUGE-2 15.0 - - 5.8 - - 15.4 -  ROUGE-L 16.6 - - 8.7 - - 15.7 -  BERTScore 26.1 - - 8.2 - - 23.6 -  MoverScore 18.2 - - 1.2 - - 17.2 -  PRISM 25.9 - - 9.1 - - 27.4 -  BARTSCORE 26.1 - - 8.2 - - 23.6 -  +CNN 26.2 - - 8.1 - - 28.7 -  +CNN+Para 31.0 - - 10.8 - - 29.9 -   GPT3  GPT3-a01 24.9 23.7 27.9t,‡ 5.9 6.3t 11.6+,‡ 27.0 24.1 24.4±  GPT3-b01 25.9 25.0 29.8�,‡ 10.7 10.8 14.0+,‡ 29.4 29.6 31.2+,‡  GPT3-c01 29.4 30.3† 30.2+ 10.7 9.3 17.9t,‡ 33.3 34.8t 34.5t  GPT3-d01 28.6 26.5 31.2+,‡ 11.3 8.6 17.5+,‡ 32.0 32.5t 38.3t,‡  GPT3-d03 27.2 30.1t 29.5t 18.0 17.1 21.3 t,‡ 29.9 34.8t 32.8t  Avg. 27.2 27.1 29.7�,‡ 11.3 10.4 16.4+,‡ 30.3 31.2† 32.3 T ,‡  GPT2  GPT2-M 25.7 24.6 29.6+,‡ 8.6 9.4t 15.1+,‡ 32.1 29.4 34.1+,‡  GPT2-L 27.2 28.5t 32.2+,‡ 11.1 10.4 14.9t,‡ 31.2 30.9 33.9+,‡  GPT2-XL 24.2 27.6+ 29.7+,‡ 9.4 12.0+ 17.4+,‡ 28.6 32.2t 35.8+,‡  GPT-J-6B 26.2 27.2+ 29.5+,‡ 9.9 11.2+ 15.9+,‡ 28.5 28.8� 30.3+,‡  Avg. 25.8 27.0+ 30.3t,‡ 9.8 10.8+ 15.8t,‡ 30.1 30.3t 33.5t,‡  OPT  OPT-350M 29.3 28.1 28.6‡ 11.7 11.9 15.7+,‡ 31.5 32.5t 31.8  OPT-1.3B 27.9 27.7 28.0‡ 8.8 13.3t 15.9+,‡ 32.6 33.6t 32.9t  OPT-6.7B 29.6 30.7t 30.6+ 10.7 12.2+ 15.0+,‡ 34.2 36.4t 36.9+,±  OPT-13B 27.5 29.5t 30.8+,‡ 9.6 11.7t 17.9+,‡ 31.9 35.5t 37.5+,‡  OPT-66B 29.5 31.0+ 33.4+,‡ 9.1 12.1t 16.8+,‡ 32.1 35.3t 36.4+,‡  Avg. 28.7 29.4t 30.3t,‡ 10.0 12.2† 16.3t,‡ 32.5 34.6+ 35.1 t,‡  FLAN-T5  FT5-S 27.6 28.7t 27.0 12.6 9.4 15.0+,± 33.5 33.3 31.3  FT5-B 25.5 25.4 27.4+,‡ 10.4 10.2 15.9+,‡ 29.8 29.6 30.0±  FT5-L 28.5 28.5 28.8+,‡ 7.9 13.0+ 15.6+,‡ 30.7 31.6+ 32.1+,‡  FT5-XL 28.1 27.0 28.1‡ 9.4 10.2+ 14.0+,± 30.4 33.5t 34.2+,‡  FT5-XXL 29.0 29.4t 30.5+,‡ 7.6 12.2+ 16.2+,‡ 30.7 33.3t 33.8+,‡  Avg. 27.7 27.8 28.3�,‡ 9.6 11.0+ 15.4+,‡ 31.0 32.3† 32.3†  Overall Avg 27.4 27.8t 29.7�,‡ 10.2 11.1+ 16.0+,‡ 31.0 32.1t"
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 2581
                },
                {
                    "x": 2266,
                    "y": 2581
                },
                {
                    "x": 2266,
                    "y": 2776
                },
                {
                    "x": 220,
                    "y": 2776
                }
            ],
            "category": "caption",
            "html": "<br><caption id='235' style='font-size:18px'>Table 14. Spearman correlations on MQM-2020 dataset for machine translation task. VAL, IST, and IDM denote the evaluator with<br>vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with 1 denote the evaluator with instruction<br>significantly outperforms with vanilla, and values with : denote the evaluator with the combination of instruction and demonstration<br>significantly outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).</caption>",
            "id": 235,
            "page": 20,
            "text": "Table 14. Spearman correlations on MQM-2020 dataset for machine translation task. VAL, IST, and IDM denote the evaluator with vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with 1 denote the evaluator with instruction significantly outperforms with vanilla, and values with : denote the evaluator with the combination of instruction and demonstration significantly outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family)."
        },
        {
            "bounding_box": [
                {
                    "x": 958,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 958,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='236' style='font-size:18px'>GPTScore: Evaluate as You Desire</header>",
            "id": 236,
            "page": 21,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 427,
                    "y": 488
                },
                {
                    "x": 2051,
                    "y": 488
                },
                {
                    "x": 2051,
                    "y": 2546
                },
                {
                    "x": 427,
                    "y": 2546
                }
            ],
            "category": "table",
            "html": "<table id='237' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">INF</td><td colspan=\"3\">NAT</td><td colspan=\"3\">FLU</td></tr><tr><td>VAL</td><td>IST</td><td>IST+DM</td><td>VAL</td><td>IST</td><td>IST+DM</td><td>VAL</td><td>IST</td><td>IST+DM</td></tr><tr><td>ROUGE-1</td><td>28.7</td><td>-</td><td>-</td><td>5.0</td><td>-</td><td>-</td><td>8.3</td><td>-</td><td></td></tr><tr><td>ROUGE-2</td><td>24.0</td><td>、</td><td>-</td><td>15.2</td><td>-</td><td>-</td><td>16.0</td><td>-</td><td>-</td></tr><tr><td>ROUGE-L</td><td>26.3</td><td>-</td><td>-</td><td>10.5</td><td>-</td><td>-</td><td>11.0</td><td>-</td><td>-</td></tr><tr><td>BERTScore</td><td>37.2</td><td>-</td><td>-</td><td>16.0</td><td>-</td><td>-</td><td>18.7</td><td>-</td><td>-</td></tr><tr><td>MoverScore</td><td>30.7</td><td>-</td><td>-</td><td>20.4</td><td>-</td><td>-</td><td>14.8</td><td>-</td><td>-</td></tr><tr><td>PRISM</td><td>36.8</td><td>-</td><td>-</td><td>28.7</td><td>-</td><td>-</td><td>34.4</td><td>-</td><td>-</td></tr><tr><td>BARTSCORE</td><td>29.5</td><td>-</td><td>-</td><td>24.0</td><td>-</td><td>-</td><td>29.7</td><td>-</td><td>-</td></tr><tr><td>+CNN</td><td>37.7</td><td>-</td><td>-</td><td>30.1</td><td>-</td><td>-</td><td>34.4</td><td>-</td><td>-</td></tr><tr><td>+CNN+Para</td><td>39.2</td><td>-</td><td>-</td><td>31.0</td><td>-</td><td>-</td><td>44.9</td><td>-</td><td>-</td></tr><tr><td>GPT3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT3-a01</td><td>33.3</td><td>37.0+</td><td>42.5+,‡</td><td>20.5</td><td>28.7t</td><td>41.7t,‡</td><td>28.8</td><td>35.1t</td><td>40.2 t,‡</td></tr><tr><td>GPT3-b01</td><td>39.2</td><td>44.5+</td><td>42.2+</td><td>18.2</td><td>29.8t</td><td>39.1+,‡</td><td>30.0</td><td>33.8t</td><td>40.3 + ,↕</td></tr><tr><td>GPT3-c01</td><td>30.6</td><td>40.9t</td><td>47.5t,‡</td><td>24.8</td><td>26.5t</td><td>39.9t,‡</td><td>27.4</td><td>34.2t</td><td>44.21,1</td></tr><tr><td>GPT3-d01</td><td>41.2</td><td>39.4</td><td>43.6+,‡</td><td>25.4</td><td>26.2+</td><td>36.6+,‡</td><td>29.7</td><td>27.1</td><td>47.9 1,1</td></tr><tr><td>GPT3-d03</td><td>32.9</td><td>29.8</td><td>42.0+,‡</td><td>19.5</td><td>21.4t</td><td>27.5+,‡</td><td>36.6</td><td>34.2</td><td>44.4+,‡</td></tr><tr><td>Avg.</td><td>35.4</td><td>38.3†</td><td>43.6t,‡</td><td>21.7</td><td>26.5t</td><td>36.9t,‡</td><td>30.5</td><td>32.9†</td><td>43.4 T ,‡</td></tr><tr><td>GPT2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT2-M</td><td>39.4</td><td>42.9t</td><td>38.6</td><td>31.2</td><td>33.2t</td><td>34.3+,‡</td><td>38.9</td><td>38.9</td><td>39.6+,‡</td></tr><tr><td>GPT2-L</td><td>39.7</td><td>42.2†</td><td>41.8+</td><td>30.1</td><td>33.5t</td><td>33.1t</td><td>34.0</td><td>40.0+</td><td>39.6t</td></tr><tr><td>GPT2-XL</td><td>41.2</td><td>42.0+</td><td>38.7</td><td>31.7</td><td>33.7t</td><td>34.8�,±</td><td>38.0</td><td>40.6+</td><td>44.21,1</td></tr><tr><td>GPT-J-6B</td><td>42.8</td><td>45.6+</td><td>41.6</td><td>32.5</td><td>31.5</td><td>31.9±</td><td>35.9</td><td>37.7t</td><td>42.0+ ,↕</td></tr><tr><td>Avg.</td><td>40.8</td><td>43.2†</td><td>40.2</td><td>31.4</td><td>33.0+</td><td>33.5+,‡</td><td>36.7</td><td>39.3†</td><td>41.3 †,‡</td></tr><tr><td>OPT</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OPT-350M</td><td>37.0</td><td>36.8</td><td>37.9t,‡</td><td>33.9</td><td>32.5</td><td>31.1</td><td>39.9</td><td>39.5</td><td>39.9‡</td></tr><tr><td>OPT-1.3B</td><td>36.7</td><td>39.3t</td><td>38.2t</td><td>28.8</td><td>30.0+</td><td>32.9+,‡</td><td>37.3</td><td>34.9</td><td>40.9+,‡</td></tr><tr><td>OPT-6.7B</td><td>40.4</td><td>39.3</td><td>38.3</td><td>31.6</td><td>27.2</td><td>35.2+,‡</td><td>36.0</td><td>34.4</td><td>43.6+,‡</td></tr><tr><td>OPT-13B</td><td>37.9</td><td>37.6</td><td>38.9�,‡</td><td>31.4</td><td>30.3</td><td>34.6+,‡</td><td>39.2</td><td>39.0</td><td>41.2+,‡</td></tr><tr><td>OPT-66B</td><td>41.4</td><td>43.2+</td><td>39.6</td><td>31.3</td><td>30.2</td><td>34.7+,‡</td><td>36.3</td><td>37.6t</td><td>42.0+ ,↕</td></tr><tr><td>Avg.</td><td>38.7</td><td>39.3</td><td>38.6</td><td>31.4</td><td>30.0</td><td>33.7t,‡</td><td>37.7</td><td>37.1</td><td>41.5 †,‡</td></tr><tr><td>FLAN-T5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FT5-S</td><td>39.8</td><td>37.6</td><td>38.2</td><td>33.0</td><td>29.5</td><td>26.6</td><td>46.1</td><td>34.7</td><td>36.1‡</td></tr><tr><td>FT5-B</td><td>39.7</td><td>43.6t</td><td>37.7</td><td>26.4</td><td>30.3t</td><td>27.3t</td><td>37.8</td><td>40.6+</td><td>37.9</td></tr><tr><td>FT5-L</td><td>42.0</td><td>42.8t</td><td>38.9</td><td>23.6</td><td>31.0+</td><td>32.6+,‡</td><td>35.3</td><td>43.3t</td><td>44.5t,±</td></tr><tr><td>FT5-XL</td><td>41.0</td><td>42.8t</td><td>43.3+,‡</td><td>24.8</td><td>28.9t</td><td>27.8t</td><td>37.4</td><td>44.4+</td><td>41.9t</td></tr><tr><td>FT5-XXL</td><td>44.9</td><td>40.7</td><td>37.4</td><td>24.8</td><td>28.8t</td><td>28.4t</td><td>34.2</td><td>42.5t</td><td>41.3t</td></tr><tr><td>Avg.</td><td>41.5</td><td>41.5</td><td>39.1</td><td>26.5</td><td>29.7t</td><td>28.6�</td><td>38.1</td><td>41.1+</td><td>40.3†</td></tr><tr><td>Overall Avg</td><td>39.1</td><td>40.6+</td><td>40.3t</td><td>27.7</td><td>29.8t</td><td>33.2+,‡</td><td>35.8</td><td>37.6t</td><td>41.6+,‡</td></tr></table>",
            "id": 237,
            "page": 21,
            "text": "Model INF NAT FLU  VAL IST IST+DM VAL IST IST+DM VAL IST IST+DM  ROUGE-1 28.7 - - 5.0 - - 8.3 -   ROUGE-2 24.0 、 - 15.2 - - 16.0 -  ROUGE-L 26.3 - - 10.5 - - 11.0 -  BERTScore 37.2 - - 16.0 - - 18.7 -  MoverScore 30.7 - - 20.4 - - 14.8 -  PRISM 36.8 - - 28.7 - - 34.4 -  BARTSCORE 29.5 - - 24.0 - - 29.7 -  +CNN 37.7 - - 30.1 - - 34.4 -  +CNN+Para 39.2 - - 31.0 - - 44.9 -  GPT3           GPT3-a01 33.3 37.0+ 42.5+,‡ 20.5 28.7t 41.7t,‡ 28.8 35.1t 40.2 t,‡  GPT3-b01 39.2 44.5+ 42.2+ 18.2 29.8t 39.1+,‡ 30.0 33.8t 40.3 + ,↕  GPT3-c01 30.6 40.9t 47.5t,‡ 24.8 26.5t 39.9t,‡ 27.4 34.2t 44.21,1  GPT3-d01 41.2 39.4 43.6+,‡ 25.4 26.2+ 36.6+,‡ 29.7 27.1 47.9 1,1  GPT3-d03 32.9 29.8 42.0+,‡ 19.5 21.4t 27.5+,‡ 36.6 34.2 44.4+,‡  Avg. 35.4 38.3† 43.6t,‡ 21.7 26.5t 36.9t,‡ 30.5 32.9† 43.4 T ,‡  GPT2           GPT2-M 39.4 42.9t 38.6 31.2 33.2t 34.3+,‡ 38.9 38.9 39.6+,‡  GPT2-L 39.7 42.2† 41.8+ 30.1 33.5t 33.1t 34.0 40.0+ 39.6t  GPT2-XL 41.2 42.0+ 38.7 31.7 33.7t 34.8�,± 38.0 40.6+ 44.21,1  GPT-J-6B 42.8 45.6+ 41.6 32.5 31.5 31.9± 35.9 37.7t 42.0+ ,↕  Avg. 40.8 43.2† 40.2 31.4 33.0+ 33.5+,‡ 36.7 39.3† 41.3 †,‡  OPT           OPT-350M 37.0 36.8 37.9t,‡ 33.9 32.5 31.1 39.9 39.5 39.9‡  OPT-1.3B 36.7 39.3t 38.2t 28.8 30.0+ 32.9+,‡ 37.3 34.9 40.9+,‡  OPT-6.7B 40.4 39.3 38.3 31.6 27.2 35.2+,‡ 36.0 34.4 43.6+,‡  OPT-13B 37.9 37.6 38.9�,‡ 31.4 30.3 34.6+,‡ 39.2 39.0 41.2+,‡  OPT-66B 41.4 43.2+ 39.6 31.3 30.2 34.7+,‡ 36.3 37.6t 42.0+ ,↕  Avg. 38.7 39.3 38.6 31.4 30.0 33.7t,‡ 37.7 37.1 41.5 †,‡  FLAN-T5           FT5-S 39.8 37.6 38.2 33.0 29.5 26.6 46.1 34.7 36.1‡  FT5-B 39.7 43.6t 37.7 26.4 30.3t 27.3t 37.8 40.6+ 37.9  FT5-L 42.0 42.8t 38.9 23.6 31.0+ 32.6+,‡ 35.3 43.3t 44.5t,±  FT5-XL 41.0 42.8t 43.3+,‡ 24.8 28.9t 27.8t 37.4 44.4+ 41.9t  FT5-XXL 44.9 40.7 37.4 24.8 28.8t 28.4t 34.2 42.5t 41.3t  Avg. 41.5 41.5 39.1 26.5 29.7t 28.6� 38.1 41.1+ 40.3†  Overall Avg 39.1 40.6+ 40.3t 27.7 29.8t 33.2+,‡ 35.8 37.6t"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2573
                },
                {
                    "x": 2267,
                    "y": 2573
                },
                {
                    "x": 2267,
                    "y": 2761
                },
                {
                    "x": 221,
                    "y": 2761
                }
            ],
            "category": "caption",
            "html": "<caption id='238' style='font-size:18px'>Table 15. Spearman correlations on BAGEL dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction,<br>and the combination of instruction and demonstration, respectively. Values with 1 denote the evaluator with instruction significantly<br>outperforms with vanilla, and values with 1 denote the evaluator with the combination of instruction and demonstration significantly<br>outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).</caption>",
            "id": 238,
            "page": 21,
            "text": "Table 15. Spearman correlations on BAGEL dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with 1 denote the evaluator with instruction significantly outperforms with vanilla, and values with 1 denote the evaluator with the combination of instruction and demonstration significantly outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family)."
        },
        {
            "bounding_box": [
                {
                    "x": 959,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 190
                },
                {
                    "x": 1526,
                    "y": 235
                },
                {
                    "x": 959,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='239' style='font-size:18px'>GPTScore: Evaluate as You Desire</header>",
            "id": 239,
            "page": 22,
            "text": "GPTScore: Evaluate as You Desire"
        },
        {
            "bounding_box": [
                {
                    "x": 430,
                    "y": 480
                },
                {
                    "x": 2055,
                    "y": 480
                },
                {
                    "x": 2055,
                    "y": 2553
                },
                {
                    "x": 430,
                    "y": 2553
                }
            ],
            "category": "table",
            "html": "<table id='240' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">INF</td><td colspan=\"3\">NAT</td><td colspan=\"3\">FLU</td></tr><tr><td>VAL</td><td>IST</td><td>IST+DM</td><td>VAL</td><td>IST</td><td>IST+DM</td><td>VAL</td><td>IST</td><td>IST+DM</td></tr><tr><td>ROUGE-1</td><td>24.2</td><td>-</td><td>、</td><td>24.2</td><td>-</td><td>-</td><td>15.1</td><td>-</td><td>-</td></tr><tr><td>ROUGE-2</td><td>21.9</td><td>-</td><td>-</td><td>25.9</td><td>-</td><td>-</td><td>11.4</td><td>-</td><td>-</td></tr><tr><td>ROUGE-L</td><td>18.5</td><td>-</td><td>-</td><td>20.2</td><td>-</td><td>-</td><td>1.7</td><td>-</td><td>-</td></tr><tr><td>BERTScore</td><td>25.8</td><td>-</td><td>-</td><td>28.0</td><td>-</td><td>-</td><td>11.8</td><td>-</td><td>-</td></tr><tr><td>MoverScore</td><td>17.9</td><td>-</td><td>-</td><td>24.4</td><td>-</td><td>-</td><td>5.0</td><td>-</td><td>-</td></tr><tr><td>PRISM</td><td>27.4</td><td>-</td><td>-</td><td>33.1</td><td>-</td><td>-</td><td>14.2</td><td>-</td><td>-</td></tr><tr><td>BARTSCORE</td><td>22.4</td><td>-</td><td>-</td><td>25.5</td><td>-</td><td>-</td><td>6.9</td><td>-</td><td>-</td></tr><tr><td>+CNN</td><td>24.2</td><td>-</td><td>-</td><td>30.6</td><td>-</td><td>-</td><td>17.2</td><td>-</td><td>-</td></tr><tr><td>+CNN+Para</td><td>25.0</td><td>-</td><td>-</td><td>30.2</td><td>-</td><td>、</td><td>19.5</td><td>-</td><td></td></tr><tr><td>GPT3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT3-a01</td><td>25.4</td><td>19.1</td><td>25.6±</td><td>28.7</td><td>34.0†</td><td>37.7+,‡</td><td>30.7</td><td>27.0</td><td>26.6</td></tr><tr><td>GPT3-b01</td><td>37.5</td><td>28.4</td><td>26.5</td><td>21.5</td><td>30.6+</td><td>26.1t</td><td>24.6</td><td>28.9t</td><td>21.1</td></tr><tr><td>GPT3-c01</td><td>29.8</td><td>21.3</td><td>33.7+,‡</td><td>24.7</td><td>28.5t</td><td>28.6t</td><td>31.1</td><td>27.1</td><td>27.6‡</td></tr><tr><td>GPT3-d01</td><td>32.6</td><td>27.0</td><td>33.9t , 1</td><td>27.3</td><td>31.7t</td><td>21.9</td><td>35.8</td><td>39.7t</td><td>27.1</td></tr><tr><td>GPT3-d03</td><td>26.6</td><td>29.6t</td><td>37.6 t , 1</td><td>22.6</td><td>27.0+</td><td>18.2</td><td>33.9</td><td>31.9</td><td>28.2</td></tr><tr><td>Avg.</td><td>30.4</td><td>25.1</td><td>31.5t,‡</td><td>25.0</td><td>30.4�</td><td>26.5�</td><td>31.2</td><td>30.9</td><td>26.1</td></tr><tr><td>GPT2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT2-M</td><td>24.7</td><td>23.1</td><td>18.2</td><td>28.7</td><td>32.7t</td><td>35.2+,‡</td><td>18.7</td><td>34.8t</td><td>33.6t</td></tr><tr><td>GPT2-L</td><td>19.6</td><td>28.1t</td><td>20.2+</td><td>31.2</td><td>32.4t</td><td>37.8+,‡</td><td>18.6</td><td>33.1t</td><td>35.9+,‡</td></tr><tr><td>GPT2-XL</td><td>22.0</td><td>23.6t</td><td>23.8t</td><td>29.7</td><td>29.1</td><td>38.0+,‡</td><td>18.2</td><td>29.8t</td><td>37.1+,‡</td></tr><tr><td>GPT-J-6B</td><td>23.9</td><td>25.6+</td><td>19.6</td><td>34.3</td><td>33.3</td><td>36.8+,‡</td><td>24.4</td><td>34.5t</td><td>38.4+,‡</td></tr><tr><td>Avg.</td><td>22.5</td><td>25.1†</td><td>20.5</td><td>31.0</td><td>31.9t</td><td>37.0+,‡</td><td>20.0</td><td>33.1t</td><td>36.2+,‡</td></tr><tr><td>OPT</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OPT-350M</td><td>26.1</td><td>28.7t</td><td>25.4</td><td>27.0</td><td>29.5t</td><td>35.0+,‡</td><td>21.7</td><td>26.6+</td><td>27.3+,‡</td></tr><tr><td>OPT-1.3B</td><td>26.1</td><td>28.3t</td><td>23.5</td><td>26.0</td><td>30.5t</td><td>38.7+,‡</td><td>23.0</td><td>26.9t</td><td>29.8�,‡</td></tr><tr><td>OPT-6.7B</td><td>26.2</td><td>26.0</td><td>24.2</td><td>26.7</td><td>31.0+</td><td>36.5+,‡</td><td>21.7</td><td>25.8t</td><td>35.9+,‡</td></tr><tr><td>OPT-13B</td><td>27.7</td><td>26.9</td><td>26.0</td><td>24.4</td><td>30.1t</td><td>38.0+,‡</td><td>20.2</td><td>29.6+</td><td>34.91,‡</td></tr><tr><td>OPT-66B</td><td>20.1</td><td>24.7t</td><td>22.4+</td><td>26.8</td><td>29.1t</td><td>34.6+,‡</td><td>19.8</td><td>19.1</td><td>25.3 t,‡</td></tr><tr><td>Avg.</td><td>25.2</td><td>26.9†</td><td>24.3</td><td>26.2</td><td>30.0+</td><td>36.6t,‡</td><td>21.3</td><td>25.6�</td><td>30.6+,‡</td></tr><tr><td>FLAN-T5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>FT5-S</td><td>19.7</td><td>16.9</td><td>17.0</td><td>33.6</td><td>33.1</td><td>33.0</td><td>19.4</td><td>17.2</td><td>15.9</td></tr><tr><td>FT5-B</td><td>24.2</td><td>23.7</td><td>20.9</td><td>31.7</td><td>32.5t</td><td>33.4+,‡</td><td>14.2</td><td>15.5t</td><td>16.8+,‡</td></tr><tr><td>FT5-L</td><td>24.9</td><td>22.3</td><td>20.6</td><td>36.2</td><td>37.1t</td><td>38.6+,‡</td><td>24.3</td><td>18.1</td><td>21.1‡</td></tr><tr><td>FT5-XL</td><td>26.1</td><td>23.7</td><td>19.5</td><td>38.4</td><td>35.6</td><td>37.4±</td><td>28.4</td><td>21.0</td><td>22.5±</td></tr><tr><td>FT5-XXL</td><td>24.9</td><td>22.9</td><td>20.3</td><td>31.9</td><td>34.7t</td><td>41.7t,‡</td><td>23.8</td><td>16.9</td><td>22.2±</td></tr><tr><td>Avg.</td><td>24.0</td><td>21.9</td><td>19.7</td><td>34.3</td><td>34.6+</td><td>36.8t,‡</td><td>22.0</td><td>17.8</td><td>19.7‡</td></tr><tr><td>Overall Avg</td><td>25.5</td><td>24.7</td><td>24.0</td><td>29.1</td><td>31.7</td><td>34.2+,‡</td><td>23.6</td><td>26.8t</td><td>28.2+,‡</td></tr></table>",
            "id": 240,
            "page": 22,
            "text": "Model INF NAT FLU  VAL IST IST+DM VAL IST IST+DM VAL IST IST+DM  ROUGE-1 24.2 - 、 24.2 - - 15.1 -  ROUGE-2 21.9 - - 25.9 - - 11.4 -  ROUGE-L 18.5 - - 20.2 - - 1.7 -  BERTScore 25.8 - - 28.0 - - 11.8 -  MoverScore 17.9 - - 24.4 - - 5.0 -  PRISM 27.4 - - 33.1 - - 14.2 -  BARTSCORE 22.4 - - 25.5 - - 6.9 -  +CNN 24.2 - - 30.6 - - 17.2 -  +CNN+Para 25.0 - - 30.2 - 、 19.5 -   GPT3           GPT3-a01 25.4 19.1 25.6± 28.7 34.0† 37.7+,‡ 30.7 27.0 26.6  GPT3-b01 37.5 28.4 26.5 21.5 30.6+ 26.1t 24.6 28.9t 21.1  GPT3-c01 29.8 21.3 33.7+,‡ 24.7 28.5t 28.6t 31.1 27.1 27.6‡  GPT3-d01 32.6 27.0 33.9t , 1 27.3 31.7t 21.9 35.8 39.7t 27.1  GPT3-d03 26.6 29.6t 37.6 t , 1 22.6 27.0+ 18.2 33.9 31.9 28.2  Avg. 30.4 25.1 31.5t,‡ 25.0 30.4� 26.5� 31.2 30.9 26.1  GPT2           GPT2-M 24.7 23.1 18.2 28.7 32.7t 35.2+,‡ 18.7 34.8t 33.6t  GPT2-L 19.6 28.1t 20.2+ 31.2 32.4t 37.8+,‡ 18.6 33.1t 35.9+,‡  GPT2-XL 22.0 23.6t 23.8t 29.7 29.1 38.0+,‡ 18.2 29.8t 37.1+,‡  GPT-J-6B 23.9 25.6+ 19.6 34.3 33.3 36.8+,‡ 24.4 34.5t 38.4+,‡  Avg. 22.5 25.1† 20.5 31.0 31.9t 37.0+,‡ 20.0 33.1t 36.2+,‡  OPT           OPT-350M 26.1 28.7t 25.4 27.0 29.5t 35.0+,‡ 21.7 26.6+ 27.3+,‡  OPT-1.3B 26.1 28.3t 23.5 26.0 30.5t 38.7+,‡ 23.0 26.9t 29.8�,‡  OPT-6.7B 26.2 26.0 24.2 26.7 31.0+ 36.5+,‡ 21.7 25.8t 35.9+,‡  OPT-13B 27.7 26.9 26.0 24.4 30.1t 38.0+,‡ 20.2 29.6+ 34.91,‡  OPT-66B 20.1 24.7t 22.4+ 26.8 29.1t 34.6+,‡ 19.8 19.1 25.3 t,‡  Avg. 25.2 26.9† 24.3 26.2 30.0+ 36.6t,‡ 21.3 25.6� 30.6+,‡  FLAN-T5           FT5-S 19.7 16.9 17.0 33.6 33.1 33.0 19.4 17.2 15.9  FT5-B 24.2 23.7 20.9 31.7 32.5t 33.4+,‡ 14.2 15.5t 16.8+,‡  FT5-L 24.9 22.3 20.6 36.2 37.1t 38.6+,‡ 24.3 18.1 21.1‡  FT5-XL 26.1 23.7 19.5 38.4 35.6 37.4± 28.4 21.0 22.5±  FT5-XXL 24.9 22.9 20.3 31.9 34.7t 41.7t,‡ 23.8 16.9 22.2±  Avg. 24.0 21.9 19.7 34.3 34.6+ 36.8t,‡ 22.0 17.8 19.7‡  Overall Avg 25.5 24.7 24.0 29.1 31.7 34.2+,‡ 23.6 26.8t"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2578
                },
                {
                    "x": 2269,
                    "y": 2578
                },
                {
                    "x": 2269,
                    "y": 2774
                },
                {
                    "x": 219,
                    "y": 2774
                }
            ],
            "category": "caption",
            "html": "<caption id='241' style='font-size:18px'>Table 16. Spearman correlations on SFRES dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction,<br>and the combination of instruction and demonstration, respectively. Values with 1 denote the evaluator with instruction significantly<br>outperforms with vanilla, and values with 1 denote the evaluator with the combination of instruction and demonstration significantly<br>outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).</caption>",
            "id": 241,
            "page": 22,
            "text": "Table 16. Spearman correlations on SFRES dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with 1 denote the evaluator with instruction significantly outperforms with vanilla, and values with 1 denote the evaluator with the combination of instruction and demonstration significantly outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family)."
        }
    ]
}