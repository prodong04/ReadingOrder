{
    "id": "32abd7ec-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/BF00992698.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 565,
                    "y": 206
                },
                {
                    "x": 1698,
                    "y": 206
                },
                {
                    "x": 1698,
                    "y": 294
                },
                {
                    "x": 565,
                    "y": 294
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:14px'>Machine Learning, 8, 279-292 (1992)<br>1992 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.</p>",
            "id": 0,
            "page": 1,
            "text": "Machine Learning, 8, 279-292 (1992) 1992 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 502
                },
                {
                    "x": 619,
                    "y": 502
                },
                {
                    "x": 619,
                    "y": 566
                },
                {
                    "x": 194,
                    "y": 566
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Technical Note</p>",
            "id": 1,
            "page": 1,
            "text": "Technical Note"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 577
                },
                {
                    "x": 515,
                    "y": 577
                },
                {
                    "x": 515,
                    "y": 652
                },
                {
                    "x": 193,
                    "y": 652
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:22px'>Q-Learning</p>",
            "id": 2,
            "page": 1,
            "text": "Q-Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 699
                },
                {
                    "x": 1000,
                    "y": 699
                },
                {
                    "x": 1000,
                    "y": 785
                },
                {
                    "x": 191,
                    "y": 785
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:16px'>CHRISTOPHER J.C.H. WATKINS<br>25b Framfield Road, Highbury, London N5 IUU, England</p>",
            "id": 3,
            "page": 1,
            "text": "CHRISTOPHER J.C.H. WATKINS 25b Framfield Road, Highbury, London N5 IUU, England"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 811
                },
                {
                    "x": 422,
                    "y": 811
                },
                {
                    "x": 422,
                    "y": 848
                },
                {
                    "x": 193,
                    "y": 848
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:14px'>PETER DAYAN</p>",
            "id": 4,
            "page": 1,
            "text": "PETER DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 851
                },
                {
                    "x": 1665,
                    "y": 851
                },
                {
                    "x": 1665,
                    "y": 894
                },
                {
                    "x": 191,
                    "y": 894
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:16px'>Centre for Cognitive Science, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9EH, Scotland</p>",
            "id": 5,
            "page": 1,
            "text": "Centre for Cognitive Science, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9EH, Scotland"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 977
                },
                {
                    "x": 1696,
                    "y": 977
                },
                {
                    "x": 1696,
                    "y": 1106
                },
                {
                    "x": 191,
                    "y": 1106
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:14px'>Abstract. Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian<br>domains. It amounts to an incremental method for dynamic programming which imposes limited computational<br>demands. It works by successively improving its evaluations of the quality of particular actions at particular states.</p>",
            "id": 6,
            "page": 1,
            "text": "Abstract. Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 1103
                },
                {
                    "x": 1699,
                    "y": 1103
                },
                {
                    "x": 1699,
                    "y": 1314
                },
                {
                    "x": 189,
                    "y": 1314
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:14px'>This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins<br>(1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions<br>are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions<br>to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed<br>each iteration, rather than just one.</p>",
            "id": 7,
            "page": 1,
            "text": "This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1353
                },
                {
                    "x": 1659,
                    "y": 1353
                },
                {
                    "x": 1659,
                    "y": 1404
                },
                {
                    "x": 192,
                    "y": 1404
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>Keywords. Q-learning, reinforcement learning, temporal differences, asynchronous dynamic programming</p>",
            "id": 8,
            "page": 1,
            "text": "Keywords. Q-learning, reinforcement learning, temporal differences, asynchronous dynamic programming"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1501
                },
                {
                    "x": 475,
                    "y": 1501
                },
                {
                    "x": 475,
                    "y": 1552
                },
                {
                    "x": 192,
                    "y": 1552
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:20px'>1. Introduction</p>",
            "id": 9,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1602
                },
                {
                    "x": 1698,
                    "y": 1602
                },
                {
                    "x": 1698,
                    "y": 1802
                },
                {
                    "x": 190,
                    "y": 1802
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:20px'>Q-learning (Watkins, 1989) is a form of model-free reinforcement learning. It can also be<br>viewed as a method of asynchronous dynamic programming (DP). It provides agents with<br>the capability of learning to act optimally in Markovian domains by experiencing the con-<br>sequences of actions, without requiring them to build maps of the domains.</p>",
            "id": 10,
            "page": 1,
            "text": "Q-learning (Watkins, 1989) is a form of model-free reinforcement learning. It can also be viewed as a method of asynchronous dynamic programming (DP). It provides agents with the capability of learning to act optimally in Markovian domains by experiencing the consequences of actions, without requiring them to build maps of the domains."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 1807
                },
                {
                    "x": 1698,
                    "y": 1807
                },
                {
                    "x": 1698,
                    "y": 2254
                },
                {
                    "x": 189,
                    "y": 2254
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:20px'>Learning proceeds similarly to Sutton's (1984; 1988) method of temporal differences<br>(TD): an agent tries an action at a particular state, and evaluates its consequences in terms<br>of the immediate reward or penalty it receives and its estimate of the value of the state<br>to which it is taken. By trying all actions in all states repeatedly, it learns which are best<br>overall, judged by long-term discounted reward. Q-learning is a primitive (Watkins, 1989)<br>form of learning, but, as such, it can operate as the basis of far more sophisticated devices.<br>Examples of its use include Barto and Singh (1990), Sutton (1990), Chapman and Kael-<br>bling (1991), Mahadevan and Connell (1991), and Lin (1992), who developed it inde-<br>pendently. There are also various industrial applications.</p>",
            "id": 11,
            "page": 1,
            "text": "Learning proceeds similarly to Sutton's (1984; 1988) method of temporal differences (TD): an agent tries an action at a particular state, and evaluates its consequences in terms of the immediate reward or penalty it receives and its estimate of the value of the state to which it is taken. By trying all actions in all states repeatedly, it learns which are best overall, judged by long-term discounted reward. Q-learning is a primitive (Watkins, 1989) form of learning, but, as such, it can operate as the basis of far more sophisticated devices. Examples of its use include Barto and Singh (1990), Sutton (1990), Chapman and Kaelbling (1991), Mahadevan and Connell (1991), and Lin (1992), who developed it independently. There are also various industrial applications."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 2259
                },
                {
                    "x": 1700,
                    "y": 2259
                },
                {
                    "x": 1700,
                    "y": 2554
                },
                {
                    "x": 189,
                    "y": 2554
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:18px'>This paper presents the proof outlined by Watkins (1989) that Q-learning converges. Sec-<br>tion 2 describes the problem, the method, and the notation, section 3 gives an overview<br>of the proof, and section 4 discusses two extensions. Formal details are left as far as pos-<br>sible to the appendix. Watkins (1989) should be consulted for a more extensive discussion<br>of Q-learning, including its relationship with dynamic programming and TD. See also Werbos<br>(1977).</p>",
            "id": 12,
            "page": 1,
            "text": "This paper presents the proof outlined by Watkins (1989) that Q-learning converges. Section 2 describes the problem, the method, and the notation, section 3 gives an overview of the proof, and section 4 discusses two extensions. Formal details are left as far as possible to the appendix. Watkins (1989) should be consulted for a more extensive discussion of Q-learning, including its relationship with dynamic programming and TD. See also Werbos (1977)."
        },
        {
            "bounding_box": [
                {
                    "x": 1643,
                    "y": 2751
                },
                {
                    "x": 1693,
                    "y": 2751
                },
                {
                    "x": 1693,
                    "y": 2796
                },
                {
                    "x": 1643,
                    "y": 2796
                }
            ],
            "category": "footer",
            "html": "<footer id='13' style='font-size:18px'>55</footer>",
            "id": 13,
            "page": 1,
            "text": "55"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 207
                },
                {
                    "x": 270,
                    "y": 207
                },
                {
                    "x": 270,
                    "y": 253
                },
                {
                    "x": 194,
                    "y": 253
                }
            ],
            "category": "header",
            "html": "<header id='14' style='font-size:16px'>280</header>",
            "id": 14,
            "page": 2,
            "text": "280"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 215
                },
                {
                    "x": 1697,
                    "y": 215
                },
                {
                    "x": 1697,
                    "y": 259
                },
                {
                    "x": 1259,
                    "y": 259
                }
            ],
            "category": "header",
            "html": "<br><header id='15' style='font-size:14px'>C. WATKINS AND P. DAYAN</header>",
            "id": 15,
            "page": 2,
            "text": "C. WATKINS AND P. DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 341
                },
                {
                    "x": 689,
                    "y": 341
                },
                {
                    "x": 689,
                    "y": 393
                },
                {
                    "x": 194,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:18px'>2. The task for Q-learning</p>",
            "id": 16,
            "page": 2,
            "text": "2. The task for Q-learning"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 440
                },
                {
                    "x": 1699,
                    "y": 440
                },
                {
                    "x": 1699,
                    "y": 751
                },
                {
                    "x": 193,
                    "y": 751
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:16px'>Consider a computational agent moving around some discrete, finite world, choosing one<br>from a finite collection of actions at every time step. The world constitutes a controlled<br>Markov process with the agent as a controller. At step n, the agent is equipped to register<br>the state Xn (E X) of the world, an can choose its action an (E A)1 accordingly. The agent<br>receives a probabilistic reward rn, whose mean value Rxn (an) depends only on the state<br>and action, and the state of the world changes probabilistically to yn according to the law:</p>",
            "id": 17,
            "page": 2,
            "text": "Consider a computational agent moving around some discrete, finite world, choosing one from a finite collection of actions at every time step. The world constitutes a controlled Markov process with the agent as a controller. At step n, the agent is equipped to register the state Xn (E X) of the world, an can choose its action an (E A)1 accordingly. The agent receives a probabilistic reward rn, whose mean value Rxn (an) depends only on the state and action, and the state of the world changes probabilistically to yn according to the law:"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 896
                },
                {
                    "x": 1697,
                    "y": 896
                },
                {
                    "x": 1697,
                    "y": 1100
                },
                {
                    "x": 191,
                    "y": 1100
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>The task facing the agent is that of determining an optimal policy, one that maximizes total<br>discounted expected reward. By discounted reward, we mean that rewards received s steps<br>hence are worth less than rewards received now, by a factor of 25 (0 < 2 < 1). Under<br>a policy �, the value of state x is</p>",
            "id": 18,
            "page": 2,
            "text": "The task facing the agent is that of determining an optimal policy, one that maximizes total discounted expected reward. By discounted reward, we mean that rewards received s steps hence are worth less than rewards received now, by a factor of 25 (0 < 2 < 1). Under a policy �, the value of state x is"
        },
        {
            "bounding_box": [
                {
                    "x": 1016,
                    "y": 1244
                },
                {
                    "x": 1041,
                    "y": 1244
                },
                {
                    "x": 1041,
                    "y": 1273
                },
                {
                    "x": 1016,
                    "y": 1273
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:14px'>y</p>",
            "id": 19,
            "page": 2,
            "text": "y"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1327
                },
                {
                    "x": 1696,
                    "y": 1327
                },
                {
                    "x": 1696,
                    "y": 1530
                },
                {
                    "x": 191,
                    "y": 1530
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>because the agent expects to receive Rx(�(x)) immediately for performing the action �<br>recommends, and then moves to a state that is 'worth' V� (y) to it, with probability<br>Pxy [� (x)]. The theory of DP (Bellman & Dreyfus, 1962; Ross, 1983) assures us that there<br>is at least one optimal stationary policy �* which is such that</p>",
            "id": 20,
            "page": 2,
            "text": "because the agent expects to receive Rx(�(x)) immediately for performing the action � recommends, and then moves to a state that is 'worth' V� (y) to it, with probability Pxy [� (x)]. The theory of DP (Bellman & Dreyfus, 1962; Ross, 1983) assures us that there is at least one optimal stationary policy �* which is such that"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1781
                },
                {
                    "x": 1697,
                    "y": 1781
                },
                {
                    "x": 1697,
                    "y": 2228
                },
                {
                    "x": 190,
                    "y": 2228
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:18px'>is as well as an agent can do from state x. Although this might look circular, it is actually<br>*<br>well defined, and DP provides a number of methods for calculating V* and one � , assum-<br>ing that Rx (a) and P [a] are known. The task facing a Q learner is that of determining<br>xy<br>a � without initially knowing these values. There are traditional methods (e.g., Sato, Abe<br>& Takeda, 1988) for learning Rx(a) and Pxy [a] while concurrently performing DP, but<br>any assumption of certainty equivalence, i.e., calculating actions as if the current model<br>were accurate, costs dearly in the early stages of learning (Barto & Singh, 1990). Watkins<br>(1989) classes Q-learning as incremental dynamic programming, because of the step-by-<br>step manner in which it determines the optimal policy.</p>",
            "id": 21,
            "page": 2,
            "text": "is as well as an agent can do from state x. Although this might look circular, it is actually * well defined, and DP provides a number of methods for calculating V* and one � , assuming that Rx (a) and P [a] are known. The task facing a Q learner is that of determining xy a � without initially knowing these values. There are traditional methods (e.g., Sato, Abe & Takeda, 1988) for learning Rx(a) and Pxy [a] while concurrently performing DP, but any assumption of certainty equivalence, i.e., calculating actions as if the current model were accurate, costs dearly in the early stages of learning (Barto & Singh, 1990). Watkins (1989) classes Q-learning as incremental dynamic programming, because of the step-bystep manner in which it determines the optimal policy."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2229
                },
                {
                    "x": 1150,
                    "y": 2229
                },
                {
                    "x": 1150,
                    "y": 2283
                },
                {
                    "x": 226,
                    "y": 2283
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:16px'>For a policy �, define Q values (or action-values) as:</p>",
            "id": 22,
            "page": 2,
            "text": "For a policy �, define Q values (or action-values) as:"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2501
                },
                {
                    "x": 1694,
                    "y": 2501
                },
                {
                    "x": 1694,
                    "y": 2607
                },
                {
                    "x": 190,
                    "y": 2607
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:16px'>In other words, the Q value is the expected discounted reward for executing action a at<br>state x and following policy � thereafter. The object in Q-learning is to estimate the Q</p>",
            "id": 23,
            "page": 2,
            "text": "In other words, the Q value is the expected discounted reward for executing action a at state x and following policy � thereafter. The object in Q-learning is to estimate the Q"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2708
                },
                {
                    "x": 241,
                    "y": 2708
                },
                {
                    "x": 241,
                    "y": 2749
                },
                {
                    "x": 192,
                    "y": 2749
                }
            ],
            "category": "footer",
            "html": "<footer id='24' style='font-size:16px'>56</footer>",
            "id": 24,
            "page": 2,
            "text": "56"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 214
                },
                {
                    "x": 409,
                    "y": 214
                },
                {
                    "x": 409,
                    "y": 260
                },
                {
                    "x": 190,
                    "y": 260
                }
            ],
            "category": "header",
            "html": "<header id='25' style='font-size:14px'>Q-LEARNING</header>",
            "id": 25,
            "page": 3,
            "text": "Q-LEARNING"
        },
        {
            "bounding_box": [
                {
                    "x": 1621,
                    "y": 207
                },
                {
                    "x": 1691,
                    "y": 207
                },
                {
                    "x": 1691,
                    "y": 252
                },
                {
                    "x": 1621,
                    "y": 252
                }
            ],
            "category": "header",
            "html": "<br><header id='26' style='font-size:18px'>281</header>",
            "id": 26,
            "page": 3,
            "text": "281"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 340
                },
                {
                    "x": 1698,
                    "y": 340
                },
                {
                    "x": 1698,
                    "y": 646
                },
                {
                    "x": 190,
                    "y": 646
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>values for an optimal policy. For convenience, define these as Q*(x, a) ≡ Q�* (x, a), Vx, a.<br>* is an action at which<br>It is straightforward to show that V*(x) = maxa Q*(x, a) and that if a<br>the maximum is attained, then an optimal policy can be formed as � (x) ≡ a* . Herein<br>lies the utility of the Q values-if an agent can learn them, it can easily decide what it<br>* the Q* values<br>is optimal to do. Although there may be more than one optimal policy or a<br>,<br>are unique.</p>",
            "id": 27,
            "page": 3,
            "text": "values for an optimal policy. For convenience, define these as Q*(x, a) ≡ Q�* (x, a), Vx, a. * is an action at which It is straightforward to show that V*(x) = maxa Q*(x, a) and that if a the maximum is attained, then an optimal policy can be formed as � (x) ≡ a* . Herein lies the utility of the Q values-if an agent can learn them, it can easily decide what it * the Q* values is optimal to do. Although there may be more than one optimal policy or a , are unique."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 647
                },
                {
                    "x": 1695,
                    "y": 647
                },
                {
                    "x": 1695,
                    "y": 749
                },
                {
                    "x": 189,
                    "y": 749
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:18px'>In Q-learning, the agent's experience consists of a sequence of distinct stages or episodes.<br>th episode, the agent:<br>In the n</p>",
            "id": 28,
            "page": 3,
            "text": "In Q-learning, the agent's experience consists of a sequence of distinct stages or episodes. th episode, the agent: In the n"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 798
                },
                {
                    "x": 1340,
                    "y": 798
                },
                {
                    "x": 1340,
                    "y": 1057
                },
                {
                    "x": 192,
                    "y": 1057
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:14px'>● observes its current state Xn,<br>● selects and performs an action an,<br>● observes the subsequent state yn,<br>· receives an immediate payoff rn, and<br>· adjusts its Qn-1 values using a learning factor �n, according to:</p>",
            "id": 29,
            "page": 3,
            "text": "● observes its current state Xn, ● selects and performs an action an, ● observes the subsequent state yn, · receives an immediate payoff rn, and · adjusts its Qn-1 values using a learning factor �n, according to:"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 1305
                },
                {
                    "x": 345,
                    "y": 1305
                },
                {
                    "x": 345,
                    "y": 1349
                },
                {
                    "x": 227,
                    "y": 1349
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:22px'>where</p>",
            "id": 30,
            "page": 3,
            "text": "where"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1537
                },
                {
                    "x": 1698,
                    "y": 1537
                },
                {
                    "x": 1698,
                    "y": 1741
                },
                {
                    "x": 226,
                    "y": 1741
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>is the best the agent thinks it can do from state y. Of course, in the early stages of learn-<br>ing, the Q values may not accurately reflect the policy they implicitly define (the maxi-<br>mizing actions in equation 2). The initial Q values, Qo(x, a), for all states and actions<br>are assumed given.</p>",
            "id": 31,
            "page": 3,
            "text": "is the best the agent thinks it can do from state y. Of course, in the early stages of learning, the Q values may not accurately reflect the policy they implicitly define (the maximizing actions in equation 2). The initial Q values, Qo(x, a), for all states and actions are assumed given."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1788
                },
                {
                    "x": 1696,
                    "y": 1788
                },
                {
                    "x": 1696,
                    "y": 1892
                },
                {
                    "x": 190,
                    "y": 1892
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:20px'>Note that this description assumes a look-up table representation for the Qn (x, a).<br>Watkins (1989) shows that Q-learning may not converge correctly for other representations.</p>",
            "id": 32,
            "page": 3,
            "text": "Note that this description assumes a look-up table representation for the Qn (x, a). Watkins (1989) shows that Q-learning may not converge correctly for other representations."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 1894
                },
                {
                    "x": 1699,
                    "y": 1894
                },
                {
                    "x": 1699,
                    "y": 2240
                },
                {
                    "x": 189,
                    "y": 2240
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:18px'>The most important condition implicit in the convergence theorem given below is that<br>the sequence of episodes that forms the basis of learning must include an infinite number<br>of episodes for each starting state and action. This may be considered a strong condition<br>on the way states and actions are selected-however, under the stochastic conditions of<br>the theorem, no method could be guaranteed to find an optimal policy under weaker con-<br>ditions. Note, however, that the episodes need not form a continuous sequence- that is<br>the y of one episode need not be the x of the next episode.</p>",
            "id": 33,
            "page": 3,
            "text": "The most important condition implicit in the convergence theorem given below is that the sequence of episodes that forms the basis of learning must include an infinite number of episodes for each starting state and action. This may be considered a strong condition on the way states and actions are selected-however, under the stochastic conditions of the theorem, no method could be guaranteed to find an optimal policy under weaker conditions. Note, however, that the episodes need not form a continuous sequence- that is the y of one episode need not be the x of the next episode."
        },
        {
            "bounding_box": [
                {
                    "x": 188,
                    "y": 2241
                },
                {
                    "x": 1699,
                    "y": 2241
                },
                {
                    "x": 1699,
                    "y": 2345
                },
                {
                    "x": 188,
                    "y": 2345
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:14px'>The following theorem defines a set of conditions under which Qn(x, a) → Q*(x, a)<br>as n → 00. Define ni(x, a) as the index of the ith time that action a is tried in state x.</p>",
            "id": 34,
            "page": 3,
            "text": "The following theorem defines a set of conditions under which Qn(x, a) → Q*(x, a) as n → 00. Define ni(x, a) as the index of the ith time that action a is tried in state x."
        },
        {
            "bounding_box": [
                {
                    "x": 1643,
                    "y": 2713
                },
                {
                    "x": 1695,
                    "y": 2713
                },
                {
                    "x": 1695,
                    "y": 2758
                },
                {
                    "x": 1643,
                    "y": 2758
                }
            ],
            "category": "footer",
            "html": "<footer id='35' style='font-size:18px'>57</footer>",
            "id": 35,
            "page": 3,
            "text": "57"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 207
                },
                {
                    "x": 269,
                    "y": 207
                },
                {
                    "x": 269,
                    "y": 252
                },
                {
                    "x": 194,
                    "y": 252
                }
            ],
            "category": "header",
            "html": "<header id='36' style='font-size:20px'>282</header>",
            "id": 36,
            "page": 4,
            "text": "282"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 214
                },
                {
                    "x": 1696,
                    "y": 214
                },
                {
                    "x": 1696,
                    "y": 258
                },
                {
                    "x": 1260,
                    "y": 258
                }
            ],
            "category": "header",
            "html": "<br><header id='37' style='font-size:14px'>C. WATKINS AND P. DAYAN</header>",
            "id": 37,
            "page": 4,
            "text": "C. WATKINS AND P. DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 342
                },
                {
                    "x": 367,
                    "y": 342
                },
                {
                    "x": 367,
                    "y": 390
                },
                {
                    "x": 197,
                    "y": 390
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:20px'>Theorem</p>",
            "id": 38,
            "page": 4,
            "text": "Theorem"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 440
                },
                {
                    "x": 1383,
                    "y": 440
                },
                {
                    "x": 1383,
                    "y": 498
                },
                {
                    "x": 196,
                    "y": 498
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>Given bounded rewards rn I ≤ R, learning rates 0 ≤ �n < 1, and</p>",
            "id": 39,
            "page": 4,
            "text": "Given bounded rewards rn I ≤ R, learning rates 0 ≤ �n < 1, and"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 729
                },
                {
                    "x": 1327,
                    "y": 729
                },
                {
                    "x": 1327,
                    "y": 782
                },
                {
                    "x": 196,
                    "y": 782
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:16px'>then Qn (x, a) → Q (x, a) as n → 00, Ax, a, with probability 1.</p>",
            "id": 40,
            "page": 4,
            "text": "then Qn (x, a) → Q (x, a) as n → 00, Ax, a, with probability 1."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 853
                },
                {
                    "x": 664,
                    "y": 853
                },
                {
                    "x": 664,
                    "y": 907
                },
                {
                    "x": 194,
                    "y": 907
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:20px'>3. The convergence proof</p>",
            "id": 41,
            "page": 4,
            "text": "3. The convergence proof"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 956
                },
                {
                    "x": 1697,
                    "y": 956
                },
                {
                    "x": 1697,
                    "y": 1108
                },
                {
                    "x": 193,
                    "y": 1108
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:20px'>The key to the convergence proof is an artificial controlled Markov process called the action-<br>replay process ARP, which is constructed from the episode sequence and the learning rate<br>sequence �n.</p>",
            "id": 42,
            "page": 4,
            "text": "The key to the convergence proof is an artificial controlled Markov process called the actionreplay process ARP, which is constructed from the episode sequence and the learning rate sequence �n."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1109
                },
                {
                    "x": 1697,
                    "y": 1109
                },
                {
                    "x": 1697,
                    "y": 1460
                },
                {
                    "x": 193,
                    "y": 1460
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>A formal description of the ARP is given in the appendix, but the easiest way to think<br>of it is in terms of a card game. Imagine each episode <xt, at, yt, rt, �t> written on a card.<br>All the cards together form an infinite deck, with the first episode-card next-to-bottom<br>and stretching infinitely upwards, in order. The bottom card (numbered 0) has written on<br>it the agent's initial values Qo(x, a) for all pairs of x and a. A state of the ARP, <x, n〉,<br>consists of a card number (or level) n, together with a state x from the real process. The<br>actions permitted in the ARP are the same as those permitted in the real process.</p>",
            "id": 43,
            "page": 4,
            "text": "A formal description of the ARP is given in the appendix, but the easiest way to think of it is in terms of a card game. Imagine each episode <xt, at, yt, rt, �t> written on a card. All the cards together form an infinite deck, with the first episode-card next-to-bottom and stretching infinitely upwards, in order. The bottom card (numbered 0) has written on it the agent's initial values Qo(x, a) for all pairs of x and a. A state of the ARP, <x, n〉, consists of a card number (or level) n, together with a state x from the real process. The actions permitted in the ARP are the same as those permitted in the real process."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1464
                },
                {
                    "x": 1698,
                    "y": 1464
                },
                {
                    "x": 1698,
                    "y": 1913
                },
                {
                    "x": 192,
                    "y": 1913
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>The next state of the ARP, given current state (x, n> and action a, is determined as follows.<br>First, all the cards for episodes later than n are eliminated, leaving just a finite deck. Cards<br>are then removed one at a time from top of this deck and examined until one is found whose<br>starting state and action match x and a, say at episode t. Then a biased coin is flipped,<br>with probability at of coming out heads, and 1 - at of tails. If the coin turns up heads,<br>the episode recorded on this card is replayed, a process described below; if the coin turns<br>up tails, this card too is thrown away and the search continues for another card matching<br>x and a. If the bottom card is reached, the game stops in a special, absorbing, state, and<br>just provides the reward written on this card for x, a, namely Qo(x, a).</p>",
            "id": 44,
            "page": 4,
            "text": "The next state of the ARP, given current state (x, n> and action a, is determined as follows. First, all the cards for episodes later than n are eliminated, leaving just a finite deck. Cards are then removed one at a time from top of this deck and examined until one is found whose starting state and action match x and a, say at episode t. Then a biased coin is flipped, with probability at of coming out heads, and 1 - at of tails. If the coin turns up heads, the episode recorded on this card is replayed, a process described below; if the coin turns up tails, this card too is thrown away and the search continues for another card matching x and a. If the bottom card is reached, the game stops in a special, absorbing, state, and just provides the reward written on this card for x, a, namely Qo(x, a)."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1918
                },
                {
                    "x": 1698,
                    "y": 1918
                },
                {
                    "x": 1698,
                    "y": 2114
                },
                {
                    "x": 192,
                    "y": 2114
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:16px'>Replaying the episode on card t consists of emitting the reward, rt, written on the card,<br>and then moving to the next state 〈yt, t - 1) in the ARP, where yt is the state to which<br>the real process went on that episode. Card t itself is thrown away. The next state transition<br>of the ARP will be taken based on just the remaining deck.</p>",
            "id": 45,
            "page": 4,
            "text": "Replaying the episode on card t consists of emitting the reward, rt, written on the card, and then moving to the next state 〈yt, t - 1) in the ARP, where yt is the state to which the real process went on that episode. Card t itself is thrown away. The next state transition of the ARP will be taken based on just the remaining deck."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2117
                },
                {
                    "x": 1699,
                    "y": 2117
                },
                {
                    "x": 1699,
                    "y": 2267
                },
                {
                    "x": 193,
                    "y": 2267
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:20px'>The above completely specifies how state transitions and rewards are determined in the<br>ARP. Define P(x.n),(y.m) [a] and R(m)(a) as the transition-probability matrices and expected<br>rewards of the ARP. Also define:</p>",
            "id": 46,
            "page": 4,
            "text": "The above completely specifies how state transitions and rewards are determined in the ARP. Define P(x.n),(y.m) [a] and R(m)(a) as the transition-probability matrices and expected rewards of the ARP. Also define:"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2500
                },
                {
                    "x": 1696,
                    "y": 2500
                },
                {
                    "x": 1696,
                    "y": 2599
                },
                {
                    "x": 193,
                    "y": 2599
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:14px'>as the probabilities that, for each x, n and a, executing action a at state <x,n> in the ARP<br>leads to state y of the real process at some lower level in the deck.</p>",
            "id": 47,
            "page": 4,
            "text": "as the probabilities that, for each x, n and a, executing action a at state <x,n> in the ARP leads to state y of the real process at some lower level in the deck."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2711
                },
                {
                    "x": 245,
                    "y": 2711
                },
                {
                    "x": 245,
                    "y": 2753
                },
                {
                    "x": 195,
                    "y": 2753
                }
            ],
            "category": "footer",
            "html": "<footer id='48' style='font-size:18px'>58</footer>",
            "id": 48,
            "page": 4,
            "text": "58"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 213
                },
                {
                    "x": 410,
                    "y": 213
                },
                {
                    "x": 410,
                    "y": 258
                },
                {
                    "x": 191,
                    "y": 258
                }
            ],
            "category": "header",
            "html": "<header id='49' style='font-size:14px'>Q-LEARNING</header>",
            "id": 49,
            "page": 5,
            "text": "Q-LEARNING"
        },
        {
            "bounding_box": [
                {
                    "x": 1623,
                    "y": 207
                },
                {
                    "x": 1694,
                    "y": 207
                },
                {
                    "x": 1694,
                    "y": 253
                },
                {
                    "x": 1623,
                    "y": 253
                }
            ],
            "category": "header",
            "html": "<br><header id='50' style='font-size:22px'>283</header>",
            "id": 50,
            "page": 5,
            "text": "283"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 342
                },
                {
                    "x": 1700,
                    "y": 342
                },
                {
                    "x": 1700,
                    "y": 597
                },
                {
                    "x": 190,
                    "y": 597
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:18px'>As defined above, the ARP is as much a controlled Markov process as is the real pro-<br>cess. One can therefore consider sequences of states and controls, and also optimal dis-<br>counted Q* values for the ARP.2 Note that during such a sequence, episode cards are only<br>removed from the deck, and are never replaced. Therefore, after a finite number of actions,<br>the bottom card will always be reached.</p>",
            "id": 51,
            "page": 5,
            "text": "As defined above, the ARP is as much a controlled Markov process as is the real process. One can therefore consider sequences of states and controls, and also optimal discounted Q* values for the ARP.2 Note that during such a sequence, episode cards are only removed from the deck, and are never replaced. Therefore, after a finite number of actions, the bottom card will always be reached."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 646
                },
                {
                    "x": 415,
                    "y": 646
                },
                {
                    "x": 415,
                    "y": 695
                },
                {
                    "x": 192,
                    "y": 695
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:20px'>3.1. Lemmas</p>",
            "id": 52,
            "page": 5,
            "text": "3.1. Lemmas"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 746
                },
                {
                    "x": 1702,
                    "y": 746
                },
                {
                    "x": 1702,
                    "y": 999
                },
                {
                    "x": 191,
                    "y": 999
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>Two lemmas form the heart of the proof. One shows that, effectively by construction, the<br>optimal Q value for ARP state (x, n) and action a is just Qn (x, a). The next shows that<br>for almost all possible decks, P(n)[a] converge to P xy [a] and R(m)(a) converge to Rx(a)<br>as n → 00. Informal statements of the lemmas and outlines of their proofs are given below;<br>consult the appendix for the formal statements.</p>",
            "id": 53,
            "page": 5,
            "text": "Two lemmas form the heart of the proof. One shows that, effectively by construction, the optimal Q value for ARP state (x, n) and action a is just Qn (x, a). The next shows that for almost all possible decks, P(n)[a] converge to P xy [a] and R(m)(a) converge to Rx(a) as n → 00. Informal statements of the lemmas and outlines of their proofs are given below; consult the appendix for the formal statements."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1047
                },
                {
                    "x": 382,
                    "y": 1047
                },
                {
                    "x": 382,
                    "y": 1098
                },
                {
                    "x": 192,
                    "y": 1098
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:20px'>Lemma A</p>",
            "id": 54,
            "page": 5,
            "text": "Lemma A"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1148
                },
                {
                    "x": 1612,
                    "y": 1148
                },
                {
                    "x": 1612,
                    "y": 1204
                },
                {
                    "x": 193,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:20px'>Qn (x, a) are the optimal action values for ARP states <x, n> and ARP actions a.</p>",
            "id": 55,
            "page": 5,
            "text": "Qn (x, a) are the optimal action values for ARP states <x, n> and ARP actions a."
        },
        {
            "bounding_box": [
                {
                    "x": 240,
                    "y": 1250
                },
                {
                    "x": 1697,
                    "y": 1250
                },
                {
                    "x": 1697,
                    "y": 1354
                },
                {
                    "x": 240,
                    "y": 1354
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>The ARP was directly constructed to have this property. The proof proceeds by backwards<br>induction, following the ARP down through the stack of past episodes.</p>",
            "id": 56,
            "page": 5,
            "text": "The ARP was directly constructed to have this property. The proof proceeds by backwards induction, following the ARP down through the stack of past episodes."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1401
                },
                {
                    "x": 379,
                    "y": 1401
                },
                {
                    "x": 379,
                    "y": 1450
                },
                {
                    "x": 192,
                    "y": 1450
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>Lemma B</p>",
            "id": 57,
            "page": 5,
            "text": "Lemma B"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1502
                },
                {
                    "x": 1698,
                    "y": 1502
                },
                {
                    "x": 1698,
                    "y": 1654
                },
                {
                    "x": 191,
                    "y": 1654
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:16px'>Lemma B concerns the convergence of the ARP to the real process. The first two steps<br>are preparatory; the next two specify the form of the convergence and provide foundations<br>for proving that it occurs.</p>",
            "id": 58,
            "page": 5,
            "text": "Lemma B concerns the convergence of the ARP to the real process. The first two steps are preparatory; the next two specify the form of the convergence and provide foundations for proving that it occurs."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1703
                },
                {
                    "x": 257,
                    "y": 1703
                },
                {
                    "x": 257,
                    "y": 1747
                },
                {
                    "x": 194,
                    "y": 1747
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:20px'>B.1</p>",
            "id": 59,
            "page": 5,
            "text": "B.1"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1804
                },
                {
                    "x": 1699,
                    "y": 1804
                },
                {
                    "x": 1699,
                    "y": 1959
                },
                {
                    "x": 190,
                    "y": 1959
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:16px'>Consider a discounted, bounded-reward, finite Markov process. From any starting state<br>x, the difference between the value of that state under the finite sequence of s actions and<br>its value under that same sequence followed by any other actions tends to 0 as S → 00.</p>",
            "id": 60,
            "page": 5,
            "text": "Consider a discounted, bounded-reward, finite Markov process. From any starting state x, the difference between the value of that state under the finite sequence of s actions and its value under that same sequence followed by any other actions tends to 0 as S → 00."
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 2004
                },
                {
                    "x": 1698,
                    "y": 2004
                },
                {
                    "x": 1698,
                    "y": 2109
                },
                {
                    "x": 239,
                    "y": 2109
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:14px'>This follows from the presence of the discount factor which weighs the (s + 1)th state<br>by ys → 0 as s → 00.</p>",
            "id": 61,
            "page": 5,
            "text": "This follows from the presence of the discount factor which weighs the (s + 1)th state by ys → 0 as s → 00."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2156
                },
                {
                    "x": 262,
                    "y": 2156
                },
                {
                    "x": 262,
                    "y": 2200
                },
                {
                    "x": 192,
                    "y": 2200
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>B.2</p>",
            "id": 62,
            "page": 5,
            "text": "B.2"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2254
                },
                {
                    "x": 1698,
                    "y": 2254
                },
                {
                    "x": 1698,
                    "y": 2406
                },
                {
                    "x": 190,
                    "y": 2406
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:18px'>Given any level 1, there exists another yet higher level, h, such that the probability can<br>be made arbitrarily small of straying below 1 after taking S actions in the ARP, starting<br>from above h.</p>",
            "id": 63,
            "page": 5,
            "text": "Given any level 1, there exists another yet higher level, h, such that the probability can be made arbitrarily small of straying below 1 after taking S actions in the ARP, starting from above h."
        },
        {
            "bounding_box": [
                {
                    "x": 238,
                    "y": 2456
                },
                {
                    "x": 1699,
                    "y": 2456
                },
                {
                    "x": 1699,
                    "y": 2610
                },
                {
                    "x": 238,
                    "y": 2610
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>The probability, starting at level h of the ARP of straying below any fixed level / tends<br>to 0 as h → 00. Therefore there is some sufficiently high level for which s actions can<br>be safely accommodated, with an arbitrarily high probability of leaving the ARP above 1.</p>",
            "id": 64,
            "page": 5,
            "text": "The probability, starting at level h of the ARP of straying below any fixed level / tends to 0 as h → 00. Therefore there is some sufficiently high level for which s actions can be safely accommodated, with an arbitrarily high probability of leaving the ARP above 1."
        },
        {
            "bounding_box": [
                {
                    "x": 1643,
                    "y": 2711
                },
                {
                    "x": 1695,
                    "y": 2711
                },
                {
                    "x": 1695,
                    "y": 2756
                },
                {
                    "x": 1643,
                    "y": 2756
                }
            ],
            "category": "footer",
            "html": "<footer id='65' style='font-size:18px'>59</footer>",
            "id": 65,
            "page": 5,
            "text": "59"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 207
                },
                {
                    "x": 268,
                    "y": 207
                },
                {
                    "x": 268,
                    "y": 253
                },
                {
                    "x": 193,
                    "y": 253
                }
            ],
            "category": "header",
            "html": "<header id='66' style='font-size:20px'>284</header>",
            "id": 66,
            "page": 6,
            "text": "284"
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 214
                },
                {
                    "x": 1693,
                    "y": 214
                },
                {
                    "x": 1693,
                    "y": 258
                },
                {
                    "x": 1258,
                    "y": 258
                }
            ],
            "category": "header",
            "html": "<br><header id='67' style='font-size:14px'>C. WATKINS AND P. DAYAN</header>",
            "id": 67,
            "page": 6,
            "text": "C. WATKINS AND P. DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 342
                },
                {
                    "x": 265,
                    "y": 342
                },
                {
                    "x": 265,
                    "y": 386
                },
                {
                    "x": 194,
                    "y": 386
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:22px'>B.3</p>",
            "id": 68,
            "page": 6,
            "text": "B.3"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 438
                },
                {
                    "x": 1696,
                    "y": 438
                },
                {
                    "x": 1696,
                    "y": 695
                },
                {
                    "x": 193,
                    "y": 695
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:20px'>With probability 1, the probabilities P(n)[a] and expected rewards R(n)(a) in the ARP con-<br>verge and tend to the transition matrices and expected rewards in the real process as the<br>level n increases to infinity. This, together with B.2, makes it appropriate to consider<br>P(n)[a] rather than the ARP transition matrices P(x,n),(y,m) [a], i.e., essentially ignoring the<br>level at which the ARP enters state y.</p>",
            "id": 69,
            "page": 6,
            "text": "With probability 1, the probabilities P(n)[a] and expected rewards R(n)(a) in the ARP converge and tend to the transition matrices and expected rewards in the real process as the level n increases to infinity. This, together with B.2, makes it appropriate to consider P(n)[a] rather than the ARP transition matrices P(x,n),(y,m) [a], i.e., essentially ignoring the level at which the ARP enters state y."
        },
        {
            "bounding_box": [
                {
                    "x": 242,
                    "y": 736
                },
                {
                    "x": 1695,
                    "y": 736
                },
                {
                    "x": 1695,
                    "y": 894
                },
                {
                    "x": 242,
                    "y": 894
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:16px'>The ARP effectively estimates the mean rewards and transitions of the real process over<br>all the episodes. Since its raw data are unbiased, the conditions on the sums and sums<br>of squares of the learning rates �ni (x,a) ensure the convergence with probability one.</p>",
            "id": 70,
            "page": 6,
            "text": "The ARP effectively estimates the mean rewards and transitions of the real process over all the episodes. Since its raw data are unbiased, the conditions on the sums and sums of squares of the learning rates �ni (x,a) ensure the convergence with probability one."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 935
                },
                {
                    "x": 260,
                    "y": 935
                },
                {
                    "x": 260,
                    "y": 981
                },
                {
                    "x": 194,
                    "y": 981
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:22px'>B.4</p>",
            "id": 71,
            "page": 6,
            "text": "B.4"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1031
                },
                {
                    "x": 1697,
                    "y": 1031
                },
                {
                    "x": 1697,
                    "y": 1237
                },
                {
                    "x": 192,
                    "y": 1237
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:20px'>Consider executing a series of s actions in the ARP and in the real process. If the proba-<br>bilities P(n)[a] and expected rewards R(m)(a) at appropriate levels of the ARP for each<br>of the actions, are close to Pxy [a] and Rx(a), va, x, y, respectively, then the value of the<br>series of actions in the ARP will be close to its value in the real process.</p>",
            "id": 72,
            "page": 6,
            "text": "Consider executing a series of s actions in the ARP and in the real process. If the probabilities P(n)[a] and expected rewards R(m)(a) at appropriate levels of the ARP for each of the actions, are close to Pxy [a] and Rx(a), va, x, y, respectively, then the value of the series of actions in the ARP will be close to its value in the real process."
        },
        {
            "bounding_box": [
                {
                    "x": 240,
                    "y": 1281
                },
                {
                    "x": 1696,
                    "y": 1281
                },
                {
                    "x": 1696,
                    "y": 1482
                },
                {
                    "x": 240,
                    "y": 1482
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>The discrepancy in the action values over a finite number s of actions between the values<br>of two approximately equal Markov processes grows at most quadratically with s. So,<br>if the transition probabilities and rewards are close, then the values of the actions must<br>be close too.</p>",
            "id": 73,
            "page": 6,
            "text": "The discrepancy in the action values over a finite number s of actions between the values of two approximately equal Markov processes grows at most quadratically with s. So, if the transition probabilities and rewards are close, then the values of the actions must be close too."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1533
                },
                {
                    "x": 498,
                    "y": 1533
                },
                {
                    "x": 498,
                    "y": 1582
                },
                {
                    "x": 194,
                    "y": 1582
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:20px'>3.2. The theorem</p>",
            "id": 74,
            "page": 6,
            "text": "3.2. The theorem"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1629
                },
                {
                    "x": 1695,
                    "y": 1629
                },
                {
                    "x": 1695,
                    "y": 1783
                },
                {
                    "x": 191,
                    "y": 1783
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:20px'>Putting these together, the ARP tends towards the real process, and so its optimal Q values<br>th level of the ARP (by Lemma A),<br>do too. But Qn (a, x) are the optimal Q values for the n<br>and SO tend to Q*(x, a).</p>",
            "id": 75,
            "page": 6,
            "text": "Putting these together, the ARP tends towards the real process, and so its optimal Q values th level of the ARP (by Lemma A), do too. But Qn (a, x) are the optimal Q values for the n and SO tend to Q*(x, a)."
        },
        {
            "bounding_box": [
                {
                    "x": 244,
                    "y": 1827
                },
                {
                    "x": 1661,
                    "y": 1827
                },
                {
                    "x": 1661,
                    "y": 1878
                },
                {
                    "x": 244,
                    "y": 1878
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>Assume, without loss of generality, that Qo(x, a) < R/(1 -- 1) and that R ≥ 1.</p>",
            "id": 76,
            "page": 6,
            "text": "Assume, without loss of generality, that Qo(x, a) < R/(1 -- 1) and that R ≥ 1."
        },
        {
            "bounding_box": [
                {
                    "x": 241,
                    "y": 1923
                },
                {
                    "x": 819,
                    "y": 1923
                },
                {
                    "x": 819,
                    "y": 1974
                },
                {
                    "x": 241,
                    "y": 1974
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:14px'>Given E > 0, choose S such that</p>",
            "id": 77,
            "page": 6,
            "text": "Given E > 0, choose S such that"
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 2163
                },
                {
                    "x": 1694,
                    "y": 2163
                },
                {
                    "x": 1694,
                    "y": 2267
                },
                {
                    "x": 239,
                    "y": 2267
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>By B.3, with probability 1, it is possible to choose 1 sufficiently large such that for<br>n > 1, and va, x, y,</p>",
            "id": 78,
            "page": 6,
            "text": "By B.3, with probability 1, it is possible to choose 1 sufficiently large such that for n > 1, and va, x, y,"
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 2455
                },
                {
                    "x": 1696,
                    "y": 2455
                },
                {
                    "x": 1696,
                    "y": 2605
                },
                {
                    "x": 239,
                    "y": 2605
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:16px'>By B.2, choose h sufficiently large such that for n > h, the probability, after taking<br>s actions, of ending up at a level lower than 1 is less than min {(E(1 - y)/6sR),<br>(E/3s(s + 1)R)}. This means that</p>",
            "id": 79,
            "page": 6,
            "text": "By B.2, choose h sufficiently large such that for n > h, the probability, after taking s actions, of ending up at a level lower than 1 is less than min {(E(1 - y)/6sR), (E/3s(s + 1)R)}. This means that"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 2711
                },
                {
                    "x": 244,
                    "y": 2711
                },
                {
                    "x": 244,
                    "y": 2752
                },
                {
                    "x": 194,
                    "y": 2752
                }
            ],
            "category": "footer",
            "html": "<footer id='80' style='font-size:20px'>60</footer>",
            "id": 80,
            "page": 6,
            "text": "60"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 214
                },
                {
                    "x": 412,
                    "y": 214
                },
                {
                    "x": 412,
                    "y": 258
                },
                {
                    "x": 192,
                    "y": 258
                }
            ],
            "category": "header",
            "html": "<header id='81' style='font-size:14px'>Q-LEARNING</header>",
            "id": 81,
            "page": 7,
            "text": "Q-LEARNING"
        },
        {
            "bounding_box": [
                {
                    "x": 1623,
                    "y": 207
                },
                {
                    "x": 1695,
                    "y": 207
                },
                {
                    "x": 1695,
                    "y": 253
                },
                {
                    "x": 1623,
                    "y": 253
                }
            ],
            "category": "header",
            "html": "<br><header id='82' style='font-size:22px'>285</header>",
            "id": 82,
            "page": 7,
            "text": "285"
        },
        {
            "bounding_box": [
                {
                    "x": 243,
                    "y": 493
                },
                {
                    "x": 1699,
                    "y": 493
                },
                {
                    "x": 1699,
                    "y": 596
                },
                {
                    "x": 243,
                    "y": 596
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:16px'>and R ,(n) indicate that these are conditional on the level in<br>where the primes on p1(n)<br>the ARP after the �th step being greater than 1.</p>",
            "id": 83,
            "page": 7,
            "text": "and R ,(n) indicate that these are conditional on the level in where the primes on p1(n) the ARP after the �th step being greater than 1."
        },
        {
            "bounding_box": [
                {
                    "x": 242,
                    "y": 643
                },
                {
                    "x": 1700,
                    "y": 643
                },
                {
                    "x": 1700,
                    "y": 799
                },
                {
                    "x": 242,
                    "y": 799
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:14px'>Then, for n > h, by B.4, compare the value QARP((X, n〉, a1, · . · , as) of taking ac-<br>tions a1, . · · , as at state x in the ARP, with Q(x, a1, · · · , as) of taking them in the<br>real process:3</p>",
            "id": 84,
            "page": 7,
            "text": "Then, for n > h, by B.4, compare the value QARP((X, n〉, a1, · . · , as) of taking actions a1, . · · , as at state x in the ARP, with Q(x, a1, · · · , as) of taking them in the real process:3"
        },
        {
            "bounding_box": [
                {
                    "x": 242,
                    "y": 1072
                },
                {
                    "x": 1698,
                    "y": 1072
                },
                {
                    "x": 1698,
                    "y": 1227
                },
                {
                    "x": 242,
                    "y": 1227
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:18px'>Where, in equation 4, the first term counts the cost of conditions for B.2 not holding,<br>as the cost of straying below l is bounded by 2sR /(1 - 2). The second term is the cost,<br>from B.4, of the incorrect rewards and transition probabilities.</p>",
            "id": 85,
            "page": 7,
            "text": "Where, in equation 4, the first term counts the cost of conditions for B.2 not holding, as the cost of straying below l is bounded by 2sR /(1 - 2). The second term is the cost, from B.4, of the incorrect rewards and transition probabilities."
        },
        {
            "bounding_box": [
                {
                    "x": 242,
                    "y": 1275
                },
                {
                    "x": 1699,
                    "y": 1275
                },
                {
                    "x": 1699,
                    "y": 1480
                },
                {
                    "x": 242,
                    "y": 1480
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>However, by B.1, the effect of taking only s actions makes a difference of less than e/6<br>for both the ARP and the real process. Also since equation 4 applies to any set of ac-<br>tions, it applies perforce to a set of actions optimal for either the ARP or the real proc-<br>ess. Therefore</p>",
            "id": 86,
            "page": 7,
            "text": "However, by B.1, the effect of taking only s actions makes a difference of less than e/6 for both the ARP and the real process. Also since equation 4 applies to any set of actions, it applies perforce to a set of actions optimal for either the ARP or the real process. Therefore"
        },
        {
            "bounding_box": [
                {
                    "x": 242,
                    "y": 1627
                },
                {
                    "x": 1447,
                    "y": 1627
                },
                {
                    "x": 1447,
                    "y": 1684
                },
                {
                    "x": 242,
                    "y": 1684
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:14px'>So, with probability 1, Qn(x, a) → Q.(x, a) as n → ○○ as required.</p>",
            "id": 87,
            "page": 7,
            "text": "So, with probability 1, Qn(x, a) → Q.(x, a) as n → ○○ as required."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1779
                },
                {
                    "x": 757,
                    "y": 1779
                },
                {
                    "x": 757,
                    "y": 1831
                },
                {
                    "x": 192,
                    "y": 1831
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:20px'>4. Discussions and conclusions</p>",
            "id": 88,
            "page": 7,
            "text": "4. Discussions and conclusions"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1882
                },
                {
                    "x": 1700,
                    "y": 1882
                },
                {
                    "x": 1700,
                    "y": 2184
                },
                {
                    "x": 191,
                    "y": 2184
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>For the sake of clarity, the theorem proved above was somewhat restricted. Two par-<br>ticular extensions to the version of Q-learning described above have been used in prac-<br>tice. One is the non-discounted case (V = 1), but for a Markov process with absorbing<br>goal states, and the other is to the case where many of the Q values are updated in each<br>iteration rather than just one (Barto, Bradtke & Singh, 1991). The convergence result holds<br>for both of these, and this section sketches the modifications to the proof that are necessary.</p>",
            "id": 89,
            "page": 7,
            "text": "For the sake of clarity, the theorem proved above was somewhat restricted. Two particular extensions to the version of Q-learning described above have been used in practice. One is the non-discounted case (V = 1), but for a Markov process with absorbing goal states, and the other is to the case where many of the Q values are updated in each iteration rather than just one (Barto, Bradtke & Singh, 1991). The convergence result holds for both of these, and this section sketches the modifications to the proof that are necessary."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2187
                },
                {
                    "x": 1699,
                    "y": 2187
                },
                {
                    "x": 1699,
                    "y": 2431
                },
                {
                    "x": 191,
                    "y": 2431
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>A process with absorbing goal states has one or more states which are bound in the end<br>to trap the agent. This ultimate certainty of being trapped plays the role that Y < 1 played<br>in the earlier proof, in ensuring that the value of state x under any policy �, V� (x), is<br>bounded, and that lemma B.1 holds, i.e., that the difference between considering infinite<br>and finite (s) numbers of actions tends to 0 as S → 00.</p>",
            "id": 90,
            "page": 7,
            "text": "A process with absorbing goal states has one or more states which are bound in the end to trap the agent. This ultimate certainty of being trapped plays the role that Y < 1 played in the earlier proof, in ensuring that the value of state x under any policy �, V� (x), is bounded, and that lemma B.1 holds, i.e., that the difference between considering infinite and finite (s) numbers of actions tends to 0 as S → 00."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2435
                },
                {
                    "x": 1700,
                    "y": 2435
                },
                {
                    "x": 1700,
                    "y": 2587
                },
                {
                    "x": 190,
                    "y": 2587
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>Since the process would always get trapped were it allowed to run, for every state x there<br>is some number of actions u(x) such that no matter what they are, there is a probability<br>p(x) > 0 of having reached one of the goal states after executing those actions. Take</p>",
            "id": 91,
            "page": 7,
            "text": "Since the process would always get trapped were it allowed to run, for every state x there is some number of actions u(x) such that no matter what they are, there is a probability p(x) > 0 of having reached one of the goal states after executing those actions. Take"
        },
        {
            "bounding_box": [
                {
                    "x": 1643,
                    "y": 2716
                },
                {
                    "x": 1690,
                    "y": 2716
                },
                {
                    "x": 1690,
                    "y": 2759
                },
                {
                    "x": 1643,
                    "y": 2759
                }
            ],
            "category": "footer",
            "html": "<footer id='92' style='font-size:20px'>61</footer>",
            "id": 92,
            "page": 7,
            "text": "61"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 207
                },
                {
                    "x": 266,
                    "y": 207
                },
                {
                    "x": 266,
                    "y": 252
                },
                {
                    "x": 192,
                    "y": 252
                }
            ],
            "category": "header",
            "html": "<header id='93' style='font-size:20px'>286</header>",
            "id": 93,
            "page": 8,
            "text": "286"
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 214
                },
                {
                    "x": 1691,
                    "y": 214
                },
                {
                    "x": 1691,
                    "y": 257
                },
                {
                    "x": 1254,
                    "y": 257
                }
            ],
            "category": "header",
            "html": "<br><header id='94' style='font-size:14px'>C. WATKINS AND P. DAYAN</header>",
            "id": 94,
            "page": 8,
            "text": "C. WATKINS AND P. DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 341
                },
                {
                    "x": 1693,
                    "y": 341
                },
                {
                    "x": 1693,
                    "y": 442
                },
                {
                    "x": 189,
                    "y": 442
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:16px'>u * = maxx {u(x)}, and p * = minx {p(x)} > 0 (since there is only a finite number of<br>states). Then a crude upper bound for V� (x) is</p>",
            "id": 95,
            "page": 8,
            "text": "u * = maxx {u(x)}, and p * = minx {p(x)} > 0 (since there is only a finite number of states). Then a crude upper bound for V� (x) is"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 746
                },
                {
                    "x": 1693,
                    "y": 746
                },
                {
                    "x": 1693,
                    "y": 944
                },
                {
                    "x": 190,
                    "y": 944
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:14px'>* steps the agent earns a reward of less than u *R and has probability less<br>since in each u<br>than (1 - p *) of not having been trapped. Similarly, the effect of measuring the reward<br>after only ⌀u * steps is less than (1 - p *) � u *R → 0 as � → 00, and so an equivalent<br>of lemma B.1 does hold.</p>",
            "id": 96,
            "page": 8,
            "text": "* steps the agent earns a reward of less than u *R and has probability less since in each u than (1 - p *) of not having been trapped. Similarly, the effect of measuring the reward after only ⌀u * steps is less than (1 - p *) � u *R → 0 as � → 00, and so an equivalent of lemma B.1 does hold."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 949
                },
                {
                    "x": 1694,
                    "y": 949
                },
                {
                    "x": 1694,
                    "y": 1351
                },
                {
                    "x": 189,
                    "y": 1351
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:18px'>Changing more than one Q value on each iteration requires a minor modification to the<br>action replay process ARP such that an action can be taken at any level at which it was<br>executed in the real process-i.e., more than one action can be taken at each level. As<br>long as the stochastic convergence conditions in equation 3 are still satisfied, the proof<br>requires no non-trivial modification. The Qn (x, a) values are still optimal for the modified<br>ARP, and this still tends to the real process in the original manner. Intuitively, the proof<br>relies on the ARP estimating rewards and transition functions based on many episodes,<br>and this is just speeded up by changing more than one Q value per iteration.</p>",
            "id": 97,
            "page": 8,
            "text": "Changing more than one Q value on each iteration requires a minor modification to the action replay process ARP such that an action can be taken at any level at which it was executed in the real process-i.e., more than one action can be taken at each level. As long as the stochastic convergence conditions in equation 3 are still satisfied, the proof requires no non-trivial modification. The Qn (x, a) values are still optimal for the modified ARP, and this still tends to the real process in the original manner. Intuitively, the proof relies on the ARP estimating rewards and transition functions based on many episodes, and this is just speeded up by changing more than one Q value per iteration."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1354
                },
                {
                    "x": 1693,
                    "y": 1354
                },
                {
                    "x": 1693,
                    "y": 1806
                },
                {
                    "x": 190,
                    "y": 1806
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='98' style='font-size:20px'>Although the paper has so far presented an apparent dichotomy between Q-learning and<br>methods based on certainty equivalence, such as Sato, Abe and Takeda (1988), in fact there<br>is more of a continuum. If the agent can remember the details of its learning episodes,<br>then, after altering the learning rates, it can use each of them more than once (which is<br>equivalent to putting cards that were thrown away, back in, lower down on the ARP stack).<br>This biases the Q-learning process towards the particular sample of the rewards and transi-<br>tions that it has experienced. In the limit of re-presenting 'old' cards infinitely often, this<br>reuse amounts to the certainty equivalence step of calculating the optimal actions for the<br>observed sample of the Markovian environment rather than the actual environment itself.</p>",
            "id": 98,
            "page": 8,
            "text": "Although the paper has so far presented an apparent dichotomy between Q-learning and methods based on certainty equivalence, such as Sato, Abe and Takeda (1988), in fact there is more of a continuum. If the agent can remember the details of its learning episodes, then, after altering the learning rates, it can use each of them more than once (which is equivalent to putting cards that were thrown away, back in, lower down on the ARP stack). This biases the Q-learning process towards the particular sample of the rewards and transitions that it has experienced. In the limit of re-presenting 'old' cards infinitely often, this reuse amounts to the certainty equivalence step of calculating the optimal actions for the observed sample of the Markovian environment rather than the actual environment itself."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 1809
                },
                {
                    "x": 1693,
                    "y": 1809
                },
                {
                    "x": 1693,
                    "y": 2108
                },
                {
                    "x": 189,
                    "y": 2108
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:18px'>The theorem above only proves the convergence of a restricted version of Watkins' (1989)<br>comprehensive Q-learning algorithm, since it does not permit updates based on the rewards<br>from more than one iteration. This addition was pioneered by Sutton (1984; 1988) in his<br>TD(入) algorithm, in which a reward from a step taken r iterations previously is weighted<br>by V, where 入 < 1. Unfortunately, the theorem does not extend trivially to this case, and<br>alternative proof methods such as those in Kushner and Clark (1978) may be required.</p>",
            "id": 99,
            "page": 8,
            "text": "The theorem above only proves the convergence of a restricted version of Watkins' (1989) comprehensive Q-learning algorithm, since it does not permit updates based on the rewards from more than one iteration. This addition was pioneered by Sutton (1984; 1988) in his TD(入) algorithm, in which a reward from a step taken r iterations previously is weighted by V, where 入 < 1. Unfortunately, the theorem does not extend trivially to this case, and alternative proof methods such as those in Kushner and Clark (1978) may be required."
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 2110
                },
                {
                    "x": 1694,
                    "y": 2110
                },
                {
                    "x": 1694,
                    "y": 2310
                },
                {
                    "x": 189,
                    "y": 2310
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:18px'>This paper has presented the proof outlined by Watkins (1989) that Q-learning converges<br>with probability one under reasonable conditions on the learning rates and the Markovian<br>environment. Such a guarantee has previously eluded most methods of reinforcement<br>learning.</p>",
            "id": 100,
            "page": 8,
            "text": "This paper has presented the proof outlined by Watkins (1989) that Q-learning converges with probability one under reasonable conditions on the learning rates and the Markovian environment. Such a guarantee has previously eluded most methods of reinforcement learning."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2358
                },
                {
                    "x": 523,
                    "y": 2358
                },
                {
                    "x": 523,
                    "y": 2408
                },
                {
                    "x": 191,
                    "y": 2408
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:22px'>Acknowledgments</p>",
            "id": 101,
            "page": 8,
            "text": "Acknowledgments"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2455
                },
                {
                    "x": 1696,
                    "y": 2455
                },
                {
                    "x": 1696,
                    "y": 2607
                },
                {
                    "x": 190,
                    "y": 2607
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:14px'>We are very grateful to Andy Barto, Graeme Mitchison, Steve Nowlan, Satinder Singh,<br>Rich Sutton and three anonymous reviewers for their valuable comments on multifarious<br>aspects of Q-learning and this paper. Such clarity as it possesses owes to Rich Sutton's</p>",
            "id": 102,
            "page": 8,
            "text": "We are very grateful to Andy Barto, Graeme Mitchison, Steve Nowlan, Satinder Singh, Rich Sutton and three anonymous reviewers for their valuable comments on multifarious aspects of Q-learning and this paper. Such clarity as it possesses owes to Rich Sutton's"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2710
                },
                {
                    "x": 242,
                    "y": 2710
                },
                {
                    "x": 242,
                    "y": 2751
                },
                {
                    "x": 192,
                    "y": 2751
                }
            ],
            "category": "footer",
            "html": "<footer id='103' style='font-size:18px'>62</footer>",
            "id": 103,
            "page": 8,
            "text": "62"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 213
                },
                {
                    "x": 409,
                    "y": 213
                },
                {
                    "x": 409,
                    "y": 260
                },
                {
                    "x": 190,
                    "y": 260
                }
            ],
            "category": "header",
            "html": "<header id='104' style='font-size:14px'>Q-LEARNING</header>",
            "id": 104,
            "page": 9,
            "text": "Q-LEARNING"
        },
        {
            "bounding_box": [
                {
                    "x": 1621,
                    "y": 207
                },
                {
                    "x": 1692,
                    "y": 207
                },
                {
                    "x": 1692,
                    "y": 252
                },
                {
                    "x": 1621,
                    "y": 252
                }
            ],
            "category": "header",
            "html": "<br><header id='105' style='font-size:20px'>287</header>",
            "id": 105,
            "page": 9,
            "text": "287"
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 343
                },
                {
                    "x": 1695,
                    "y": 343
                },
                {
                    "x": 1695,
                    "y": 448
                },
                {
                    "x": 189,
                    "y": 448
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:20px'>tireless efforts. Support was from Philips Research Laboratories and SERC. PD's current<br>address is CNL, The Salk Institute, PO Box 85800, San Diego, CA 92186-5800, USA.</p>",
            "id": 106,
            "page": 9,
            "text": "tireless efforts. Support was from Philips Research Laboratories and SERC. PD's current address is CNL, The Salk Institute, PO Box 85800, San Diego, CA 92186-5800, USA."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 497
                },
                {
                    "x": 301,
                    "y": 497
                },
                {
                    "x": 301,
                    "y": 545
                },
                {
                    "x": 190,
                    "y": 545
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:20px'>Notes</p>",
            "id": 107,
            "page": 9,
            "text": "Notes"
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 586
                },
                {
                    "x": 1697,
                    "y": 586
                },
                {
                    "x": 1697,
                    "y": 756
                },
                {
                    "x": 189,
                    "y": 756
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:14px'>1. In general, the set of available actions may differ from state to state. Here we assume it does not, to simplify<br>the notation. The theorem we present can straightfowardly be extended to the general case.<br>2. The discount factor for the ARP will be taken to be Y, the same as for the real process.<br>3. The bars over the Q indicate that the sum is over only a finite number of actions, with 0 terminal reward.</p>",
            "id": 108,
            "page": 9,
            "text": "1. In general, the set of available actions may differ from state to state. Here we assume it does not, to simplify the notation. The theorem we present can straightfowardly be extended to the general case. 2. The discount factor for the ARP will be taken to be Y, the same as for the real process. 3. The bars over the Q indicate that the sum is over only a finite number of actions, with 0 terminal reward."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 807
                },
                {
                    "x": 375,
                    "y": 807
                },
                {
                    "x": 375,
                    "y": 858
                },
                {
                    "x": 191,
                    "y": 858
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:22px'>Appendix</p>",
            "id": 109,
            "page": 9,
            "text": "Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 907
                },
                {
                    "x": 668,
                    "y": 907
                },
                {
                    "x": 668,
                    "y": 962
                },
                {
                    "x": 190,
                    "y": 962
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:20px'>The action-replay process</p>",
            "id": 110,
            "page": 9,
            "text": "The action-replay process"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1009
                },
                {
                    "x": 1697,
                    "y": 1009
                },
                {
                    "x": 1697,
                    "y": 1210
                },
                {
                    "x": 190,
                    "y": 1210
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>The definition of the ARP is contingent on a particular sequence of episodes observed<br>in the real process. The state space of the ARP is {<x, n)}, for x a state of the real process<br>and n ≥ 1, together with one, special, absorbing state, and the action space is {a} for<br>a an action from the real process.</p>",
            "id": 111,
            "page": 9,
            "text": "The definition of the ARP is contingent on a particular sequence of episodes observed in the real process. The state space of the ARP is {<x, n)}, for x a state of the real process and n ≥ 1, together with one, special, absorbing state, and the action space is {a} for a an action from the real process."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1214
                },
                {
                    "x": 1698,
                    "y": 1214
                },
                {
                    "x": 1698,
                    "y": 1363
                },
                {
                    "x": 190,
                    "y": 1363
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='112' style='font-size:16px'>The stochastic reward and state transition consequent on performing action a at state<br><x, n> is given as follows. For convenience, define ni ≡ ni(x, a), as the index of the ith<br>time action a was tried at state x. Define</p>",
            "id": 112,
            "page": 9,
            "text": "The stochastic reward and state transition consequent on performing action a at state <x, n> is given as follows. For convenience, define ni ≡ ni(x, a), as the index of the ith time action a was tried at state x. Define"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1616
                },
                {
                    "x": 1694,
                    "y": 1616
                },
                {
                    "x": 1694,
                    "y": 1718
                },
                {
                    "x": 190,
                    "y": 1718
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>such that ni* is the last time before episode n that x, a was exeucted in the real process.<br>If i* = 0, the reward is set as Qo(x, a), and the ARP absorbs. Otherwise, let</p>",
            "id": 113,
            "page": 9,
            "text": "such that ni* is the last time before episode n that x, a was exeucted in the real process. If i* = 0, the reward is set as Qo(x, a), and the ARP absorbs. Otherwise, let"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2318
                },
                {
                    "x": 1699,
                    "y": 2318
                },
                {
                    "x": 1699,
                    "y": 2622
                },
                {
                    "x": 190,
                    "y": 2622
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:18px'>be the index of the episode that is replayed or taken, chosen probabilistically from the<br>collection of existing samples from the real process. If ie = 0, then the reward is set at<br>Qo(x, a) and the ARP absorbs, as above, Otherwise, taking ie provides reward rnie, and<br>causes a state transition to (ynie, nie - 1> which is at level nie - 1. This last point is<br>crucial, taking an action in the ARP always causes a state transition to a lower level-so<br>it ultimately terminates. The discount factor in the ARP is 7, the same as in the real process.</p>",
            "id": 114,
            "page": 9,
            "text": "be the index of the episode that is replayed or taken, chosen probabilistically from the collection of existing samples from the real process. If ie = 0, then the reward is set at Qo(x, a) and the ARP absorbs, as above, Otherwise, taking ie provides reward rnie, and causes a state transition to (ynie, nie - 1> which is at level nie - 1. This last point is crucial, taking an action in the ARP always causes a state transition to a lower level-so it ultimately terminates. The discount factor in the ARP is 7, the same as in the real process."
        },
        {
            "bounding_box": [
                {
                    "x": 1642,
                    "y": 2713
                },
                {
                    "x": 1694,
                    "y": 2713
                },
                {
                    "x": 1694,
                    "y": 2758
                },
                {
                    "x": 1642,
                    "y": 2758
                }
            ],
            "category": "footer",
            "html": "<footer id='115' style='font-size:18px'>63</footer>",
            "id": 115,
            "page": 9,
            "text": "63"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 206
                },
                {
                    "x": 269,
                    "y": 206
                },
                {
                    "x": 269,
                    "y": 252
                },
                {
                    "x": 193,
                    "y": 252
                }
            ],
            "category": "header",
            "html": "<header id='116' style='font-size:18px'>288</header>",
            "id": 116,
            "page": 10,
            "text": "288"
        },
        {
            "bounding_box": [
                {
                    "x": 1257,
                    "y": 212
                },
                {
                    "x": 1695,
                    "y": 212
                },
                {
                    "x": 1695,
                    "y": 257
                },
                {
                    "x": 1257,
                    "y": 257
                }
            ],
            "category": "header",
            "html": "<br><header id='117' style='font-size:14px'>C. WATKINS AND P. DAYAN</header>",
            "id": 117,
            "page": 10,
            "text": "C. WATKINS AND P. DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 340
                },
                {
                    "x": 930,
                    "y": 340
                },
                {
                    "x": 930,
                    "y": 391
                },
                {
                    "x": 194,
                    "y": 391
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:20px'>Lemma A: Qn are optimal for the ARP</p>",
            "id": 118,
            "page": 10,
            "text": "Lemma A: Qn are optimal for the ARP"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 437
                },
                {
                    "x": 1693,
                    "y": 437
                },
                {
                    "x": 1693,
                    "y": 495
                },
                {
                    "x": 195,
                    "y": 495
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>Qn(x, a) are the optimal action values for ARP states (x, n> and ARP actions a. That is</p>",
            "id": 119,
            "page": 10,
            "text": "Qn(x, a) are the optimal action values for ARP states (x, n> and ARP actions a. That is"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 643
                },
                {
                    "x": 305,
                    "y": 643
                },
                {
                    "x": 305,
                    "y": 689
                },
                {
                    "x": 193,
                    "y": 689
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:20px'>Proof</p>",
            "id": 120,
            "page": 10,
            "text": "Proof"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 742
                },
                {
                    "x": 1695,
                    "y": 742
                },
                {
                    "x": 1695,
                    "y": 847
                },
                {
                    "x": 193,
                    "y": 847
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:20px'>By induction. From the construction of the ARP, Qo(x, a) is the optimal-indeed the only<br>possible-action value of (x, 0〉, a. Therefore,</p>",
            "id": 121,
            "page": 10,
            "text": "By induction. From the construction of the ARP, Qo(x, a) is the optimal-indeed the only possible-action value of (x, 0〉, a. Therefore,"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 996
                },
                {
                    "x": 830,
                    "y": 996
                },
                {
                    "x": 830,
                    "y": 1044
                },
                {
                    "x": 192,
                    "y": 1044
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:16px'>Hence the theorem holds for n = 0.</p>",
            "id": 122,
            "page": 10,
            "text": "Hence the theorem holds for n = 0."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1048
                },
                {
                    "x": 1695,
                    "y": 1048
                },
                {
                    "x": 1695,
                    "y": 1149
                },
                {
                    "x": 192,
                    "y": 1149
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:18px'>Suppose that the values of Qn-1, as produced by the Q-learning rule, are the optimal<br>action values for the ARP at level n - 1, that is</p>",
            "id": 123,
            "page": 10,
            "text": "Suppose that the values of Qn-1, as produced by the Q-learning rule, are the optimal action values for the ARP at level n - 1, that is"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1296
                },
                {
                    "x": 1693,
                    "y": 1296
                },
                {
                    "x": 1693,
                    "y": 1400
                },
                {
                    "x": 192,
                    "y": 1400
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:18px'>This implies that the Vn-1(x) are the optimal values V* for the ARP at the n - 1th level,<br>that is</p>",
            "id": 124,
            "page": 10,
            "text": "This implies that the Vn-1(x) are the optimal values V* for the ARP at the n - 1th level, that is"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1581
                },
                {
                    "x": 1697,
                    "y": 1581
                },
                {
                    "x": 1697,
                    "y": 1686
                },
                {
                    "x": 191,
                    "y": 1686
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Now consider the cases in trying to perform action a in (x, n〉. Ifx, a ≠ Xn, an, then this<br>is the same as performing a in (x, n - 1〉, and Qn(x, a) == Qn-1(x, a). Therefore,</p>",
            "id": 125,
            "page": 10,
            "text": "Now consider the cases in trying to perform action a in (x, n〉. Ifx, a ≠ Xn, an, then this is the same as performing a in (x, n - 1〉, and Qn(x, a) == Qn-1(x, a). Therefore,"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1831
                },
                {
                    "x": 817,
                    "y": 1831
                },
                {
                    "x": 817,
                    "y": 1889
                },
                {
                    "x": 191,
                    "y": 1889
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:22px'>Otherwise, performing an in <xn, n)</p>",
            "id": 126,
            "page": 10,
            "text": "Otherwise, performing an in <xn, n)"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1932
                },
                {
                    "x": 1623,
                    "y": 1932
                },
                {
                    "x": 1623,
                    "y": 2040
                },
                {
                    "x": 191,
                    "y": 2040
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:14px'>● with probability 1 - an is exactly the same as performing an in <xn, n - 1), or<br>● with probability �n yields immediate reward rn and new state 〈yn, n - 1).</p>",
            "id": 127,
            "page": 10,
            "text": "● with probability 1 - an is exactly the same as performing an in <xn, n - 1), or ● with probability �n yields immediate reward rn and new state 〈yn, n - 1)."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2084
                },
                {
                    "x": 1292,
                    "y": 2084
                },
                {
                    "x": 1292,
                    "y": 2140
                },
                {
                    "x": 190,
                    "y": 2140
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:18px'>Therefore the optimal action value in the ARP of <xn, n〉, an is</p>",
            "id": 128,
            "page": 10,
            "text": "Therefore the optimal action value in the ARP of <xn, n〉, an is"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2182
                },
                {
                    "x": 1682,
                    "y": 2182
                },
                {
                    "x": 1682,
                    "y": 2242
                },
                {
                    "x": 202,
                    "y": 2242
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:16px'>QARP((xn, n〉, an) == (1 - �n)QARP((Xn, n - 1〉, an) + �n(rn + yV*((yn, n - 1〉))</p>",
            "id": 129,
            "page": 10,
            "text": "QARP((xn, n〉, an) == (1 - �n)QARP((Xn, n - 1〉, an) + �n(rn + yV*((yn, n - 1〉))"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2483
                },
                {
                    "x": 1492,
                    "y": 2483
                },
                {
                    "x": 1492,
                    "y": 2588
                },
                {
                    "x": 192,
                    "y": 2588
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:18px'>from the induction hypothesis and the Qn interation formula in equation 1.<br>Hence, Qn(x, a) == QARP((X, n〉, a), Va, x, as required.</p>",
            "id": 130,
            "page": 10,
            "text": "from the induction hypothesis and the Qn interation formula in equation 1. Hence, Qn(x, a) == QARP((X, n〉, a), Va, x, as required."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2708
                },
                {
                    "x": 243,
                    "y": 2708
                },
                {
                    "x": 243,
                    "y": 2750
                },
                {
                    "x": 193,
                    "y": 2750
                }
            ],
            "category": "footer",
            "html": "<footer id='131' style='font-size:18px'>64</footer>",
            "id": 131,
            "page": 10,
            "text": "64"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 214
                },
                {
                    "x": 410,
                    "y": 214
                },
                {
                    "x": 410,
                    "y": 259
                },
                {
                    "x": 191,
                    "y": 259
                }
            ],
            "category": "header",
            "html": "<header id='132' style='font-size:14px'>Q-LEARNING</header>",
            "id": 132,
            "page": 11,
            "text": "Q-LEARNING"
        },
        {
            "bounding_box": [
                {
                    "x": 1621,
                    "y": 208
                },
                {
                    "x": 1694,
                    "y": 208
                },
                {
                    "x": 1694,
                    "y": 252
                },
                {
                    "x": 1621,
                    "y": 252
                }
            ],
            "category": "header",
            "html": "<br><header id='133' style='font-size:18px'>289</header>",
            "id": 133,
            "page": 11,
            "text": "289"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 341
                },
                {
                    "x": 377,
                    "y": 341
                },
                {
                    "x": 377,
                    "y": 391
                },
                {
                    "x": 191,
                    "y": 391
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:16px'>Lemma B</p>",
            "id": 134,
            "page": 11,
            "text": "Lemma B"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 443
                },
                {
                    "x": 787,
                    "y": 443
                },
                {
                    "x": 787,
                    "y": 495
                },
                {
                    "x": 192,
                    "y": 495
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:22px'>B.1 Discounting infinite sequences</p>",
            "id": 135,
            "page": 11,
            "text": "B.1 Discounting infinite sequences"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 543
                },
                {
                    "x": 1698,
                    "y": 543
                },
                {
                    "x": 1698,
                    "y": 747
                },
                {
                    "x": 190,
                    "y": 747
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:14px'>Consider a discounted, bounded-reward, finite Markov process with transition matrix<br>Pxy[a]. From any starting state x, the difference between the value of that state under any<br>set of s actions and under those same s actions followed by any arbitrary policy tends to<br>0 as s → 00.</p>",
            "id": 136,
            "page": 11,
            "text": "Consider a discounted, bounded-reward, finite Markov process with transition matrix Pxy[a]. From any starting state x, the difference between the value of that state under any set of s actions and under those same s actions followed by any arbitrary policy tends to 0 as s → 00."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 798
                },
                {
                    "x": 303,
                    "y": 798
                },
                {
                    "x": 303,
                    "y": 845
                },
                {
                    "x": 192,
                    "y": 845
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:20px'>Proof</p>",
            "id": 137,
            "page": 11,
            "text": "Proof"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 896
                },
                {
                    "x": 1204,
                    "y": 896
                },
                {
                    "x": 1204,
                    "y": 953
                },
                {
                    "x": 191,
                    "y": 953
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:14px'>Ignoring the value of the s + 1th state incurs a penalty of</p>",
            "id": 138,
            "page": 11,
            "text": "Ignoring the value of the s + 1th state incurs a penalty of"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1147
                },
                {
                    "x": 1396,
                    "y": 1147
                },
                {
                    "x": 1396,
                    "y": 1202
                },
                {
                    "x": 192,
                    "y": 1202
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:14px'>But if all rewards are bounded by R, [V\"(x)] < R/(1 - Y), and so</p>",
            "id": 139,
            "page": 11,
            "text": "But if all rewards are bounded by R, [V\"(x)] < R/(1 - Y), and so"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1402
                },
                {
                    "x": 1697,
                    "y": 1402
                },
                {
                    "x": 1697,
                    "y": 1503
                },
                {
                    "x": 191,
                    "y": 1503
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>B.2 The probability of straying below level 1 is executing s actions can be make arbitrarily<br>small</p>",
            "id": 140,
            "page": 11,
            "text": "B.2 The probability of straying below level 1 is executing s actions can be make arbitrarily small"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1553
                },
                {
                    "x": 1697,
                    "y": 1553
                },
                {
                    "x": 1697,
                    "y": 1705
                },
                {
                    "x": 191,
                    "y": 1705
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>Given any level 1, there exists another yet higher level, h, such that the probability can<br>be made arbitrarily small of straying below 1 after taking s actions in the ARP, starting<br>from above h.</p>",
            "id": 141,
            "page": 11,
            "text": "Given any level 1, there exists another yet higher level, h, such that the probability can be made arbitrarily small of straying below 1 after taking s actions in the ARP, starting from above h."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1754
                },
                {
                    "x": 303,
                    "y": 1754
                },
                {
                    "x": 303,
                    "y": 1801
                },
                {
                    "x": 192,
                    "y": 1801
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:22px'>Proof</p>",
            "id": 142,
            "page": 11,
            "text": "Proof"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1856
                },
                {
                    "x": 1698,
                    "y": 1856
                },
                {
                    "x": 1698,
                    "y": 2008
                },
                {
                    "x": 191,
                    "y": 2008
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:16px'>Define in as the largest i such that ni(x, a) ≤ n, and il as the smallest such that ni(x, a) ≥ 1.<br>Then, defining �n0 = 1, the probability of straying below I starting from <x, n〉, n > 1<br>executing action a is:</p>",
            "id": 143,
            "page": 11,
            "text": "Define in as the largest i such that ni(x, a) ≤ n, and il as the smallest such that ni(x, a) ≥ 1. Then, defining �n0 = 1, the probability of straying below I starting from <x, n〉, n > 1 executing action a is:"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2246
                },
                {
                    "x": 1700,
                    "y": 2246
                },
                {
                    "x": 1700,
                    "y": 2552
                },
                {
                    "x": 190,
                    "y": 2552
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:14px'>where, as before, ni ≡ ni(x, a). But II (1 - �ni) < exp(- Eit=il �ni) → 0 as n and<br>hence in → 00. Furthermore, since the state and action spaces are finite, given 7, there<br>exists some level n1 such that starting above there from any (x, a) leads to a level above<br>1 with probability at least 1 - 7. This argument iterates for the second action with n1 as<br>the new lower limit. 7 can be chosen appropriately to set the overall probability of straying<br>below 1 less than any arbitrary 6 > 0.</p>",
            "id": 144,
            "page": 11,
            "text": "where, as before, ni ≡ ni(x, a). But II (1 - �ni) < exp(- Eit=il �ni) → 0 as n and hence in → 00. Furthermore, since the state and action spaces are finite, given 7, there exists some level n1 such that starting above there from any (x, a) leads to a level above 1 with probability at least 1 - 7. This argument iterates for the second action with n1 as the new lower limit. 7 can be chosen appropriately to set the overall probability of straying below 1 less than any arbitrary 6 > 0."
        },
        {
            "bounding_box": [
                {
                    "x": 1642,
                    "y": 2715
                },
                {
                    "x": 1693,
                    "y": 2715
                },
                {
                    "x": 1693,
                    "y": 2757
                },
                {
                    "x": 1642,
                    "y": 2757
                }
            ],
            "category": "footer",
            "html": "<footer id='145' style='font-size:18px'>65</footer>",
            "id": 145,
            "page": 11,
            "text": "65"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 207
                },
                {
                    "x": 270,
                    "y": 207
                },
                {
                    "x": 270,
                    "y": 253
                },
                {
                    "x": 195,
                    "y": 253
                }
            ],
            "category": "header",
            "html": "<header id='146' style='font-size:20px'>290</header>",
            "id": 146,
            "page": 12,
            "text": "290"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 214
                },
                {
                    "x": 1695,
                    "y": 214
                },
                {
                    "x": 1695,
                    "y": 257
                },
                {
                    "x": 1259,
                    "y": 257
                }
            ],
            "category": "header",
            "html": "<br><header id='147' style='font-size:14px'>C. WATKINS AND P. DAYAN</header>",
            "id": 147,
            "page": 12,
            "text": "C. WATKINS AND P. DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 340
                },
                {
                    "x": 1416,
                    "y": 340
                },
                {
                    "x": 1416,
                    "y": 394
                },
                {
                    "x": 195,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:20px'>B.3 Rewards and transition probabilities converge with probabability 1</p>",
            "id": 148,
            "page": 12,
            "text": "B.3 Rewards and transition probabilities converge with probabability 1"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 441
                },
                {
                    "x": 1697,
                    "y": 441
                },
                {
                    "x": 1697,
                    "y": 596
                },
                {
                    "x": 194,
                    "y": 596
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:20px'>With probability 1, the probabilities P(n)[a] and expected rewards R(n)(a) in the ARP con-<br>verge and tend to the transition matrices and expected rewards in the real process as the<br>level n increases to infinity.</p>",
            "id": 149,
            "page": 12,
            "text": "With probability 1, the probabilities P(n)[a] and expected rewards R(n)(a) in the ARP converge and tend to the transition matrices and expected rewards in the real process as the level n increases to infinity."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 646
                },
                {
                    "x": 305,
                    "y": 646
                },
                {
                    "x": 305,
                    "y": 691
                },
                {
                    "x": 195,
                    "y": 691
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:20px'>Proof</p>",
            "id": 150,
            "page": 12,
            "text": "Proof"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 747
                },
                {
                    "x": 1695,
                    "y": 747
                },
                {
                    "x": 1695,
                    "y": 847
                },
                {
                    "x": 196,
                    "y": 847
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:20px'>A standard theorem in stochastic convergence (e.g., theorem 2.3.1 of Kushner & Clark,<br>1978) states that if Xn are updated according to</p>",
            "id": 151,
            "page": 12,
            "text": "A standard theorem in stochastic convergence (e.g., theorem 2.3.1 of Kushner & Clark, 1978) states that if Xn are updated according to"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 991
                },
                {
                    "x": 1695,
                    "y": 991
                },
                {
                    "x": 1695,
                    "y": 1101
                },
                {
                    "x": 193,
                    "y": 1101
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:20px'>where 0 ≤ Bn < 1, �i=1 Bn == 00, �/=1 B2 < 8, and En are bounded random variables<br>with mean E, then</p>",
            "id": 152,
            "page": 12,
            "text": "where 0 ≤ Bn < 1, �i=1 Bn == 00, �/=1 B2 < 8, and En are bounded random variables with mean E, then"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1249
                },
                {
                    "x": 1694,
                    "y": 1249
                },
                {
                    "x": 1694,
                    "y": 1355
                },
                {
                    "x": 192,
                    "y": 1355
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>If R(x,n)(a) is the expected immediate reward for performing action a from state x at level<br>n in the ARP, then R(x,n)(a) satisfies</p>",
            "id": 153,
            "page": 12,
            "text": "If R(x,n)(a) is the expected immediate reward for performing action a from state x at level n in the ARP, then R(x,n)(a) satisfies"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1502
                },
                {
                    "x": 1696,
                    "y": 1502
                },
                {
                    "x": 1696,
                    "y": 1707
                },
                {
                    "x": 192,
                    "y": 1707
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>where the R and the a satisfy the conditions of the theorem with � = Rx(a), and<br>remembering that ni is the ith occasion on which action a was tried at state x. Therefore<br>R(x,n) (a) → Rx(a) as n → 00, with probbility one. Also, since there is only a finite num-<br>ber of states and actions, the convergence is uniform.</p>",
            "id": 154,
            "page": 12,
            "text": "where the R and the a satisfy the conditions of the theorem with � = Rx(a), and remembering that ni is the ith occasion on which action a was tried at state x. Therefore R(x,n) (a) → Rx(a) as n → 00, with probbility one. Also, since there is only a finite number of states and actions, the convergence is uniform."
        },
        {
            "bounding_box": [
                {
                    "x": 229,
                    "y": 1707
                },
                {
                    "x": 522,
                    "y": 1707
                },
                {
                    "x": 522,
                    "y": 1754
                },
                {
                    "x": 229,
                    "y": 1754
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:22px'>Similarly, define</p>",
            "id": 155,
            "page": 12,
            "text": "Similarly, define"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2003
                },
                {
                    "x": 1694,
                    "y": 2003
                },
                {
                    "x": 1694,
                    "y": 2157
                },
                {
                    "x": 191,
                    "y": 2157
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:16px'>th transition, mean value Pxy (a). Then,<br>as a (random variable) indicator function of the n<br>with P(m)[a] as the probability of ending up at state y based on a transition from state x<br>using action a at level n in the ARP,</p>",
            "id": 156,
            "page": 12,
            "text": "th transition, mean value Pxy (a). Then, as a (random variable) indicator function of the n with P(m)[a] as the probability of ending up at state y based on a transition from state x using action a at level n in the ARP,"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2306
                },
                {
                    "x": 1694,
                    "y": 2306
                },
                {
                    "x": 1694,
                    "y": 2405
                },
                {
                    "x": 192,
                    "y": 2405
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:16px'>and so, by the theorem, P(m)[a] → Pxy[a] (the transition matrix in the real process) as<br>n → 00, with probability one.</p>",
            "id": 157,
            "page": 12,
            "text": "and so, by the theorem, P(m)[a] → Pxy[a] (the transition matrix in the real process) as n → 00, with probability one."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2408
                },
                {
                    "x": 1693,
                    "y": 2408
                },
                {
                    "x": 1693,
                    "y": 2609
                },
                {
                    "x": 191,
                    "y": 2609
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:16px'>Since, in addition, all observations from the real process are independent, and, by B.2,<br>the probability of straying below a fixed level k can be made arbitrarily small, the transi-<br>tion probabilities and expected rewards for a single step conditional on ending up at a level<br>greater than k also converge to Pxy [a] and Rx(a) as n → 00.</p>",
            "id": 158,
            "page": 12,
            "text": "Since, in addition, all observations from the real process are independent, and, by B.2, the probability of straying below a fixed level k can be made arbitrarily small, the transition probabilities and expected rewards for a single step conditional on ending up at a level greater than k also converge to Pxy [a] and Rx(a) as n → 00."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2707
                },
                {
                    "x": 242,
                    "y": 2707
                },
                {
                    "x": 242,
                    "y": 2750
                },
                {
                    "x": 193,
                    "y": 2750
                }
            ],
            "category": "footer",
            "html": "<footer id='159' style='font-size:20px'>66</footer>",
            "id": 159,
            "page": 12,
            "text": "66"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 213
                },
                {
                    "x": 410,
                    "y": 213
                },
                {
                    "x": 410,
                    "y": 260
                },
                {
                    "x": 191,
                    "y": 260
                }
            ],
            "category": "header",
            "html": "<header id='160' style='font-size:14px'>Q-LEARNING</header>",
            "id": 160,
            "page": 13,
            "text": "Q-LEARNING"
        },
        {
            "bounding_box": [
                {
                    "x": 1621,
                    "y": 208
                },
                {
                    "x": 1691,
                    "y": 208
                },
                {
                    "x": 1691,
                    "y": 253
                },
                {
                    "x": 1621,
                    "y": 253
                }
            ],
            "category": "header",
            "html": "<br><header id='161' style='font-size:18px'>291</header>",
            "id": 161,
            "page": 13,
            "text": "291"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 343
                },
                {
                    "x": 1112,
                    "y": 343
                },
                {
                    "x": 1112,
                    "y": 395
                },
                {
                    "x": 190,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:18px'>B.4 Close rewards and transitions imply close values</p>",
            "id": 162,
            "page": 13,
            "text": "B.4 Close rewards and transitions imply close values"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 441
                },
                {
                    "x": 1701,
                    "y": 441
                },
                {
                    "x": 1701,
                    "y": 799
                },
                {
                    "x": 190,
                    "y": 799
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:16px'>Let Pxy[a], for i = 1 · · · s be the transition matrices of s Markov chains, and Ri(a) be<br>the reward functions. Consider the s-step chain formed from the concatenation of these-<br>i.e., starting from state x1, move to state x2 according to Px1x2 [a1], then state X3 according<br>P2x2x3<br>to [a2], and SO on, with commensurate rewards. Given 7 > 0, if Pi[a] are within<br>7/R of Pxy[a], va, x, y, and R.(a) . . · Rx(a) are within 7 of Rx(a), va, x, then the<br>value of the s actions in the concatenated chain is within ns(s + 1)/2 of their value in the<br>real process.</p>",
            "id": 163,
            "page": 13,
            "text": "Let Pxy[a], for i = 1 · · · s be the transition matrices of s Markov chains, and Ri(a) be the reward functions. Consider the s-step chain formed from the concatenation of thesei.e., starting from state x1, move to state x2 according to Px1x2 [a1], then state X3 according P2x2x3 to [a2], and SO on, with commensurate rewards. Given 7 > 0, if Pi[a] are within 7/R of Pxy[a], va, x, y, and R.(a) . . · Rx(a) are within 7 of Rx(a), va, x, then the value of the s actions in the concatenated chain is within ns(s + 1)/2 of their value in the real process."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 846
                },
                {
                    "x": 304,
                    "y": 846
                },
                {
                    "x": 304,
                    "y": 895
                },
                {
                    "x": 191,
                    "y": 895
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:20px'>Proof</p>",
            "id": 164,
            "page": 13,
            "text": "Proof"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 950
                },
                {
                    "x": 327,
                    "y": 950
                },
                {
                    "x": 327,
                    "y": 996
                },
                {
                    "x": 192,
                    "y": 996
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:16px'>Define:</p>",
            "id": 165,
            "page": 13,
            "text": "Define:"
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 1204
                },
                {
                    "x": 1697,
                    "y": 1204
                },
                {
                    "x": 1697,
                    "y": 1305
                },
                {
                    "x": 189,
                    "y": 1305
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:16px'>as the expected reward in the real process for executing two actions, a1 and a2 at state x,<br>and</p>",
            "id": 166,
            "page": 13,
            "text": "as the expected reward in the real process for executing two actions, a1 and a2 at state x, and"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1513
                },
                {
                    "x": 1444,
                    "y": 1513
                },
                {
                    "x": 1444,
                    "y": 1562
                },
                {
                    "x": 190,
                    "y": 1562
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:16px'>as the equivalent in the concatenated chain for exactly the same actions.</p>",
            "id": 167,
            "page": 13,
            "text": "as the equivalent in the concatenated chain for exactly the same actions."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1563
                },
                {
                    "x": 1615,
                    "y": 1563
                },
                {
                    "x": 1615,
                    "y": 1618
                },
                {
                    "x": 218,
                    "y": 1618
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:16px'>Then, since | Ri(a) - Rx(a)| < 7 and Piy[a] - Pxy[a] < 7/R, va, i, x, y,</p>",
            "id": 168,
            "page": 13,
            "text": "Then, since | Ri(a) - Rx(a)| < 7 and Piy[a] - Pxy[a] < 7/R, va, i, x, y,"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2197
                },
                {
                    "x": 605,
                    "y": 2197
                },
                {
                    "x": 605,
                    "y": 2250
                },
                {
                    "x": 190,
                    "y": 2250
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:18px'>Similarly, for s actions,</p>",
            "id": 169,
            "page": 13,
            "text": "Similarly, for s actions,"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2444
                },
                {
                    "x": 1698,
                    "y": 2444
                },
                {
                    "x": 1698,
                    "y": 2599
                },
                {
                    "x": 190,
                    "y": 2599
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:16px'>This applies to the ARP if the rewards and transition matrices at the successively lower<br>levels are sufficiently close to those in the real process-the main body of the theorem<br>quantifies the cost of this condition failing.</p>",
            "id": 170,
            "page": 13,
            "text": "This applies to the ARP if the rewards and transition matrices at the successively lower levels are sufficiently close to those in the real process-the main body of the theorem quantifies the cost of this condition failing."
        },
        {
            "bounding_box": [
                {
                    "x": 1642,
                    "y": 2712
                },
                {
                    "x": 1692,
                    "y": 2712
                },
                {
                    "x": 1692,
                    "y": 2756
                },
                {
                    "x": 1642,
                    "y": 2756
                }
            ],
            "category": "footer",
            "html": "<footer id='171' style='font-size:16px'>67</footer>",
            "id": 171,
            "page": 13,
            "text": "67"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 207
                },
                {
                    "x": 267,
                    "y": 207
                },
                {
                    "x": 267,
                    "y": 253
                },
                {
                    "x": 195,
                    "y": 253
                }
            ],
            "category": "header",
            "html": "<header id='172' style='font-size:20px'>292</header>",
            "id": 172,
            "page": 14,
            "text": "292"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 216
                },
                {
                    "x": 1693,
                    "y": 216
                },
                {
                    "x": 1693,
                    "y": 257
                },
                {
                    "x": 1259,
                    "y": 257
                }
            ],
            "category": "header",
            "html": "<br><header id='173' style='font-size:14px'>C. WATKINS AND P. DAYAN</header>",
            "id": 173,
            "page": 14,
            "text": "C. WATKINS AND P. DAYAN"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 344
                },
                {
                    "x": 394,
                    "y": 344
                },
                {
                    "x": 394,
                    "y": 386
                },
                {
                    "x": 197,
                    "y": 386
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:22px'>References</p>",
            "id": 174,
            "page": 14,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 424
                },
                {
                    "x": 1696,
                    "y": 424
                },
                {
                    "x": 1696,
                    "y": 1570
                },
                {
                    "x": 192,
                    "y": 1570
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:16px'>Barto, A.G., Bradtke, S.J. & Singh, S.P. (1991). Real-time learning and control using asynchronous dynamic<br>programming. (COINS technical report 91-57). Amherst: University of Massachusetts.<br>Barto, A.G. & Singh, S.P. (1990). On the computational economics of reinforcement learning. In D.S. Touretzky,<br>J. Elman, T.J. Sejnowski & G.E. Hinton, (Eds.), Proceedings of the 1990 Connectionist Models Summer School.<br>San Mateo, CA: Morgan Kaufmann.<br>Bellman, R.E. & Dreyfus, S.E. (1962). Applied dynamic programming. RAND Corporation.<br>Chapman, D. & Kaelbling, L.P. (1991). Input generalization in delayed reinforcement learning: An algorithm<br>and performance comparisons. Proceedings of the 1991 International Joint Conference on Artificial Intelligence<br>(pp. 726-731).<br>Kushner, H. & Clark, D. (1978). Stochastic approximation methods for constrained and unconstrained systems.<br>Berlin, Germany: Springer-Verlag.<br>Lin, L. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine<br>Learning, 8.<br>Mahadevan & Connell (1991). Automatic programming of behavior-based robots using reinforcement learning.<br>Proceedings of the 1991 National Conference on AI (pp. 768-773).<br>Ross, S. (1983). Introduction to stochastic dynamic programming. New York, Academic Press.<br>Sato, M2, Abe, K. & Takeda, H. (1988). Learning control of finite Markov chains with explicit trade-off between<br>estimation and control. IEEE Transactions on Systems, Man and Cybernetics, 18, pp. 677-684.<br>Sutton, R.S. (1984). Temporal credit assignment in reinforcement learning. PhD Thesis, University of Massachusetts,<br>Amherst, MA.<br>Sutton, R.S. (1988). Learning to predict by the methods of temporal difference. Machine Learning, 3, pp. 9-44.<br>Sutton. R.S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic<br>programming. Proceedings of the Seventh International Conference on Machine Learning. San Mateo, CA:<br>Morgan Kaufmann.<br>Watkins, C.J.C.H. (1989). Learning from delayed rewards. PhD Thesis, University of Cambridge, England.<br>Werbos, P.J. (1977). Advanced forecasting methods for global crisis warning and models of intelligence. General<br>Systems Yearbook, 22, pp. 25-38.</p>",
            "id": 175,
            "page": 14,
            "text": "Barto, A.G., Bradtke, S.J. & Singh, S.P. (1991). Real-time learning and control using asynchronous dynamic programming. (COINS technical report 91-57). Amherst: University of Massachusetts. Barto, A.G. & Singh, S.P. (1990). On the computational economics of reinforcement learning. In D.S. Touretzky, J. Elman, T.J. Sejnowski & G.E. Hinton, (Eds.), Proceedings of the 1990 Connectionist Models Summer School. San Mateo, CA: Morgan Kaufmann. Bellman, R.E. & Dreyfus, S.E. (1962). Applied dynamic programming. RAND Corporation. Chapman, D. & Kaelbling, L.P. (1991). Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. Proceedings of the 1991 International Joint Conference on Artificial Intelligence (pp. 726-731). Kushner, H. & Clark, D. (1978). Stochastic approximation methods for constrained and unconstrained systems. Berlin, Germany: Springer-Verlag. Lin, L. (1992). Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 8. Mahadevan & Connell (1991). Automatic programming of behavior-based robots using reinforcement learning. Proceedings of the 1991 National Conference on AI (pp. 768-773). Ross, S. (1983). Introduction to stochastic dynamic programming. New York, Academic Press. Sato, M2, Abe, K. & Takeda, H. (1988). Learning control of finite Markov chains with explicit trade-off between estimation and control. IEEE Transactions on Systems, Man and Cybernetics, 18, pp. 677-684. Sutton, R.S. (1984). Temporal credit assignment in reinforcement learning. PhD Thesis, University of Massachusetts, Amherst, MA. Sutton, R.S. (1988). Learning to predict by the methods of temporal difference. Machine Learning, 3, pp. 9-44. Sutton. R.S. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. Proceedings of the Seventh International Conference on Machine Learning. San Mateo, CA: Morgan Kaufmann. Watkins, C.J.C.H. (1989). Learning from delayed rewards. PhD Thesis, University of Cambridge, England. Werbos, P.J. (1977). Advanced forecasting methods for global crisis warning and models of intelligence. General Systems Yearbook, 22, pp. 25-38."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2707
                },
                {
                    "x": 273,
                    "y": 2707
                },
                {
                    "x": 273,
                    "y": 2748
                },
                {
                    "x": 223,
                    "y": 2748
                }
            ],
            "category": "footer",
            "html": "<footer id='176' style='font-size:18px'>68</footer>",
            "id": 176,
            "page": 14,
            "text": "68"
        }
    ]
}