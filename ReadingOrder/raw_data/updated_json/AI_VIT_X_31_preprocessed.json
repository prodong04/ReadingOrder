{
    "id": "32c0fffa-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1511.06581v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 396,
                    "y": 368
                },
                {
                    "x": 2089,
                    "y": 368
                },
                {
                    "x": 2089,
                    "y": 443
                },
                {
                    "x": 396,
                    "y": 443
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Dueling Network Architectures for Deep Reinforcement Learning</p>",
            "id": 0,
            "page": 1,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 576
                },
                {
                    "x": 778,
                    "y": 576
                },
                {
                    "x": 778,
                    "y": 944
                },
                {
                    "x": 222,
                    "y": 944
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:18px'>Ziyu Wang<br>Tom Schaul<br>Matteo Hessel<br>Hado van Hasselt<br>Marc Lanctot<br>Nando de Freitas<br>Google DeepMind, London, UK</p>",
            "id": 1,
            "page": 1,
            "text": "Ziyu Wang Tom Schaul Matteo Hessel Hado van Hasselt Marc Lanctot Nando de Freitas Google DeepMind, London, UK"
        },
        {
            "bounding_box": [
                {
                    "x": 1642,
                    "y": 585
                },
                {
                    "x": 2233,
                    "y": 585
                },
                {
                    "x": 2233,
                    "y": 883
                },
                {
                    "x": 1642,
                    "y": 883
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:16px'>ZIYU@GOOGLE.COM<br>SCHAUL@GOOGLE.COM<br>MTTHSS@GOOGLE.COM<br>HADO@GOOGLE.COM<br>LANCTOT@GOOGLE.COM<br>NANDODEFREITAS @GMAIL.COM</p>",
            "id": 2,
            "page": 1,
            "text": "ZIYU@GOOGLE.COM SCHAUL@GOOGLE.COM MTTHSS@GOOGLE.COM HADO@GOOGLE.COM LANCTOT@GOOGLE.COM NANDODEFREITAS @GMAIL.COM"
        },
        {
            "bounding_box": [
                {
                    "x": 619,
                    "y": 1056
                },
                {
                    "x": 819,
                    "y": 1056
                },
                {
                    "x": 819,
                    "y": 1114
                },
                {
                    "x": 619,
                    "y": 1114
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 228,
                    "y": 2208
                },
                {
                    "x": 554,
                    "y": 2208
                },
                {
                    "x": 554,
                    "y": 2263
                },
                {
                    "x": 228,
                    "y": 2263
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>1. Introduction</p>",
            "id": 4,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 305,
                    "y": 1147
                },
                {
                    "x": 1136,
                    "y": 1147
                },
                {
                    "x": 1136,
                    "y": 2099
                },
                {
                    "x": 305,
                    "y": 2099
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:16px'>In recent years there have been many successes<br>of using deep representations in reinforcement<br>learning. Still, many of these applications use<br>conventional architectures, such as convolutional<br>networks, LSTMs, or auto-encoders. In this pa-<br>per, we present a new neural network architec-<br>ture for model-free reinforcement learning. Our<br>dueling network represents two separate estima-<br>tors: one for the state value function and one for<br>the state-dependent action advantage function.<br>The main benefit of this factoring is to general-<br>ize learning across actions without imposing any<br>change to the underlying reinforcement learning<br>algorithm. Our results show that this architec-<br>ture leads to better policy evaluation in the pres-<br>ence of many similar-valued actions. Moreover,<br>the dueling architecture enables our RL agent to<br>outperform the state-of-the-art on the Atari 2600<br>domain.</p>",
            "id": 5,
            "page": 1,
            "text": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2294
                },
                {
                    "x": 1217,
                    "y": 2294
                },
                {
                    "x": 1217,
                    "y": 2995
                },
                {
                    "x": 222,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Over the past years, deep learning has contributed to dra-<br>matic advances in scalability and performance of machine<br>learning (LeCun et al., 2015). One exciting application<br>is the sequential decision-making setting of reinforcement<br>learning (RL) and control. Notable examples include deep<br>Q-learning (Mnih et al., 2015), deep visuomotor policies<br>(Levine et al., 2015), attention with recurrent networks (Ba<br>et al., 2015), and model predictive control with embeddings<br>(Watter et al., 2015). Other recent successes include mas-<br>sively parallel frameworks (Nair et al., 2015) and expert<br>move prediction in the game of Go (Maddison et al., 2015),<br>which produced policies matching those of Monte Carlo<br>tree search programs, and squarely beaten a professional<br>player when combined with search (Silver et al., 2016).</p>",
            "id": 6,
            "page": 1,
            "text": "Over the past years, deep learning has contributed to dramatic advances in scalability and performance of machine learning (LeCun , 2015). One exciting application is the sequential decision-making setting of reinforcement learning (RL) and control. Notable examples include deep Q-learning (Mnih , 2015), deep visuomotor policies (Levine , 2015), attention with recurrent networks (Ba , 2015), and model predictive control with embeddings (Watter , 2015). Other recent successes include massively parallel frameworks (Nair , 2015) and expert move prediction in the game of Go (Maddison , 2015), which produced policies matching those of Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver , 2016)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1063
                },
                {
                    "x": 2264,
                    "y": 1063
                },
                {
                    "x": 2264,
                    "y": 1664
                },
                {
                    "x": 1272,
                    "y": 1664
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:16px'>In spite of this, most of the approaches for RL use standard<br>neural networks, such as convolutional networks, MLPs,<br>LSTMs and autoencoders. The focus in these recent ad-<br>vances has been on designing improved control and RL al-<br>gorithms, or simply on incorporating existing neural net-<br>work architectures into RL methods. Here, we take an al-<br>ternative but complementary approach of focusing primar-<br>ily on innovating a neural network architecture that is better<br>suited for model-free RL. This approach has the benefit that<br>the new network can be easily combined with existing and<br>future algorithms for RL. Thatis, this paper advances a new<br>network (Figure 1), but uses already published algorithms.</p>",
            "id": 7,
            "page": 1,
            "text": "In spite of this, most of the approaches for RL use standard neural networks, such as convolutional networks, MLPs, LSTMs and autoencoders. The focus in these recent advances has been on designing improved control and RL algorithms, or simply on incorporating existing neural network architectures into RL methods. Here, we take an alternative but complementary approach of focusing primarily on innovating a neural network architecture that is better suited for model-free RL. This approach has the benefit that the new network can be easily combined with existing and future algorithms for RL. Thatis, this paper advances a new network (Figure 1), but uses already published algorithms."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1734
                },
                {
                    "x": 2264,
                    "y": 1734
                },
                {
                    "x": 2264,
                    "y": 1986
                },
                {
                    "x": 1274,
                    "y": 1986
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>The proposed network architecture, which we name the du-<br>eling architecture, explicitly separates the representation of<br>state values and (state-dependent) action advantages. The<br>dueling architecture consists of two streams that represent<br>the value and advantage functions, while sharing a common</p>",
            "id": 8,
            "page": 1,
            "text": "The proposed network architecture, which we name the dueling architecture, explicitly separates the representation of state values and (state-dependent) action advantages. The dueling architecture consists of two streams that represent the value and advantage functions, while sharing a common"
        },
        {
            "bounding_box": [
                {
                    "x": 1362,
                    "y": 2055
                },
                {
                    "x": 2175,
                    "y": 2055
                },
                {
                    "x": 2175,
                    "y": 2724
                },
                {
                    "x": 1362,
                    "y": 2724
                }
            ],
            "category": "figure",
            "html": "<figure><img id='9' alt=\"\" data-coord=\"top-left:(1362,2055); bottom-right:(2175,2724)\" /></figure>",
            "id": 9,
            "page": 1,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2752
                },
                {
                    "x": 2266,
                    "y": 2752
                },
                {
                    "x": 2266,
                    "y": 2988
                },
                {
                    "x": 1271,
                    "y": 2988
                }
            ],
            "category": "caption",
            "html": "<caption id='10' style='font-size:14px'>Figure 1. A popular single stream Q-network (top) and the duel-<br>ing Q-network (bottom). The dueling network has two streams<br>to separately estimate (scalar) state-value and the advantages for<br>each action; the green output module implements equation (9) to<br>combine them. Both networks output Q-values for each action.</caption>",
            "id": 10,
            "page": 1,
            "text": "Figure 1. A popular single stream Q-network (top) and the dueling Q-network (bottom). The dueling network has two streams to separately estimate (scalar) state-value and the advantages for each action; the green output module implements equation (9) to combine them. Both networks output Q-values for each action."
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 891
                },
                {
                    "x": 151,
                    "y": 891
                },
                {
                    "x": 151,
                    "y": 2326
                },
                {
                    "x": 64,
                    "y": 2326
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2016<br>Apr<br>5<br>[cs.LG]<br>arXiv:1511.06581v3</footer>",
            "id": 11,
            "page": 1,
            "text": "2016 Apr 5 [cs.LG] arXiv:1511.06581v3"
        },
        {
            "bounding_box": [
                {
                    "x": 714,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 236
                },
                {
                    "x": 714,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='12' style='font-size:22px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 12,
            "page": 2,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 284
                },
                {
                    "x": 1214,
                    "y": 284
                },
                {
                    "x": 1214,
                    "y": 782
                },
                {
                    "x": 223,
                    "y": 782
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:20px'>convolutional feature learning module. The two streams<br>are combined via a special aggregating layer to produce an<br>estimate of the state-action value function Q as shown in<br>Figure 1. This dueling network should be understood as a<br>single Q network with two streams that replaces the popu-<br>lar single-stream Q network in existing algorithms such as<br>Deep Q-Networks (DQN; Mnih et al., 2015). The dueling<br>network automatically produces separate estimates of the<br>state value function and advantage function, without any<br>extra supervision.</p>",
            "id": 13,
            "page": 2,
            "text": "convolutional feature learning module. The two streams are combined via a special aggregating layer to produce an estimate of the state-action value function Q as shown in Figure 1. This dueling network should be understood as a single Q network with two streams that replaces the popular single-stream Q network in existing algorithms such as Deep Q-Networks (DQN; Mnih , 2015). The dueling network automatically produces separate estimates of the state value function and advantage function, without any extra supervision."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 804
                },
                {
                    "x": 1215,
                    "y": 804
                },
                {
                    "x": 1215,
                    "y": 1855
                },
                {
                    "x": 224,
                    "y": 1855
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:18px'>Intuitively, the dueling architecture can learn which states<br>are (or are not) valuable, without having to learn the effect<br>of each action for each state. This is particularly useful<br>in states where its actions do not affect the environment in<br>any relevant way. To illustrate this, consider the saliency<br>maps shown in Figure 21. These maps were generated by<br>computing the Jacobians of the trained value and advan-<br>tage streams with respect to the input video, following the<br>method proposed by Simonyan et al. (2013). (The experi-<br>mental section describes this methodology in more detail.)<br>The figure shows the value and advantage saliency maps for<br>two different time steps. In one time step (leftmost pair of<br>images), we see that the value network stream pays atten-<br>tion to the road and in particular to the horizon, where new<br>cars appear. It also pays attention to the score. The advan-<br>tage stream on the other hand does not pay much attention<br>to the visual input because its action choice is practically<br>irrelevant when there are no cars in front. However, in the<br>second time step (rightmost pair of images) the advantage<br>stream pays attention as there is a car immediately in front,<br>making its choice of action very relevant.</p>",
            "id": 14,
            "page": 2,
            "text": "Intuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful in states where its actions do not affect the environment in any relevant way. To illustrate this, consider the saliency maps shown in Figure 21. These maps were generated by computing the Jacobians of the trained value and advantage streams with respect to the input video, following the method proposed by Simonyan  (2013). (The experimental section describes this methodology in more detail.) The figure shows the value and advantage saliency maps for two different time steps. In one time step (leftmost pair of images), we see that the value network stream pays attention to the road and in particular to the horizon, where new cars appear. It also pays attention to the score. The advantage stream on the other hand does not pay much attention to the visual input because its action choice is practically irrelevant when there are no cars in front. However, in the second time step (rightmost pair of images) the advantage stream pays attention as there is a car immediately in front, making its choice of action very relevant."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1878
                },
                {
                    "x": 1213,
                    "y": 1878
                },
                {
                    "x": 1213,
                    "y": 2078
                },
                {
                    "x": 224,
                    "y": 2078
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:18px'>In the experiments, we demonstrate that the dueling archi-<br>tecture can more quickly identify the correct action during<br>policy evaluation as redundant or similar actions are added<br>to the learning problem.</p>",
            "id": 15,
            "page": 2,
            "text": "In the experiments, we demonstrate that the dueling architecture can more quickly identify the correct action during policy evaluation as redundant or similar actions are added to the learning problem."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2101
                },
                {
                    "x": 1213,
                    "y": 2101
                },
                {
                    "x": 1213,
                    "y": 2603
                },
                {
                    "x": 223,
                    "y": 2603
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>We also evaluate the gains brought in by the dueling archi-<br>tecture on the challenging Atari 2600 testbed. Here, an RL<br>agent with the same structure and hyper-parameters must<br>be able to play 57 different games by observing image pix-<br>els and game scores only. The results illustrate vast im-<br>provements over the single-stream baselines of Mnih et al.<br>(2015) and van Hasselt et al. (2015). The combination of<br>prioritized replay (Schaul et al., 2016) with the proposed<br>dueling network results in the new state-of-the-art for this<br>popular domain.</p>",
            "id": 16,
            "page": 2,
            "text": "We also evaluate the gains brought in by the dueling architecture on the challenging Atari 2600 testbed. Here, an RL agent with the same structure and hyper-parameters must be able to play 57 different games by observing image pixels and game scores only. The results illustrate vast improvements over the single-stream baselines of Mnih  (2015) and van Hasselt  (2015). The combination of prioritized replay (Schaul , 2016) with the proposed dueling network results in the new state-of-the-art for this popular domain."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2654
                },
                {
                    "x": 555,
                    "y": 2654
                },
                {
                    "x": 555,
                    "y": 2703
                },
                {
                    "x": 225,
                    "y": 2703
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:18px'>1.1. Related Work</p>",
            "id": 17,
            "page": 2,
            "text": "1.1. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2734
                },
                {
                    "x": 1213,
                    "y": 2734
                },
                {
                    "x": 1213,
                    "y": 2834
                },
                {
                    "x": 224,
                    "y": 2834
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>The notion of maintaining separate value and advantage<br>functions goes back to Baird (1993). In Baird's original</p>",
            "id": 18,
            "page": 2,
            "text": "The notion of maintaining separate value and advantage functions goes back to Baird (1993). In Baird's original"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2861
                },
                {
                    "x": 1158,
                    "y": 2861
                },
                {
                    "x": 1158,
                    "y": 2954
                },
                {
                    "x": 223,
                    "y": 2954
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:14px'>1https : / / www · youtube · com/playlist ?list=<br>PINF2y03532Pau0gEhimwTalmutywWyFEB</p>",
            "id": 19,
            "page": 2,
            "text": "1https : / / www · youtube · com/playlist ?list= PINF2y03532Pau0gEhimwTalmutywWyFEB"
        },
        {
            "bounding_box": [
                {
                    "x": 1384,
                    "y": 258
                },
                {
                    "x": 2154,
                    "y": 258
                },
                {
                    "x": 2154,
                    "y": 1335
                },
                {
                    "x": 1384,
                    "y": 1335
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='20' style='font-size:14px' alt=\"VALUE ADVANTAGE\n0003台 0003片\n1 加198 1 尔1 98\nACTIVISION ACTIVISION\nVALUE ADVANTAGE\n2\n0006년 0006년\n1 加193 1 ¥1 93\nACTIVISION ACTIVISION\" data-coord=\"top-left:(1384,258); bottom-right:(2154,1335)\" /></figure>",
            "id": 20,
            "page": 2,
            "text": "VALUE ADVANTAGE 0003台 0003片 1 加198 1 尔1 98 ACTIVISION ACTIVISION VALUE ADVANTAGE 2 0006년 0006년 1 加193 1 ¥1 93 ACTIVISION ACTIVISION"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1399
                },
                {
                    "x": 2265,
                    "y": 1399
                },
                {
                    "x": 2265,
                    "y": 1631
                },
                {
                    "x": 1273,
                    "y": 1631
                }
            ],
            "category": "caption",
            "html": "<caption id='21' style='font-size:16px'>Figure 2. See, attend and drive: Value and advantage saliency<br>maps (red-tinted overlay) on the Atari game Enduro, for a trained<br>dueling architecture. The value stream learns to pay attention to<br>the road. The advantage stream learns to pay attention only when<br>there are cars immediately in front, so as to avoid collisions.</caption>",
            "id": 21,
            "page": 2,
            "text": "Figure 2. See, attend and drive: Value and advantage saliency maps (red-tinted overlay) on the Atari game Enduro, for a trained dueling architecture. The value stream learns to pay attention to the road. The advantage stream learns to pay attention only when there are cars immediately in front, so as to avoid collisions."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1722
                },
                {
                    "x": 2265,
                    "y": 1722
                },
                {
                    "x": 2265,
                    "y": 2121
                },
                {
                    "x": 1273,
                    "y": 2121
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:20px'>advantage updating algorithm, the shared Bellman resid-<br>ual update equation is decomposed into two updates: one<br>for a state value function, and one for its associated ad-<br>vantage function. Advantage updating was shown to con-<br>verge faster than Q-learning in simple continuous time do-<br>mains in (Harmon et al., 1995). Its successor, the advan-<br>tage learning algorithm, represents only a single advantage<br>function (Harmon & Baird, 1996).</p>",
            "id": 22,
            "page": 2,
            "text": "advantage updating algorithm, the shared Bellman residual update equation is decomposed into two updates: one for a state value function, and one for its associated advantage function. Advantage updating was shown to converge faster than Q-learning in simple continuous time domains in (Harmon , 1995). Its successor, the advantage learning algorithm, represents only a single advantage function (Harmon & Baird, 1996)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2142
                },
                {
                    "x": 2264,
                    "y": 2142
                },
                {
                    "x": 2264,
                    "y": 2495
                },
                {
                    "x": 1272,
                    "y": 2495
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:22px'>The dueling architecture represents both the value V(s)<br>and advantage A(s, a) functions with a single deep model<br>whose output combines the two to produce a state-action<br>value Q(s, a). Unlike in advantage updating, the represen-<br>tation and algorithm are decoupled by construction. Con-<br>sequently, the dueling architecture can be used in combina-<br>tion with a myriad of model free RL algorithms.</p>",
            "id": 23,
            "page": 2,
            "text": "The dueling architecture represents both the value V(s) and advantage A(s, a) functions with a single deep model whose output combines the two to produce a state-action value Q(s, a). Unlike in advantage updating, the representation and algorithm are decoupled by construction. Consequently, the dueling architecture can be used in combination with a myriad of model free RL algorithms."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2518
                },
                {
                    "x": 2264,
                    "y": 2518
                },
                {
                    "x": 2264,
                    "y": 2769
                },
                {
                    "x": 1273,
                    "y": 2769
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:22px'>There is a long history of advantage functions in policy gra-<br>dients, starting with (Sutton et al., 2000). As a recent ex-<br>ample of this line of work, Schulman et al. (2015) estimate<br>advantage values online to reduce the variance of policy<br>gradient algorithms.</p>",
            "id": 24,
            "page": 2,
            "text": "There is a long history of advantage functions in policy gradients, starting with (Sutton , 2000). As a recent example of this line of work, Schulman  (2015) estimate advantage values online to reduce the variance of policy gradient algorithms."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2792
                },
                {
                    "x": 2264,
                    "y": 2792
                },
                {
                    "x": 2264,
                    "y": 2994
                },
                {
                    "x": 1273,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:20px'>There have been several attempts at playing Atari with deep<br>reinforcement learning, including Mnih et al. (2015); Guo<br>et al. (2014); Stadie et al. (2015); Nair et al. (2015); van<br>Hasselt et al. (2015); Bellemare et al. (2016) and Schaul</p>",
            "id": 25,
            "page": 2,
            "text": "There have been several attempts at playing Atari with deep reinforcement learning, including Mnih  (2015); Guo  (2014); Stadie  (2015); Nair  (2015); van Hasselt  (2015); Bellemare  (2016) and Schaul"
        },
        {
            "bounding_box": [
                {
                    "x": 715,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 236
                },
                {
                    "x": 715,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='26' style='font-size:16px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 26,
            "page": 3,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 286
                },
                {
                    "x": 1214,
                    "y": 286
                },
                {
                    "x": 1214,
                    "y": 382
                },
                {
                    "x": 223,
                    "y": 382
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:14px'>et al. (2016). The results of Schaul et al. (2016) are the<br>current published state-of-the-art.</p>",
            "id": 27,
            "page": 3,
            "text": " (2016). The results of Schaul  (2016) are the current published state-of-the-art."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 445
                },
                {
                    "x": 548,
                    "y": 445
                },
                {
                    "x": 548,
                    "y": 504
                },
                {
                    "x": 223,
                    "y": 504
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:20px'>2. Background</p>",
            "id": 28,
            "page": 3,
            "text": "2. Background"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 531
                },
                {
                    "x": 1214,
                    "y": 531
                },
                {
                    "x": 1214,
                    "y": 933
                },
                {
                    "x": 223,
                    "y": 933
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:14px'>We consider a sequential decision making setup, in which<br>an agent interacts with an environment 3 over discrete time<br>steps, see Sutton & Barto (1998) for an introduction. In the<br>Atari domain, for example, the agent perceives a video St<br>consisting of M image frames: St = (xt-M+1 , · · · , xt) E<br>S at time step t. The agent then chooses an action from a<br>discrete set at E A = {1, · · · , A} and observes a reward<br>signal rt produced by the game emulator.</p>",
            "id": 29,
            "page": 3,
            "text": "We consider a sequential decision making setup, in which an agent interacts with an environment 3 over discrete time steps, see Sutton & Barto (1998) for an introduction. In the Atari domain, for example, the agent perceives a video St consisting of M image frames: St = (xt-M+1 , · · · , xt) E S at time step t. The agent then chooses an action from a discrete set at E A = {1, · · · , A} and observes a reward signal rt produced by the game emulator."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 957
                },
                {
                    "x": 1213,
                    "y": 957
                },
                {
                    "x": 1213,
                    "y": 1204
                },
                {
                    "x": 223,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:14px'>The agent seeks maximize the expected discounted re-<br>turn, where we define the discounted return as Rt <br>ET=t ��-t��. In this formulation, 2 E [0, 1] is a discount<br>factor that trades-off the importance of immediate and fu-<br>ture rewards.</p>",
            "id": 30,
            "page": 3,
            "text": "The agent seeks maximize the expected discounted return, where we define the discounted return as Rt  ET=t ��-t��. In this formulation, 2 E  is a discount factor that trades-off the importance of immediate and future rewards."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1231
                },
                {
                    "x": 1214,
                    "y": 1231
                },
                {
                    "x": 1214,
                    "y": 1378
                },
                {
                    "x": 223,
                    "y": 1378
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:14px'>For an agent behaving according to a stochastic policy �,<br>the values of the state-action pair (s,a) and the state s are<br>defined as follows</p>",
            "id": 31,
            "page": 3,
            "text": "For an agent behaving according to a stochastic policy �, the values of the state-action pair (s,a) and the state s are defined as follows"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1625
                },
                {
                    "x": 1211,
                    "y": 1625
                },
                {
                    "x": 1211,
                    "y": 1776
                },
                {
                    "x": 223,
                    "y": 1776
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>The preceding state-action value function (Q function for<br>short) can be computed recursively with dynamic program-<br>ming:</p>",
            "id": 32,
            "page": 3,
            "text": "The preceding state-action value function (Q function for short) can be computed recursively with dynamic programming:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1929
                },
                {
                    "x": 1214,
                    "y": 1929
                },
                {
                    "x": 1214,
                    "y": 2179
                },
                {
                    "x": 223,
                    "y": 2179
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>We define the optimal Q*(s,a) = max� Q�(s,a). Un-<br>der the deterministic policy a = arg maxa'EA Q*(s, a'),<br>it follows that V*(s) = maxa Q*(s, a). From this, it also<br>follows that the optimal Q function satisfies the Bellman<br>equation:</p>",
            "id": 33,
            "page": 3,
            "text": "We define the optimal Q*(s,a) = max� Q�(s,a). Under the deterministic policy a = arg maxa'EA Q*(s, a'), it follows that V*(s) = maxa Q*(s, a). From this, it also follows that the optimal Q function satisfies the Bellman equation:"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2386
                },
                {
                    "x": 1211,
                    "y": 2386
                },
                {
                    "x": 1211,
                    "y": 2487
                },
                {
                    "x": 224,
                    "y": 2487
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>We define another important quantity, the advantage func-<br>tion, relating the value and Q functions:</p>",
            "id": 34,
            "page": 3,
            "text": "We define another important quantity, the advantage function, relating the value and Q functions:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2641
                },
                {
                    "x": 1213,
                    "y": 2641
                },
                {
                    "x": 1213,
                    "y": 2993
                },
                {
                    "x": 223,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:14px'>Note that Ea~�(s) [A�(s,a)] = 0. Intuitively, the value<br>function V measures the how good it is to be in a particular<br>state s. The Q function, however, measures the the value<br>of choosing a particular action when in this state. The ad-<br>vantage function subtracts the value of the state from the Q<br>function to obtain a relative measure of the importance of<br>each action.</p>",
            "id": 35,
            "page": 3,
            "text": "Note that Ea~�(s) [A�(s,a)] = 0. Intuitively, the value function V measures the how good it is to be in a particular state s. The Q function, however, measures the the value of choosing a particular action when in this state. The advantage function subtracts the value of the state from the Q function to obtain a relative measure of the importance of each action."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 285
                },
                {
                    "x": 1668,
                    "y": 285
                },
                {
                    "x": 1668,
                    "y": 333
                },
                {
                    "x": 1274,
                    "y": 333
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:18px'>2.1. Deep Q-networks</p>",
            "id": 36,
            "page": 3,
            "text": "2.1. Deep Q-networks"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 361
                },
                {
                    "x": 2265,
                    "y": 361
                },
                {
                    "x": 2265,
                    "y": 610
                },
                {
                    "x": 1272,
                    "y": 610
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>The value functions as described in the preceding section<br>are high dimensional objects. To approximate them, we can<br>use a deep Q-network: Q(s, a; 0) with parameters 0. To<br>estimate this network, we optimize the following sequence<br>of loss functions at iteration i:</p>",
            "id": 37,
            "page": 3,
            "text": "The value functions as described in the preceding section are high dimensional objects. To approximate them, we can use a deep Q-network: Q(s, a; 0) with parameters 0. To estimate this network, we optimize the following sequence of loss functions at iteration i:"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 787
                },
                {
                    "x": 1360,
                    "y": 787
                },
                {
                    "x": 1360,
                    "y": 829
                },
                {
                    "x": 1276,
                    "y": 829
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:14px'>with</p>",
            "id": 38,
            "page": 3,
            "text": "with"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 930
                },
                {
                    "x": 2265,
                    "y": 930
                },
                {
                    "x": 2265,
                    "y": 1427
                },
                {
                    "x": 1272,
                    "y": 1427
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>where 0- represents the parameters of a fixed and sepa-<br>rate target network. We could attempt to use standard Q-<br>learning to learn the parameters of the network Q(s, a;0)<br>online. However, this estimator performs poorly in prac-<br>tice. A key innovation in (Mnih et al., 2015) was to freeze<br>the parameters of the target network Q(s', a'; 0-) for a<br>fixed number of iterations while updating the online net-<br>work Q(s, a;0i) by gradient descent. (This greatly im-<br>proves the stability of the algorithm.) The specific gradient<br>update is</p>",
            "id": 39,
            "page": 3,
            "text": "where 0- represents the parameters of a fixed and separate target network. We could attempt to use standard Qlearning to learn the parameters of the network Q(s, a;0) online. However, this estimator performs poorly in practice. A key innovation in (Mnih , 2015) was to freeze the parameters of the target network Q(s', a'; 0-) for a fixed number of iterations while updating the online network Q(s, a;0i) by gradient descent. (This greatly improves the stability of the algorithm.) The specific gradient update is"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1608
                },
                {
                    "x": 2262,
                    "y": 1608
                },
                {
                    "x": 2262,
                    "y": 1857
                },
                {
                    "x": 1274,
                    "y": 1857
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:14px'>This approach is model free in the sense that the states and<br>rewards are produced by the environment. It is also off-<br>policy because these states and rewards are obtained with<br>a behavior policy (epsilon greedy in DQN) different from<br>the online policy that is being learned.</p>",
            "id": 40,
            "page": 3,
            "text": "This approach is model free in the sense that the states and rewards are produced by the environment. It is also offpolicy because these states and rewards are obtained with a behavior policy (epsilon greedy in DQN) different from the online policy that is being learned."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1882
                },
                {
                    "x": 2264,
                    "y": 1882
                },
                {
                    "x": 2264,
                    "y": 2332
                },
                {
                    "x": 1274,
                    "y": 2332
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>Another key ingredient behind the success of DQN is expe-<br>rience replay (Lin, 1993; Mnih et al., 2015). During learn-<br>ing, the agent accumulates a dataset Dt = {e1, e2, · · · , et}<br>of experiences et = (St, at, rt, St+1) from many episodes.<br>When training the Q-network, instead only using the<br>current experience as prescribed by standard temporal-<br>difference learning, the network is trained by sampling<br>mini-batches of experiences from D uniformly at random.<br>The sequence of losses thus takes the form</p>",
            "id": 41,
            "page": 3,
            "text": "Another key ingredient behind the success of DQN is experience replay (Lin, 1993; Mnih , 2015). During learning, the agent accumulates a dataset Dt = {e1, e2, · · · , et} of experiences et = (St, at, rt, St+1) from many episodes. When training the Q-network, instead only using the current experience as prescribed by standard temporaldifference learning, the network is trained by sampling mini-batches of experiences from D uniformly at random. The sequence of losses thus takes the form"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2510
                },
                {
                    "x": 2263,
                    "y": 2510
                },
                {
                    "x": 2263,
                    "y": 2761
                },
                {
                    "x": 1272,
                    "y": 2761
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>Experience replay increases data efficiency through re-use<br>of experience samples in multiple updates and, importantly,<br>it reduces variance as uniform sampling from the replay<br>buffer reduces the correlation among the samples used in<br>the update.</p>",
            "id": 42,
            "page": 3,
            "text": "Experience replay increases data efficiency through re-use of experience samples in multiple updates and, importantly, it reduces variance as uniform sampling from the replay buffer reduces the correlation among the samples used in the update."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2815
                },
                {
                    "x": 1805,
                    "y": 2815
                },
                {
                    "x": 1805,
                    "y": 2865
                },
                {
                    "x": 1273,
                    "y": 2865
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:16px'>2.2. Double Deep Q-networks</p>",
            "id": 43,
            "page": 3,
            "text": "2.2. Double Deep Q-networks"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2893
                },
                {
                    "x": 2263,
                    "y": 2893
                },
                {
                    "x": 2263,
                    "y": 2993
                },
                {
                    "x": 1273,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:14px'>The previous section described the main components of<br>DQN as presented in (Mnih et al., 2015). In this paper,</p>",
            "id": 44,
            "page": 3,
            "text": "The previous section described the main components of DQN as presented in (Mnih , 2015). In this paper,"
        },
        {
            "bounding_box": [
                {
                    "x": 715,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 236
                },
                {
                    "x": 715,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='45' style='font-size:16px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 45,
            "page": 4,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 285
                },
                {
                    "x": 1213,
                    "y": 285
                },
                {
                    "x": 1213,
                    "y": 584
                },
                {
                    "x": 223,
                    "y": 584
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:14px'>we use the improved Double DQN (DDQN) learning al-<br>gorithm of van Hasselt et al. (2015). In Q-learning and<br>DQN, the max operator uses the same values to both select<br>and evaluate an action. This can therefore lead to overopti-<br>mistic value estimates (van Hasselt, 2010). To mitigate this<br>problem, DDQN uses the following target:</p>",
            "id": 46,
            "page": 4,
            "text": "we use the improved Double DQN (DDQN) learning algorithm of van Hasselt  (2015). In Q-learning and DQN, the max operator uses the same values to both select and evaluate an action. This can therefore lead to overoptimistic value estimates (van Hasselt, 2010). To mitigate this problem, DDQN uses the following target:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 716
                },
                {
                    "x": 1212,
                    "y": 716
                },
                {
                    "x": 1212,
                    "y": 870
                },
                {
                    "x": 223,
                    "y": 870
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:14px'>DDQN is the same as for DQN (see Mnih et al. (2015)), but<br>DQN<br>with the target Yi replaced by yDDQN<br>· The pseudo-<br>code for DDQN is presented in Appendix A.</p>",
            "id": 47,
            "page": 4,
            "text": "DDQN is the same as for DQN (see Mnih  (2015)), but DQN with the target Yi replaced by yDDQN · The pseudocode for DDQN is presented in Appendix A."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 920
                },
                {
                    "x": 632,
                    "y": 920
                },
                {
                    "x": 632,
                    "y": 972
                },
                {
                    "x": 222,
                    "y": 972
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>2.3. Prioritized Replay</p>",
            "id": 48,
            "page": 4,
            "text": "2.3. Prioritized Replay"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 999
                },
                {
                    "x": 1214,
                    "y": 999
                },
                {
                    "x": 1214,
                    "y": 1449
                },
                {
                    "x": 223,
                    "y": 1449
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:16px'>A recent innovation in prioritized experience re-<br>play (Schaul et al., 2016) built on top of DDQN and<br>further improved the state-of-the-art. Their key idea was<br>to increase the replay probability of experience tuples<br>that have a high expected learning progress (as measured<br>via the proxy of absolute TD-error). This led to both<br>faster learning and to better final policy quality across<br>most games of the Atari benchmark suite, as compared to<br>uniform experience replay.</p>",
            "id": 49,
            "page": 4,
            "text": "A recent innovation in prioritized experience replay (Schaul , 2016) built on top of DDQN and further improved the state-of-the-art. Their key idea was to increase the replay probability of experience tuples that have a high expected learning progress (as measured via the proxy of absolute TD-error). This led to both faster learning and to better final policy quality across most games of the Atari benchmark suite, as compared to uniform experience replay."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1472
                },
                {
                    "x": 1213,
                    "y": 1472
                },
                {
                    "x": 1213,
                    "y": 1774
                },
                {
                    "x": 223,
                    "y": 1774
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:16px'>To strengthen the claim that our dueling architecture is<br>complementary to algorithmic innovations, we show that<br>it improves performance for both the uniform and the pri-<br>oritized replay baselines (for which we picked the easier<br>to implement rank-based variant), with the resulting priori-<br>tized dueling variant holding the new state-of-the-art.</p>",
            "id": 50,
            "page": 4,
            "text": "To strengthen the claim that our dueling architecture is complementary to algorithmic innovations, we show that it improves performance for both the uniform and the prioritized replay baselines (for which we picked the easier to implement rank-based variant), with the resulting prioritized dueling variant holding the new state-of-the-art."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1833
                },
                {
                    "x": 1026,
                    "y": 1833
                },
                {
                    "x": 1026,
                    "y": 1891
                },
                {
                    "x": 223,
                    "y": 1891
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:20px'>3. The Dueling Network Architecture</p>",
            "id": 51,
            "page": 4,
            "text": "3. The Dueling Network Architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1920
                },
                {
                    "x": 1213,
                    "y": 1920
                },
                {
                    "x": 1213,
                    "y": 2420
                },
                {
                    "x": 223,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:14px'>The key insight behind our new architecture, as illustrated<br>in Figure 2, is that for many states, it is unnecessary to es-<br>timate the value of each action choice. For example, in<br>the Enduro game setting, knowing whether to move left or<br>right only matters when a collision is eminent. In some<br>states, it is of paramount importance to know which action<br>to take, but in many other states the choice of action has no<br>repercussion on what happens. For bootstrapping based al-<br>gorithms, however, the estimation of state values is of great<br>importance for every state.</p>",
            "id": 52,
            "page": 4,
            "text": "The key insight behind our new architecture, as illustrated in Figure 2, is that for many states, it is unnecessary to estimate the value of each action choice. For example, in the Enduro game setting, knowing whether to move left or right only matters when a collision is eminent. In some states, it is of paramount importance to know which action to take, but in many other states the choice of action has no repercussion on what happens. For bootstrapping based algorithms, however, the estimation of state values is of great importance for every state."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2443
                },
                {
                    "x": 1213,
                    "y": 2443
                },
                {
                    "x": 1213,
                    "y": 2996
                },
                {
                    "x": 224,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:16px'>To bring this insight to fruition, we design a single Q-<br>network architecture, as illustrated in Figure 1, which we<br>refer to as the dueling network. The lower layers of the<br>dueling network are convolutional as in the original DQNs<br>(Mnih et al., 2015). However, instead of following the con-<br>volutional layers with a single sequence of fully connected<br>layers, we instead use two sequences (or streams) of fully<br>connected layers. The streams are constructed such that<br>they have they have the capability of providing separate es-<br>timates of the value and advantage functions. Finally, the<br>two streams are combined to produce a single output Q</p>",
            "id": 53,
            "page": 4,
            "text": "To bring this insight to fruition, we design a single Qnetwork architecture, as illustrated in Figure 1, which we refer to as the dueling network. The lower layers of the dueling network are convolutional as in the original DQNs (Mnih , 2015). However, instead of following the convolutional layers with a single sequence of fully connected layers, we instead use two sequences (or streams) of fully connected layers. The streams are constructed such that they have they have the capability of providing separate estimates of the value and advantage functions. Finally, the two streams are combined to produce a single output Q"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 285
                },
                {
                    "x": 2262,
                    "y": 285
                },
                {
                    "x": 2262,
                    "y": 383
                },
                {
                    "x": 1272,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:14px'>function. As in (Mnih et al., 2015), the output of the net-<br>work is a set of Q values, one for each action.</p>",
            "id": 54,
            "page": 4,
            "text": "function. As in (Mnih , 2015), the output of the network is a set of Q values, one for each action."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 410
                },
                {
                    "x": 2263,
                    "y": 410
                },
                {
                    "x": 2263,
                    "y": 707
                },
                {
                    "x": 1271,
                    "y": 707
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>Since the output of the dueling network is a Q function,<br>it can be trained with the many existing algorithms, such<br>as DDQN and SARSA. In addition, it can take advantage<br>of any improvements to these algorithms, including better<br>replay memories, better exploration policies, intrinsic mo-<br>tivation, and SO on.</p>",
            "id": 55,
            "page": 4,
            "text": "Since the output of the dueling network is a Q function, it can be trained with the many existing algorithms, such as DDQN and SARSA. In addition, it can take advantage of any improvements to these algorithms, including better replay memories, better exploration policies, intrinsic motivation, and SO on."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 732
                },
                {
                    "x": 2261,
                    "y": 732
                },
                {
                    "x": 2261,
                    "y": 882
                },
                {
                    "x": 1273,
                    "y": 882
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:16px'>The module that combines the two streams of fully-<br>connected layers to output a Q estimate requires very<br>thoughtful design.</p>",
            "id": 56,
            "page": 4,
            "text": "The module that combines the two streams of fullyconnected layers to output a Q estimate requires very thoughtful design."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 906
                },
                {
                    "x": 2263,
                    "y": 906
                },
                {
                    "x": 2263,
                    "y": 1156
                },
                {
                    "x": 1273,
                    "y": 1156
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:18px'>From the expressions for advantage Q�(s,a) = V�(s) +<br>A� (s,a) and state-value V� (s) = Ea~�(s) [Q�(s,a)], it<br>follows that Ea~�(s) [A�(s, a)] = 0. Moreover, for a de-<br>terministic policy, a* = arg maxa'EA Q(s, a'), it follows<br>that Q(s, a*) = V(s) and hence A(s, a*) = 0.</p>",
            "id": 57,
            "page": 4,
            "text": "From the expressions for advantage Q�(s,a) = V�(s) + A� (s,a) and state-value V� (s) = Ea~�(s) [Q�(s,a)], it follows that Ea~�(s) [A�(s, a)] = 0. Moreover, for a deterministic policy, a* = arg maxa'EA Q(s, a'), it follows that Q(s, a*) = V(s) and hence A(s, a*) = 0."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1181
                },
                {
                    "x": 2264,
                    "y": 1181
                },
                {
                    "x": 2264,
                    "y": 1480
                },
                {
                    "x": 1271,
                    "y": 1480
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:14px'>Let us consider the dueling network shown in Figure 1,<br>where we make one stream of fully-connected layers out-<br>put a scalar V(s; 0, B), and the other stream output an |A|-<br>dimensional vector A(s, a; 0, �). Here, 0 denotes the pa-<br>rameters of the convolutional layers, while a and B are the<br>parameters of the two streams of fully-connected layers.</p>",
            "id": 58,
            "page": 4,
            "text": "Let us consider the dueling network shown in Figure 1, where we make one stream of fully-connected layers output a scalar V(s; 0, B), and the other stream output an |A|dimensional vector A(s, a; 0, �). Here, 0 denotes the parameters of the convolutional layers, while a and B are the parameters of the two streams of fully-connected layers."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1504
                },
                {
                    "x": 2261,
                    "y": 1504
                },
                {
                    "x": 2261,
                    "y": 1603
                },
                {
                    "x": 1274,
                    "y": 1603
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:16px'>Using the definition of advantage, we might be tempted to<br>construct the aggregating module as follows:</p>",
            "id": 59,
            "page": 4,
            "text": "Using the definition of advantage, we might be tempted to construct the aggregating module as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1704
                },
                {
                    "x": 2261,
                    "y": 1704
                },
                {
                    "x": 2261,
                    "y": 1853
                },
                {
                    "x": 1273,
                    "y": 1853
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:16px'>Note that this expression applies to all (s, a) instances; that<br>is, to express equation (7) in matrix form we need to repli-<br>cate the scalar, V(s;0,B), |A| times.</p>",
            "id": 60,
            "page": 4,
            "text": "Note that this expression applies to all (s, a) instances; that is, to express equation (7) in matrix form we need to replicate the scalar, V(s;0,B), |A| times."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1877
                },
                {
                    "x": 2262,
                    "y": 1877
                },
                {
                    "x": 2262,
                    "y": 2174
                },
                {
                    "x": 1273,
                    "y": 2174
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>However, we need to keep in mind that Q(s, a;0,�,B)<br>is only a parameterized estimate of the true Q-function.<br>Moreover, it would be wrong to conclude that V(s; 0,B)<br>is a good estimator of the state-value function, or likewise<br>that A(s, a; 0, a) provides a reasonable estimate of the ad-<br>vantage function.</p>",
            "id": 61,
            "page": 4,
            "text": "However, we need to keep in mind that Q(s, a;0,�,B) is only a parameterized estimate of the true Q-function. Moreover, it would be wrong to conclude that V(s; 0,B) is a good estimator of the state-value function, or likewise that A(s, a; 0, a) provides a reasonable estimate of the advantage function."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2202
                },
                {
                    "x": 2264,
                    "y": 2202
                },
                {
                    "x": 2264,
                    "y": 2549
                },
                {
                    "x": 1271,
                    "y": 2549
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:16px'>Equation (7) is unidentifiable in the sense that given Q<br>we cannot recover V and A uniquely. To see this, add a<br>constant to V(s;0,B) and subtract the same constant from<br>A(s, a; 0, a). This constant cancels out resulting in the<br>same Q value. This lack of identifiability is mirrored by<br>poor practical performance when this equation is used di-<br>rectly.</p>",
            "id": 62,
            "page": 4,
            "text": "Equation (7) is unidentifiable in the sense that given Q we cannot recover V and A uniquely. To see this, add a constant to V(s;0,B) and subtract the same constant from A(s, a; 0, a). This constant cancels out resulting in the same Q value. This lack of identifiability is mirrored by poor practical performance when this equation is used directly."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2573
                },
                {
                    "x": 2263,
                    "y": 2573
                },
                {
                    "x": 2263,
                    "y": 2775
                },
                {
                    "x": 1274,
                    "y": 2775
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:14px'>To address this issue ofidentifiability, we can force the ad-<br>vantage function estimator to have zero advantage at the<br>chosen action. That is, we let the last module of the net-<br>work implement the forward mapping</p>",
            "id": 63,
            "page": 4,
            "text": "To address this issue ofidentifiability, we can force the advantage function estimator to have zero advantage at the chosen action. That is, we let the last module of the network implement the forward mapping"
        },
        {
            "bounding_box": [
                {
                    "x": 714,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 236
                },
                {
                    "x": 714,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='64' style='font-size:18px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 64,
            "page": 5,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 282
                },
                {
                    "x": 1212,
                    "y": 282
                },
                {
                    "x": 1212,
                    "y": 533
                },
                {
                    "x": 223,
                    "y": 533
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:16px'>Now, for a* = arg maxa'EA Q(s, a';0,�,B) =<br>arg maxa'EA A(s, a'; 0, a), we obtain Q(s, a* ; 0,a, B) =<br>V(s; 0, B). Hence, the stream V(s; 0, B) provides an esti-<br>mate of the value function, while the other stream produces<br>an estimate of the advantage function.</p>",
            "id": 65,
            "page": 5,
            "text": "Now, for a* = arg maxa'EA Q(s, a';0,�,B) = arg maxa'EA A(s, a'; 0, a), we obtain Q(s, a* ; 0,a, B) = V(s; 0, B). Hence, the stream V(s; 0, B) provides an estimate of the value function, while the other stream produces an estimate of the advantage function."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 558
                },
                {
                    "x": 1215,
                    "y": 558
                },
                {
                    "x": 1215,
                    "y": 658
                },
                {
                    "x": 223,
                    "y": 658
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:14px'>An alternative module replaces the max operator with an<br>average:</p>",
            "id": 66,
            "page": 5,
            "text": "An alternative module replaces the max operator with an average:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 941
                },
                {
                    "x": 1214,
                    "y": 941
                },
                {
                    "x": 1214,
                    "y": 1440
                },
                {
                    "x": 223,
                    "y": 1440
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:16px'>On the one hand this loses the original semantics of V and<br>A because they are now off-target by a constant, but on<br>the other hand it increases the stability of the optimization:<br>with (9) the advantages only need to change as fast as the<br>mean, instead of having to compensate any change to the<br>optimal action's advantage in (8). We also experimented<br>with a softmax version of equation (8), but found it to de-<br>liver similar results to the simpler module of equation (9).<br>Hence, all the experiments reported in this paper use the<br>module of equation (9).</p>",
            "id": 67,
            "page": 5,
            "text": "On the one hand this loses the original semantics of V and A because they are now off-target by a constant, but on the other hand it increases the stability of the optimization: with (9) the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action's advantage in (8). We also experimented with a softmax version of equation (8), but found it to deliver similar results to the simpler module of equation (9). Hence, all the experiments reported in this paper use the module of equation (9)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1464
                },
                {
                    "x": 1213,
                    "y": 1464
                },
                {
                    "x": 1213,
                    "y": 1761
                },
                {
                    "x": 223,
                    "y": 1761
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>Note that while subtracting the mean in equation (9) helps<br>with identifiability, it does not change the relative rank of<br>the A (and hence Q) values, preserving any greedy or E-<br>greedy policy based on Q values from equation (7). When<br>acting, it suffices to evaluate the advantage stream to make<br>decisions.</p>",
            "id": 68,
            "page": 5,
            "text": "Note that while subtracting the mean in equation (9) helps with identifiability, it does not change the relative rank of the A (and hence Q) values, preserving any greedy or Egreedy policy based on Q values from equation (7). When acting, it suffices to evaluate the advantage stream to make decisions."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1788
                },
                {
                    "x": 1211,
                    "y": 1788
                },
                {
                    "x": 1211,
                    "y": 2186
                },
                {
                    "x": 223,
                    "y": 2186
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>It is important to note that equation (9) is viewed and im-<br>plemented as part of the network and not as a separate algo-<br>rithmic step. Training of the dueling architectures, as with<br>standard Q networks (e.g. the deep Q-network of Mnih<br>et al. (2015)), requires only back-propagation. The esti-<br>mates V(s;0, B) and A(s, a; 0, a) are computed automati-<br>cally without any extra supervision or algorithmic modifi-<br>cations.</p>",
            "id": 69,
            "page": 5,
            "text": "It is important to note that equation (9) is viewed and implemented as part of the network and not as a separate algorithmic step. Training of the dueling architectures, as with standard Q networks (e.g. the deep Q-network of Mnih  (2015)), requires only back-propagation. The estimates V(s;0, B) and A(s, a; 0, a) are computed automatically without any extra supervision or algorithmic modifications."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2212
                },
                {
                    "x": 1212,
                    "y": 2212
                },
                {
                    "x": 1212,
                    "y": 2411
                },
                {
                    "x": 223,
                    "y": 2411
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:16px'>As the dueling architecture shares the same input-output in-<br>terface with standard Q networks, we can recycle all learn-<br>ing algorithms with Q networks (e.g., DDQN and SARSA)<br>to train the dueling architecture.</p>",
            "id": 70,
            "page": 5,
            "text": "As the dueling architecture shares the same input-output interface with standard Q networks, we can recycle all learning algorithms with Q networks (e.g., DDQN and SARSA) to train the dueling architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2473
                },
                {
                    "x": 558,
                    "y": 2473
                },
                {
                    "x": 558,
                    "y": 2530
                },
                {
                    "x": 224,
                    "y": 2530
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:20px'>4. Experiments</p>",
            "id": 71,
            "page": 5,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2560
                },
                {
                    "x": 1212,
                    "y": 2560
                },
                {
                    "x": 1212,
                    "y": 2763
                },
                {
                    "x": 224,
                    "y": 2763
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:16px'>We now show the practical performance of the dueling net-<br>work. We start with a simple policy evaluation task and<br>then show larger scale results for learning policies for gen-<br>eral Atari game-playing.</p>",
            "id": 72,
            "page": 5,
            "text": "We now show the practical performance of the dueling network. We start with a simple policy evaluation task and then show larger scale results for learning policies for general Atari game-playing."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2814
                },
                {
                    "x": 608,
                    "y": 2814
                },
                {
                    "x": 608,
                    "y": 2864
                },
                {
                    "x": 224,
                    "y": 2864
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:20px'>4.1. Policy evaluation</p>",
            "id": 73,
            "page": 5,
            "text": "4.1. Policy evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2892
                },
                {
                    "x": 1210,
                    "y": 2892
                },
                {
                    "x": 1210,
                    "y": 2993
                },
                {
                    "x": 224,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:14px'>We start by measuring the performance of the dueling ar-<br>chitecture on a policy evaluation task. We choose this par-</p>",
            "id": 74,
            "page": 5,
            "text": "We start by measuring the performance of the dueling architecture on a policy evaluation task. We choose this par-"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 286
                },
                {
                    "x": 2264,
                    "y": 286
                },
                {
                    "x": 2264,
                    "y": 483
                },
                {
                    "x": 1272,
                    "y": 483
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:16px'>ticular task because it is very useful for evaluating network<br>architectures, as it is devoid of confounding factors such as<br>the choice of exploration strategy, and the interaction be-<br>tween policy improvement and policy evaluation.</p>",
            "id": 75,
            "page": 5,
            "text": "ticular task because it is very useful for evaluating network architectures, as it is devoid of confounding factors such as the choice of exploration strategy, and the interaction between policy improvement and policy evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 509
                },
                {
                    "x": 2264,
                    "y": 509
                },
                {
                    "x": 2264,
                    "y": 757
                },
                {
                    "x": 1273,
                    "y": 757
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>In this experiment, we employ temporal difference learning<br>(without eligibility traces, i.e., 入 = 0) to learn Q values.<br>More specifically, given a behavior policy �, we seek to<br>estimate the state-action value Q�(., · ) by optimizing the<br>sequence of costs of equation (4), with target</p>",
            "id": 76,
            "page": 5,
            "text": "In this experiment, we employ temporal difference learning (without eligibility traces, i.e., 入 = 0) to learn Q values. More specifically, given a behavior policy �, we seek to estimate the state-action value Q�(., · ) by optimizing the sequence of costs of equation (4), with target"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 894
                },
                {
                    "x": 2262,
                    "y": 894
                },
                {
                    "x": 2262,
                    "y": 1042
                },
                {
                    "x": 1273,
                    "y": 1042
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:16px'>The above update rule is the same as that of Expected<br>SARSA (van Seijen et al., 2009). We, however, do not<br>modify the behavior policy as in Expected SARSA.</p>",
            "id": 77,
            "page": 5,
            "text": "The above update rule is the same as that of Expected SARSA (van Seijen , 2009). We, however, do not modify the behavior policy as in Expected SARSA."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1067
                },
                {
                    "x": 2263,
                    "y": 1067
                },
                {
                    "x": 2263,
                    "y": 1665
                },
                {
                    "x": 1270,
                    "y": 1665
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:14px'>To evaluate the learned Q values, we choose a simple envi-<br>ronment where the exact Q�(s, a) values can be computed<br>separately for all (s, a) E S x A. This environment, which<br>we call the corridor is composed of three connected cor-<br>ridors. A schematic drawing of the corridor environment<br>is shown in Figure 3, The agent starts from the bottom left<br>corner of the environment and must move to the top right<br>to get the largest reward. A total of 5 actions are available:<br>go up, down, left, right and no-op. We also have the free-<br>dom of adding an arbitrary number of no-op actions. In our<br>setup, the two vertical sections both have 10 states while<br>the horizontal section has 50.</p>",
            "id": 78,
            "page": 5,
            "text": "To evaluate the learned Q values, we choose a simple environment where the exact Q�(s, a) values can be computed separately for all (s, a) E S x A. This environment, which we call the corridor is composed of three connected corridors. A schematic drawing of the corridor environment is shown in Figure 3, The agent starts from the bottom left corner of the environment and must move to the top right to get the largest reward. A total of 5 actions are available: go up, down, left, right and no-op. We also have the freedom of adding an arbitrary number of no-op actions. In our setup, the two vertical sections both have 10 states while the horizontal section has 50."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1690
                },
                {
                    "x": 2264,
                    "y": 1690
                },
                {
                    "x": 2264,
                    "y": 1937
                },
                {
                    "x": 1271,
                    "y": 1937
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:14px'>We use an e-greedy policy as the behavior policy �, which<br>chooses a random action with probability E or an action<br>according to the optimal Q function arg maxaEA Q*(s,a)<br>with probability 1 - E. In our experiments, E is chosen to<br>be 0.001.</p>",
            "id": 79,
            "page": 5,
            "text": "We use an e-greedy policy as the behavior policy �, which chooses a random action with probability E or an action according to the optimal Q function arg maxaEA Q*(s,a) with probability 1 - E. In our experiments, E is chosen to be 0.001."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1965
                },
                {
                    "x": 2264,
                    "y": 1965
                },
                {
                    "x": 2264,
                    "y": 2617
                },
                {
                    "x": 1270,
                    "y": 2617
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:16px'>We compare a single-stream Q architecture with the duel-<br>ing architecture on three variants of the corridor environ-<br>ment with 5, 10 and 20 actions respectively. The 10 and 20<br>action variants are formed by adding no-ops to the original<br>environment. We measure performance by Squared Error<br>(SE) against the true state values: Eses,aEA(Q(s, a; 0) -<br>Q� (s, a))2. The single-stream architecture is a three layer<br>MLP with 50 units on each hidden layer. The dueling ar-<br>chitecture is also composed of three layers. After the first<br>hidden layer of 50 units, however, the network branches off<br>into two streams each of them a two layer MLP with 25 hid-<br>den units. The results of the comparison are summarized in<br>Figure 3.</p>",
            "id": 80,
            "page": 5,
            "text": "We compare a single-stream Q architecture with the dueling architecture on three variants of the corridor environment with 5, 10 and 20 actions respectively. The 10 and 20 action variants are formed by adding no-ops to the original environment. We measure performance by Squared Error (SE) against the true state values: Eses,aEA(Q(s, a; 0) Q� (s, a))2. The single-stream architecture is a three layer MLP with 50 units on each hidden layer. The dueling architecture is also composed of three layers. After the first hidden layer of 50 units, however, the network branches off into two streams each of them a two layer MLP with 25 hidden units. The results of the comparison are summarized in Figure 3."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2644
                },
                {
                    "x": 2264,
                    "y": 2644
                },
                {
                    "x": 2264,
                    "y": 2994
                },
                {
                    "x": 1272,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:14px'>The results show that with 5 actions, both architectures<br>converge at about the same speed. However, when we in-<br>crease the number of actions, the dueling architecture per-<br>forms better than the traditional Q-network. In the dueling<br>network, the stream V(s; 0, B) learns a general value that<br>is shared across many similar actions at s, hence leading<br>to faster convergence. This is a very promising result be-</p>",
            "id": 81,
            "page": 5,
            "text": "The results show that with 5 actions, both architectures converge at about the same speed. However, when we increase the number of actions, the dueling architecture performs better than the traditional Q-network. In the dueling network, the stream V(s; 0, B) learns a general value that is shared across many similar actions at s, hence leading to faster convergence. This is a very promising result be-"
        },
        {
            "bounding_box": [
                {
                    "x": 713,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 236
                },
                {
                    "x": 713,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='82' style='font-size:22px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 82,
            "page": 6,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 262
                },
                {
                    "x": 2125,
                    "y": 262
                },
                {
                    "x": 2125,
                    "y": 750
                },
                {
                    "x": 328,
                    "y": 750
                }
            ],
            "category": "figure",
            "html": "<figure><img id='83' style='font-size:14px' alt=\"CORRIDOR ENVIRONMENT 5 ACTIONS 10 ACTIONS 20 ACTIONS\n103 103 103\n102 102 10\nSE\n101 101 101\nSingle\nDuel\n10° 10° 10°\n103 104 103 104 103 10°\nNo. Iterations No. Iterations No. Iterations\n(a) (b) (c) (d)\" data-coord=\"top-left:(328,262); bottom-right:(2125,750)\" /></figure>",
            "id": 83,
            "page": 6,
            "text": "CORRIDOR ENVIRONMENT 5 ACTIONS 10 ACTIONS 20 ACTIONS 103 103 103 102 102 10 SE 101 101 101 Single Duel 10° 10° 10° 103 104 103 104 103 10° No. Iterations No. Iterations No. Iterations (a) (b) (c) (d)"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 761
                },
                {
                    "x": 2266,
                    "y": 761
                },
                {
                    "x": 2266,
                    "y": 993
                },
                {
                    "x": 222,
                    "y": 993
                }
            ],
            "category": "caption",
            "html": "<br><caption id='84' style='font-size:14px'>Figure 3. (a) The corridor environment. The star marks the starting state. The redness of a state signifies the reward the agent receives<br>upon arrival. The game terminates upon reaching either reward state. The agent's actions are going up, down, left, right and no action.<br>Plots (b), (c) and (d) shows squared error for policy evaluation with 5, 10, and 20 actions on a log-log scale. The dueling network<br>(Duel) consistently outperforms a conventional single-stream network (Single), with the performance gap increasing with the number of<br>actions.</caption>",
            "id": 84,
            "page": 6,
            "text": "Figure 3. (a) The corridor environment. The star marks the starting state. The redness of a state signifies the reward the agent receives upon arrival. The game terminates upon reaching either reward state. The agent's actions are going up, down, left, right and no action. Plots (b), (c) and (d) shows squared error for policy evaluation with 5, 10, and 20 actions on a log-log scale. The dueling network (Duel) consistently outperforms a conventional single-stream network (Single), with the performance gap increasing with the number of actions."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1068
                },
                {
                    "x": 1214,
                    "y": 1068
                },
                {
                    "x": 1214,
                    "y": 1419
                },
                {
                    "x": 223,
                    "y": 1419
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:16px'>cause many control tasks with large action spaces have this<br>property, and consequently we should expect that the du-<br>eling network will often lead to much faster convergence<br>than a traditional single stream network. In the following<br>section, we will indeed see that the dueling network results<br>in substantial gains in performance in a wide-range of Atari<br>games.</p>",
            "id": 85,
            "page": 6,
            "text": "cause many control tasks with large action spaces have this property, and consequently we should expect that the dueling network will often lead to much faster convergence than a traditional single stream network. In the following section, we will indeed see that the dueling network results in substantial gains in performance in a wide-range of Atari games."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1469
                },
                {
                    "x": 819,
                    "y": 1469
                },
                {
                    "x": 819,
                    "y": 1521
                },
                {
                    "x": 223,
                    "y": 1521
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:22px'>4.2. General Atari Game-Playing</p>",
            "id": 86,
            "page": 6,
            "text": "4.2. General Atari Game-Playing"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1547
                },
                {
                    "x": 1214,
                    "y": 1547
                },
                {
                    "x": 1214,
                    "y": 1998
                },
                {
                    "x": 224,
                    "y": 1998
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:18px'>We perform a comprehensive evaluation of our proposed<br>method on the Arcade Learning Environment (Bellemare<br>et al., 2013), which is composed of 57 Atari games. The<br>challenge is to deploy a single algorithm and architecture,<br>with a fixed set of hyper-parameters, to learn to play all<br>the games given only raw pixel observations and game re-<br>wards. This environment is very demanding because it is<br>both comprised of a large number of highly diverse games<br>and the observations are high-dimensional.</p>",
            "id": 87,
            "page": 6,
            "text": "We perform a comprehensive evaluation of our proposed method on the Arcade Learning Environment (Bellemare , 2013), which is composed of 57 Atari games. The challenge is to deploy a single algorithm and architecture, with a fixed set of hyper-parameters, to learn to play all the games given only raw pixel observations and game rewards. This environment is very demanding because it is both comprised of a large number of highly diverse games and the observations are high-dimensional."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2020
                },
                {
                    "x": 1213,
                    "y": 2020
                },
                {
                    "x": 1213,
                    "y": 2319
                },
                {
                    "x": 223,
                    "y": 2319
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:20px'>We follow closely the setup of van Hasselt et al. (2015) and<br>compare to their results using single-stream Q-networks.<br>We train the dueling network with the DDQN algorithm<br>as presented in Appendix A. At the end of this section,<br>we incorporate prioritized experience replay (Schaul et al.,<br>2016).</p>",
            "id": 88,
            "page": 6,
            "text": "We follow closely the setup of van Hasselt  (2015) and compare to their results using single-stream Q-networks. We train the dueling network with the DDQN algorithm as presented in Appendix A. At the end of this section, we incorporate prioritized experience replay (Schaul , 2016)."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2345
                },
                {
                    "x": 1213,
                    "y": 2345
                },
                {
                    "x": 1213,
                    "y": 2947
                },
                {
                    "x": 224,
                    "y": 2947
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>Our network architecture has the same low-level convolu-<br>tional structure of DQN (Mnih et al., 2015; van Hasselt<br>et al., 2015). There are 3 convolutional layers followed by<br>2 fully-connected layers. The first convolutional layer has<br>32 8 x 8 filters with stride 4, the second 64 4 x 4 filters with<br>stride 2, and the third and final convolutional layer consists<br>64 3 x 3 filters with stride 1. As shown in Figure 1, the<br>dueling network splits into two streams of fully connected<br>layers. The value and advantage streams both have a fully-<br>connected layer with 512 units. The final hidden layers of<br>the value and advantage streams are both fully-connected<br>with the value stream having one output and the advantage</p>",
            "id": 89,
            "page": 6,
            "text": "Our network architecture has the same low-level convolutional structure of DQN (Mnih , 2015; van Hasselt , 2015). There are 3 convolutional layers followed by 2 fully-connected layers. The first convolutional layer has 32 8 x 8 filters with stride 4, the second 64 4 x 4 filters with stride 2, and the third and final convolutional layer consists 64 3 x 3 filters with stride 1. As shown in Figure 1, the dueling network splits into two streams of fully connected layers. The value and advantage streams both have a fullyconnected layer with 512 units. The final hidden layers of the value and advantage streams are both fully-connected with the value stream having one output and the advantage"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1066
                },
                {
                    "x": 2262,
                    "y": 1066
                },
                {
                    "x": 2262,
                    "y": 1266
                },
                {
                    "x": 1273,
                    "y": 1266
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>as many outputs as there are valid actions2. We combine the<br>value and advantage streams using the module described by<br>Equation (9). Rectifier non-linearities (Fukushima, 1980)<br>are inserted between all adjacent layers.</p>",
            "id": 90,
            "page": 6,
            "text": "as many outputs as there are valid actions2. We combine the value and advantage streams using the module described by Equation (9). Rectifier non-linearities (Fukushima, 1980) are inserted between all adjacent layers."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1288
                },
                {
                    "x": 2264,
                    "y": 1288
                },
                {
                    "x": 2264,
                    "y": 1891
                },
                {
                    "x": 1271,
                    "y": 1891
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>We adopt the optimizers and hyper-parameters of van Has-<br>selt et al. (2015), with the exception of the learning rate<br>which we chose to be slightly lower (we do not do this for<br>double DQN as it can deteriorate its performance). Since<br>both the advantage and the value stream propagate gradi-<br>ents to the last convolutional layer in the backward pass,<br>we rescale the combined gradient entering the last convo-<br>lutional layer by 1/V2. This simple heuristic mildly in-<br>creases stability. In addition, we clip the gradients to have<br>their norm less than or equal to 10. This clipping is not<br>standard practice in deep RL, but common in recurrent net-<br>work training (Bengio et al., 2013).</p>",
            "id": 91,
            "page": 6,
            "text": "We adopt the optimizers and hyper-parameters of van Hasselt  (2015), with the exception of the learning rate which we chose to be slightly lower (we do not do this for double DQN as it can deteriorate its performance). Since both the advantage and the value stream propagate gradients to the last convolutional layer in the backward pass, we rescale the combined gradient entering the last convolutional layer by 1/V2. This simple heuristic mildly increases stability. In addition, we clip the gradients to have their norm less than or equal to 10. This clipping is not standard practice in deep RL, but common in recurrent network training (Bengio , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1914
                },
                {
                    "x": 2264,
                    "y": 1914
                },
                {
                    "x": 2264,
                    "y": 2362
                },
                {
                    "x": 1274,
                    "y": 2362
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:18px'>To isolate the contributions of the dueling architecture, we<br>re-train DDQN with a single stream network using exactly<br>the same procedure as described above. Specifically, we<br>apply gradient clipping, and use 1024 hidden units for the<br>first fully-connected layer of the network SO that both archi-<br>tectures (dueling and single) have roughly the same number<br>of parameters. We refer to this re-trained model as Single<br>Clip, while the original trained model of van Hasselt et al.<br>(2015) is referred to as Single.</p>",
            "id": 92,
            "page": 6,
            "text": "To isolate the contributions of the dueling architecture, we re-train DDQN with a single stream network using exactly the same procedure as described above. Specifically, we apply gradient clipping, and use 1024 hidden units for the first fully-connected layer of the network SO that both architectures (dueling and single) have roughly the same number of parameters. We refer to this re-trained model as Single Clip, while the original trained model of van Hasselt  (2015) is referred to as Single."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2387
                },
                {
                    "x": 2264,
                    "y": 2387
                },
                {
                    "x": 2264,
                    "y": 2636
                },
                {
                    "x": 1274,
                    "y": 2636
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:16px'>As in (van Hasselt et al., 2015), we start the game with up<br>to 30 no-op actions to provide random starting positions for<br>the agent. To evaluate our approach, we measure improve-<br>ment in percentage (positive or negative) in score over the<br>better of human and baseline agent scores:</p>",
            "id": 93,
            "page": 6,
            "text": "As in (van Hasselt , 2015), we start the game with up to 30 no-op actions to provide random starting positions for the agent. To evaluate our approach, we measure improvement in percentage (positive or negative) in score over the better of human and baseline agent scores:"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2773
                },
                {
                    "x": 2262,
                    "y": 2773
                },
                {
                    "x": 2262,
                    "y": 2877
                },
                {
                    "x": 1275,
                    "y": 2877
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>We took the maximum over human and baseline agent<br>scores as it prevents insignificant changes to appear as</p>",
            "id": 94,
            "page": 6,
            "text": "We took the maximum over human and baseline agent scores as it prevents insignificant changes to appear as"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2904
                },
                {
                    "x": 2263,
                    "y": 2904
                },
                {
                    "x": 2263,
                    "y": 2990
                },
                {
                    "x": 1276,
                    "y": 2990
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:14px'>2The number of actions ranges between 3-18 actions in the<br>ALE environment.</p>",
            "id": 95,
            "page": 6,
            "text": "2The number of actions ranges between 3-18 actions in the ALE environment."
        },
        {
            "bounding_box": [
                {
                    "x": 713,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 236
                },
                {
                    "x": 713,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='96' style='font-size:22px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 96,
            "page": 7,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 1123,
                    "y": 292
                },
                {
                    "x": 1191,
                    "y": 292
                },
                {
                    "x": 1191,
                    "y": 310
                },
                {
                    "x": 1123,
                    "y": 310
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:14px'>296.67%</p>",
            "id": 97,
            "page": 7,
            "text": "296.67%"
        },
        {
            "bounding_box": [
                {
                    "x": 241,
                    "y": 280
                },
                {
                    "x": 1154,
                    "y": 280
                },
                {
                    "x": 1154,
                    "y": 1216
                },
                {
                    "x": 241,
                    "y": 1216
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='98' style='font-size:14px' alt=\"Atlantis\nTennis 180.00%\nSpace Invaders 164.11%\nUp and PDown 94.33%\n97.90%\nEnduro 86.35%\nChopper Command 82.20%\nSeaquest 80.51%\nYars' Revenge 73.63%\nFrostbite 70.02%\nTime Pilot 69.73%\nAsterix 63.17%\nRoad Runner 57.57%\nBank Heist 57.19%\nKrull 55.85%\nMs. Pac-Man 53.76%\nStar Gunner 48.92%\nSurround 44.24%\nDouble Dunk 42.75%\nRiver Raid 39.79%\nVenture 33.60%\nAmidar 31.40%\nFishing Derby 28.82%\nQ*Bert 27.68%\nZaxxon 27.45%\nIce Hockey 26.45%\nCrazentipede 21.68%\nClimber 24.68%\nDefender 21.18%\nName This Game 16.28%\nBattle Zone 15.65%\nKung-Fu Master 15.56%\nKangaroo 14.39%\nAlien 10.34%\nBerzerk 9.86%\nBoxing 8.52%\nGopher 6.02%\nGravitar 5.54%\nWizard Of Wor 5.24%\nDemon Attack 4.78%\nAsteroids 4.51%\nH.E.R.O. 2.31%\nSkiing 1.29%\n0.45%\nRobotank 0.32%\nPong 0.24%\nMontezuma's Revenge 0.00%\nPrivate Eye -0.04%\nBowling -1.89%\nTutankham -3.38%\nJames Bond -3.42%\nSolaris -7.37%\nBeam Rider -9.71%\nAssault -14.93%\nBreakout -17.56%\nVideo Pinball -68.31%\nFreeway -100.00%\" data-coord=\"top-left:(241,280); bottom-right:(1154,1216)\" /></figure>",
            "id": 98,
            "page": 7,
            "text": "Atlantis Tennis 180.00% Space Invaders 164.11% Up and PDown 94.33% 97.90% Enduro 86.35% Chopper Command 82.20% Seaquest 80.51% Yars' Revenge 73.63% Frostbite 70.02% Time Pilot 69.73% Asterix 63.17% Road Runner 57.57% Bank Heist 57.19% Krull 55.85% Ms. Pac-Man 53.76% Star Gunner 48.92% Surround 44.24% Double Dunk 42.75% River Raid 39.79% Venture 33.60% Amidar 31.40% Fishing Derby 28.82% Q*Bert 27.68% Zaxxon 27.45% Ice Hockey 26.45% Crazentipede 21.68% Climber 24.68% Defender 21.18% Name This Game 16.28% Battle Zone 15.65% Kung-Fu Master 15.56% Kangaroo 14.39% Alien 10.34% Berzerk 9.86% Boxing 8.52% Gopher 6.02% Gravitar 5.54% Wizard Of Wor 5.24% Demon Attack 4.78% Asteroids 4.51% H.E.R.O. 2.31% Skiing 1.29% 0.45% Robotank 0.32% Pong 0.24% Montezuma's Revenge 0.00% Private Eye -0.04% Bowling -1.89% Tutankham -3.38% James Bond -3.42% Solaris -7.37% Beam Rider -9.71% Assault -14.93% Breakout -17.56% Video Pinball -68.31% Freeway -100.00%"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1286
                },
                {
                    "x": 1214,
                    "y": 1286
                },
                {
                    "x": 1214,
                    "y": 1475
                },
                {
                    "x": 224,
                    "y": 1475
                }
            ],
            "category": "caption",
            "html": "<caption id='99' style='font-size:16px'>Figure 4. Improvements of dueling architecture over the baseline<br>Single network of van Hasselt et al. (2015), using the metric de-<br>scribed in Equation (10). Bars to the right indicate by how much<br>the dueling network outperforms the single-stream network.</caption>",
            "id": 99,
            "page": 7,
            "text": "Figure 4. Improvements of dueling architecture over the baseline Single network of van Hasselt  (2015), using the metric described in Equation (10). Bars to the right indicate by how much the dueling network outperforms the single-stream network."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1597
                },
                {
                    "x": 1213,
                    "y": 1597
                },
                {
                    "x": 1213,
                    "y": 2046
                },
                {
                    "x": 224,
                    "y": 2046
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:20px'>large improvements when neither the agent in question nor<br>the baseline are doing well. For example, an agent that<br>achieves 2% human performance should not be interpreted<br>as two times better when the baseline agent achieves 1%<br>human performance. We also chose not to measure perfor-<br>mance in terms of percentage of human performance alone<br>because a tiny difference relative to the baseline on some<br>games can translate into hundreds of percent in human per-<br>formance difference.</p>",
            "id": 100,
            "page": 7,
            "text": "large improvements when neither the agent in question nor the baseline are doing well. For example, an agent that achieves 2% human performance should not be interpreted as two times better when the baseline agent achieves 1% human performance. We also chose not to measure performance in terms of percentage of human performance alone because a tiny difference relative to the baseline on some games can translate into hundreds of percent in human performance difference."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2071
                },
                {
                    "x": 1212,
                    "y": 2071
                },
                {
                    "x": 1212,
                    "y": 2172
                },
                {
                    "x": 223,
                    "y": 2172
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:18px'>The results for the wide suite of 57 games are summarized<br>in Table 1. Detailed results are presented in the Appendix.</p>",
            "id": 101,
            "page": 7,
            "text": "The results for the wide suite of 57 games are summarized in Table 1. Detailed results are presented in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2196
                },
                {
                    "x": 1213,
                    "y": 2196
                },
                {
                    "x": 1213,
                    "y": 2543
                },
                {
                    "x": 224,
                    "y": 2543
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='102' style='font-size:20px'>Using this 30 no-ops performance measure, it is clear that<br>the dueling network (Duel Clip) does substantially better<br>than the Single Clip network of similar capacity. It also<br>does considerably better than the baseline (Single) of van<br>Hasselt et al. (2015). For comparison we also show results<br>for the deep Q-network of Mnih et al. (2015), referred to as<br>Nature DQN.</p>",
            "id": 102,
            "page": 7,
            "text": "Using this 30 no-ops performance measure, it is clear that the dueling network (Duel Clip) does substantially better than the Single Clip network of similar capacity. It also does considerably better than the baseline (Single) of van Hasselt  (2015). For comparison we also show results for the deep Q-network of Mnih  (2015), referred to as Nature DQN."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2568
                },
                {
                    "x": 1212,
                    "y": 2568
                },
                {
                    "x": 1212,
                    "y": 2768
                },
                {
                    "x": 224,
                    "y": 2768
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:20px'>Figure 4 shows the improvement of the dueling network<br>over the baseline Single network of van Hasselt et al.<br>(2015). Again, we seen that the improvements are often<br>very dramatic.</p>",
            "id": 103,
            "page": 7,
            "text": "Figure 4 shows the improvement of the dueling network over the baseline Single network of van Hasselt  (2015). Again, we seen that the improvements are often very dramatic."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2793
                },
                {
                    "x": 1212,
                    "y": 2793
                },
                {
                    "x": 1212,
                    "y": 2995
                },
                {
                    "x": 223,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:22px'>As shown in Table 1, Single Clip performs better than Sin-<br>gle. We verified that this gain was mostly brought in by<br>gradient clipping. For this reason, we incorporate gradient<br>clipping in all the new approaches.</p>",
            "id": 104,
            "page": 7,
            "text": "As shown in Table 1, Single Clip performs better than Single. We verified that this gain was mostly brought in by gradient clipping. For this reason, we incorporate gradient clipping in all the new approaches."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 313
                },
                {
                    "x": 2261,
                    "y": 313
                },
                {
                    "x": 2261,
                    "y": 403
                },
                {
                    "x": 1277,
                    "y": 403
                }
            ],
            "category": "caption",
            "html": "<br><caption id='105' style='font-size:16px'>Table 1. Mean and median scores across all 57 Atari games, mea-<br>sured in percentages of human performance.</caption>",
            "id": 105,
            "page": 7,
            "text": "Table 1. Mean and median scores across all 57 Atari games, measured in percentages of human performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 420
                },
                {
                    "x": 2255,
                    "y": 420
                },
                {
                    "x": 2255,
                    "y": 765
                },
                {
                    "x": 1275,
                    "y": 765
                }
            ],
            "category": "table",
            "html": "<br><table id='106' style='font-size:16px'><tr><td></td><td colspan=\"2\">30 no-ops</td><td colspan=\"2\">Human Starts</td></tr><tr><td></td><td>Mean</td><td>Median</td><td>Mean</td><td>Median</td></tr><tr><td>Prior. Duel Clip</td><td>591.9%</td><td>172.1%</td><td>567.0%</td><td>115.3%</td></tr><tr><td>Prior. Single</td><td>434.6%</td><td>123.7%</td><td>386.7%</td><td>112.9%</td></tr><tr><td>Duel Clip</td><td>373.1%</td><td>151.5%</td><td>343.8%</td><td>117.1%</td></tr><tr><td>Single Clip</td><td>341.2%</td><td>132.6%</td><td>302.8%</td><td>114.1%</td></tr><tr><td>Single</td><td>307.3%</td><td>117.8%</td><td>332.9%</td><td>110.9%</td></tr><tr><td>Nature DQN</td><td>227.9%</td><td>79.1%</td><td>219.6%</td><td>68.5%</td></tr></table>",
            "id": 106,
            "page": 7,
            "text": "30 no-ops Human Starts   Mean Median Mean Median  Prior. Duel Clip 591.9% 172.1% 567.0% 115.3%  Prior. Single 434.6% 123.7% 386.7% 112.9%  Duel Clip 373.1% 151.5% 343.8% 117.1%  Single Clip 341.2% 132.6% 302.8% 114.1%  Single 307.3% 117.8% 332.9% 110.9%  Nature DQN 227.9% 79.1% 219.6%"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 900
                },
                {
                    "x": 2265,
                    "y": 900
                },
                {
                    "x": 2265,
                    "y": 1400
                },
                {
                    "x": 1273,
                    "y": 1400
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:20px'>Duel Clip does better than Single Clip on 75.4% of the<br>games (43 out of 57). It also achieves higher scores com-<br>pared to the Single baseline on 80.7% (46 out of 57) of the<br>games. Of all the games with 18 actions, Duel Clip is better<br>86.6% of the time (26 out of 30). This is consistent with the<br>findings of the previous section. Overall, our agent (Duel<br>Clip) achieves human level performance on 42 out of 57<br>games. Raw scores for all the games, as well as measure-<br>ments in human performance percentage, are presented in<br>the Appendix.</p>",
            "id": 107,
            "page": 7,
            "text": "Duel Clip does better than Single Clip on 75.4% of the games (43 out of 57). It also achieves higher scores compared to the Single baseline on 80.7% (46 out of 57) of the games. Of all the games with 18 actions, Duel Clip is better 86.6% of the time (26 out of 30). This is consistent with the findings of the previous section. Overall, our agent (Duel Clip) achieves human level performance on 42 out of 57 games. Raw scores for all the games, as well as measurements in human performance percentage, are presented in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1422
                },
                {
                    "x": 2263,
                    "y": 1422
                },
                {
                    "x": 2263,
                    "y": 1724
                },
                {
                    "x": 1273,
                    "y": 1724
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:20px'>Robustness to human starts. One shortcoming of the 30<br>no-ops metric is that an agent does not necessarily have to<br>generalize well to play the Atari games. Due to the deter-<br>ministic nature of the Atari environment, from an unique<br>starting point, an agent could learn to achieve good perfor-<br>mance by simply remembering sequences of actions.</p>",
            "id": 108,
            "page": 7,
            "text": "Robustness to human starts. One shortcoming of the 30 no-ops metric is that an agent does not necessarily have to generalize well to play the Atari games. Due to the deterministic nature of the Atari environment, from an unique starting point, an agent could learn to achieve good performance by simply remembering sequences of actions."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1747
                },
                {
                    "x": 2265,
                    "y": 1747
                },
                {
                    "x": 2265,
                    "y": 2097
                },
                {
                    "x": 1273,
                    "y": 2097
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:18px'>To obtain a more robust measure, we adopt the methodol-<br>ogy of Nair et al. (2015). Specifically, for each game, we<br>use 100 starting points sampled from a human expert's tra-<br>jectory. From each of these points, an evaluation episode<br>is launched for up to 108,000 frames. The agents are eval-<br>uated only on rewards accrued after the starting point. We<br>refer to this metric as Human Starts.</p>",
            "id": 109,
            "page": 7,
            "text": "To obtain a more robust measure, we adopt the methodology of Nair  (2015). Specifically, for each game, we use 100 starting points sampled from a human expert's trajectory. From each of these points, an evaluation episode is launched for up to 108,000 frames. The agents are evaluated only on rewards accrued after the starting point. We refer to this metric as Human Starts."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2120
                },
                {
                    "x": 2262,
                    "y": 2120
                },
                {
                    "x": 2262,
                    "y": 2369
                },
                {
                    "x": 1274,
                    "y": 2369
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:20px'>As shown in Table 1, under the Human Starts metric, Duel<br>Clip once again outperforms the single stream variants. In<br>particular, our agent does better than the Single baseline on<br>70.2% (40 out of 57) games and on games of 18 actions,<br>Duel Clip is 83.3% better (25 out of 30).</p>",
            "id": 110,
            "page": 7,
            "text": "As shown in Table 1, under the Human Starts metric, Duel Clip once again outperforms the single stream variants. In particular, our agent does better than the Single baseline on 70.2% (40 out of 57) games and on games of 18 actions, Duel Clip is 83.3% better (25 out of 30)."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2392
                },
                {
                    "x": 2264,
                    "y": 2392
                },
                {
                    "x": 2264,
                    "y": 2995
                },
                {
                    "x": 1270,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:22px'>Combining with Prioritized Experience Replay. The du-<br>eling architecture can be easily combined with other algo-<br>rithmic improvements. In particular, prioritization of the<br>experience replay has been shown to significantly improve<br>performance of Atari games (Schaul et al., 2016). Further-<br>more, as prioritization and the dueling architecture address<br>very different aspects of the learning process, their combi-<br>nation is promising. So in our final experiment, we inves-<br>tigate the integration of the dueling architecture with pri-<br>oritized experience replay. We use the prioritized variant<br>of DDQN (Prior. Single) as the new baseline algorithm,<br>which replaces with the uniform sampling of the experi-</p>",
            "id": 111,
            "page": 7,
            "text": "Combining with Prioritized Experience Replay. The dueling architecture can be easily combined with other algorithmic improvements. In particular, prioritization of the experience replay has been shown to significantly improve performance of Atari games (Schaul , 2016). Furthermore, as prioritization and the dueling architecture address very different aspects of the learning process, their combination is promising. So in our final experiment, we investigate the integration of the dueling architecture with prioritized experience replay. We use the prioritized variant of DDQN (Prior. Single) as the new baseline algorithm, which replaces with the uniform sampling of the experi-"
        },
        {
            "bounding_box": [
                {
                    "x": 714,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 236
                },
                {
                    "x": 714,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='112' style='font-size:20px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 112,
            "page": 8,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 244,
                    "y": 284
                },
                {
                    "x": 1158,
                    "y": 284
                },
                {
                    "x": 1158,
                    "y": 1217
                },
                {
                    "x": 244,
                    "y": 1217
                }
            ],
            "category": "figure",
            "html": "<figure><img id='113' style='font-size:14px' alt=\"Asterix 1097.02%\nSpace Invaders 457.93%\nPhoenix 281.56%\n223.03%\nWizard Gopwer 178.13%\nUp and Down 113.47%\nYars' Revenge 113.16%\nStar Gunner 98.69%\nBerzerk 83.91%\nFrostbite 70.29%\nVideo Pinball 69.92%\nChopper Command 58.87%\nAssault 51.07%\nBank Heist 43.11%\nRiver Raid 38.56%\nDefender 35.33%\nName This Game 33.09%\nZaxxon 32.74%\nCentipede 32.48%\nBeam Rider 29.94%\nAmidar 24.98%\nKung-Fu-Master Master 22.36%\n21.38%\nCrazy Climber 16.16%\nQ*Bert 15.56%\nBattle Zone 11.46%\nAtlantis 11.16%\nEnduro 10.20%\nKrull 7.95%\nRoad Runner 7.89%\nPitfall! 5.33%\nBoxing 3.46%\nDemon Attack 1.44%\nFishing Derby 1.37%\nPong 0.73%\nPrivate Eye 0.01%\nMontezuma's Revenge 0.00%\n0.00%\nVenture -0.51%\nBowling -0.87%\nFreeway -2.08%\nBreakout -2.12%\nAsteroids 3.13%\nAlien -3.81%\nH.E.R.O. -6.72%\nGravitar -9.77%\nIce Hockey -13.60%\nTime Pilot -29.21%\nSolaris -37.65%\nSurround -40.74%\nMs. Pac-Man -48.03%\nRobotank -58.11%\n-60.56%\n-77.99%\nDouble  -83.56%\nJamesBand -89.22%\n-84.70%\" data-coord=\"top-left:(244,284); bottom-right:(1158,1217)\" /></figure>",
            "id": 113,
            "page": 8,
            "text": "Asterix 1097.02% Space Invaders 457.93% Phoenix 281.56% 223.03% Wizard Gopwer 178.13% Up and Down 113.47% Yars' Revenge 113.16% Star Gunner 98.69% Berzerk 83.91% Frostbite 70.29% Video Pinball 69.92% Chopper Command 58.87% Assault 51.07% Bank Heist 43.11% River Raid 38.56% Defender 35.33% Name This Game 33.09% Zaxxon 32.74% Centipede 32.48% Beam Rider 29.94% Amidar 24.98% Kung-Fu-Master Master 22.36% 21.38% Crazy Climber 16.16% Q*Bert 15.56% Battle Zone 11.46% Atlantis 11.16% Enduro 10.20% Krull 7.95% Road Runner 7.89% Pitfall! 5.33% Boxing 3.46% Demon Attack 1.44% Fishing Derby 1.37% Pong 0.73% Private Eye 0.01% Montezuma's Revenge 0.00% 0.00% Venture -0.51% Bowling -0.87% Freeway -2.08% Breakout -2.12% Asteroids 3.13% Alien -3.81% H.E.R.O. -6.72% Gravitar -9.77% Ice Hockey -13.60% Time Pilot -29.21% Solaris -37.65% Surround -40.74% Ms. Pac-Man -48.03% Robotank -58.11% -60.56% -77.99% Double  -83.56% JamesBand -89.22% -84.70%"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1286
                },
                {
                    "x": 1215,
                    "y": 1286
                },
                {
                    "x": 1215,
                    "y": 1474
                },
                {
                    "x": 224,
                    "y": 1474
                }
            ],
            "category": "caption",
            "html": "<caption id='114' style='font-size:14px'>Figure 5. Improvements of dueling architecture over Prioritized<br>DDQN baseline, using the same metric as Figure 4. Again, the<br>dueling architecture leads to significant improvements over the<br>single-stream baseline on the majority of games.</caption>",
            "id": 114,
            "page": 8,
            "text": "Figure 5. Improvements of dueling architecture over Prioritized DDQN baseline, using the same metric as Figure 4. Again, the dueling architecture leads to significant improvements over the single-stream baseline on the majority of games."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1598
                },
                {
                    "x": 1212,
                    "y": 1598
                },
                {
                    "x": 1212,
                    "y": 1948
                },
                {
                    "x": 223,
                    "y": 1948
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:18px'>ence tuples by rank-based prioritized sampling. We keep<br>all the parameters of the prioritized replay as described<br>in (Schaul et al., 2016), namely a priority exponent of 0.7,<br>and an annealing schedule on the importance sampling ex-<br>ponent from 0.5 to 1. We combine this baseline with our<br>dueling architecture (as above), and again use gradient clip-<br>ping (Prior. Duel Clip).</p>",
            "id": 115,
            "page": 8,
            "text": "ence tuples by rank-based prioritized sampling. We keep all the parameters of the prioritized replay as described in (Schaul , 2016), namely a priority exponent of 0.7, and an annealing schedule on the importance sampling exponent from 0.5 to 1. We combine this baseline with our dueling architecture (as above), and again use gradient clipping (Prior. Duel Clip)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1969
                },
                {
                    "x": 1212,
                    "y": 1969
                },
                {
                    "x": 1212,
                    "y": 2471
                },
                {
                    "x": 223,
                    "y": 2471
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:18px'>Note that, although orthogonal in their objectives, these<br>extensions (prioritization, dueling and gradient clipping)<br>interact in subtle ways. For example, prioritization inter-<br>acts with gradient clipping, as sampling transitions with<br>high absolute TD-errors more often leads to gradients with<br>higher norms. To avoid adverse interactions, we roughly<br>re-tuned the learning rate and the gradient clipping norm on<br>a subset of 9 games. As a result of rough tuning, we settled<br>on 6.25 x 10-5 for the learning rate and 10 for the gradient<br>clipping norm (the same as in the previous section).</p>",
            "id": 116,
            "page": 8,
            "text": "Note that, although orthogonal in their objectives, these extensions (prioritization, dueling and gradient clipping) interact in subtle ways. For example, prioritization interacts with gradient clipping, as sampling transitions with high absolute TD-errors more often leads to gradients with higher norms. To avoid adverse interactions, we roughly re-tuned the learning rate and the gradient clipping norm on a subset of 9 games. As a result of rough tuning, we settled on 6.25 x 10-5 for the learning rate and 10 for the gradient clipping norm (the same as in the previous section)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2492
                },
                {
                    "x": 1213,
                    "y": 2492
                },
                {
                    "x": 1213,
                    "y": 2996
                },
                {
                    "x": 223,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:18px'>When evaluated on all 57 Atari games, our prioritized du-<br>eling agent performs significantly better than both the pri-<br>oritized baseline agent and the dueling agent alone. The<br>full mean and median performance against the human per-<br>formance percentage is shown in Table 1. When initializ-<br>ing the games using up to 30 no-ops action, we observe<br>mean and median scores of 591% and 172% respectively.<br>The direct comparison between the prioritized baseline and<br>prioritized dueling versions, using the metric described in<br>Equation 10, is presented in Figure 5.</p>",
            "id": 117,
            "page": 8,
            "text": "When evaluated on all 57 Atari games, our prioritized dueling agent performs significantly better than both the prioritized baseline agent and the dueling agent alone. The full mean and median performance against the human performance percentage is shown in Table 1. When initializing the games using up to 30 no-ops action, we observe mean and median scores of 591% and 172% respectively. The direct comparison between the prioritized baseline and prioritized dueling versions, using the metric described in Equation 10, is presented in Figure 5."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 283
                },
                {
                    "x": 2262,
                    "y": 283
                },
                {
                    "x": 2262,
                    "y": 434
                },
                {
                    "x": 1273,
                    "y": 434
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:18px'>The combination of prioritized replay and the dueling net-<br>work results in vast improvements over the previous state-<br>of-the-art in the popular ALE benchmark.</p>",
            "id": 118,
            "page": 8,
            "text": "The combination of prioritized replay and the dueling network results in vast improvements over the previous stateof-the-art in the popular ALE benchmark."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 456
                },
                {
                    "x": 2264,
                    "y": 456
                },
                {
                    "x": 2264,
                    "y": 1007
                },
                {
                    "x": 1271,
                    "y": 1007
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>Saliency maps. To better understand the roles of the value<br>and the advantage streams, we compute saliency maps (Si-<br>monyan et al., 2013). More specifically, to visualize the<br>salient part of the image as seen by the value stream, we<br>compute the absolute value of the Jacobian of V with re-<br>spect to the input frames: 1▼ s V (s; 0)|. Similarly, to visu-<br>alize the salient part of the image as seen by the advan-<br>tage stream, we compute |▽ s A(s, arg max�, A(s, a'); 0)1.<br>Both quantities are of the same dimensionality as the input<br>frames and therefore can be visualized easily alongside the<br>input frames.</p>",
            "id": 119,
            "page": 8,
            "text": "Saliency maps. To better understand the roles of the value and the advantage streams, we compute saliency maps (Simonyan , 2013). More specifically, to visualize the salient part of the image as seen by the value stream, we compute the absolute value of the Jacobian of V with respect to the input frames: 1▼ s V (s; 0)|. Similarly, to visualize the salient part of the image as seen by the advantage stream, we compute |▽ s A(s, arg max�, A(s, a'); 0)1. Both quantities are of the same dimensionality as the input frames and therefore can be visualized easily alongside the input frames."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1031
                },
                {
                    "x": 2265,
                    "y": 1031
                },
                {
                    "x": 2265,
                    "y": 1531
                },
                {
                    "x": 1273,
                    "y": 1531
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:16px'>Here, we place the gray scale input frames in the green and<br>blue channel and the saliency maps in the red channel. All<br>three channels together form an RGB image. Figure 2 de-<br>picts the value and advantage saliency maps on the Enduro<br>game for two different time steps. As observed in the in-<br>troduction, the value stream pays attention to the horizon<br>where the appearance of a car could affect future perfor-<br>mance. The value stream also pays attention to the score.<br>The advantage stream, on the other hand, cares more about<br>cars that are on an immediate collision course.</p>",
            "id": 120,
            "page": 8,
            "text": "Here, we place the gray scale input frames in the green and blue channel and the saliency maps in the red channel. All three channels together form an RGB image. Figure 2 depicts the value and advantage saliency maps on the Enduro game for two different time steps. As observed in the introduction, the value stream pays attention to the horizon where the appearance of a car could affect future performance. The value stream also pays attention to the score. The advantage stream, on the other hand, cares more about cars that are on an immediate collision course."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1592
                },
                {
                    "x": 1559,
                    "y": 1592
                },
                {
                    "x": 1559,
                    "y": 1647
                },
                {
                    "x": 1274,
                    "y": 1647
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:22px'>5. Discussion</p>",
            "id": 121,
            "page": 8,
            "text": "5. Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1677
                },
                {
                    "x": 2264,
                    "y": 1677
                },
                {
                    "x": 2264,
                    "y": 2428
                },
                {
                    "x": 1270,
                    "y": 2428
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>The advantage of the dueling architecture lies partly in its<br>ability to learn the state-value function efficiently. With<br>every update of the Q values in the dueling architecture,<br>the value stream V is updated - this contrasts with the up-<br>dates in a single-stream architecture where only the value<br>for one of the actions is updated, the values for all other<br>actions remain untouched. This more frequent updating of<br>the value stream in our approach allocates more resources<br>to V, and thus allows for better approximation of the state<br>values, which in turn need to be accurate for temporal-<br>difference-based methods like Q-learning to work (Sutton<br>& Barto, 1998). This phenomenon is reflected in the ex-<br>periments, where the advantage of the dueling architecture<br>over single-stream Q networks grows when the number of<br>actions is large.</p>",
            "id": 122,
            "page": 8,
            "text": "The advantage of the dueling architecture lies partly in its ability to learn the state-value function efficiently. With every update of the Q values in the dueling architecture, the value stream V is updated - this contrasts with the updates in a single-stream architecture where only the value for one of the actions is updated, the values for all other actions remain untouched. This more frequent updating of the value stream in our approach allocates more resources to V, and thus allows for better approximation of the state values, which in turn need to be accurate for temporaldifference-based methods like Q-learning to work (Sutton & Barto, 1998). This phenomenon is reflected in the experiments, where the advantage of the dueling architecture over single-stream Q networks grows when the number of actions is large."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2450
                },
                {
                    "x": 2265,
                    "y": 2450
                },
                {
                    "x": 2265,
                    "y": 2955
                },
                {
                    "x": 1271,
                    "y": 2955
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:18px'>Furthermore, the differences between Q-values for a given<br>state are often very small relative to the magnitude of Q.<br>For example, after training with DDQN on the game of<br>Seaquest, the average action gap (the gap between the Q<br>values of the best and the second best action in a given<br>state) across visited states is roughly 0.04, whereas the av-<br>erage state value across those states is about 15. This differ-<br>ence in scales can lead to small amounts of noise in the up-<br>dates can lead to reorderings of the actions, and thus make<br>the nearly greedy policy switch abruptly. The dueling ar-</p>",
            "id": 123,
            "page": 8,
            "text": "Furthermore, the differences between Q-values for a given state are often very small relative to the magnitude of Q. For example, after training with DDQN on the game of Seaquest, the average action gap (the gap between the Q values of the best and the second best action in a given state) across visited states is roughly 0.04, whereas the average state value across those states is about 15. This difference in scales can lead to small amounts of noise in the updates can lead to reorderings of the actions, and thus make the nearly greedy policy switch abruptly. The dueling ar-"
        },
        {
            "bounding_box": [
                {
                    "x": 714,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 236
                },
                {
                    "x": 714,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='124' style='font-size:16px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 124,
            "page": 9,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 286
                },
                {
                    "x": 1213,
                    "y": 286
                },
                {
                    "x": 1213,
                    "y": 383
                },
                {
                    "x": 224,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:14px'>chitecture with its separate advantage stream is robust to<br>such effects.</p>",
            "id": 125,
            "page": 9,
            "text": "chitecture with its separate advantage stream is robust to such effects."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 447
                },
                {
                    "x": 543,
                    "y": 447
                },
                {
                    "x": 543,
                    "y": 501
                },
                {
                    "x": 224,
                    "y": 501
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:20px'>6. Conclusions</p>",
            "id": 126,
            "page": 9,
            "text": "6. Conclusions"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 531
                },
                {
                    "x": 1213,
                    "y": 531
                },
                {
                    "x": 1213,
                    "y": 933
                },
                {
                    "x": 223,
                    "y": 933
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:14px'>We introduced a new neural network architecture that de-<br>couples value and advantage in deep Q-networks, while<br>sharing a common feature learning module. The new duel-<br>ing architecture, in combination with some algorithmic im-<br>provements, leads to dramatic improvements over existing<br>approaches for deep RL in the challenging Atari domain.<br>The results presented in this paper are the new state-of-the-<br>art in this popular domain.</p>",
            "id": 127,
            "page": 9,
            "text": "We introduced a new neural network architecture that decouples value and advantage in deep Q-networks, while sharing a common feature learning module. The new dueling architecture, in combination with some algorithmic improvements, leads to dramatic improvements over existing approaches for deep RL in the challenging Atari domain. The results presented in this paper are the new state-of-theart in this popular domain."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 994
                },
                {
                    "x": 468,
                    "y": 994
                },
                {
                    "x": 468,
                    "y": 1049
                },
                {
                    "x": 225,
                    "y": 1049
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:22px'>References</p>",
            "id": 128,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1075
                },
                {
                    "x": 1213,
                    "y": 1075
                },
                {
                    "x": 1213,
                    "y": 1175
                },
                {
                    "x": 225,
                    "y": 1175
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:18px'>Ba, J., Mnih, V., and Kavukcuoglu, K. Multiple object<br>recognition with visual attention. In ICLR, 2015.</p>",
            "id": 129,
            "page": 9,
            "text": "Ba, J., Mnih, V., and Kavukcuoglu, K. Multiple object recognition with visual attention. In ICLR, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1196
                },
                {
                    "x": 1210,
                    "y": 1196
                },
                {
                    "x": 1210,
                    "y": 1295
                },
                {
                    "x": 225,
                    "y": 1295
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='130' style='font-size:18px'>Baird, L.C. Advantage updating. Technical Report WL-<br>TR-93-1146, Wright-Patterson Air Force Base, 1993.</p>",
            "id": 130,
            "page": 9,
            "text": "Baird, L.C. Advantage updating. Technical Report WLTR-93-1146, Wright-Patterson Air Force Base, 1993."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1316
                },
                {
                    "x": 1212,
                    "y": 1316
                },
                {
                    "x": 1212,
                    "y": 1514
                },
                {
                    "x": 225,
                    "y": 1514
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:18px'>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.<br>The arcade learning environment: An evaluation plat-<br>form for general agents. Journal of Artificial Intelligence<br>Research, 47:253-279, 2013.</p>",
            "id": 131,
            "page": 9,
            "text": "Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1537
                },
                {
                    "x": 1213,
                    "y": 1537
                },
                {
                    "x": 1213,
                    "y": 1686
                },
                {
                    "x": 225,
                    "y": 1686
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:16px'>Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P. S.,<br>and Munos, R. Increasing the action gap: New operators<br>for reinforcement learning. In AAAI, 2016. To appear.</p>",
            "id": 132,
            "page": 9,
            "text": "Bellemare, M. G., Ostrovski, G., Guez, A., Thomas, P. S., and Munos, R. Increasing the action gap: New operators for reinforcement learning. In AAAI, 2016. To appear."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1706
                },
                {
                    "x": 1210,
                    "y": 1706
                },
                {
                    "x": 1210,
                    "y": 1856
                },
                {
                    "x": 225,
                    "y": 1856
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:16px'>Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R.<br>Advances in optimizing recurrent networks. In ICASSP,<br>pp. 8624-8628, 2013.</p>",
            "id": 133,
            "page": 9,
            "text": "Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. Advances in optimizing recurrent networks. In ICASSP, pp. 8624-8628, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1878
                },
                {
                    "x": 1213,
                    "y": 1878
                },
                {
                    "x": 1213,
                    "y": 2073
                },
                {
                    "x": 224,
                    "y": 2073
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:18px'>Fukushima, K. Neocognitron: A self-organizing neural<br>network model for a mechanism of pattern recognition<br>unaffected by shift in position. Biological Cybernetics,<br>36:193-202, 1980.</p>",
            "id": 134,
            "page": 9,
            "text": "Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36:193-202, 1980."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2098
                },
                {
                    "x": 1211,
                    "y": 2098
                },
                {
                    "x": 1211,
                    "y": 2293
                },
                {
                    "x": 224,
                    "y": 2293
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='135' style='font-size:18px'>Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X.<br>Deep learning for real-time Atari game play using offline<br>Monte-Carlo tree search planning. In NIPS, pp. 3338-<br>3346. 2014.</p>",
            "id": 135,
            "page": 9,
            "text": "Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning. In NIPS, pp. 33383346. 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2319
                },
                {
                    "x": 1211,
                    "y": 2319
                },
                {
                    "x": 1211,
                    "y": 2513
                },
                {
                    "x": 224,
                    "y": 2513
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:18px'>Harmon, M.E. and Baird, L.C. Multi-player residual ad-<br>vantage learning with general function approximation.<br>Technical Report WL-TR-1065, Wright-Patterson Air<br>Force Base, 1996.</p>",
            "id": 136,
            "page": 9,
            "text": "Harmon, M.E. and Baird, L.C. Multi-player residual advantage learning with general function approximation. Technical Report WL-TR-1065, Wright-Patterson Air Force Base, 1996."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2537
                },
                {
                    "x": 1211,
                    "y": 2537
                },
                {
                    "x": 1211,
                    "y": 2684
                },
                {
                    "x": 224,
                    "y": 2684
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:18px'>Harmon, M.E., Baird, L.C., and Klopf, A.H. Advantage<br>updating applied to a differential game. In G. Tesauro,<br>D.S. Touretzky and Leen, T.K. (eds.), NIPS, 1995.</p>",
            "id": 137,
            "page": 9,
            "text": "Harmon, M.E., Baird, L.C., and Klopf, A.H. Advantage updating applied to a differential game. In G. Tesauro, D.S. Touretzky and Leen, T.K. (eds.), NIPS, 1995."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2707
                },
                {
                    "x": 1210,
                    "y": 2707
                },
                {
                    "x": 1210,
                    "y": 2804
                },
                {
                    "x": 222,
                    "y": 2804
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:18px'>LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na-<br>ture, 521(7553):436-444, 2015.</p>",
            "id": 138,
            "page": 9,
            "text": "LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature, 521(7553):436-444, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2827
                },
                {
                    "x": 1212,
                    "y": 2827
                },
                {
                    "x": 1212,
                    "y": 2976
                },
                {
                    "x": 223,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:18px'>Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-<br>end training of deep visuomotor policies. arXiv preprint<br>arXiv:1504.00702, 2015.</p>",
            "id": 139,
            "page": 9,
            "text": "Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-toend training of deep visuomotor policies. arXiv preprint arXiv:1504.00702, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 283
                },
                {
                    "x": 2263,
                    "y": 283
                },
                {
                    "x": 2263,
                    "y": 430
                },
                {
                    "x": 1271,
                    "y": 430
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:18px'>Lin, L.J. Reinforcement learning for robots using neu-<br>ral networks. PhD thesis, School of Computer Science,<br>Carnegie Mellon University, 1993.</p>",
            "id": 140,
            "page": 9,
            "text": "Lin, L.J. Reinforcement learning for robots using neural networks. PhD thesis, School of Computer Science, Carnegie Mellon University, 1993."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 453
                },
                {
                    "x": 2263,
                    "y": 453
                },
                {
                    "x": 2263,
                    "y": 602
                },
                {
                    "x": 1273,
                    "y": 602
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:16px'>Maddison, C. J., Huang, A., Sutskever, I., and Silver, D.<br>Move Evaluation in Go Using Deep Convolutional Neu-<br>ral Networks. In ICLR, 2015.</p>",
            "id": 141,
            "page": 9,
            "text": "Maddison, C. J., Huang, A., Sutskever, I., and Silver, D. Move Evaluation in Go Using Deep Convolutional Neural Networks. In ICLR, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 624
                },
                {
                    "x": 2264,
                    "y": 624
                },
                {
                    "x": 2264,
                    "y": 971
                },
                {
                    "x": 1274,
                    "y": 971
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:18px'>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-<br>ness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,<br>Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,<br>Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wier-<br>stra, D., Legg, S., and Hassabis, D. Human-level con-<br>trol through deep reinforcement learning. Nature, 518<br>(7540):529-533, 2015.</p>",
            "id": 142,
            "page": 9,
            "text": "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518 (7540):529-533, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 993
                },
                {
                    "x": 2264,
                    "y": 993
                },
                {
                    "x": 2264,
                    "y": 1292
                },
                {
                    "x": 1272,
                    "y": 1292
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:18px'>Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C.,<br>Fearon, R., Maria, A. De, Panneershelvam, V., Suley-<br>man, M., Beattie, C., Petersen, S., Legg, S., Mnih,<br>V., Kavukcuoglu, K., and Silver, D. Massively paral-<br>lel methods for deep reinforcement learning. In Deep<br>Learning Workshop, ICML, 2015.</p>",
            "id": 143,
            "page": 9,
            "text": "Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., Maria, A. De, Panneershelvam, V., Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih, V., Kavukcuoglu, K., and Silver, D. Massively parallel methods for deep reinforcement learning. In Deep Learning Workshop, ICML, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1314
                },
                {
                    "x": 2259,
                    "y": 1314
                },
                {
                    "x": 2259,
                    "y": 1412
                },
                {
                    "x": 1276,
                    "y": 1412
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:18px'>Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-<br>tized experience replay. In ICLR, 2016.</p>",
            "id": 144,
            "page": 9,
            "text": "Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. In ICLR, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1436
                },
                {
                    "x": 2263,
                    "y": 1436
                },
                {
                    "x": 2263,
                    "y": 1631
                },
                {
                    "x": 1275,
                    "y": 1631
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='145' style='font-size:18px'>Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and<br>Abbeel, P. High-dimensional continuous control us-<br>ing generalized advantage estimation. arXiv preprint<br>arXiv:1506.02438, 2015.</p>",
            "id": 145,
            "page": 9,
            "text": "Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1654
                },
                {
                    "x": 2263,
                    "y": 1654
                },
                {
                    "x": 2263,
                    "y": 2048
                },
                {
                    "x": 1273,
                    "y": 2048
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='146' style='font-size:18px'>Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L.,<br>van den Driessche, G., Schrittwieser, J., Antonoglou, I.,<br>Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe,<br>D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,<br>T., Leach, M., Kavukcuoglu, K., Graepel, T., and Has-<br>sabis, D. Mastering the game of go with deep neural<br>networks and tree search. Nature, 529(7587):484-489,<br>01 2016.</p>",
            "id": 146,
            "page": 9,
            "text": "Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 01 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2073
                },
                {
                    "x": 2262,
                    "y": 2073
                },
                {
                    "x": 2262,
                    "y": 2270
                },
                {
                    "x": 1273,
                    "y": 2270
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='147' style='font-size:18px'>Simonyan, K., Vedaldi, A., and Zisserman, A. Deep in-<br>side convolutional networks: Visualising image clas-<br>sification models and saliency maps. arXiv preprint<br>arXiv:1312.6034, 2013.</p>",
            "id": 147,
            "page": 9,
            "text": "Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2294
                },
                {
                    "x": 2263,
                    "y": 2294
                },
                {
                    "x": 2263,
                    "y": 2441
                },
                {
                    "x": 1273,
                    "y": 2441
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:18px'>Stadie, B. C., Levine, S., and Abbeel, P. Incentivizing ex-<br>ploration in reinforcement learning with deep predictive<br>models. arXiv preprint arXiv:1507.00814, 2015.</p>",
            "id": 148,
            "page": 9,
            "text": "Stadie, B. C., Levine, S., and Abbeel, P. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2464
                },
                {
                    "x": 2260,
                    "y": 2464
                },
                {
                    "x": 2260,
                    "y": 2561
                },
                {
                    "x": 1274,
                    "y": 2561
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='149' style='font-size:14px'>Sutton, R. S. and Barto, A. G. Introduction to reinforce-<br>ment learning. MIT Press, 1998.</p>",
            "id": 149,
            "page": 9,
            "text": "Sutton, R. S. and Barto, A. G. Introduction to reinforcement learning. MIT Press, 1998."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2585
                },
                {
                    "x": 2261,
                    "y": 2585
                },
                {
                    "x": 2261,
                    "y": 2731
                },
                {
                    "x": 1273,
                    "y": 2731
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:18px'>Sutton, R. S., Mcallester, D., Singh, S., and Mansour, Y.<br>Policy gradient methods for reinforcement learning with<br>function approximation. In NIPS, pp. 1057-1063, 2000.</p>",
            "id": 150,
            "page": 9,
            "text": "Sutton, R. S., Mcallester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pp. 1057-1063, 2000."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2758
                },
                {
                    "x": 2260,
                    "y": 2758
                },
                {
                    "x": 2260,
                    "y": 2847
                },
                {
                    "x": 1275,
                    "y": 2847
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>van Hasselt, H. Double Q-learning. NIPS, 23:2613-2621,<br>2010.</p>",
            "id": 151,
            "page": 9,
            "text": "van Hasselt, H. Double Q-learning. NIPS, 23:2613-2621, 2010."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2877
                },
                {
                    "x": 2264,
                    "y": 2877
                },
                {
                    "x": 2264,
                    "y": 2977
                },
                {
                    "x": 1274,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-<br>ment learning with double Q-learning. arXiv preprint</p>",
            "id": 152,
            "page": 9,
            "text": "van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double Q-learning. arXiv preprint"
        },
        {
            "bounding_box": [
                {
                    "x": 715,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 191
                },
                {
                    "x": 1771,
                    "y": 236
                },
                {
                    "x": 715,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='153' style='font-size:14px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 153,
            "page": 10,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 265,
                    "y": 286
                },
                {
                    "x": 694,
                    "y": 286
                },
                {
                    "x": 694,
                    "y": 329
                },
                {
                    "x": 265,
                    "y": 329
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>arXiv:1509.06461, 2015.</p>",
            "id": 154,
            "page": 10,
            "text": "arXiv:1509.06461, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 356
                },
                {
                    "x": 1213,
                    "y": 356
                },
                {
                    "x": 1213,
                    "y": 601
                },
                {
                    "x": 224,
                    "y": 601
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:14px'>van Seijen, H., van Hasselt, H., Whiteson, S., and Wier-<br>ing, M. A theoretical and empirical analysis of Expected<br>Sarsa. In IEEE Symposium on Adaptive Dynamic Pro-<br>gramming and Reinforcement Learning, pp. 177-184.<br>2009.</p>",
            "id": 155,
            "page": 10,
            "text": "van Seijen, H., van Hasselt, H., Whiteson, S., and Wiering, M. A theoretical and empirical analysis of Expected Sarsa. In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, pp. 177-184. 2009."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 624
                },
                {
                    "x": 1214,
                    "y": 624
                },
                {
                    "x": 1214,
                    "y": 823
                },
                {
                    "x": 225,
                    "y": 823
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:14px'>Watter, M., Springenberg, J. T., Boedecker, J., and Ried-<br>miller, M. A. Embed to control: A locally linear latent<br>dynamics model for control from raw images. In NIPS,<br>2015.</p>",
            "id": 156,
            "page": 10,
            "text": "Watter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. A. Embed to control: A locally linear latent dynamics model for control from raw images. In NIPS, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 715,
                    "y": 190
                },
                {
                    "x": 1770,
                    "y": 190
                },
                {
                    "x": 1770,
                    "y": 236
                },
                {
                    "x": 715,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='157' style='font-size:20px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 157,
            "page": 11,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 278
                },
                {
                    "x": 805,
                    "y": 278
                },
                {
                    "x": 805,
                    "y": 334
                },
                {
                    "x": 225,
                    "y": 334
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:22px'>A. Double DQN Algorithm</p>",
            "id": 158,
            "page": 11,
            "text": "A. Double DQN Algorithm"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 402
                },
                {
                    "x": 890,
                    "y": 402
                },
                {
                    "x": 890,
                    "y": 451
                },
                {
                    "x": 225,
                    "y": 451
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:22px'>Algorithm 1: Double DQN Algorithm.</p>",
            "id": 159,
            "page": 11,
            "text": "Algorithm 1: Double DQN Algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 469
                },
                {
                    "x": 1850,
                    "y": 469
                },
                {
                    "x": 1850,
                    "y": 598
                },
                {
                    "x": 223,
                    "y": 598
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:14px'>input : D - empty replay buffer; 0 - initial network parameters, 0- - copy of 0<br>input : Nr - replay buffer maximum size; No - training batch size; N - - target network replacement freq.<br>for episode e E {1, 2, . . , M } do</p>",
            "id": 160,
            "page": 11,
            "text": "input : D - empty replay buffer; 0 - initial network parameters, 0- - copy of 0 input : Nr - replay buffer maximum size; No - training batch size; N - - target network replacement freq. for episode e E {1, 2, . . , M } do"
        },
        {
            "bounding_box": [
                {
                    "x": 271,
                    "y": 550
                },
                {
                    "x": 799,
                    "y": 550
                },
                {
                    "x": 799,
                    "y": 677
                },
                {
                    "x": 271,
                    "y": 677
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:14px'>Initialize frame sequence x ← ()<br>for t E {0, 1, · · · } do</p>",
            "id": 161,
            "page": 11,
            "text": "Initialize frame sequence x ← () for t E {0, 1, · · · } do"
        },
        {
            "bounding_box": [
                {
                    "x": 347,
                    "y": 678
                },
                {
                    "x": 1808,
                    "y": 678
                },
                {
                    "x": 1808,
                    "y": 865
                },
                {
                    "x": 347,
                    "y": 865
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='162' style='font-size:16px'>Set state s ← x, sample action a ~ �B<br>Sample next frame xt from environment 3 given (s, a) and receive reward r, and append xt to x<br>if |x| > Nf then delete oldest frame Xtmin x end<br>from<br>Set s' ← X, and add transition tuple (s, a,r, s') to D,</p>",
            "id": 162,
            "page": 11,
            "text": "Set state s ← x, sample action a ~ �B Sample next frame xt from environment 3 given (s, a) and receive reward r, and append xt to x if |x| > Nf then delete oldest frame Xtmin x end from Set s' ← X, and add transition tuple (s, a,r, s') to D,"
        },
        {
            "bounding_box": [
                {
                    "x": 436,
                    "y": 852
                },
                {
                    "x": 1016,
                    "y": 852
                },
                {
                    "x": 1016,
                    "y": 896
                },
                {
                    "x": 436,
                    "y": 896
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='163' style='font-size:20px'>replacing the oldest tuple if |D| ≥ Nr</p>",
            "id": 163,
            "page": 11,
            "text": "replacing the oldest tuple if |D| ≥ Nr"
        },
        {
            "bounding_box": [
                {
                    "x": 347,
                    "y": 898
                },
                {
                    "x": 1207,
                    "y": 898
                },
                {
                    "x": 1207,
                    "y": 1032
                },
                {
                    "x": 347,
                    "y": 1032
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:18px'>Sample a minibatch of Nb tuples (s, a, r, s') ~ Unif(D)<br>Construct target values, one for each of the Nb tuples:<br>max (s'; 0) = arg maxa, Q(s' a'; 0)<br>Define a</p>",
            "id": 164,
            "page": 11,
            "text": "Sample a minibatch of Nb tuples (s, a, r, s') ~ Unif(D) Construct target values, one for each of the Nb tuples: max (s'; 0) = arg maxa, Q(s' a'; 0) Define a"
        },
        {
            "bounding_box": [
                {
                    "x": 350,
                    "y": 1035
                },
                {
                    "x": 1214,
                    "y": 1035
                },
                {
                    "x": 1214,
                    "y": 1123
                },
                {
                    "x": 350,
                    "y": 1123
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:16px'>r if s' is terminal<br>Yj =<br>r 十 YQ(s', amax (s'; 0); 0-), otherwise.</p>",
            "id": 165,
            "page": 11,
            "text": "r if s' is terminal Yj = r 十 YQ(s', amax (s'; 0); 0-), otherwise."
        },
        {
            "bounding_box": [
                {
                    "x": 349,
                    "y": 1129
                },
                {
                    "x": 1189,
                    "y": 1129
                },
                {
                    "x": 1189,
                    "y": 1224
                },
                {
                    "x": 349,
                    "y": 1224
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='166' style='font-size:18px'>Do a gradient descent step with loss ||yj - Q(s, a; �)||2<br>Replace target parameters 0- ← 0 every N- steps</p>",
            "id": 166,
            "page": 11,
            "text": "Do a gradient descent step with loss ||yj - Q(s, a; �)||2 Replace target parameters 0- ← 0 every N- steps"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1223
                },
                {
                    "x": 355,
                    "y": 1223
                },
                {
                    "x": 355,
                    "y": 1262
                },
                {
                    "x": 287,
                    "y": 1262
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='167' style='font-size:16px'>end</p>",
            "id": 167,
            "page": 11,
            "text": "end"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1272
                },
                {
                    "x": 292,
                    "y": 1272
                },
                {
                    "x": 292,
                    "y": 1306
                },
                {
                    "x": 225,
                    "y": 1306
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:16px'>end</p>",
            "id": 168,
            "page": 11,
            "text": "end"
        },
        {
            "bounding_box": [
                {
                    "x": 713,
                    "y": 190
                },
                {
                    "x": 1772,
                    "y": 190
                },
                {
                    "x": 1772,
                    "y": 237
                },
                {
                    "x": 713,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='169' style='font-size:20px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 169,
            "page": 12,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 717,
                    "y": 469
                },
                {
                    "x": 1770,
                    "y": 469
                },
                {
                    "x": 1770,
                    "y": 516
                },
                {
                    "x": 717,
                    "y": 516
                }
            ],
            "category": "caption",
            "html": "<caption id='170' style='font-size:14px'>Table 2. Raw scores across all games. Starting with 30 no-op actions.</caption>",
            "id": 170,
            "page": 12,
            "text": "Table 2. Raw scores across all games. Starting with 30 no-op actions."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 528
                },
                {
                    "x": 2350,
                    "y": 528
                },
                {
                    "x": 2350,
                    "y": 2969
                },
                {
                    "x": 219,
                    "y": 2969
                }
            ],
            "category": "table",
            "html": "<br><table id='171' style='font-size:16px'><tr><td>GAMES</td><td>No. ACTIONS</td><td>RANDOM 533.4 0.0</td><td>HUMAN</td><td>DQN</td><td>DDQN</td><td>DUEL</td><td>PRIOR.</td><td>PRIOR. DUEL.</td></tr><tr><td>Alien</td><td>18</td><td>227.8</td><td>7,127.7</td><td>1,620.0</td><td>3,747.7</td><td>4,461.4</td><td>4,203.8</td><td>3,941.0</td></tr><tr><td>Amidar</td><td>10</td><td>5.8</td><td>1,719.5</td><td>978.0</td><td>1,793.3</td><td>2,354.5</td><td>1,838.9</td><td>2,296.8</td></tr><tr><td>Assault</td><td>7</td><td>222.4</td><td>742.0</td><td>4,280.4</td><td>5,393.2</td><td>4,621.0</td><td>7,672.1</td><td>11,477.0</td></tr><tr><td>Asterix</td><td>9</td><td>210.0</td><td>8,503.3</td><td>4,359.0</td><td>17,356.5</td><td>28,188.0</td><td>31,527.0</td><td>375,080.0</td></tr><tr><td>Asteroids</td><td>14</td><td>719.1</td><td>47,388.7</td><td>1,364.5</td><td>734.7</td><td>2,837.7</td><td>2,654.3</td><td>1,192.7</td></tr><tr><td>Atlantis</td><td>4</td><td>12,850.0</td><td>29,028.1</td><td>279,987.0</td><td>106,056.0</td><td>382,572.0</td><td>357,324.0</td><td>395,762.0</td></tr><tr><td>Bank Heist</td><td>18</td><td>14.2</td><td>753.1</td><td>455.0</td><td>1,030.6</td><td>1,611.9</td><td>1,054.6</td><td>1,503.1</td></tr><tr><td>Battle Zone</td><td>18</td><td>2,360.0</td><td>37,187.5</td><td>29,900.0</td><td>31,700.0</td><td>37,150.0</td><td>31,530.0</td><td>35,520.0</td></tr><tr><td>Beam Rider</td><td>9</td><td>363.9</td><td>16,926.5</td><td>8,627.5</td><td>13,772.8</td><td>12,164.0</td><td>23,384.2</td><td>30,276.5</td></tr><tr><td>Berzerk</td><td>18</td><td>123.7</td><td>2,630.4</td><td>585.6</td><td>1,225.4</td><td>1,472.6</td><td>1,305.6</td><td>3,409.0</td></tr><tr><td>Bowling</td><td>6</td><td>23.1</td><td>160.7</td><td>50.4</td><td>68.1</td><td>65.5</td><td>47.9</td><td>46.7</td></tr><tr><td>Boxing</td><td>18</td><td>0.1</td><td>12.1</td><td>88.0</td><td>91.6</td><td>99.4</td><td>95.6</td><td>98.9</td></tr><tr><td>Breakout</td><td>4</td><td>1.7</td><td>30.5</td><td>385.5</td><td>418.5</td><td>345.3</td><td>373.9</td><td>366.0</td></tr><tr><td>Centipede 11.4</td><td>18</td><td>2,090.9</td><td>12,017.0</td><td>4,657.7</td><td>5,409.4</td><td>7,561.4</td><td>4,463.2</td><td>7,687.5</td></tr><tr><td>Chopper Command</td><td>18</td><td>811.0</td><td>7,387.8</td><td>6,126.0</td><td>5,809.0</td><td>11,215.0</td><td>8,600.0</td><td>13,185.0</td></tr><tr><td>Crazy Climber</td><td>9</td><td>10,780.5</td><td>35,829.4</td><td>110,763.0</td><td>117,282.0</td><td>143,570.0</td><td>141,161.0</td><td>162,224.0</td></tr><tr><td>Defender</td><td>18</td><td>2,874.5</td><td>18,688.9</td><td>23,633.0</td><td>35,338.5</td><td>42,214.0</td><td>31,286.5</td><td>41,324.5</td></tr><tr><td>Demon Attack</td><td>6</td><td>152.1</td><td>1,971.0</td><td>12,149.4</td><td>58,044.2</td><td>60,813.3</td><td>71,846.4 60,142.0</td><td>72,878.6</td></tr><tr><td>Double Dunk</td><td>18</td><td>-18.6</td><td>-16.4</td><td>-6.6</td><td>-5.5</td><td>0.1</td><td>18.5</td><td>-12.5</td></tr><tr><td>Enduro</td><td>9</td><td>0.0</td><td>860.5</td><td>729.0</td><td>1,211.8</td><td>2,258.2</td><td>2,093.0</td><td>2,306.4</td></tr><tr><td>Fishing Derby</td><td>18</td><td>-91.7</td><td>-38.7</td><td>-4.9</td><td>15.5</td><td>46.4</td><td>39.5</td><td>41.3</td></tr><tr><td>Freeway</td><td>3</td><td>0.0</td><td>29.6</td><td>30.8</td><td>33.3</td><td>0.0</td><td>33.7</td><td>33.0</td></tr><tr><td>Frostbite</td><td>18</td><td>65.2</td><td>4,334.7</td><td>797.4</td><td>1,683.3</td><td>4,672.8</td><td>4,380.1 8,339.0</td><td>7,413.0</td></tr><tr><td>Gopher Star Gunner</td><td>8 18</td><td>257.6 664.0</td><td>2,412.5</td><td>8,777.4</td><td>14,840.8</td><td>15,718.4</td><td>32,487.2 63,302.0 8 218.4</td><td>104,368.2</td></tr><tr><td>Gravitar Surround</td><td>18 5</td><td>173.0 -10.0</td><td>3,351.4</td><td>473.0</td><td>412.0 -2.9</td><td>588.0 4.4</td><td>548.5 8.9</td><td>238.0</td></tr><tr><td>H.E.R.O. Tennis</td><td>18 18</td><td>1,027.0 -23.8</td><td>30,826.4</td><td>20,437.8 -8.3</td><td>20,130.2 -22.8</td><td>20,818.2 5.1</td><td>23,037.7 0.0</td><td>21,036.5 12.2</td></tr><tr><td>Ice Hockey Time Pilot</td><td>18 10 3,568.0</td><td>-11.2</td><td>0.9</td><td>-1.9 5,229.2</td><td>-2.7 11,666.0</td><td>0.5 9,197.0</td><td>1.3</td><td>-0.4 4,870.0</td></tr><tr><td>James Bond</td><td>18</td><td>29.0</td><td>302.8 Tutankham</td><td>768.5 167.6</td><td>1,358.0 211.4</td><td>1,312.5 204.6</td><td>5,148.0</td><td>812.0 68.1</td></tr><tr><td>Kangaroo</td><td>18</td><td>52.0</td><td>3,035.0</td><td>7,259.0</td><td>12,992.0</td><td>14,854.0</td><td>16,200.0</td><td>1,792.0 245.9</td></tr><tr><td>Krull Up and Down</td><td>18</td><td>1,598.0</td><td>2,665.5 11,693.2</td><td>8,422.3 9,989.9</td><td>7,920.5 22,972.2</td><td>11,451.9 44,939.6 54.0</td><td>9,728.0 16,154.1</td><td>10,374.4 33,879.1</td></tr><tr><td>Kung-Fu Master</td><td>14 6 12,352.0</td><td>258.5</td><td>22,736.3</td><td>26,059.0</td><td>29,710.0</td><td>34,294.0</td><td>39,581.0</td><td>48,375.0 48.0</td></tr><tr><td>Montezuma's Revenge Venture</td><td>18 18</td><td>0.0</td><td>4,753.3 1,187.5</td><td>0.0 163.0</td><td>0.0 98.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>Ms. Pac-Man</td><td>9 9,173.3</td><td>307.3</td><td>6,951.6 17,667.9</td><td>3,085.6 196,760.4 13,886.0</td><td>2,711.4 309,941.9</td><td>6,283.5</td><td>6,518.7 282,007.3</td><td>3,327.3 479,197.0</td></tr><tr><td>Name This Game Video Pinball</td><td>6</td><td>2,292.3</td><td>8,049.0</td><td>8,207.8</td><td>10,616.0</td><td>11,971.1</td><td>12,270.5</td><td>15,572.5</td></tr><tr><td>Phoenix Wizard Of Wor</td><td>8</td><td>761.4 563.5</td><td>7,242.6 4,756.5</td><td>8,485.2 2,704.0</td><td>12,252.5 7,492.0</td><td>23,092.2 7,855.0 Yars' Revenge 54,576.9</td><td>18,992.7 4,802.0 11,357.0</td><td>70,324.3</td></tr><tr><td>Pitfall!</td><td>18 18</td><td>-229.4 3,092.9</td><td>6,463.7</td><td>-286.1 18,098.9</td><td>-29.9 11,712.6</td><td>0.0 49,622.1</td><td>-356.5</td><td>0.0</td></tr><tr><td>Pong</td><td>3</td><td>-20.7</td><td>14.6</td><td>19.5</td><td>20.9</td><td>21.0</td><td>20.6</td><td>20.9</td></tr><tr><td>Private Eye Zaxxon</td><td>18</td><td>24.9 32.5</td><td>69,571.3</td><td>146.7 5,363.0</td><td>129.7 10,163.0</td><td>103.0 12,944.0</td><td>200.0</td><td>206.0</td></tr><tr><td>Q*Bert</td><td>6 18</td><td>163.9</td><td>13,455.0</td><td>13,117.3</td><td>15,088.5</td><td>19,220.3</td><td>16,256.5</td><td>18,760.3</td></tr><tr><td>River Raid</td><td>18</td><td>1,338.5</td><td>17,118.0</td><td>7,377.6</td><td>14,884.5</td><td>21,162.6</td><td>14,522.3</td><td>20,607.6</td></tr><tr><td>Road Runner</td><td>18</td><td>11.5</td><td>7,845.0</td><td>39,544.0</td><td>44,127.0</td><td>69,524.0</td><td>57,608.0</td><td>62,151.0</td></tr><tr><td>Robotank</td><td>18</td><td>2.2</td><td>11.9</td><td>63.9</td><td>65.1</td><td>65.3</td><td>62.6</td><td>27.5</td></tr><tr><td>Seaquest</td><td>18</td><td>68.4 -17,098.1</td><td>42,054.7</td><td>5,860.6</td><td>16,452.7 -9,021.8</td><td>50,254.2 -8,857.4</td><td>26,357.8 -9,996.9</td><td>931.6</td></tr><tr><td>Skiing</td><td>3 18</td><td></td><td>-4,336.9</td><td>-13,062.3</td><td>3,067.8</td><td>2,250.8</td><td></td><td>-19,949.9</td></tr><tr><td>Solaris Space Invaders</td><td></td><td></td><td>12,326.7 1,668.7</td><td>3,482.8 1,692.3</td><td>2,525.5</td><td>6,427.3 89,238.0</td><td>4,309.0 2,865.8</td><td>133.4</td></tr><tr><td></td><td>10</td><td></td><td></td><td>-5.6</td><td></td><td></td><td></td><td>15,311.5 125,117.0</td></tr><tr><td></td><td>9</td><td></td><td>6.5</td><td></td><td></td><td>497.0 98,209.5</td><td></td><td>7,553.0</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>1.2 0.0</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>69,618.1</td></tr><td></td><td>6</td><td>1,236.3 148.0 16,256.9</td><td>10,250.0</td><td>54,282.0 10,469.0</td><td></td></table>",
            "id": 171,
            "page": 12,
            "text": "GAMES No. ACTIONS RANDOM 533.4 0.0 HUMAN DQN DDQN DUEL PRIOR. PRIOR. DUEL.  Alien 18 227.8 7,127.7 1,620.0 3,747.7 4,461.4 4,203.8 3,941.0  Amidar 10 5.8 1,719.5 978.0 1,793.3 2,354.5 1,838.9 2,296.8  Assault 7 222.4 742.0 4,280.4 5,393.2 4,621.0 7,672.1 11,477.0  Asterix 9 210.0 8,503.3 4,359.0 17,356.5 28,188.0 31,527.0 375,080.0  Asteroids 14 719.1 47,388.7 1,364.5 734.7 2,837.7 2,654.3 1,192.7  Atlantis 4 12,850.0 29,028.1 279,987.0 106,056.0 382,572.0 357,324.0 395,762.0  Bank Heist 18 14.2 753.1 455.0 1,030.6 1,611.9 1,054.6 1,503.1  Battle Zone 18 2,360.0 37,187.5 29,900.0 31,700.0 37,150.0 31,530.0 35,520.0  Beam Rider 9 363.9 16,926.5 8,627.5 13,772.8 12,164.0 23,384.2 30,276.5  Berzerk 18 123.7 2,630.4 585.6 1,225.4 1,472.6 1,305.6 3,409.0  Bowling 6 23.1 160.7 50.4 68.1 65.5 47.9 46.7  Boxing 18 0.1 12.1 88.0 91.6 99.4 95.6 98.9  Breakout 4 1.7 30.5 385.5 418.5 345.3 373.9 366.0  Centipede 11.4 18 2,090.9 12,017.0 4,657.7 5,409.4 7,561.4 4,463.2 7,687.5  Chopper Command 18 811.0 7,387.8 6,126.0 5,809.0 11,215.0 8,600.0 13,185.0  Crazy Climber 9 10,780.5 35,829.4 110,763.0 117,282.0 143,570.0 141,161.0 162,224.0  Defender 18 2,874.5 18,688.9 23,633.0 35,338.5 42,214.0 31,286.5 41,324.5  Demon Attack 6 152.1 1,971.0 12,149.4 58,044.2 60,813.3 71,846.4 60,142.0 72,878.6  Double Dunk 18 -18.6 -16.4 -6.6 -5.5 0.1 18.5 -12.5  Enduro 9 0.0 860.5 729.0 1,211.8 2,258.2 2,093.0 2,306.4  Fishing Derby 18 -91.7 -38.7 -4.9 15.5 46.4 39.5 41.3  Freeway 3 0.0 29.6 30.8 33.3 0.0 33.7 33.0  Frostbite 18 65.2 4,334.7 797.4 1,683.3 4,672.8 4,380.1 8,339.0 7,413.0  Gopher Star Gunner 8 18 257.6 664.0 2,412.5 8,777.4 14,840.8 15,718.4 32,487.2 63,302.0 8 218.4 104,368.2  Gravitar Surround 18 5 173.0 -10.0 3,351.4 473.0 412.0 -2.9 588.0 4.4 548.5 8.9 238.0  H.E.R.O. Tennis 18 18 1,027.0 -23.8 30,826.4 20,437.8 -8.3 20,130.2 -22.8 20,818.2 5.1 23,037.7 0.0 21,036.5 12.2  Ice Hockey Time Pilot 18 10 3,568.0 -11.2 0.9 -1.9 5,229.2 -2.7 11,666.0 0.5 9,197.0 1.3 -0.4 4,870.0  James Bond 18 29.0 302.8 Tutankham 768.5 167.6 1,358.0 211.4 1,312.5 204.6 5,148.0 812.0 68.1  Kangaroo 18 52.0 3,035.0 7,259.0 12,992.0 14,854.0 16,200.0 1,792.0 245.9  Krull Up and Down 18 1,598.0 2,665.5 11,693.2 8,422.3 9,989.9 7,920.5 22,972.2 11,451.9 44,939.6 54.0 9,728.0 16,154.1 10,374.4 33,879.1  Kung-Fu Master 14 6 12,352.0 258.5 22,736.3 26,059.0 29,710.0 34,294.0 39,581.0 48,375.0 48.0  Montezuma's Revenge Venture 18 18 0.0 4,753.3 1,187.5 0.0 163.0 0.0 98.0 0.0 0.0 0.0  Ms. Pac-Man 9 9,173.3 307.3 6,951.6 17,667.9 3,085.6 196,760.4 13,886.0 2,711.4 309,941.9 6,283.5 6,518.7 282,007.3 3,327.3 479,197.0  Name This Game Video Pinball 6 2,292.3 8,049.0 8,207.8 10,616.0 11,971.1 12,270.5 15,572.5  Phoenix Wizard Of Wor 8 761.4 563.5 7,242.6 4,756.5 8,485.2 2,704.0 12,252.5 7,492.0 23,092.2 7,855.0 Yars' Revenge 54,576.9 18,992.7 4,802.0 11,357.0 70,324.3  Pitfall! 18 18 -229.4 3,092.9 6,463.7 -286.1 18,098.9 -29.9 11,712.6 0.0 49,622.1 -356.5 0.0  Pong 3 -20.7 14.6 19.5 20.9 21.0 20.6 20.9  Private Eye Zaxxon 18 24.9 32.5 69,571.3 146.7 5,363.0 129.7 10,163.0 103.0 12,944.0 200.0 206.0  Q*Bert 6 18 163.9 13,455.0 13,117.3 15,088.5 19,220.3 16,256.5 18,760.3  River Raid 18 1,338.5 17,118.0 7,377.6 14,884.5 21,162.6 14,522.3 20,607.6  Road Runner 18 11.5 7,845.0 39,544.0 44,127.0 69,524.0 57,608.0 62,151.0  Robotank 18 2.2 11.9 63.9 65.1 65.3 62.6 27.5  Seaquest 18 68.4 -17,098.1 42,054.7 5,860.6 16,452.7 -9,021.8 50,254.2 -8,857.4 26,357.8 -9,996.9 931.6  Skiing 3 18  -4,336.9 -13,062.3 3,067.8 2,250.8  -19,949.9  Solaris Space Invaders   12,326.7 1,668.7 3,482.8 1,692.3 2,525.5 6,427.3 89,238.0 4,309.0 2,865.8 133.4   10   -5.6    15,311.5 125,117.0   9  6.5   497.0 98,209.5  7,553.0          1.2 0.0          69,618.1   6 1,236.3 148.0 16,256.9 10,250.0 54,282.0 10,469.0"
        },
        {
            "bounding_box": [
                {
                    "x": 714,
                    "y": 190
                },
                {
                    "x": 1772,
                    "y": 190
                },
                {
                    "x": 1772,
                    "y": 236
                },
                {
                    "x": 714,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='172' style='font-size:20px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 172,
            "page": 13,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 734,
                    "y": 391
                },
                {
                    "x": 1752,
                    "y": 391
                },
                {
                    "x": 1752,
                    "y": 437
                },
                {
                    "x": 734,
                    "y": 437
                }
            ],
            "category": "caption",
            "html": "<caption id='173' style='font-size:14px'>Table 3. Raw scores across all games. Starting with Human starts.</caption>",
            "id": 173,
            "page": 13,
            "text": "Table 3. Raw scores across all games. Starting with Human starts."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 450
                },
                {
                    "x": 2356,
                    "y": 450
                },
                {
                    "x": 2356,
                    "y": 2889
                },
                {
                    "x": 223,
                    "y": 2889
                }
            ],
            "category": "table",
            "html": "<br><table id='174' style='font-size:16px'><tr><td>GAMES</td><td>No. ACTIONS</td><td>RANDOM 707.2 18.0</td><td>HUMAN</td><td>DQN</td><td>DDQN</td><td>DUEL</td><td>PRIOR.</td><td>PRIOR. DUEL.</td></tr><tr><td>Alien</td><td>18</td><td>128.3</td><td>6,371.3</td><td>634.0</td><td>1,033.4</td><td>1,486.5</td><td>1,334.7</td><td>823.7</td></tr><tr><td>Amidar</td><td>10</td><td>11.8</td><td>1,540.4</td><td>178.4</td><td>169.1</td><td>172.7</td><td>129.1</td><td>238.4</td></tr><tr><td>Assault</td><td>7</td><td>166.9</td><td>628.9</td><td>3,489.3</td><td>6,060.8</td><td>3,994.8</td><td>6,548.9</td><td>10,950.6</td></tr><tr><td>Asterix</td><td>9</td><td>164.5</td><td>7,536.0</td><td>3,170.5</td><td>16,837.0</td><td>15,840.0</td><td>22,484.5</td><td>364,200.0</td></tr><tr><td>Asteroids</td><td>14</td><td>871.3</td><td>36,517.3</td><td>1,458.7</td><td>1,193.2</td><td>2,035.4</td><td>1,745.1</td><td>1,021.9</td></tr><tr><td>Atlantis</td><td>4</td><td>13,463.0</td><td>26,575.0</td><td>292,491.0</td><td>319,688.0</td><td>445,360.0</td><td>330,647.0</td><td>423,252.0</td></tr><tr><td>Bank Heist</td><td>18</td><td>21.7</td><td>644.5</td><td>312.7</td><td>886.0</td><td>1,129.3</td><td>876.6</td><td>1,004.6</td></tr><tr><td>Battle Zone</td><td>18</td><td>3,560.0</td><td>33,030.0</td><td>23,750.0</td><td>24,740.0</td><td>31,320.0</td><td>25,520.0</td><td>30,650.0</td></tr><tr><td>Beam Rider</td><td>9</td><td>254.6</td><td>14,961.0</td><td>9,743.2</td><td>17,417.2</td><td>14,591.3</td><td>31,181.3</td><td>37,412.2</td></tr><tr><td>Berzerk</td><td>18</td><td>196.1</td><td>2,237.5</td><td>493.4</td><td>1,011.1</td><td>910.6</td><td>865.9</td><td>2,178.6</td></tr><tr><td>Bowling</td><td>6</td><td>35.2</td><td>146.5</td><td>56.5</td><td>69.6</td><td>65.7</td><td>52.0</td><td>50.4</td></tr><tr><td>Boxing</td><td>18</td><td>-1.5</td><td>9.6</td><td>70.3</td><td>73.5</td><td>77.3</td><td>72.3</td><td>79.2</td></tr><tr><td>Breakout</td><td>4</td><td>1.6</td><td>27.9</td><td>354.5</td><td>368.9</td><td>411.6</td><td>343.0</td><td>354.6</td></tr><tr><td>Centipede 12.7</td><td>18</td><td>1,925.5</td><td>10,321.9</td><td>3,973.9</td><td>3,853.5</td><td>4,881.0</td><td>3,489.1</td><td>5,570.2</td></tr><tr><td>Chopper Command</td><td>18</td><td>644.0</td><td>8,930.0</td><td>5,017.0</td><td>3,495.0</td><td>3,784.0</td><td>4,635.0</td><td>8,058.0</td></tr><tr><td>Crazy Climber</td><td>9</td><td>9,337.0</td><td>32,667.0</td><td>98,128.0</td><td>113,782.0</td><td>124,566.0</td><td>127,512.0</td><td>127,853.0</td></tr><tr><td>Defender</td><td>18</td><td>1,965.5</td><td>14,296.0</td><td>15,917.5</td><td>27,510.0</td><td>33,996.0</td><td>23,666.5</td><td>34,415.0</td></tr><tr><td>Demon Attack</td><td>6</td><td>208.3</td><td>3,442.8</td><td>12,550.7</td><td>69,803.4</td><td>56,322.8</td><td>61,277.5</td><td>73,371.3</td></tr><tr><td>Double Dunk</td><td>18</td><td>-16.0</td><td>-14.4</td><td>-6.0</td><td>-0.3</td><td>-0.8</td><td>16.0</td><td>-10.7</td></tr><tr><td>Enduro</td><td>9</td><td>-81.8</td><td>740.2</td><td>626.7</td><td>1,216.6</td><td>2,077.4</td><td>1,831.0</td><td>2,223.9</td></tr><tr><td>Fishing Derby</td><td>18</td><td>-77.1</td><td>5.1</td><td>-1.6</td><td>3.2</td><td>-4.1</td><td>9.8</td><td>17.0</td></tr><tr><td>Freeway</td><td>3</td><td>0.1</td><td>25.6</td><td>26.9</td><td>28.8</td><td>0.2</td><td>28.9</td><td>28.2</td></tr><tr><td>Frostbite</td><td>18</td><td>66.4 4,786.0</td><td>4,202.8</td><td>496.1</td><td>1,448.1</td><td>2,332.4</td><td>3,510.0 6,608.0</td><td>4,038.4</td></tr><tr><td>Gopher Star Gunner</td><td>8 18</td><td>250.0 697.0</td><td>2,311.0</td><td>8,190.4</td><td>15,253.0</td><td>20,051.4</td><td>34,858.8 8 92.2</td><td>105,148.4</td></tr><tr><td>Gravitar Surround</td><td>18 5</td><td>245.5 -9.7</td><td>3,116.0</td><td>298.0</td><td>200.5 1.9</td><td>297.0 4.0</td><td>269.5 5.9</td><td>167.0</td></tr><tr><td>H.E.R.O. Tennis</td><td>18 18</td><td>1,580.3 -21.4</td><td>25,839.4</td><td>14,992.9 -6.7</td><td>14,892.5 -7.8</td><td>15,207.9 4.4 -5.3</td><td>20,889.9</td><td>15,459.2 11.1</td></tr><tr><td>Ice Hockey Time Pilot</td><td>18 10 3,273.0</td><td>-9.7</td><td>0.5</td><td>-1.6 5,650.0</td><td>-2.5 6,601.0</td><td>-1.3</td><td>-0.2</td><td>0.5</td></tr><tr><td>James Bond</td><td>18</td><td>33.5</td><td>368.5 Tutankham</td><td>697.5 138.3</td><td>573.0 48.0</td><td>835.5</td><td>3,961.0</td><td>585.0 45.6</td></tr><tr><td>Kangaroo</td><td>18</td><td>100.0</td><td>2,739.0</td><td>4,496.0</td><td>11,204.0</td><td>10,334.0</td><td>12,185.0</td><td>861.0 108.6</td></tr><tr><td>Krull Up and Down</td><td>18</td><td>1,151.9</td><td>2,109.1 9,896.1</td><td>6,206.0 8,038.5</td><td>6,796.1 19,086.9</td><td>8,051.6 24,759.2 94.0</td><td>6,872.8 12,157.4</td><td>7,658.6 22,681.3</td></tr><tr><td>Kung-Fu Master</td><td>14 6 10,471.0</td><td>304.0</td><td>20,786.8</td><td>20,882.0</td><td>30,207.0</td><td>24,288.0 200.0</td><td>31,676.0</td><td>37,484.0 29.0</td></tr><tr><td>Montezuma's Revenge Venture</td><td>18 18</td><td>25.0</td><td>4,182.0 1,039.0</td><td>47.0 136.0</td><td>42.0 21.0</td><td>22.0</td><td>51.0</td><td>24.0</td></tr><tr><td>Ms. Pac-Man</td><td>9 8,443.0 4,412.0</td><td>197.8</td><td>15,375.0 15,641.1</td><td>1,092.3 154,414.1 11,320.0</td><td>1,241.3 367,823.7</td><td>2,250.6</td><td>1,865.9 295,972.8</td><td>1,007.8</td></tr><tr><td>Name This Game Pinball</td><td>6</td><td>1,747.8</td><td>6,796.0</td><td>6,738.8</td><td>8,960.3</td><td>11,185.1</td><td>10,497.6</td><td>13,637.9</td></tr><tr><td>Phoenix Wizard Of Wor</td><td>8</td><td>1,134.4 804.0</td><td>6,686.2 4,556.0</td><td>7,484.8 1,609.0</td><td>12,366.5 6,201.0</td><td>20,410.5 7,054.0 Yars' Revenge 47,135.2</td><td>16,903.6 5,727.0</td><td>63,597.0</td></tr><tr><td>Pitfall!</td><td>18 18</td><td>-348.8 1,476.9</td><td>5,998.9</td><td>-113.2 4,577.5</td><td>-186.7 6,270.6</td><td>-46.9 25,976.5 4,687.4</td><td>-427.0</td><td>-243.6</td></tr><tr><td>Pong</td><td>3</td><td>-18.0</td><td>15.5</td><td>18.0</td><td>19.1</td><td>18.8</td><td>18.9</td><td>18.4</td></tr><tr><td>Private Eye</td><td>18</td><td>662.8 475.0</td><td>64,169.1</td><td>207.9</td><td>-575.5 8,593.0</td><td>292.6 10,164.0 9,474.0</td><td>670.7</td><td>1,277.6</td></tr><tr><td>Q*Bert Zaxxon</td><td>6 18</td><td>183.0</td><td>12,085.0</td><td>9,271.5</td><td>11,020.8</td><td>14,175.8</td><td>9,944.0</td><td>14,063.0</td></tr><tr><td>River Raid</td><td>18</td><td>588.3</td><td>14,382.2</td><td>4,748.5</td><td>10,838.4</td><td>16,569.4</td><td>11,807.2</td><td>16,496.8</td></tr><tr><td>Road Runner</td><td>18</td><td>200.0</td><td>6,878.0</td><td>35,215.0</td><td>43,156.0</td><td>58,549.0</td><td>52,264.0</td><td>54,630.0</td></tr><tr><td>Robotank</td><td>18</td><td>2.4</td><td>8.9</td><td>58.7</td><td>59.1</td><td>62.0</td><td>56.2</td><td>24.7</td></tr><tr><td>Seaquest</td><td>18</td><td>215.5</td><td>40,425.8</td><td>4,216.7</td><td>14,498.0</td><td>37,361.6</td><td>25,463.7</td><td>1,431.2</td></tr><tr><td>Skiing</td><td>3</td><td>-15,287.4</td><td>-3,686.6</td><td>-12,142.1</td><td>-11,490.4</td><td>-11,928.0</td><td>-10,169.1</td><td>-18,955.8</td></tr><tr><td>Solaris</td><td>18 6</td><td>2,047.2 182.6</td><td>11,032.6 1,464.9</td><td>1,295.4</td><td>810.0</td><td>1,768.4</td><td>2,272.8</td><td>280.6</td></tr><tr><td>Space Invaders</td><td></td><td></td><td></td><td>1,293.8 52,970.0</td><td>2,628.7 58,365.0</td><td>5,993.1 90,804.0</td><td>3,912.1 61,582.0</td><td>8,978.0 127,073.0</td></tr><tr><td>Video</td><td>9 10</td><td></td><td></td><td></td><td></td><td>110,976.2</td><td>5,963.0 56.9</td><td></td></tr><tr><td></td><td></td><td></td><td>5.4</td><td>-6.0</td><td></td><td></td><td></td><td>-0.2 -13.2 4,871.0</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>447,408.6 58,145.9</td></tr><td></td><td></td><td>20,452.0</td><td>9,528.0</td><td></td><td></td></table>",
            "id": 174,
            "page": 13,
            "text": "GAMES No. ACTIONS RANDOM 707.2 18.0 HUMAN DQN DDQN DUEL PRIOR. PRIOR. DUEL.  Alien 18 128.3 6,371.3 634.0 1,033.4 1,486.5 1,334.7 823.7  Amidar 10 11.8 1,540.4 178.4 169.1 172.7 129.1 238.4  Assault 7 166.9 628.9 3,489.3 6,060.8 3,994.8 6,548.9 10,950.6  Asterix 9 164.5 7,536.0 3,170.5 16,837.0 15,840.0 22,484.5 364,200.0  Asteroids 14 871.3 36,517.3 1,458.7 1,193.2 2,035.4 1,745.1 1,021.9  Atlantis 4 13,463.0 26,575.0 292,491.0 319,688.0 445,360.0 330,647.0 423,252.0  Bank Heist 18 21.7 644.5 312.7 886.0 1,129.3 876.6 1,004.6  Battle Zone 18 3,560.0 33,030.0 23,750.0 24,740.0 31,320.0 25,520.0 30,650.0  Beam Rider 9 254.6 14,961.0 9,743.2 17,417.2 14,591.3 31,181.3 37,412.2  Berzerk 18 196.1 2,237.5 493.4 1,011.1 910.6 865.9 2,178.6  Bowling 6 35.2 146.5 56.5 69.6 65.7 52.0 50.4  Boxing 18 -1.5 9.6 70.3 73.5 77.3 72.3 79.2  Breakout 4 1.6 27.9 354.5 368.9 411.6 343.0 354.6  Centipede 12.7 18 1,925.5 10,321.9 3,973.9 3,853.5 4,881.0 3,489.1 5,570.2  Chopper Command 18 644.0 8,930.0 5,017.0 3,495.0 3,784.0 4,635.0 8,058.0  Crazy Climber 9 9,337.0 32,667.0 98,128.0 113,782.0 124,566.0 127,512.0 127,853.0  Defender 18 1,965.5 14,296.0 15,917.5 27,510.0 33,996.0 23,666.5 34,415.0  Demon Attack 6 208.3 3,442.8 12,550.7 69,803.4 56,322.8 61,277.5 73,371.3  Double Dunk 18 -16.0 -14.4 -6.0 -0.3 -0.8 16.0 -10.7  Enduro 9 -81.8 740.2 626.7 1,216.6 2,077.4 1,831.0 2,223.9  Fishing Derby 18 -77.1 5.1 -1.6 3.2 -4.1 9.8 17.0  Freeway 3 0.1 25.6 26.9 28.8 0.2 28.9 28.2  Frostbite 18 66.4 4,786.0 4,202.8 496.1 1,448.1 2,332.4 3,510.0 6,608.0 4,038.4  Gopher Star Gunner 8 18 250.0 697.0 2,311.0 8,190.4 15,253.0 20,051.4 34,858.8 8 92.2 105,148.4  Gravitar Surround 18 5 245.5 -9.7 3,116.0 298.0 200.5 1.9 297.0 4.0 269.5 5.9 167.0  H.E.R.O. Tennis 18 18 1,580.3 -21.4 25,839.4 14,992.9 -6.7 14,892.5 -7.8 15,207.9 4.4 -5.3 20,889.9 15,459.2 11.1  Ice Hockey Time Pilot 18 10 3,273.0 -9.7 0.5 -1.6 5,650.0 -2.5 6,601.0 -1.3 -0.2 0.5  James Bond 18 33.5 368.5 Tutankham 697.5 138.3 573.0 48.0 835.5 3,961.0 585.0 45.6  Kangaroo 18 100.0 2,739.0 4,496.0 11,204.0 10,334.0 12,185.0 861.0 108.6  Krull Up and Down 18 1,151.9 2,109.1 9,896.1 6,206.0 8,038.5 6,796.1 19,086.9 8,051.6 24,759.2 94.0 6,872.8 12,157.4 7,658.6 22,681.3  Kung-Fu Master 14 6 10,471.0 304.0 20,786.8 20,882.0 30,207.0 24,288.0 200.0 31,676.0 37,484.0 29.0  Montezuma's Revenge Venture 18 18 25.0 4,182.0 1,039.0 47.0 136.0 42.0 21.0 22.0 51.0 24.0  Ms. Pac-Man 9 8,443.0 4,412.0 197.8 15,375.0 15,641.1 1,092.3 154,414.1 11,320.0 1,241.3 367,823.7 2,250.6 1,865.9 295,972.8 1,007.8  Name This Game Pinball 6 1,747.8 6,796.0 6,738.8 8,960.3 11,185.1 10,497.6 13,637.9  Phoenix Wizard Of Wor 8 1,134.4 804.0 6,686.2 4,556.0 7,484.8 1,609.0 12,366.5 6,201.0 20,410.5 7,054.0 Yars' Revenge 47,135.2 16,903.6 5,727.0 63,597.0  Pitfall! 18 18 -348.8 1,476.9 5,998.9 -113.2 4,577.5 -186.7 6,270.6 -46.9 25,976.5 4,687.4 -427.0 -243.6  Pong 3 -18.0 15.5 18.0 19.1 18.8 18.9 18.4  Private Eye 18 662.8 475.0 64,169.1 207.9 -575.5 8,593.0 292.6 10,164.0 9,474.0 670.7 1,277.6  Q*Bert Zaxxon 6 18 183.0 12,085.0 9,271.5 11,020.8 14,175.8 9,944.0 14,063.0  River Raid 18 588.3 14,382.2 4,748.5 10,838.4 16,569.4 11,807.2 16,496.8  Road Runner 18 200.0 6,878.0 35,215.0 43,156.0 58,549.0 52,264.0 54,630.0  Robotank 18 2.4 8.9 58.7 59.1 62.0 56.2 24.7  Seaquest 18 215.5 40,425.8 4,216.7 14,498.0 37,361.6 25,463.7 1,431.2  Skiing 3 -15,287.4 -3,686.6 -12,142.1 -11,490.4 -11,928.0 -10,169.1 -18,955.8  Solaris 18 6 2,047.2 182.6 11,032.6 1,464.9 1,295.4 810.0 1,768.4 2,272.8 280.6  Space Invaders    1,293.8 52,970.0 2,628.7 58,365.0 5,993.1 90,804.0 3,912.1 61,582.0 8,978.0 127,073.0  Video 9 10     110,976.2 5,963.0 56.9      5.4 -6.0    -0.2 -13.2 4,871.0          447,408.6 58,145.9    20,452.0 9,528.0"
        },
        {
            "bounding_box": [
                {
                    "x": 714,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 191
                },
                {
                    "x": 1772,
                    "y": 236
                },
                {
                    "x": 714,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='175' style='font-size:20px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 175,
            "page": 14,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 662,
                    "y": 407
                },
                {
                    "x": 1824,
                    "y": 407
                },
                {
                    "x": 1824,
                    "y": 452
                },
                {
                    "x": 662,
                    "y": 452
                }
            ],
            "category": "caption",
            "html": "<caption id='176' style='font-size:14px'>Table 4. Normalized scores across all games. Starting with 30 no-op actions.</caption>",
            "id": 176,
            "page": 14,
            "text": "Table 4. Normalized scores across all games. Starting with 30 no-op actions."
        },
        {
            "bounding_box": [
                {
                    "x": 527,
                    "y": 436
                },
                {
                    "x": 1953,
                    "y": 436
                },
                {
                    "x": 1953,
                    "y": 2876
                },
                {
                    "x": 527,
                    "y": 2876
                }
            ],
            "category": "table",
            "html": "<br><table id='177' style='font-size:16px'><tr><td>GAMES</td><td>DQN</td><td>DDQN</td><td>DUEL</td><td>PRIOR.</td><td>PRIOR. DUEL.</td></tr><tr><td>Alien</td><td>20.2%</td><td>51.0%</td><td>61.4%</td><td>57.6%</td><td>53.8%</td></tr><tr><td>Amidar</td><td>56.7%</td><td>104.3%</td><td>137.1%</td><td>107.0%</td><td>133.7%</td></tr><tr><td>Assault</td><td>781.0%</td><td>995.1%</td><td>846.5%</td><td>1433.7%</td><td>2166.0%</td></tr><tr><td>Asterix</td><td>50.0%</td><td>206.8%</td><td>337.4%</td><td>377.6%</td><td>4520.1%</td></tr><tr><td>Asteroids</td><td>1.4%</td><td>0.0%</td><td>4.5%</td><td>4.1%</td><td>1.0%</td></tr><tr><td>Atlantis</td><td>1651.2%</td><td>576.1%</td><td>2285.3%</td><td>2129.3%</td><td>2366.9%</td></tr><tr><td>Bank Heist</td><td>59.7%</td><td>137.6%</td><td>216.2%</td><td>140.8%</td><td>201.5%</td></tr><tr><td>Battle Zone</td><td>79.1%</td><td>84.2%</td><td>99.9%</td><td>83.8%</td><td>95.2%</td></tr><tr><td>Beam Rider</td><td>49.9%</td><td>81.0%</td><td>71.2%</td><td>139.0%</td><td>180.6%</td></tr><tr><td>Berzerk</td><td>18.4%</td><td>44.0%</td><td>53.8%</td><td>47.2%</td><td>131.1%</td></tr><tr><td>Bowling</td><td>19.8%</td><td>32.7%</td><td>30.8%</td><td>18.0%</td><td>17.1%</td></tr><tr><td>Boxing</td><td>732.5%</td><td>762.1%</td><td>827.1%</td><td>795.5%</td><td>823.1%</td></tr><tr><td>Breakout</td><td>1334.5%</td><td>1449.2%</td><td>1194.5%</td><td>1294.3%</td><td>1266.6%</td></tr><tr><td>Centipede</td><td>25.9%</td><td>33.4%</td><td>55.1%</td><td>23.9%</td><td>56.4%</td></tr><tr><td>Chopper Command</td><td>80.8%</td><td>76.0%</td><td>158.2%</td><td>118.4%</td><td>188.1%</td></tr><tr><td>Crazy Climber</td><td>399.1%</td><td>425.2%</td><td>530.1%</td><td>520.5%</td><td>604.6%</td></tr><tr><td>Defender</td><td>131.3%</td><td>205.3%</td><td>248.8%</td><td>179.7%</td><td>243.1%</td></tr><tr><td>Demon Attack</td><td>659.6%</td><td>3182.8%</td><td>3335.0%</td><td>3941.6%</td><td>3998.3%</td></tr><tr><td>Double Dunk</td><td>557.7%</td><td>607.9%</td><td>866.5%</td><td>1723.3%</td><td>280.5%</td></tr><tr><td>Enduro</td><td>84.7%</td><td>140.8%</td><td>262.4%</td><td>243.2%</td><td>268.0%</td></tr><tr><td>Fishing Derby</td><td>163.8%</td><td>202.4%</td><td>260.7%</td><td>247.7%</td><td>251.1%</td></tr><tr><td>Freeway</td><td>104.0%</td><td>112.5%</td><td>0.1%</td><td>114.0%</td><td>111.3%</td></tr><tr><td>Frostbite</td><td>17.1%</td><td>37.9%</td><td>107.9%</td><td>101.1%</td><td>172.1%</td></tr><tr><td>Gopher</td><td>395.4%</td><td>676.7%</td><td>717.5%</td><td>1495.6%</td><td>4831.3%</td></tr><tr><td>Gravitar</td><td>9.4%</td><td>7.5%</td><td>13.1%</td><td>11.8%</td><td>2.0%</td></tr><tr><td>H.E.R.O. Tennis</td><td>65.1%</td><td>64.1%</td><td>66.4%</td><td>73.9%</td><td>67.1%</td></tr><tr><td>Ice Hockey</td><td>76.9%</td><td>70.0%</td><td>96.4%</td><td>103.2%</td><td>89.6%</td></tr><tr><td>James Bond</td><td>270.1%</td><td>485.4%</td><td>468.8%</td><td>1869.8%</td><td>286.0%</td></tr><tr><td>Kangaroo</td><td>241.6%</td><td>433.8%</td><td>496.2%</td><td>541.3%</td><td>58.3%</td></tr><tr><td>Krull</td><td>639.3%</td><td>592.3%</td><td>923.1 %</td><td>761.6% 140.0%</td><td>822.2% 298.8%</td></tr><tr><td>Kung-Fu Master</td><td>114.8%</td><td>131.0%</td><td>151.4%</td><td>174.9%</td><td>214.1%</td></tr><tr><td>Montezuma's Revenge Venture</td><td>0.0%</td><td>0.0%</td><td>0.0%</td><td>0.0%</td><td>0.0%</td></tr><tr><td>Ms. Pac-Man</td><td>41.8%</td><td>36.2%</td><td>89.9%</td><td>93.5%</td><td>45.5%</td></tr><tr><td>Name This Game</td><td>102.8%</td><td>144.6%</td><td>168.1%</td><td>173.3%</td><td>230.7%</td></tr><tr><td>Phoenix</td><td>119.2%</td><td>177.3%</td><td>344.5%</td><td>281.3%</td><td>1073.3%</td></tr><tr><td>Pitfall!</td><td>-0.8%</td><td>3.0%</td><td>3.4%</td><td>-1.9%</td><td>3.4%</td></tr><tr><td>Pong</td><td>114.0%</td><td>117.8%</td><td>118.2%</td><td>117.1%</td><td>118.0%</td></tr><tr><td>Private Eye Q*Bert</td><td>0.2%</td><td>0.2%</td><td>0.1%</td><td>0.3%</td><td>0.3%</td></tr><tr><td></td><td>97.5%</td><td>112.3%</td><td>143.4%</td><td>121.1%</td><td>139.9%</td></tr><tr><td>River Raid</td><td>38.3%</td><td>85.8%</td><td>125.6%</td><td>83.6%</td><td>122.1%</td></tr><tr><td>Road Runner</td><td>504.7%</td><td>563.2%</td><td>887.4%</td><td>735.3%</td><td>793.3%</td></tr><tr><td>Robotank</td><td>631.5%</td><td>643.7%</td><td></td><td></td><td>259.5% 2.1%</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td>645.1%</td><td>617.5%</td><td></td></tr><tr><td>Seaquest</td><td>13.8%</td><td>39.0%</td><td>119.5%</td><td>62.6%</td><td></td></tr><tr><td>Skiing</td><td>31.6%</td><td>63.3%</td><td>64.6%</td><td>55.6%</td><td>-22.3% -9.9%</td></tr><tr><td>Solaris</td><td>20.3%</td><td>16.5%</td><td>9.1%</td><td>27.7% 178.7%</td><td>997.2%</td></tr><tr><td>Space Invaders Star Gunner</td><td>101.6% 559.3%</td><td>156.3% 620.5%</td><td>412.9% 924.0%</td><td>653.4%</td><td>1298.3% 67.6%</td></tr><tr><td>Surround</td><td>26.5% 231.3% 78.4% 36.3%</td><td>43.2% 6.8% 287.2%</td><td>86.9% 186.2% 487.5% 128.1%</td><td>114.6% 153.2% 338.9% 123.7%</td><td>153.2% 239.9% 150.1%</td></tr><tr><td>Time Pilot Tutankham Up and Down</td><td>84.7% 13.7% 1113.7%</td><td>132.5% 201.1% 8.3%</td><td>397.9% 41.9%</td><td>4.5% 1596.2%</td><td>4.0% 2712.2%</td></tr><tr><td>Video Pinball Wizard Of Wor</td><td>51.0%</td><td>1754.3% 165.2%</td><td>555.9% 173.9%</td><td>101.1%</td><td>281.1%</td></tr><tr><td>Yars' Revenge</td><td>29.1%</td><td>16.7%</td><td>90.4%</td><td>16.1%</td><td>129.2%</td></tr><tr><td>Zaxxon</td><td>58.3%</td><td>110.8%</td><td>141.3%</td><td>114.2%</td><td>151.6%</td></tr></table>",
            "id": 177,
            "page": 14,
            "text": "GAMES DQN DDQN DUEL PRIOR. PRIOR. DUEL.  Alien 20.2% 51.0% 61.4% 57.6% 53.8%  Amidar 56.7% 104.3% 137.1% 107.0% 133.7%  Assault 781.0% 995.1% 846.5% 1433.7% 2166.0%  Asterix 50.0% 206.8% 337.4% 377.6% 4520.1%  Asteroids 1.4% 0.0% 4.5% 4.1% 1.0%  Atlantis 1651.2% 576.1% 2285.3% 2129.3% 2366.9%  Bank Heist 59.7% 137.6% 216.2% 140.8% 201.5%  Battle Zone 79.1% 84.2% 99.9% 83.8% 95.2%  Beam Rider 49.9% 81.0% 71.2% 139.0% 180.6%  Berzerk 18.4% 44.0% 53.8% 47.2% 131.1%  Bowling 19.8% 32.7% 30.8% 18.0% 17.1%  Boxing 732.5% 762.1% 827.1% 795.5% 823.1%  Breakout 1334.5% 1449.2% 1194.5% 1294.3% 1266.6%  Centipede 25.9% 33.4% 55.1% 23.9% 56.4%  Chopper Command 80.8% 76.0% 158.2% 118.4% 188.1%  Crazy Climber 399.1% 425.2% 530.1% 520.5% 604.6%  Defender 131.3% 205.3% 248.8% 179.7% 243.1%  Demon Attack 659.6% 3182.8% 3335.0% 3941.6% 3998.3%  Double Dunk 557.7% 607.9% 866.5% 1723.3% 280.5%  Enduro 84.7% 140.8% 262.4% 243.2% 268.0%  Fishing Derby 163.8% 202.4% 260.7% 247.7% 251.1%  Freeway 104.0% 112.5% 0.1% 114.0% 111.3%  Frostbite 17.1% 37.9% 107.9% 101.1% 172.1%  Gopher 395.4% 676.7% 717.5% 1495.6% 4831.3%  Gravitar 9.4% 7.5% 13.1% 11.8% 2.0%  H.E.R.O. Tennis 65.1% 64.1% 66.4% 73.9% 67.1%  Ice Hockey 76.9% 70.0% 96.4% 103.2% 89.6%  James Bond 270.1% 485.4% 468.8% 1869.8% 286.0%  Kangaroo 241.6% 433.8% 496.2% 541.3% 58.3%  Krull 639.3% 592.3% 923.1 % 761.6% 140.0% 822.2% 298.8%  Kung-Fu Master 114.8% 131.0% 151.4% 174.9% 214.1%  Montezuma's Revenge Venture 0.0% 0.0% 0.0% 0.0% 0.0%  Ms. Pac-Man 41.8% 36.2% 89.9% 93.5% 45.5%  Name This Game 102.8% 144.6% 168.1% 173.3% 230.7%  Phoenix 119.2% 177.3% 344.5% 281.3% 1073.3%  Pitfall! -0.8% 3.0% 3.4% -1.9% 3.4%  Pong 114.0% 117.8% 118.2% 117.1% 118.0%  Private Eye Q*Bert 0.2% 0.2% 0.1% 0.3% 0.3%   97.5% 112.3% 143.4% 121.1% 139.9%  River Raid 38.3% 85.8% 125.6% 83.6% 122.1%  Road Runner 504.7% 563.2% 887.4% 735.3% 793.3%  Robotank 631.5% 643.7%   259.5% 2.1%                                        645.1% 617.5%   Seaquest 13.8% 39.0% 119.5% 62.6%   Skiing 31.6% 63.3% 64.6% 55.6% -22.3% -9.9%  Solaris 20.3% 16.5% 9.1% 27.7% 178.7% 997.2%  Space Invaders Star Gunner 101.6% 559.3% 156.3% 620.5% 412.9% 924.0% 653.4% 1298.3% 67.6%  Surround 26.5% 231.3% 78.4% 36.3% 43.2% 6.8% 287.2% 86.9% 186.2% 487.5% 128.1% 114.6% 153.2% 338.9% 123.7% 153.2% 239.9% 150.1%  Time Pilot Tutankham Up and Down 84.7% 13.7% 1113.7% 132.5% 201.1% 8.3% 397.9% 41.9% 4.5% 1596.2% 4.0% 2712.2%  Video Pinball Wizard Of Wor 51.0% 1754.3% 165.2% 555.9% 173.9% 101.1% 281.1%  Yars' Revenge 29.1% 16.7% 90.4% 16.1% 129.2%  Zaxxon 58.3% 110.8% 141.3% 114.2%"
        },
        {
            "bounding_box": [
                {
                    "x": 714,
                    "y": 190
                },
                {
                    "x": 1773,
                    "y": 190
                },
                {
                    "x": 1773,
                    "y": 237
                },
                {
                    "x": 714,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='178' style='font-size:20px'>Dueling Network Architectures for Deep Reinforcement Learning</header>",
            "id": 178,
            "page": 15,
            "text": "Dueling Network Architectures for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 678,
                    "y": 365
                },
                {
                    "x": 1810,
                    "y": 365
                },
                {
                    "x": 1810,
                    "y": 410
                },
                {
                    "x": 678,
                    "y": 410
                }
            ],
            "category": "caption",
            "html": "<caption id='179' style='font-size:14px'>Table 5. Normalized scores across all games. Starting with Human Starts.</caption>",
            "id": 179,
            "page": 15,
            "text": "Table 5. Normalized scores across all games. Starting with Human Starts."
        },
        {
            "bounding_box": [
                {
                    "x": 528,
                    "y": 396
                },
                {
                    "x": 1961,
                    "y": 396
                },
                {
                    "x": 1961,
                    "y": 2918
                },
                {
                    "x": 528,
                    "y": 2918
                }
            ],
            "category": "table",
            "html": "<br><table id='180' style='font-size:16px'><tr><td>GAMES</td><td>DQN</td><td>DDQN</td><td>DUEL</td><td>PRIOR.</td><td>PRIOR. DUEL.</td></tr><tr><td>Alien</td><td>8.1%</td><td>14.5%</td><td>21.8%</td><td>19.3%</td><td>11.1%</td></tr><tr><td>Amidar</td><td>10.9%</td><td>10.3%</td><td>10.5%</td><td>7.7%</td><td>14.8%</td></tr><tr><td>Assault</td><td>719.2%</td><td>1275.9%</td><td>828.6%</td><td>1381.5%</td><td>2334.4%</td></tr><tr><td>Asterix</td><td>40.8%</td><td>226.2%</td><td>212.7%</td><td>302.8%</td><td>4938.4%</td></tr><tr><td>Asteroids</td><td>1.6%</td><td>0.9%</td><td>3.3%</td><td>2.5%</td><td>0.4%</td></tr><tr><td>Atlantis</td><td>2128.0%</td><td>2335.5%</td><td>3293.9%</td><td>2419.0%</td><td>3125.3%</td></tr><tr><td>Bank Heist</td><td>46.7%</td><td>138.8%</td><td>177.8%</td><td>137.3%</td><td>157.8%</td></tr><tr><td>Battle Zone</td><td>68.5%</td><td>71.9%</td><td>94.2%</td><td>74.5%</td><td>91.9%</td></tr><tr><td>Beam Rider</td><td>64.5%</td><td>116.7%</td><td>97.5%</td><td>210.3%</td><td>252.7%</td></tr><tr><td>Berzerk</td><td>14.6%</td><td>39.9%</td><td>35.0%</td><td>32.8%</td><td>97.1%</td></tr><tr><td>Bowling</td><td>19.2%</td><td>30.9%</td><td>27.5%</td><td>15.1%</td><td>13.7%</td></tr><tr><td>Boxing</td><td>648.2%</td><td>677.0%</td><td>711.2%</td><td>666.7%</td><td>728.5%</td></tr><tr><td>Breakout</td><td>1341.9%</td><td>1396.7%</td><td>1559.0%</td><td>1298.3%</td><td>1342.4%</td></tr><tr><td>Centipede</td><td>24.4%</td><td>23.0%</td><td>35.2%</td><td>18.6%</td><td>43.4%</td></tr><tr><td>Chopper Command</td><td>52.8%</td><td>34.4%</td><td>37.9%</td><td>48.2%</td><td>89.5%</td></tr><tr><td>Crazy Climber</td><td>380.6%</td><td>447.7%</td><td>493.9%</td><td>506.5%</td><td>508.0%</td></tr><tr><td>Defender</td><td>113.2%</td><td>207.2%</td><td>259.8%</td><td>176.0%</td><td>263.2%</td></tr><tr><td>Demon Attack</td><td>381.6%</td><td>2151.6%</td><td>1734.8%</td><td>1888.0%</td><td>2261.9%</td></tr><tr><td>Double Dunk</td><td>622.5%</td><td>982.5%</td><td>948.7%</td><td>1998.7%</td><td>328.7%</td></tr><tr><td>Enduro</td><td>86.2%</td><td>158.0%</td><td>262.7%</td><td>232.7%</td><td>280.5%</td></tr><tr><td>Fishing Derby</td><td>91.8%</td><td>97.7%</td><td>88.8%</td><td>105.7%</td><td>114.5%</td></tr><tr><td>Freeway</td><td>105.1%</td><td>112.5%</td><td>0.6%</td><td>113.1 %</td><td>110.2%</td></tr><tr><td>Frostbite</td><td>10.4%</td><td>33.4%</td><td>54.8%</td><td>83.2%</td><td>96.0%</td></tr><tr><td>Gopher</td><td>385.3%</td><td>727.9%</td><td>960.8%</td><td>1679.2%</td><td>5089.7%</td></tr><tr><td>Gravitar</td><td>1.8%</td><td>-1.6%</td><td>1.8%</td><td>0.8%</td><td>-2.7%</td></tr><tr><td>H.E.R.O.</td><td>55.3%</td><td>54.9%</td><td>56.2%</td><td>79.6%</td><td>57.2%</td></tr><tr><td>Ice Hockey</td><td>79.2%</td><td>70.8%</td><td>82.4%</td><td>92.5%</td><td>99.4%</td></tr><tr><td>James Bond</td><td>198.2%</td><td>161.0%</td><td>239.4%</td><td>1172.4%</td><td>164.6%</td></tr><tr><td>Kangaroo</td><td>166.6%</td><td>420.8%</td><td>387.8%</td><td>457.9%</td><td>28.8%</td></tr><tr><td>Krull Venture</td><td>528.0%</td><td>589.7% 0.3%</td><td>720.8%</td><td>597.7%</td><td>679.8%</td></tr><tr><td>Kung-Fu Master Video</td><td>100.5%</td><td>146.0% 2351.6%</td><td>117.1% 709.5%</td><td>153.2% 1892.3%</td><td>181.5% 2860.5%</td></tr><tr><td>Montezuma's Revenge Pinball</td><td>0.5%</td><td>0.4%</td><td>-0.1%</td><td>0.6%</td><td>-0.0%</td></tr><tr><td>Ms. Pac-Man</td><td>5.9%</td><td>6.9%</td><td>13.5%</td><td>11.0%</td><td>5.3%</td></tr><tr><td>Name This Game</td><td>98.9%</td><td>142.9%</td><td>186.9%</td><td>173.3%</td><td>235.5%</td></tr><tr><td>Phoenix</td><td>114.4%</td><td>202.3%</td><td>347.2%</td><td>284.0%</td><td>1125.1%</td></tr><tr><td>Pitfall!</td><td>3.7%</td><td>2.6%</td><td>4.8%</td><td>-1.2%</td><td>1.7%</td></tr><tr><td>Pong</td><td>107.6%</td><td>110.9%</td><td>110.0%</td><td>110.1%</td><td>108.6%</td></tr><tr><td>Private Eye</td><td>-0.7%</td><td>-1.9%</td><td>-0.6%</td><td>0.0%</td><td>1.0%</td></tr><tr><td>Q*Bert</td><td>76.4%</td><td>91.1%</td><td>117.6%</td><td>82.0%</td><td>116.6%</td></tr><tr><td>River Raid</td><td>30.2%</td><td>74.3%</td><td>115.9%</td><td>81.3%</td><td>115.3%</td></tr><tr><td>Road Runner</td><td>524.3%</td><td>643.2%</td><td>873.7%</td><td>779.6%</td><td>815.1%</td></tr><tr><td>Robotank</td><td>863.3%</td><td>868.7%</td><td>913.3%</td><td>824.4%</td><td>341.1%</td></tr><tr><td>Seaquest</td><td>10.0%</td><td>35.5%</td><td>92.4%</td><td>62.8%</td><td>3.0%</td></tr><tr><td>Skiing Solaris</td><td>27.1% -8.4%</td><td>32.7%</td><td>29.0% -3.1%</td><td>2.5%</td><td>-31.6% -19.7%</td></tr><tr><td></td><td></td><td>-13.8% 190.8%</td><td></td><td></td><td>685.9%</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>44.1 %</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Space Invaders</td><td>86.7%</td><td></td><td>453.1%</td><td>290.8%</td><td></td></tr><tr><td>Star Gunner</td><td>591.9%</td><td>653.0%</td><td>1020.3%</td><td>689.4%</td><td>1431.0% 62.9%</td></tr><tr><td>Surround</td><td>24.7% 220.8%</td><td>76.9% 92.1%</td><td>91.1% 175.0%</td><td>103.2% 109.6%</td><td>55.6% 67.2%</td></tr><tr><td>Tennis Time Pilot Tutankham Up and Down</td><td>63.7% 26.2% 79.8% 11.6% 987.2%</td><td>140.3% 63.3% 200.0%</td><td>140.0% 28.1% 261.8% 17.8%</td><td>113.2% 35.2% 124.6% 7.4%</td><td>76.4% 239.1% 1.1%</td></tr><tr><td>Wizard Of Wor Yars' Revenge</td><td>21.5% 6.8%</td><td>143.8% 10.5%</td><td>166.6% 53.7%</td><td>131.2% 7.0%</td><td>257.6% 124.1%</td></tr><tr><td>Zaxxon</td><td>49.4%</td><td>101.9%</td><td>121.6%</td><td>112.9%</td><td>136.1 %</td></tr><tr><td>Mean</td><td>219.6%</td><td>332.9%</td><td>343.8%</td><td>386.7%</td><td>567.0%</td></tr><tr><td>Median</td><td>68.5%</td><td>110.9%</td><td>117.1%</td><td>112.9%</td><td>115.3%</td></tr></table>",
            "id": 180,
            "page": 15,
            "text": "GAMES DQN DDQN DUEL PRIOR. PRIOR. DUEL.  Alien 8.1% 14.5% 21.8% 19.3% 11.1%  Amidar 10.9% 10.3% 10.5% 7.7% 14.8%  Assault 719.2% 1275.9% 828.6% 1381.5% 2334.4%  Asterix 40.8% 226.2% 212.7% 302.8% 4938.4%  Asteroids 1.6% 0.9% 3.3% 2.5% 0.4%  Atlantis 2128.0% 2335.5% 3293.9% 2419.0% 3125.3%  Bank Heist 46.7% 138.8% 177.8% 137.3% 157.8%  Battle Zone 68.5% 71.9% 94.2% 74.5% 91.9%  Beam Rider 64.5% 116.7% 97.5% 210.3% 252.7%  Berzerk 14.6% 39.9% 35.0% 32.8% 97.1%  Bowling 19.2% 30.9% 27.5% 15.1% 13.7%  Boxing 648.2% 677.0% 711.2% 666.7% 728.5%  Breakout 1341.9% 1396.7% 1559.0% 1298.3% 1342.4%  Centipede 24.4% 23.0% 35.2% 18.6% 43.4%  Chopper Command 52.8% 34.4% 37.9% 48.2% 89.5%  Crazy Climber 380.6% 447.7% 493.9% 506.5% 508.0%  Defender 113.2% 207.2% 259.8% 176.0% 263.2%  Demon Attack 381.6% 2151.6% 1734.8% 1888.0% 2261.9%  Double Dunk 622.5% 982.5% 948.7% 1998.7% 328.7%  Enduro 86.2% 158.0% 262.7% 232.7% 280.5%  Fishing Derby 91.8% 97.7% 88.8% 105.7% 114.5%  Freeway 105.1% 112.5% 0.6% 113.1 % 110.2%  Frostbite 10.4% 33.4% 54.8% 83.2% 96.0%  Gopher 385.3% 727.9% 960.8% 1679.2% 5089.7%  Gravitar 1.8% -1.6% 1.8% 0.8% -2.7%  H.E.R.O. 55.3% 54.9% 56.2% 79.6% 57.2%  Ice Hockey 79.2% 70.8% 82.4% 92.5% 99.4%  James Bond 198.2% 161.0% 239.4% 1172.4% 164.6%  Kangaroo 166.6% 420.8% 387.8% 457.9% 28.8%  Krull Venture 528.0% 589.7% 0.3% 720.8% 597.7% 679.8%  Kung-Fu Master Video 100.5% 146.0% 2351.6% 117.1% 709.5% 153.2% 1892.3% 181.5% 2860.5%  Montezuma's Revenge Pinball 0.5% 0.4% -0.1% 0.6% -0.0%  Ms. Pac-Man 5.9% 6.9% 13.5% 11.0% 5.3%  Name This Game 98.9% 142.9% 186.9% 173.3% 235.5%  Phoenix 114.4% 202.3% 347.2% 284.0% 1125.1%  Pitfall! 3.7% 2.6% 4.8% -1.2% 1.7%  Pong 107.6% 110.9% 110.0% 110.1% 108.6%  Private Eye -0.7% -1.9% -0.6% 0.0% 1.0%  Q*Bert 76.4% 91.1% 117.6% 82.0% 116.6%  River Raid 30.2% 74.3% 115.9% 81.3% 115.3%  Road Runner 524.3% 643.2% 873.7% 779.6% 815.1%  Robotank 863.3% 868.7% 913.3% 824.4% 341.1%  Seaquest 10.0% 35.5% 92.4% 62.8% 3.0%  Skiing Solaris 27.1% -8.4% 32.7% 29.0% -3.1% 2.5% -31.6% -19.7%    -13.8% 190.8%   685.9%                                  44.1 %          Space Invaders 86.7%  453.1% 290.8%   Star Gunner 591.9% 653.0% 1020.3% 689.4% 1431.0% 62.9%  Surround 24.7% 220.8% 76.9% 92.1% 91.1% 175.0% 103.2% 109.6% 55.6% 67.2%  Tennis Time Pilot Tutankham Up and Down 63.7% 26.2% 79.8% 11.6% 987.2% 140.3% 63.3% 200.0% 140.0% 28.1% 261.8% 17.8% 113.2% 35.2% 124.6% 7.4% 76.4% 239.1% 1.1%  Wizard Of Wor Yars' Revenge 21.5% 6.8% 143.8% 10.5% 166.6% 53.7% 131.2% 7.0% 257.6% 124.1%  Zaxxon 49.4% 101.9% 121.6% 112.9% 136.1 %  Mean 219.6% 332.9% 343.8% 386.7% 567.0%  Median 68.5% 110.9% 117.1% 112.9%"
        }
    ]
}